[
    {
        "system": "Hello! I am Kiran, an AI assistant from Tampere University, dedicated to providing you with accurate and insightful assistance. My goal is to help you understand concepts, summarize lectures, and answer questions effectively.",
        "conversations": [
            {
                "role": "host",
                "content": "Introduction to Algorithms: 6.006 Massachusetts Institute of Technology Instructors: Erik Demaine, Jason Ku, and Justin Solomon Lecture 1: Introduction Lecture 1: Introduction The goal of this class is to teach you to solve computation problems, and to communicate that your solutions are correct and efﬁcient. Problem • Binary relation from problem inputs to correct outputs • Usually don’t specify every correct output for all inputs (too many!) • Provide a veriﬁable predicate (a property) that correct outputs must satisfy • 6.006 studies problems on large general input spaces • Not general: small input instance – Example: In this room, is there a pair of students with same birthday? • General: arbitrarily large inputs – Example: Given any set of n students, is there a pair of students with same birthday? – If birthday is just one of 365, for n > 365, answer always true by pigeon-hole – Assume resolution of possible birthdays exceeds n (include year, time, etc.) Algorithm • Procedure mapping each input to a single output (deterministic) • Algorithm solves a problem if it returns a correct output for every problem input • Example: An algorithm to solve birthday matching – Maintain a record of names and birthdays (initially empty) – Interview each student in some order ∗ If birthday exists in record, return found pair! ∗ Else add name and birthday to record – Return None if last student interviewed without success 2 Lecture 1: Introduction Correctness • Programs/algorithms have ﬁxed size, so how to prove correct? • For small inputs, can use case analysis • For arbitrarily large inputs, algorithm must be recursive or loop in some way • Must use induction (why recursion is such a key concept in computer science) • Example: Proof of correctness of birthday matching algorithm – Induct on k: the number of students in record – Hypothesis: if ﬁrst k contain match, returns match before interviewing student k + 1 – Base case: k = 0, ﬁrst k contains no match – Assume for induction hypothesis holds for k = k0, and consider k = k0 + 1 – If ﬁrst k0 contains a match, already returned a match by induction – Else ﬁrst k0 do not have match, so if ﬁrst k0 + 1 has match, match contains k0 + 1 – Then algorithm checks directly whether birthday of student k0 + 1 exists in ﬁrst k0 Efﬁciency • How fast does an algorithm produce a correct output? – Could measure time, but want performance to be machine independent – Idea! Count number of ﬁxed-time operations algorithm takes to return – Expect to depend on size of input: larger input suggests longer time – Size of input is often called ‘n’, but not always! – Efﬁcient if returns in polynomial time with respect to input – Sometimes no efﬁcient algorithm exists for a problem! (See L20) • Asymptotic Notation: ignore constant factors and low order terms – Upper bounds (O), lower bounds (Ω), tight bounds (Θ) ∈, =, is, order – Time estimate below based on one operation per cycle on a 1 GHz single-core machine – Particles in universe estimated < 10100 input constant logarithmic linear log-linear quadratic polynomial exponential n Θ(1) Θ(log n) Θ(n) Θ(n log n) Θ(n2) Θ(nc) 2Θ(nc) 1000 1 ≈ 10 1000 ≈ 10,000 1,000,000 1000c 21000 ≈ 10301 Time 1 ns 10 ns 1 µs 10 µs 1 ms 103c−9 s 10281 millenia 3 Lecture 1: Introduction Model of Computation • Speciﬁcation for what operations on the machine can be performed in O(1) time • Model in this class is called the Word-RAM • Machine word: block of w bits (w is word size of a w-bit Word-RAM) • Memory: Addressable sequence of machine words • Processor supports many constant time operations on a O(1) number of words (integers): – integer arithmetic: (+, -, *, //, %) – logical operators: (&&, ||, !, ==, <, >, <=, =>) – (bitwise arithmetic: (&, |, <<, >>, ...)) – Given word a, can read word at address a, write word to address a • Memory address must be able to access every place in memory – Requirement: w ≥ # bits to represent largest memory address, i.e., log2 n – 32-bit words → max ∼ 4 GB memory, 64-bit words → max ∼ 16 exabytes of memory • Python is a more complicated model of computation, implemented on a Word-RAM Data Structure • A data structure is a way to store non-constant data, that supports a set of operations • A collection of operations is called an interface – Sequence: Extrinsic order to items (ﬁrst, last, nth) – Set: Intrinsic order to items (queries based on item keys) • Data structures may implement the same interface with different performance • Example: Static Array - ﬁxed width slots, ﬁxed length, static sequence interface – StaticArray(n): allocate static array of size n initialized to 0 in Θ(n) time – StaticArray.get at(i): return word stored at array index i in Θ(1) time – StaticArray.set at(i, x): write word x to array index i in Θ(1) time • Stored word can hold the address of a larger object • Like Python tuple plus set at(i, x), Python list is a dynamic array (see L02) 4 Lecture 1: Introduction 1 def birthday_match(students): 2 ’’’ 3 Find a pair of students with the same birthday 4 Input: tuple of student (name, bday) tuples 5 Output: tuple of student names or None 6 ’’’ 7 n = len(students) # O(1) 8 record = StaticArray(n) # O(n) 9 for k in range(n): # n 10 (name1, bday1) = students[k] # O(1) 11 # Return pair if bday1 in record 12 for i in range(k): # k 13 (name2, bday2) = record.get_at(i) # O(1) 14 if bday1 == bday2: # O(1) 15 return (name1, name2) # O(1) 16 record.set_at(k, (name1, bday1)) # O(1) 17 return None # O(1) Example: Running Time Analysis • Two loops: outer k ∈{0, . . . , n − 1}, inner is i ∈{0, . . . , k} P n−1 • Running time is O(n) + k=0 (O(1) + k · O(1)) = O(n2) • Quadratic in n is polynomial. Efﬁcient? Use different data structure for record! How to Solve an Algorithms Problem 1. Reduce to a problem you already know (use data structure or algorithm) Search Problem (Data Structures) Sort Algorithms Static Array (L01) Insertion Sort (L03) Linked List (L02) Selection Sort (L03) Dynamic Array (L02) Merge Sort (L03) Sorted Array (L03) Counting Sort (L05) Direct-Access Array (L04) Radix Sort (L05) Hash Table (L04) AVL Sort (L07) Balanced Binary Tree (L06-L07) Heap Sort (L08) Binary Heap (L08) 2. Design your own (recursive) algorithm • Brute Force • Decrease and Conquer • Divide and Conquer • Dynamic Programming (L15-L19) • Greedy / Incremental Shortest Path Algorithms Breadth First Search (L09) DAG Relaxation (L11) Depth First Search (L10) Topological Sort (L10) Bellman-Ford (L12) Dijkstra (L13) Johnson (L14) Floyd-Warshall (L18) MIT OpenCourseWare https://ocw.mit.edu 6.006 Introduction to Algorithms Spring 2020 For information about citing these materials or our Terms of Use, visit: https://ocw.mit.edu/terms Introduction to Algorithms: 6.006 Massachusetts Institute of Technology Instructors: Erik Demaine, Jason Ku, and Justin Solomon Lecture 2: Data Structures Lecture 2: Data Structures Data Structure Interfaces • A data structure is a way to store data, with algorithms that support operations on the data • Collection of supported operations is called an interface (also API or ADT) • Interface is a speciﬁcation: what operations are supported (the problem!) • Data structure is a representation: how operations are supported (the solution!) • In this class, two main interfaces: Sequence and Set Sequence Interface (L02, L07) • Maintain a sequence of items (order is extrinsic) • Ex: (x0, x1, x2, . . . , xn−1) (zero indexing) • (use n to denote the number of items stored in the data structure) • Supports sequence operations: Container build(X) len() given an iterable X, build sequence from items in X return the number of stored items Static iter seq() get at(i) set at(i, x) return the stored items one-by-one in sequence order return the ith item replace the ith item with x Dynamic insert at(i, x) delete at(i) insert first(x) delete first() insert last(x) delete last() add x as the ith item remove and return the ith item add x as the ﬁrst item remove and return the ﬁrst item add x as the last item remove and return the last item • Special case interfaces: stack insert last(x) and delete last() queue insert last(x) and delete first() 2 Lecture 2: Data Structures Set Interface (L03-L08) • Sequence about extrinsic order, set is about intrinsic order • Maintain a set of items having unique keys (e.g., item x has key x.key) • (Set or multi-set? We restrict to unique keys for now.) • Often we let key of an item be the item itself, but may want to store more info than just key • Supports set operations: Container build(X) len() given an iterable X, build sequence from items in X return the number of stored items Static find(k) return the stored item with key k Dynamic insert(x) delete(k) add x to set (replace item with key x.key if one already exists) remove and return the stored item with key k Order iter ord() find min() find max() find next(k) find prev(k) return the stored items one-by-one in key order return the stored item with smallest key return the stored item with largest key return the stored item with smallest key larger than k return the stored item with largest key smaller than k • Special case interfaces: dictionary set without the Order operations • In recitation, you will be asked to implement a Set, given a Sequence data structure. Array Sequence • Array is great for static operations! get at(i) and set at(i, x) in Θ(1) time! • But not so great at dynamic operations... • (For consistency, we maintain the invariant that array is full) • Then inserting and removing items requires: – reallocating the array – shifting all items after the modiﬁed item Data Operation, Worst Case O(·) Container Static Dynamic Structure build(X) get at(i) set at(i,x) insert first(x) delete first() insert last(x) delete last() insert at(i, x) delete at(i) Array n 1 n n n 3 Lecture 2: Data Structures Linked List Sequence • Pointer data structure (this is not related to a Python “list”) • Each item stored in a node which contains a pointer to the next node in sequence • Each node has two ﬁelds: node.item and node.next • Can manipulate nodes simply by relinking pointers! • Maintain pointers to the ﬁrst node in sequence (called the head) • Can now insert and delete from the front in Θ(1) time! Yay! • (Inserting/deleting efﬁciently from back is also possible; you will do this in PS1) • But now get at(i) and set at(i, x) each take O(n) time... :( • Can we get the best of both worlds? Yes! (Kind of...) Data Operation, Worst Case O(·) Container Static Dynamic Structure build(X) get at(i) set at(i,x) insert first(x) delete first() insert last(x) delete last() insert at(i, x) delete at(i) Linked List n n 1 n n Dynamic Array Sequence • Make an array efﬁcient for last dynamic operations • Python “list” is a dynamic array • Idea! Allocate extra space so reallocation does not occur with every dynamic operation • Fill ratio: 0 ≤ r ≤ 1 the ratio of items to space • Whenever array is full (r = 1), allocate Θ(n) extra space at end to ﬁll ratio ri (e.g., 1/2) • Will have to insert Θ(n) items before the next reallocation • A single operation can take Θ(n) time for reallocation • However, any sequence of Θ(n) operations takes Θ(n) time • So each operation takes Θ(1) time “on average” 4 Lecture 2: Data Structures Amortized Analysis • Data structure analysis technique to distribute cost over many operations • Operation has amortized cost T (n) if k operations cost at most ≤ kT (n) • “T (n) amortized” roughly means T (n) “on average” over many operations • Inserting into a dynamic array takes Θ(1) amortized time • More amortization analysis techniques in 6.046! Dynamic Array Deletion • Delete from back? Θ(1) time without effort, yay! • However, can be very wasteful in space. Want size of data structure to stay Θ(n) • Attempt: if very empty, resize to r = 1. Alternating insertion and deletion could be bad... • Idea! When r < rd, resize array to ratio ri where rd < ri (e.g., rd = 1/4, ri = 1/2) • Then Θ(n) cheap operations must be made before next expensive resize 1 rd+1 • Can limit extra space usage to (1 + ε)n for any ε > 0 (set rd = , ri = ) 1+ε 2 • Dynamic arrays only support dynamic last operations in Θ(1) time • Python List append and pop are amortized O(1) time, other operations can be O(n)! • (Inserting/deleting efﬁciently from front is also possible; you will do this in PS1) Data Operation, Worst Case O(·) Container Static Dynamic Structure build(X) get at(i) set at(i,x) insert first(x) delete first() insert last(x) delete last() insert at(i, x) delete at(i) Array n 1 n n n Linked List n n 1 n n Dynamic Array n 1 n 1(a) n MIT OpenCourseWare https://ocw.mit.edu 6.006 Introduction to Algorithms Spring 2020 For information about citing these materials or our Terms of Use, visit: https://ocw.mit.edu/terms Introduction to Algorithms: 6.006 Massachusetts Institute of Technology Instructors: Erik Demaine, Jason Ku, and Justin Solomon Lecture 3: Sorting Lecture 3: Sorting Set Interface (L03-L08) Container build(X) len() given an iterable X, build set from items in X return the number of stored items Static find(k) return the stored item with key k Dynamic insert(x) delete(k) add x to set (replace item with key x.key if one already exists) remove and return the stored item with key k Order iter ord() find min() find max() find next(k) find prev(k) return the stored items one-by-one in key order return the stored item with smallest key return the stored item with largest key return the stored item with smallest key larger than k return the stored item with largest key smaller than k • Storing items in an array in arbitrary order can implement a (not so efﬁcient) set • Stored items sorted increasing by key allows: – faster ﬁnd min/max (at ﬁrst and last index of array) – faster ﬁnds via binary search: O(log n) Set Operations O(·) Container Static Dynamic Order Data Structure build(X) find(k) insert(x) delete(k) find min() find max() find prev(k) find next(k) Array n n n n n Sorted Array n log n log n n 1 log n • But how to construct a sorted array efﬁciently? 2 Lecture 3: Sorting Sorting • Given a sorted array, we can leverage binary search to make an efﬁcient set data structure. • Input: (static) array A of n numbers • Output: (static) array B which is a sorted permutation of A – Permutation: array with same elements in a different order – Sorted: B[i − 1] ≤ B[i] for all i ∈{1, . . . , n} • Example: [8, 2, 4, 9, 3] → [2, 3, 4, 8, 9] • A sort is destructive if it overwrites A (instead of making a new array B that is a sorted version of A) • A sort is in place if it uses O(1) extra space (implies destructive: in place ⊆ destructive) Permutation Sort • There are n! permutations of A, at least one of which is sorted • For each permutation, check whether sorted in Θ(n) • Example: [2, 3, 1] →{[1, 2, 3], [1, 3, 2], [2, 1, 3], [2, 3, 1], [3, 1, 2], [3, 2, 1]} 1 def permutation_sort(A): 2 ’’’Sort A’’’ 3 for B in permutations(A): # O(n!) 4 if is_sorted(B): # O(n) 5 return B # O(1) • permutation sort analysis: – Correct by case analysis: try all possibilities (Brute Force) – Running time: Ω(n! · n) which is exponential :( Solving Recurrences • Substitution: Guess a solution, replace with representative function, recurrence holds true • Recurrence Tree: Draw a tree representing the recursive calls and sum computation at nodes • Master Theorem: A formula to solve many recurrences (R03) 3 Lecture 3: Sorting Selection Sort • Find a largest number in preﬁx A[:i + 1] and swap it to A[i] • Recursively sort preﬁx A[:i] • Example: [8, 2, 4, 9, 3], [8, 2, 4, 3, 9], [3, 2, 4, 8, 9], [3, 2, 4, 8, 9], [2, 3, 4, 8, 9] 1 def selection_sort(A, i = None): # T(i) 2 ’’’Sort A[:i + 1]’’’ 3 if i is None: i = len(A) - 1 # O(1) 4 if i > 0: # O(1) 5 j = prefix_max(A, i) # S(i) 6 A[i], A[j] = A[j], A[i] # O(1) 7 selection_sort(A, i - 1) # T(i - 1) 8 9 def prefix_max(A, i): # S(i) 10 ’’’Return index of maximum in A[:i + 1]’’’ 11 if i > 0: # O(1) 12 j = prefix_max(A, i - 1) # S(i - 1) 13 if A[i] < A[j]: # O(1) 14 return j # O(1) 15 return i # O(1) • prefix max analysis: – Base case: for i = 0, array has one element, so index of max is i – Induction: assume correct for i, maximum is either the maximum of A[:i] or A[i], returns correct index in either case. – S(1) = Θ(1), S(n) = S(n − 1) + Θ(1) ∗ Substitution: S(n) = Θ(n), cn = Θ(1) + c(n − 1) =⇒ 1 = Θ(1) P n−1 ∗ Recurrence tree: chain of n nodes with Θ(1) work per node, i=0 1 = Θ(n) • selection sort analysis: – Base case: for i = 0, array has one element so is sorted – Induction: assume correct for i, last number of a sorted output is a largest number of the array, and the algorithm puts one there; then A[:i] is sorted by induction – T (1) = Θ(1), T (n) = T (n − 1) + Θ(n) ∗ Substitution: T (n) = Θ(n2), cn2 = Θ(n) + c(n − 1)2 =⇒ c(2n − 1) = Θ(n) P n−1 ∗ Recurrence tree: chain of n nodes with Θ(i) work per node, i=0 i = Θ(n2) 4 Lecture 3: Sorting Insertion Sort • Recursively sort preﬁx A[:i] • Sort preﬁx A[:i + 1] assuming that preﬁx A[:i] is sorted by repeated swaps • Example: [8, 2, 4, 9, 3], [2, 8, 4, 9, 3], [2, 4, 8, 9, 3], [2, 4, 8, 9, 3], [2, 3, 4, 8, 9] 1 def insertion_sort(A, i = None): # T(i) 2 ’’’Sort A[:i + 1]’’’ 3 if i is None: i = len(A) - 1 # O(1) 4 if i > 0: # O(1) 5 insertion_sort(A, i - 1) # T(i - 1) 6 insert_last(A, i) # S(i) 7 8 def insert_last(A, i): # S(i) 9 ’’’Sort A[:i + 1] assuming sorted A[:i]’’’ 10 if i > 0 and A[i] < A[i - 1]: # O(1) 11 A[i], A[i - 1] = A[i - 1], A[i] # O(1) 12 insert_last(A, i - 1) # S(i - 1) • insert last analysis: – Base case: for i = 0, array has one element so is sorted – Induction: assume correct for i, if A[i] >= A[i - 1], array is sorted; otherwise, swapping last two elements allows us to sort A[:i] by induction – S(1) = Θ(1), S(n) = S(n − 1) + Θ(1) =⇒ S(n) = Θ(n) • insertion sort analysis: – Base case: for i = 0, array has one element so is sorted – Induction: assume correct for i, algorithm sorts A[:i] by induction, and then insert last correctly sorts the rest as proved above – T (1) = Θ(1), T (n) = T (n − 1) + Θ(n) =⇒ T (n) = Θ(n2) 5 Lecture 3: Sorting Merge Sort • Recursively sort ﬁrst half and second half (may assume power of two) • Merge sorted halves into one sorted list (two ﬁnger algorithm) • Example: [7, 1, 5, 6, 2, 4, 9, 3], [1, 7, 5, 6, 2, 4, 3, 9], [1, 5, 6, 7, 2, 3, 4, 9], [1, 2, 3, 4, 5, 6, 7, 9] 1 def merge_sort(A, a = 0, b = None): # T(b - a = n) 2 ’’’Sort A[a:b]’’’ 3 if b is None: b = len(A) # O(1) 4 if 1 < b - a: # O(1) 5 c = (a + b + 1) // 2 # O(1) 6 merge_sort(A, a, c) # T(n / 2) 7 merge_sort(A, c, b) # T(n / 2) 8 L, R = A[a:c], A[c:b] # O(n) 9 merge(L, R, A, len(L), len(R), a, b) # S(n) 10 11 def merge(L, R, A, i, j, a, b): # S(b - a = n) 12 ’’’Merge sorted L[:i] and R[:j] into A[a:b]’’’ 13 if a < b: # O(1) 14 if (j <= 0) or (i > 0 and L[i - 1] > R[j - 1]): # O(1) 15 A[b - 1] = L[i - 1] # O(1) 16 i = i - 1 # O(1) 17 else: # O(1) 18 A[b - 1] = R[j - 1] # O(1) 19 j = j - 1 # O(1) 20 merge(L, R, A, i, j, a, b - 1) # S(n - 1) • merge analysis: – Base case: for n = 0, arrays are empty, so vacuously correct – Induction: assume correct for n, item in A[r] must be a largest number from remaining preﬁxes of L and R, and since they are sorted, taking largest of last items sufﬁces; remainder is merged by induction – S(0) = Θ(1), S(n) = S(n − 1) + Θ(1) =⇒ S(n) = Θ(n) • merge sort analysis: – Base case: for n = 1, array has one element so is sorted – Induction: assume correct for k < n, algorithm sorts smaller halves by induction, and then merge merges into a sorted array as proved above. – T (1) = Θ(1), T (n) = 2T (n/2) + Θ(n) ∗ Substitution: Guess T (n) = Θ(n log n) cn log n = Θ(n) + 2c(n/2) log(n/2) =⇒ cn log(2) = Θ(n) ∗ Recurrence Tree: complete binary tree with depth log2 n and n leaves, level i has 2i Plog2 n Plog2 n nodes with O(n/2i) work each, total: i=0 (2i)(n/2i) = i=0 n = Θ(n log n) MIT OpenCourseWare https://ocw.mit.edu 6.006 Introduction to Algorithms Spring 2020 For information about citing these materials or our Terms of Use, visit: https://ocw.mit.edu/terms Introduction to Algorithms: 6.006 Massachusetts Institute of Technology Instructors: Erik Demaine, Jason Ku, and Justin Solomon Lecture 4: Hashing Lecture 4: Hashing Review Data Structure Operations O(·) Container Static Dynamic Order build(X) find(k) insert(x) delete(k) find min() find max() find prev(k) find next(k) Array n n n n n Sorted Array n log n log n n 1 log n • Idea! Want faster search and dynamic operations. Can we find(k) faster than Θ(log n)? • Answer is no (lower bound)! (But actually, yes...!?) Comparison Model • In this model, assume algorithm can only differentiate items via comparisons • Comparable items: black boxes only supporting comparisons between pairs • Comparisons are <, ≤, >, ≥, =, =6 , outputs are binary: True or False • Goal: Store a set of n comparable items, support find(k) operation • Running time is lower bounded by # comparisons performed, so count comparisons! Decision Tree • Any algorithm can be viewed as a decision tree of operations performed • An internal node represents a binary comparison, branching either True or False • For a comparison algorithm, the decision tree is binary (draw example) • A leaf represents algorithm termination, resulting in an algorithm output • A root-to-leaf path represents an execution of the algorithm on some input • Need at least one leaf for each algorithm output, so search requires ≥ n + 1 leaves 2 Lecture 4: Hashing Comparison Search Lower Bound • What is worst-case running time of a comparison search algorithm? • running time ≥ # comparisons ≥ max length of any root-to-leaf path ≥ height of tree • What is minimum height of any binary tree on ≥ n nodes? • Minimum height when binary tree is complete (all rows full except last) • Height ≥dlg(n + 1)e − 1 = Ω(log n), so running time of any comparison sort is Ω(log n) • Sorted arrays achieve this bound! Yay! • More generally, height of tree with Θ(n) leaves and max branching factor b is Ω(logb n) • To get faster, need an operation that allows super-constant ω(1) branching factor. How?? Direct Access Array • Exploit Word-RAM O(1) time random access indexing! Linear branching factor! • Idea! Give item unique integer key k in {0, . . . , u − 1}, store item in an array at index k • Associate a meaning with each index of array • If keys ﬁt in a machine word, i.e. u ≤ 2w , worst-case O(1) ﬁnd/dynamic operations! Yay! • 6.006: assume input numbers/strings ﬁt in a word, unless length explicitly parameterized • Anything in computer memory is a binary integer, or use (static) 64-bit address in memory • But space O(u), so really bad if n ≪ u... :( • Example: if keys are ten-letter names, for one bit per name, requires 2610 ≈ 17.6 TB space • How can we use less space? Hashing • Idea! If n ≪ u, map keys to a smaller range m = Θ(n) and use smaller direct access array • Hash function: h(k) : {0, . . . , u − 1} →{0, . . . , m − 1} (also hash map) • Direct access array called hash table, h(k) called the hash of key k • If m ≪ u, no hash function is injective by pigeonhole principle 3 Lecture 4: Hashing • Always exists keys a, b such that h(a) = h(b) → Collision! :( • Can’t store both items at same index, so where to store? Either: – store somewhere else in the array (open addressing) ∗ complicated analysis, but common and practical – store in another data structure supporting dynamic set interface (chaining) Chaining • Idea! Store collisions in another data structure (a chain) • If keys roughly evenly distributed over indices, chain size is n/m = n/Ω(n) = O(1)! • If chain has O(1) size, all operations take O(1) time! Yay! • If not, many items may map to same location, e.g. h(k) = constant, chain size is Θ(n) :( • Need good hash function! So what’s a good hash function? Hash Functions Division (bad): h(k) = (k mod m) • Heuristic, good when keys are uniformly distributed! • m should avoid symmetries of the stored keys • Large primes far from powers of 2 and 10 can be reasonable • Python uses a version of this with some additional mixing • If u ≫ n, every hash function will have some input set that will a create O(n) size chain • Idea! Don’t use a ﬁxed hash function! Choose one randomly (but carefully)! 4 Lecture 4: Hashing Universal (good, theoretically): hab(k) = (((ak + b) mod p) mod m) • Hash Family H(p, m) = {hab | a, b ∈{0, . . . , p − 1} and a 6= 0} • Parameterized by a ﬁxed prime p > u, with a and b chosen from range {0, . . . , p − 1} • H is a Universal family: Pr {h(ki) = h(kj )} ≤ 1/m ∀ki =6 kj ∈{0, . . . , u − 1} h∈H • Why is universality useful? Implies short chain lengths! (in expectation) • Xij indicator random variable over h ∈H: Xij = 1 if h(ki) = h(kj ), Xij = 0 otherwise P • Size of chain at index h(ki) is random variable Xi = j Xij • Expected size of chain at index h(ki) ( ) X X X E {Xi} = E Xij = E {Xij } = 1 + E {Xij } h∈H h∈H h∈H h∈H j j j= 6 i X = 1 + (1) Pr {h(ki) = h(kj )} + (0) Pr {h(ki) =6 h(kj )} h∈H h∈H j6=i X ≤ 1 + 1/m = 1 + (n − 1)/m j6=i • Since m = Ω(n), load factor α = n/m = O(1), so O(1) in expectation! Dynamic • If n/m far from 1, rebuild with new randomly chosen hash function for new size m • Same analysis as dynamic arrays, cost can be amortized over many dynamic operations • So a hash table can implement dynamic set operations in expected amortized O(1) time! :) Data Structure Operations O(·) Container Static Dynamic Order build(X) find(k) insert(x) delete(k) find min() find max() find prev(k) find next(k) Array n n n n n Sorted Array n log n log n n 1 log n Direct Access Array u 1 1 u u Hash Table n(e) 1(e) 1(a)(e) n n MIT OpenCourseWare https://ocw.mit.edu 6.006 Introduction to Algorithms Spring 2020 For information about citing these materials or our Terms of Use, visit: https://ocw.mit.edu/terms Introduction to Algorithms: 6.006 Massachusetts Institute of Technology Instructors: Erik Demaine, Jason Ku, and Justin Solomon Lecture 5: Linear Sorting Lecture 5: Linear Sorting Review • Comparison search lower bound: any decision tree with n nodes has height ≥dlg(n+1)e−1 • Can do faster using random access indexing: an operation with linear branching factor! • Direct access array is fast, but may use a lot of space (Θ(u)) • Solve space problem by mapping (hashing) key space u down to m = Θ(n) • Hash tables give expected O(1) time operations, amortized if dynamic • Expectation input-independent: choose hash function randomly from universal hash family • Data structure overview! • Last time we achieved faster ﬁnd. Can we also achieve faster sort? Data Structure Operations O(·) Container Static Dynamic Order build(X) find(k) insert(x) delete(k) find min() find max() find prev(k) find next(k) Array n n n n n Sorted Array n log n log n n 1 log n Direct Access Array u 1 1 u u Hash Table n(e) 1(e) 1(a)(e) n n 2 Lecture 5: Linear Sorting Comparison Sort Lower Bound • Comparison model implies that algorithm decision tree is binary (constant branching factor) • Requires # leaves L ≥ # possible outputs • Tree height lower bounded by Ω(log L), so worst-case running time is Ω(log L) • To sort array of n elements, # outputs is n! permutations • Thus height lower bounded by log(n!) ≥ log((n/2)n/2) = Ω(n log n) • So merge sort is optimal in comparison model • Can we exploit a direct access array to sort faster? Direct Access Array Sort • Example: [5, 2, 7, 0, 4] • Suppose all keys are unique non-negative integers in range {0, . . . , u − 1}, so n ≤ u • Insert each item into a direct access array with size u in Θ(n) • Return items in order they appear in direct access array in Θ(u) • Running time is Θ(u), which is Θ(n) if u = Θ(n). Yay! 1 def direct_access_sort(A): 2 \"Sort A assuming items have distinct non-negative keys\" 3 u = 1 + max([x.key for x in A]) # O(n) find maximum key 4 D = [None] * u # O(u) direct access array 5 for x in A: # O(n) insert items 6 D[x.key] = x 7 i = 0 8 for key in range(u): # O(u) read out items in order 9 if D[key] is not None: 10 A[i] = D[key] 11 i += 1 • What if keys are in larger range, like u = Ω(n2) < n2? • Idea! Represent each key k by tuple (a, b) where k = an + b and 0 ≤ b < n • Speciﬁcally a = bk/nc < n and b = (k mod n) (just a 2-digit base-n number!) • This is a built-in Python operation (a, b) = divmod(k, n) • Example: [17, 3, 24, 22, 12] ⇒ [(3,2), (0,3), (4,4), (4,2), (2,2)] ⇒ [32, 03, 44, 42, 22](n=5) • How can we sort tuples? 3 Lecture 5: Linear Sorting Tuple Sort • Item keys are tuples of equal length, i.e. item x.key = (x.k1, x.k2, x.k2, . . .). • Want to sort on all entries lexicographically, so ﬁrst key k1 is most signiﬁcant • How to sort? Idea! Use other auxiliary sorting algorithms to separately sort each key • (Like sorting rows in a spreadsheet by multiple columns) • What order to sort them in? Least signiﬁcant to most signiﬁcant! • Exercise: [32, 03, 44, 42, 22] =⇒ [42, 22, 32, 03, 44] =⇒ [03, 22, 32, 42, 44](n=5) • Idea! Use tuple sort with auxiliary direct access array sort to sort tuples (a, b). • Problem! Many integers could have the same a or b value, even if input keys distinct • Need sort allowing repeated keys which preserves input order • Want sort to be stable: repeated keys appear in output in same order as input • Direct access array sort cannot even sort arrays having repeated keys! • Can we modify direct access array sort to admit multiple keys in a way that is stable? Counting Sort • Instead of storing a single item at each array index, store a chain, just like hashing! • For stability, chain data structure should remember the order in which items were added • Use a sequence data structure which maintains insertion order • To insert item x, insert last to end of the chain at index x.key • Then to sort, read through all chains in sequence order, returning items one by one 1 def counting_sort(A): 2 \"Sort A assuming items have non-negative keys\" 3 u = 1 + max([x.key for x in A]) # O(n) find maximum key 4 D = [[] for i in range(u)] # O(u) direct access array of chains 5 for x in A: # O(n) insert into chain at x.key 6 D[x.key].append(x) 7 i = 0 8 for chain in D: # O(u) read out items in order 9 for x in chain: 10 A[i] = x 11 i += 1 4 Lecture 5: Linear Sorting Radix Sort • Idea! If u < n2 , use tuple sort with auxiliary counting sort to sort tuples (a, b) • Sort least signiﬁcant key b, then most signiﬁcant key a • Stability ensures previous sorts stay sorted • Running time for this algorithm is O(2n) = O(n). Yay! • If every key < nc for some positive c = logn(u), every key has at most c digits base n • A c-digit number can be written as a c-element tuple in O(c) time • We sort each of the c base-n digits in O(n) time • So tuple sort with auxiliary counting sort runs in O(cn) time in total • If c is constant, so each key is ≤ nc, this sort is linear O(n)! 1 def radix_sort(A): 2 \"Sort A assuming items have non-negative keys\" 3 n = len(A) 4 u = 1 + max([x.key for x in A]) # O(n) find maximum key 5 c = 1 + (u.bit_length() // n.bit_length()) 6 class Obj: pass 7 D = [Obj() for a in A] 8 for i in range(n): # O(nc) make digit tuples 9 D[i].digits = [] 10 D[i].item = A[i] 11 high = A[i].key 12 for j in range(c): # O(c) make digit tuple 13 high, low = divmod(high, n) 14 D[i].digits.append(low) 15 for i in range(c): # O(nc) sort each digit 16 for j in range(n): # O(n) assign key i to tuples 17 D[j].key = D[j].digits[i] 18 counting_sort(D) # O(n) sort on digit i 19 for i in range(n): # O(n) output to A 20 A[i] = D[i].item Algorithm Time O(·) In-place? Stable? Comments Insertion Sort 2 n Y Y O(nk) for k-proximate Selection Sort 2 n Y N O(n) swaps Merge Sort n log n N Y stable, optimal comparison Counting Sort n + u N Y O(n) when u = O(n) Radix Sort n + n log (u) n N Y O(n) when u = O(nc) MIT OpenCourseWare https://ocw.mit.edu 6.006 Introduction to Algorithms Spring 2020 For information about citing these materials or our Terms of Use, visit: https://ocw.mit.edu/terms Introduction to Algorithms: 6.006 Massachusetts Institute of Technology Instructors: Erik Demaine, Jason Ku, and Justin Solomon Lecture 6: Binary Trees I Lecture 6: Binary Trees I Previously and New Goal Sequence Data Structure Operations O(·) Container Static Dynamic build(X) get at(i) set at(i,x) insert first(x) delete first() insert last(x) delete last() insert at(i, x) delete at(i) Array n 1 n n n Linked List n n 1 n n Dynamic Array n 1 n 1(a) n Goal n log n log n log n log n Set Data Structure Operations O(·) Container Static Dynamic Order build(X) find(k) insert(x) delete(k) find min() find max() find prev(k) find next(k) Array n n n n n Sorted Array n log n log n n 1 log n Direct Access Array u 1 1 u u Hash Table n(e) 1(e) 1(a)(e) n n Goal n log n log n log n log n log n How? Binary Trees! • Pointer-based data structures (like Linked List) can achieve worst-case performance • Binary tree is pointer-based data structure with three pointers per node • Node representation: node.{item, parent, left, right} • Example: 1 2 3 4 5 ________<A>_____ __<B>_____ <C> __<D> <E> <F> node | item | parent | left | right | <A> | A | - | <B> | <C> | <B> B <A> <C> <D> | | | | | <C> | C | <A> | - | - | <D> | D | <B> | <F> | - | <E> | E | <B> | - | - | <F> F <D> - - | | | | | 2 Lecture 6: Binary Trees I Terminology • The root of a tree has no parent (Ex: <A>) • A leaf of a tree has no children (Ex: <C>, <E>, and <F>) • Deﬁne depth(<X>) of node <X> in a tree rooted at <R> to be length of path from <X> to <R> • Deﬁne height(<X>) of node <X> to be max depth of any node in the subtree rooted at <X> • Idea: Design operations to run in O(h) time for root height h, and maintain h = O(log n) • A binary tree has an inherent order: its traversal order – every node in node <X>’s left subtree is before <X> – every node in node <X>’s right subtree is after <X> • List nodes in traversal order via a recursive algorithm starting at root: – Recursively list left subtree, list self, then recursively list right subtree – Runs in O(n) time, since O(1) work is done to list each node – Example: Traversal order is (<F>, <D>, <B>, <E>, <A>, <C>) • Right now, traversal order has no meaning relative to the stored items • Later, assign semantic meaning to traversal order to implement Sequence/Set interfaces Tree Navigation • Find ﬁrst node in the traversal order of node <X>’s subtree (last is symmetric) – If <X> has left child, recursively return the ﬁrst node in the left subtree – Otherwise, <X> is the ﬁrst node, so return it – Running time is O(h) where h is the height of the tree – Example: ﬁrst node in <A>’s subtree is <F> • Find successor of node <X> in the traversal order (predecessor is symmetric) – If <X> has right child, return ﬁrst of right subtree – Otherwise, return lowest ancestor of <X> for which <X> is in its left subtree – Running time is O(h) where h is the height of the tree – Example: Successor of: <B> is <E>, <E> is <A>, and <C> is None 3 Lecture 6: Binary Trees I Dynamic Operations • Change the tree by a single item (only add or remove leaves): – add a node after another in the traversal order (before is symmetric) – remove an item from the tree • Insert node <Y> after node <X> in the traversal order – If <X> has no right child, make <Y> the right child of <X> – Otherwise, make <Y> the left child of <X>’s successor (which cannot have a left child) – Running time is O(h) where h is the height of the tree • Example: Insert node <G> before <E> in traversal order 1 _____<A>__ ________<A>__ 2 __<B>__ <C> => __<B>_____ <C> 3 __<D> <E> __<D> __<E> 4 <F> <F> <G> • Example: Insert node <H> after <A> in traversal order 1 ________<A>___ ________<A>_____ 2 __<B>_____ <C> => __<B>_____ __<C> 3 __<D> __<E> __<D> __<E> <H> 4 <F> <G> <F> <G> • Delete the item in node <X> from <X>’s subtree – If <X> is a leaf, detach from parent and return – Otherwise, <X> has a child ∗ If <X> has a left child, swap items with the predecessor of <X> and recurse ∗ Otherwise <X> has a right child, swap items with the successor of <X> and recurse – Running time is O(h) where h is the height of the tree – Example: Remove <F> (a leaf) 1 ________<A>_____ ________<A>_____ 2 __<B>_____ __<C> => __<B>_____ __<C> 3 __<D> __<E> <H> <D> __<E> <H> 4 <F> <G> <G> – Example: Remove <A> (not a leaf, so ﬁrst swap down to a leaf) 1 ________<A>_____ ________<E>_____ _____<E>_____ 2 __<B>_____ __<C> => __<B>_____ __<C> => __<B>__ __<C> 3 <D> __<E> <H> <D> __<G> <H> <D> <G> <H> 4 <G> <A> 4 Lecture 6: Binary Trees I Application: Set • Idea! Set Binary Tree (a.k.a. Binary Search Tree / BST): Traversal order is sorted order increasing by key – Equivalent to BST Property: for every node, every key in left subtree ≤ node’s key ≤ every key in right subtree • Then can ﬁnd the node with key k in node <X>’s subtree in O(h) time like binary search: – If k is smaller than the key at <X>, recurse in left subtree (or return None) – If k is larger than the key at <X>, recurse in right subtree (or return None) – Otherwise, return the item stored at <X> • Other Set operations follow a similar pattern; see recitation Application: Sequence • Idea! Sequence Binary Tree: Traversal order is sequence order • How do we ﬁnd ith node in traversal order of a subtree? Call this operation subtree at(i) • Could just iterate through entire traversal order, but that’s bad, O(n) • However, if we could compute a subtree’s size in O(1), then can solve in O(h) time – How? Check the size nL of the left subtree and compare to i – If i < nL, recurse on the left subtree – If i > nL, recurse on the right subtree with i0 = i − nL − 1 – Otherwise, i = nL, and you’ve reached the desired node! • Maintain the size of each node’s subtree at the node via augmentation – Add node.size ﬁeld to each node – When adding new leaf, add +1 to a.size for all ancestors a in O(h) time – When deleting a leaf, add −1 to a.size for all ancestors a in O(h) time • Sequence operations follow directly from a fast subtree at(i) operation • Naively, build(X) takes O(nh) time, but can be done in O(n) time; see recitation 5 Lecture 6: Binary Trees I So Far Set Data Structure Operations O(·) Container Static Dynamic Order build(X) find(k) insert(x) delete(k) find min() find max() find prev(k) find next(k) Binary Tree n log n h h h h Goal n log n log n log n log n log n Sequence Data Structure Operations O(·) Container Static Dynamic build(X) get at(i) set at(i,x) insert first(x) delete first() insert last(x) delete last() insert at(i, x) delete at(i) Binary Tree n h h h h Goal n log n log n log n log n Next Time • Keep a binary tree balanced after insertion or deletion • Reduce O(h) running times to O(log n) by keeping h = O(log n) MIT OpenCourseWare https://ocw.mit.edu 6.006 Introduction to Algorithms Spring 2020 For information about citing these materials or our Terms of Use, visit: https://ocw.mit.edu/terms Introduction to Algorithms: 6.006 Massachusetts Institute of Technology Instructors: Erik Demaine, Jason Ku, and Justin Solomon Lecture 7: Binary Trees II: AVL Lecture 7: Binary Trees II: AVL Last Time and Today’s Goal Sequence Data Structure Operations O(·) Container Static Dynamic build(X) get at(i) set at(i,x) insert first(x) delete first() insert last(x) delete last() insert at(i, x) delete at(i) Binary Tree n h h h h AVL Tree n log n log n log n log n Set Data Structure Operations O(·) Container Static Dynamic Order build(X) find(k) insert(x) delete(k) find min() find max() find prev(k) find next(k) Binary Tree n log n h h h h AVL Tree n log n log n log n log n log n Height Balance • How to maintain height h = O(log n) where n is number of nodes in tree? • A binary tree that maintains O(log n) height under dynamic operations is called balanced – There are many balancing schemes (Red-Black Trees, Splay Trees, 2-3 Trees, . . . ) – First proposed balancing scheme was the AVL Tree (Adelson-Velsky and Landis, 1962) Rotations • Need to reduce height of tree without changing its traversal order, so that we represent the same sequence of items • How to change the structure of a tree, while preserving traversal order? Rotations! 1 _____<D>__ rotate_right(<D>) __<B>_____ 2 __<B>__ <E> => <A> __<D>__ 3 <A> <C> / \\ / \\ <C> <E> 4 / \\ / \\ /___\\ <= /___\\ / \\ / \\ 5 /___\\ /___\\ rotate_left(<B>) /___\\ /___\\ • A rotation relinks O(1) pointers to modify tree structure and maintains traversal order 2 Lecture 7: Binary Trees II: AVL Rotations Sufﬁce • Claim: O(n) rotations can transform a binary tree to any other with same traversal order. • Proof: Repeatedly perform last possible right rotation in traversal order; resulting tree is a canonical chain. Each rotation increases depth of the last node by 1. Depth of last node in ﬁnal chain is n − 1, so at most n − 1 rotations are performed. Reverse canonical rotations to reach target tree. • Can maintain height-balance by using O(n) rotations to fully balance the tree, but slow :( • We will keep the tree balanced in O(log n) time per operation! AVL Trees: Height Balance • AVL trees maintain height-balance (also called the AVL Property) – A node is height-balanced if heights of its left and right subtrees differ by at most 1 – Let skew of a node be the height of its right subtree minus that of its left subtree – Then a node is height-balanced if its skew is −1, 0, or 1 • Claim: A binary tree with height-balanced nodes has height h = O(log n) (i.e., n = 2Ω(h)) • Proof: Sufﬁces to show fewest nodes F (h) in any height h tree is F (h) = 2Ω(h) F (h) ≥ 2h/2 F (0) = 1, F (1) = 2, F (h) = 1+F (h−1)+F (h−2) ≥ 2F (h−2) =⇒ • Suppose adding or removing leaf from a height-balanced tree results in imbalance – Only subtrees of the leaf’s ancestors have changed in height or skew – Heights changed by only ±1, so skews still have magnitude ≤ 2 – Idea: Fix height-balance of ancestors starting from leaf up to the root – Repeatedly rebalance lowest ancestor that is not height-balanced, wlog assume skew 2 3 Lecture 7: Binary Trees II: AVL • Local Rebalance: Given binary tree node <B>: – whose skew 2 and – every other node in <B>’s subtree is height-balanced, – then <B>’s subtree can be made height-balanced via one or two rotations – (after which <B>’s height is the same or one less than before) • Proof: – Since skew of <B> is 2, <B>’s right child <F> exists – Case 1: skew of <F> is 0 or Case 2: skew of <F> is 1 ∗ Perform a left rotation on <B> 1 __<B>______ ______<F>____ 2 <A> ___<F>___ __<B>___ <G> 3 / \\ <D> <G> => <A> <D> / \\ 4 /___\\ / \\ / \\ / \\ / \\ / \\ 5 /___\\ / \\ /___\\ /___\\ /_____\\ 6 /_____\\ /_____\\ /_____\\ ∗ Let h = height(<A>). Then height(<G>) = h + 1 and height(<D>) is h + 1 in Case 1, h in Case 2 ∗ After rotation: · the skew of <B> is either 1 in Case 1 or 0 in Case 2, so <B> is height balanced · the skew of <F> is −1, so <F> is height balanced · the height of <B> before is h + 3, then after is h + 3 in Case 1, h + 2 in Case 2 – Case 3: skew of <F> is −1, so the left child <D> of <F> exists ∗ Perform a right rotation on <F>, then a left rotation on <B> 1 __<B>___________ _____<D>______ 2 <A> _____<F>__ __<B>__ __<F>__ 3 / \\ __<D>__ <G> => <A> <C> <E> <G> 4 /___\\ <C> <E> / \\ / \\ /_\\ /_\\ / \\ 5 /_\\ /_\\ /___\\ /___\\ /___\\ /___\\ /___\\ 6 /___\\ /___\\ ∗ Let h = height(<A>). Then height(<G>) = h while height(<C>) and height(<E>) are each either h or h − 1 ∗ After rotation: · the skew of <B> is either 0 or −1, so <B> is height balanced · the skew of <F> is either 0 or 1, so <F> is height balanced · the skew of <D> is 0, so D is height balanced · the height of <B> is h + 3 before, then after is h + 2 4 Lecture 7: Binary Trees II: AVL • Global Rebalance: Add or remove a leaf from height-balanced tree T to produce tree T 0 . Then T 0 can be transformed into a height-balanced tree T 00 using at most O(log n) rotations. • Proof: – Only ancestors of the affected leaf have different height in T 0 than in T – Affected leaf has at most h = O(log n) ancestors whose subtrees may have changed – Let <X> be lowest ancestor that is not height-balanced (with skew magnitude 2) – If a leaf was added into T : ∗ Insertion increases height of <X>, so in Case 2 or 3 of Local Rebalancing ∗ Rotation decreases subtree height: balanced after one rotation – If a leaf was removed from T : ∗ Deletion decreased height of one child of <X>, not <X>, so only imbalance ∗ Could decrease height of <X> by 1; parent of <X> may now be imbalanced ∗ So may have to rebalance every ancestor of <X>, but at most h = O(log n) of them • So can maintain height-balance using only O(log n) rotations after insertion/deletion! • But requires us to evaluate whether possibly O(log n) nodes were height-balanced Computing Height • How to tell whether node <X> is height-balanced? Compute heights of subtrees! • How to compute the height of node <X>? Naive algorithm: – Recursively compute height of the left and right subtrees of <X> – Add 1 to the max of the two heights – Runs in Ω(n) time, since we recurse on every node :( • Idea: Augment each node with the height of its subtree! (Save for later!) • Height of <X> can be computed in O(1) time from the heights of its children: – Look up the stored heights of left and right subtrees in O(1) time – Add 1 to the max of the two heights • During dynamic operations, we must maintain our augmentation as the tree changes shape • Recompute subtree augmentations at every node whose subtree changes: – Update relinked nodes in a rotation operation in O(1) time (ancestors don’t change) – Update all ancestors of an inserted or deleted node in O(h) time by walking up the tree 5 Lecture 7: Binary Trees II: AVL Steps to Augment a Binary Tree • In general, to augment a binary tree with a subtree property P, you must: – State the subtree property P(<X>) you want to store at each node <X> – Show how to compute P(<X>) from the augmentations of <X>’s children in O(1) time • Then stored property P(<X>) can be maintained without changing dynamic operation costs Application: Sequence • For sequence binary tree, we needed to know subtree sizes • For just inserting/deleting a leaf, this was easy, but now need to handle rotations • Subtree size is a subtree property, so can maintain via augmentation – Can compute size from sizes of children by summing them and adding 1 Conclusion • Set AVL trees achieve O(lg n) time for all set operations, except O(n log n) time for build and O(n) time for iter • Sequence AVL trees achieve O(lg n) time for all sequence operations, except O(n) time for build and iter Application: Sorting • Any Set data structure deﬁnes a sorting algorithm: build (or repeatedly insert) then iter • For example, Direct Access Array Sort from Lecture 5 • AVL Sort is a new O(n lg n)-time sorting algorithm MIT OpenCourseWare https://ocw.mit.edu 6.006 Introduction to Algorithms Spring 2020 For information about citing these materials or our Terms of Use, visit: https://ocw.mit.edu/terms Introduction to Algorithms: 6.006 Massachusetts Institute of Technology Instructors: Erik Demaine, Jason Ku, and Justin Solomon Lecture 8: Binary Heaps Lecture 8: Binary Heaps Priority Queue Interface • Keep track of many items, quickly access/remove the most important – Example: router with limited bandwidth, must prioritize certain kinds of messages – Example: process scheduling in operating system kernels – Example: discrete-event simulation (when is next occurring event?) – Example: graph algorithms (later in the course) • Order items by key = priority so Set interface (not Sequence interface) • Optimized for a particular subset of Set operations: build(X) build priority queue from iterable X insert(x) add item x to data structure delete max() remove and return stored item with largest key find max() return stored item with largest key • (Usually optimized for max or min, not both) • Focus on insert and delete max operations: build can repeatedly insert; find max() can insert(delete min()) Priority Queue Sort • Any priority queue data structure translates into a sorting algorithm: – build(A), e.g., insert items one by one in input order – Repeatedly delete min() (or delete max()) to determine (reverse) sorted order • All the hard work happens inside the data structure • Running time is Tbuild + n · Tdelete max ≤ n · Tinsert + n · Tdelete max • Many sorting algorithms we’ve seen can be viewed as priority queue sort: Priority Queue Operations O(·) Priority Queue Sort Data Structure build(A) insert(x) delete max() Time In-place? Dynamic Array n 1(a) n 2 n Y Sorted Dynamic Array n log n n 1(a) 2 n Y Set AVL Tree n log n log n log n n log n N Goal n log n(a) log n(a) n log n Y Selection Sort Insertion Sort AVL Sort Heap Sort 2 Lecture 8: Binary Heaps Priority Queue: Set AVL Tree • Set AVL trees support insert(x), find min(), find max(), delete min(), and delete max() in O(log n) time per operation • So priority queue sort runs in O(n log n) time – This is (essentially) AVL sort from Lecture 7 • Can speed up find min() and find max() to O(1) time via subtree augmentation • But this data structure is complicated and resulting sort is not in-place • Is there a simpler data structure for just priority queue, and in-place O(n lg n) sort? YES, binary heap and heap sort • Essentially implement a Set data structure on top of a Sequence data structure (array), using what we learned about binary trees Priority Queue: Array • Store elements in an unordered dynamic array • insert(x): append x to end in amortized O(1) time • delete max(): ﬁnd max in O(n), swap max to the end and remove • insert is quick, but delete max is slow • Priority queue sort is selection sort! (plus some copying) Priority Queue: Sorted Array • Store elements in a sorted dynamic array • insert(x): append x to end, swap down to sorted position in O(n) time • delete max(): delete from end in O(1) amortized • delete max is quick, but insert is slow • Priority queue sort is insertion sort! (plus some copying) • Can we ﬁnd a compromise between these two array priority queue extremes? 3 Lecture 8: Binary Heaps Array as a Complete Binary Tree • Idea: interpret an array as a complete binary tree, with maximum 2i nodes at depth i except at the largest depth, where all nodes are left-aligned 1 d0 ______O____ 2 d1 ____O____ __O__ 3 d2 __O__ __O O O 4 d3 O O O • Equivalently, complete tree is ﬁlled densely in reading order: root to leaves, left to right • Perspective: bijection between arrays and complete binary trees 1 Q = [0,1,2,3,4,5,6,7,8,9] 2 d0 0 -> ______0____ 3 d1 1 2 -> ____1____ __2__ 4 d2 3 4 5 6 -> __3__ __4 5 6 5 d3 7 8 9 -> 7 8 9 • Height of complete tree perspective of array of n item is dlg ne, so balanced binary tree Implicit Complete Tree • Complete binary tree structure can be implicit instead of storing pointers • Root is at index 0 • Compute neighbors by index arithmetic: left(i) = 2i + 1 right(i) = 2i + 2 \u0016 \u0017 i − 1 parent(i) = 2 4 Lecture 8: Binary Heaps Binary Heaps • Idea: keep larger elements higher in tree, but only locally • Max-Heap Property at node i: Q[i] ≥ Q[j] for j ∈{left(i), right(i)} • Max-heap is an array satisfying max-heap property at all nodes • Claim: In a max-heap, every node i satisﬁes Q[i] ≥ Q[j] for all nodes j in subtree(i) • Proof: – Induction on d = depth(j) − depth(i) – Base case: d = 0 implies i = j implies Q[i] ≥ Q[j] (in fact, equal) – depth(parent(j)) − depth(i) = d − 1 < d, so Q[i] ≥ Q[parent(j)] by induction – Q[parent(j)] ≥ Q[j] by Max-Heap Property at parent(j) • In particular, max item is at root of max-heap Heap Insert • Append new item x to end of array in O(1) amortized, making it next leaf i in reading order • max heapify up(i): swap with parent until Max-Heap Property – Check whether Q[parent(i)] ≥ Q[i] (part of Max-Heap Property at parent(i)) – If not, swap items Q[i] and Q[parent(i)], and recursively max heapify up(parent(i)) • Correctness: – Max-Heap Property guarantees all nodes ≥ descendants, except Q[i] might be > some of its ancestors (unless i is the root, so we’re done) – If swap necessary, same guarantee is true with Q[parent(i)] instead of Q[i] • Running time: height of tree, so Θ(log n)! 5 Lecture 8: Binary Heaps Heap Delete Max • Can only easily remove last element from dynamic array, but max key is in root of tree • So swap item at root node i = 0 with last item at node n − 1 in heap array • max heapify down(i): swap root with larger child until Max-Heap Property – Check whether Q[i] ≥ Q[j] for j ∈{left(i), right(i)} (Max-Heap Property at i) – If not, swap Q[i] with Q[j] for child j ∈{left(i), right(i)} with maximum key, and recursively max heapify down(j) • Correctness: – Max-Heap Property guarantees all nodes ≥ descendants, except Q[i] might be < some descendants (unless i is a leaf, so we’re done) – If swap is necessary, same guarantee is true with Q[j] instead of Q[i] • Running time: height of tree, so Θ(log n)! Heap Sort • Plugging max-heap into priority queue sort gives us a new sorting algorithm • Running time is O(n log n) because each insert and delete max takes O(log n) • But often include two improvements to this sorting algorithm: In-place Priority Queue Sort • Max-heap Q is a preﬁx of a larger array A, remember how many items |Q| belong to heap • |Q| is initially zero, eventually |A| (after inserts), then zero again (after deletes) • insert() absorbs next item in array at index |Q| into heap • delete max() moves max item to end, then abandons it by decrementing |Q| • In-place priority queue sort with Array is exactly Selection Sort • In-place priority queue sort with Sorted Array is exactly Insertion Sort • In-place priority queue sort with binary Max Heap is Heap Sort 6 Lecture 8: Binary Heaps Linear Build Heap • Inserting n items into heap calls max heapify up(i) for i from 0 to n − 1 (root down): n−1 n−1 X X worst-case swaps ≈ depth(i) = lg i = lg(n!) ≥ (n/2) lg(n/2) = Ω(n lg n) i=0 i=0 • Idea! Treat full array as a complete binary tree from start, then max heapify down(i) for i from n − 1 to 0 (leaves up): n−1 n−1 \u0012 \u0013 X X n n n n worst-case swaps ≈ height(i) = (lg n−lg i) = lg = Θ lg √ = O(n) n! n(n/e)n i=0 i=0 • So can build heap in O(n) time • (Doesn’t speed up O(n lg n) performance of heap sort) Sequence AVL Tree Priority Queue • Where else have we seen linear build time for an otherwise logarithmic data structure? Sequence AVL Tree! • Store items of priority queue in Sequence AVL Tree in arbitrary order (insertion order) • Maintain max (and/or min) augmentation: node.max = pointer to node in subtree of node with maximum key – This is a subtree property, so constant factor overhead to maintain • find min() and find max() in O(1) time • delete min() and delete max() in O(log n) time • build(A) in O(n) time • Same bounds as binary heaps (and more) Set vs. Multiset • While our Set interface assumes no duplicate keys, we can use these Sets to implement Multisets that allow items with duplicate keys: – Each item in the Set is a Sequence (e.g., linked list) storing the Multiset items with the same key, which is the key of the Sequence • In fact, without this reduction, binary heaps and AVL trees work directly for duplicate-key items (where e.g. delete max deletes some item of maximum key), taking care to use ≤ constraints (instead of < in Set AVL Trees) MIT OpenCourseWare https://ocw.mit.edu 6.006 Introduction to Algorithms Spring 2020 For information about citing these materials or our Terms of Use, visit: https://ocw.mit.edu/terms Introduction to Algorithms: 6.006 Massachusetts Institute of Technology Instructors: Erik Demaine, Jason Ku, and Justin Solomon Lecture 9: Breadth-First Search Lecture 9: Breadth-First Search New Unit: Graphs! • Quiz 1 next week covers lectures L01 - L08 on Data Structures and Sorting • Today, start new unit, lectures L09 - L14 on Graph Algorithms Graph Applications • Why? Graphs are everywhere! • any network system has direct connection to graphs • e.g., road networks, computer networks, social networks • the state space of any discrete system can be represented by a transition graph • e.g., puzzle & games like Chess, Tetris, Rubik’s cube • e.g., application workﬂows, speciﬁcations Graph Deﬁnitions G1 0 1 2 3 G2 0 1 2 G3 a b s c d e f g • Graph G = (V, E) is a set of vertices V and a set of pairs of vertices E ⊆ V × V . • Directed edges are ordered pairs, e.g., (u, v) for u, v ∈ V • Undirected edges are unordered pairs, e.g., {u, v} for u, v ∈ V i.e., (u, v) and (v, u) • In this class, we assume all graphs are simple: – edges are distinct, e.g., (u, v) only occurs once in E (though (v, u) may appear), and – edges are pairs of distinct vertices, e.g., u =6 v for all (u, v) ∈ E \u0000\u0001 \u0000\u0001 |V | |V | – Simple implies |E| = O(|V |2), since |E| ≤ for undirected, ≤ 2 for directed 2 2 2 Lecture 9: Breadth-First Search Neighbor Sets/Adjacencies • The outgoing neighbor set of u ∈ V is Adj+(u) = {v ∈ V | (u, v) ∈ E} • The incoming neighbor set of u ∈ V is Adj−(u) = {v ∈ V | (v, u) ∈ E} • The out-degree of a vertex u ∈ V is deg+(u) = |Adj+(u)| • The in-degree of a vertex u ∈ V is deg−(u) = |Adj−(u)| • For undirected graphs, Adj−(u) = Adj+(u) and deg−(u) = deg+(u) • Dropping superscript defaults to outgoing, i.e., Adj(u) = Adj+(u) and deg(u) = deg+(u) Graph Representations • To store a graph G = (V, E), we need to store the outgoing edges Adj(u) for all u ∈ V • First, need a Set data structure Adj to map u to Adj(u) • Then for each u, need to store Adj(u) in another data structure called an adjacency list • Common to use direct access array or hash table for Adj, since want lookup fast by vertex • Common to use array or linked list for each Adj(u) since usually only iteration is needed1 • For the common representations, Adj has size Θ(|V |), while each Adj(u) has size Θ(deg(u)) P • Since u∈V deg(u) ≤ 2|E| by handshaking lemma, graph storable in Θ(|V | + |E|) space • Thus, for algorithms on graphs, linear time will mean Θ(|V | + |E|) (linear in size of graph) Examples • Examples 1 and 2 assume vertices are labeled {0, 1, . . . , |V | − 1}, so can use a direct access array for Adj, and store Adj(u) in an array. Example 3 uses a hash table for Adj. Ex 1 (Undirected) | Ex 2 (Directed) | Ex 3 (Undirected) G1 = [ | G2 = [ | G3 = { [2, 1], # 0 | [2], # 0 | a: [s, b], b: [a], [2, 0, 3], # 1 | [2, 0], # 1 | s: [a, c], c: [s, d, e], [1, 3, 0], # 2 | [1], # 2 | d: [c, e, f], e: [c, d, f], [1, 2], # 3 | ] | f: [d, e], g: [], ] | | } • Note that in an undirected graph, connections are symmetric as every edge is outgoing twice 1A hash table for each Adj(u) can allow checking for an edge (u, v) ∈ E in O(1)(e) time 3 Lecture 9: Breadth-First Search Paths • A path is a sequence of vertices p = (v1, v2, . . . , vk) where (vi, vi+1) ∈ E for all 1 ≤ i < k. • A path is simple if it does not repeat vertices2 • The length `(p) of a path p is the number of edges in the path • The distance δ(u, v) from u ∈ V to v ∈ V is the minimum length of any path from u to v, i.e., the length of a shortest path from u to v (by convention, δ(u, v) = ∞ if u is not connected to v) Graph Path Problems • There are many problems you might want to solve concerning paths in a graph: • SINGLE PAIR REACHABILITY(G, s, t): is there a path in G from s ∈ V to t ∈ V ? • SINGLE PAIR SHORTEST PATH(G, s, t): return distance δ(s, t), and a shortest path in G = (V, E) from s ∈ V to t ∈ V • SINGLE SOURCE SHORTEST PATHS(G, s): return δ(s, v) for all v ∈ V , and a shortest-path tree containing a shortest path from s to every v ∈ V (deﬁned below) • Each problem above is at least as hard as every problem above it (i.e., you can use a black-box that solves a lower problem to solve any higher problem) • We won’t show algorithms to solve all of these problems • Instead, show one algorithm that solves the hardest in O(|V | + |E|) time! Shortest Paths Tree • How to return a shortest path from source vertex s for every vertex in graph? • Many paths could have length Ω(|V |), so returning every path could require Ω(|V |2) time • Instead, for all v ∈ V , store its parent P (v): second to last vertex on a shortest path from s • Let P (s) be null (no second to last vertex on shortest path from s to s) • Set of parents comprise a shortest paths tree with O(|V |) size! (i.e., reversed shortest paths back to s from every vertex reachable from s) 2A path in 6.006 is a “walk” in 6.042. A “path” in 6.042 is a simple path in 6.006. 4 Lecture 9: Breadth-First Search Breadth-First Search (BFS) • How to compute δ(s, v) and P (v) for all v ∈ V ? • Store δ(s, v) and P (v) in Set data structures mapping vertices v to distance and parent • (If no path from s to v, do not store v in P and set δ(s, v) to ∞) • Idea! Explore graph nodes in increasing order of distance • Goal: Compute level sets Li = {v | v ∈ V and d(s, v) = i} (i.e., all vertices at distance i) • Claim: Every vertex v ∈ Li must be adjacent to a vertex u ∈ Li−1 (i.e., v ∈ Adj(u)) • Claim: No vertex that is in Lj for some j < i, appears in Li • Invariant: δ(s, v) and P (v) have been computed correctly for all v in any Lj for j < i • Base case (i = 1): L0 = {s}, δ(s, s) = 0, P (s) = None • Inductive Step: To compute Li: – for every vertex u in Li−1: ∗ for every vertex v ∈ Adj(u) that does not appear in any Lj for j < i: · add v to Li, set δ(s, v) = i, and set P (v) = u • Repeatedly compute Li from Lj for j < i for increasing i until Li is the empty set • Set δ(s, v) = ∞ for any v ∈ V for which δ(s, v) was not set • Breadth-ﬁrst search correctly computes all δ(s, v) and P (v) by induction • Running time analysis: – Store each Li in data structure with Θ(|Li|)-time iteration and O(1)-time insertion (i.e., in a dynamic array or linked list) – Checking for a vertex v in any Lj for j < i can be done by checking for v in P – Maintain δ and P in Set data structures supporting dictionary ops in O(1) time (i.e., direct access array or hash table) – Algorithm adds each vertex u to ≤ 1 level and spends O(1) time for each v ∈ Adj(u) P – Work upper bounded by O(1) × deg(u) = O(|E|) by handshake lemma u∈V – Spend Θ(|V |) at end to assign δ(s, v) for vertices v ∈ V not reachable from s – So breadth-ﬁrst search runs in linear time! O(|V | + |E|) • Run breadth-ﬁrst search from s in the graph in Example 3 MIT OpenCourseWare https://ocw.mit.edu 6.006 Introduction to Algorithms Spring 2020 For information about citing these materials or our Terms of Use, visit: https://ocw.mit.edu/terms Introduction to Algorithms: 6.006 Massachusetts Institute of Technology Instructors: Erik Demaine, Jason Ku, and Justin Solomon Lecture 10: Depth-First Search Lecture 10: Depth-First Search Previously • Graph deﬁnitions (directed/undirected, simple, neighbors, degree) • Graph representations (Set mapping vertices to adjacency lists) • Paths and simple paths, path length, distance, shortest path • Graph Path Problems – Single Pair Reachability(G,s,t) – Single Source Reachability(G,s) – Single Pair Shortest Path(G,s,t) – Single Source Shortest Paths(G,s) (SSSP) • Breadth-First Search (BFS) – algorithm that solves Single Source Shortest Paths – with appropriate data structures, runs in O(|V | + |E|) time (linear in input size) Examples G1 a d b e c f G2 a d b e c f 2 Lecture 10: Depth-First Search Depth-First Search (DFS) • Searches a graph from a vertex s, similar to BFS • Solves Single Source Reachability, not SSSP. Useful for solving other problems (later!) • Return (not necessarily shortest) parent tree of parent pointers back to s • Idea! Visit outgoing adjacencies recursively, but never revisit a vertex • i.e., follow any path until you get stuck, backtrack until ﬁnding an unexplored path to explore • P (s) = None, then run visit(s), where • visit(u) : – for every v ∈ Adj(u) that does not appear in P : ∗ set P (v) = u and recursively call visit(v) – (DFS ﬁnishes visiting vertex u, for use later!) • Example: Run DFS on G1 and/or G2 from a Correctness • Claim: DFS visits v and correctly sets P (v) for every vertex v reachable from s • Proof: induct on k, for claim on only vertices within distance k from s – Base case (k = 0): P (s) is set correctly for s and s is visited – Inductive step: Consider vertex v with δ(s, v) = k0 + 1 – Consider vertex u, the second to last vertex on some shortest path from s to v – By induction, since δ(s, u) = k0, DFS visits u and sets P (u) correctly – While visiting u, DFS considers v ∈ Adj(u) – Either v is in P , so has already been visited, or v will be visited while visiting u – In either case, v will be visited by DFS and will be added correctly to P Running Time • Algorithm visits each vertex u at most once and spends O(1) time for each v ∈ Adj(u) P • Work upper bounded by O(1) × deg(u) = O(|E|) u∈V • Unlike BFS, not returning a distance for each vertex, so DFS runs in O(|E|) time 3 Lecture 10: Depth-First Search Full-BFS and Full-DFS • Suppose want to explore entire graph, not just vertices reachable from one vertex • Idea! Repeat a graph search algorithm A on any unvisited vertex • Repeat the following until all vertices have been visited: – Choose an arbitrary unvisited vertex s, use A to explore all vertices reachable from s • We call this algorithm Full-A, speciﬁcally Full-BFS or Full-DFS if A is BFS or DFS • Visits every vertex once, so both Full-BFS and Full-DFS run in O(|V | + |E|) time • Example: Run Full-DFS/Full-BFS on G1 and/or G2 G1 a d b e c f G2 a d b e c f Graph Connectivity • An undirected graph is connected if there is a path connecting every pair of vertices • In a directed graph, vertex u may be reachable from v, but v may not be reachable from u • Connectivity is more complicated for directed graphs (we won’t discuss in this class) • Connectivity(G): is undirected graph G connected? • Connected Components(G): given undirected graph G = (V, E), return partition of V into subsets Vi ⊆ V (connected components) where each Vi is connected in G and there are no edges between vertices from different connected components • Consider a graph algorithm A that solves Single Source Reachability • Claim: A can be used to solve Connected Components • Proof: Run Full-A. For each run of A, put visited vertices in a connected component 4 Lecture 10: Depth-First Search Topological Sort • A Directed Acyclic Graph (DAG) is a directed graph that contains no directed cycle. • A Topological Order of a graph G = (V, E) is an ordering f on the vertices such that: every edge (u, v) ∈ E satisﬁes f(u) < f(v). • Exercise: Prove that a directed graph admits a topological ordering if and only if it is a DAG. • How to ﬁnd a topological order? • A Finishing Order is the order in which a Full-DFS ﬁnishes visiting each vertex in G • Claim: If G = (V, E) is a DAG, the reverse of a ﬁnishing order is a topological order • Proof: Need to prove, for every edge (u, v) ∈ E that u is ordered before v, i.e., the visit to v ﬁnishes before visiting u. Two cases: – If u visited before v: ∗ Before visit to u ﬁnishes, will visit v (via (u, v) or otherwise) ∗ Thus the visit to v ﬁnishes before visiting u – If v visited before u: ∗ u can’t be reached from v since graph is acyclic ∗ Thus the visit to v ﬁnishes before visiting u Cycle Detection • Full-DFS will ﬁnd a topological order if a graph G = (V, E) is acyclic • If reverse ﬁnishing order for Full-DFS is not a topological order, then G must contain a cycle • Check if G is acyclic: for each edge (u, v), check if v is before u in reverse ﬁnishing order • Can be done in O(|E|) time via a hash table or direct access array • To return such a cycle, maintain the set of ancestors along the path back to s in Full-DFS • Claim: If G contains a cycle, Full-DFS will traverse an edge from v to an ancestor of v. • Proof: Consider a cycle (v0, v1, . . . , vk, v0) in G – Without loss of generality, let v0 be the ﬁrst vertex visited by Full-DFS on the cycle – For each vi, before visit to vi ﬁnishes, will visit vi+1 and ﬁnish – Will consider edge (vi, vi+1), and if vi+1 has not been visited, it will be visited now – Thus, before visit to v0 ﬁnishes, will visit vk (for the ﬁrst time, by v0 assumption) – So, before visit to vk ﬁnishes, will consider (vk, v0), where v0 is an ancestor of vk MIT OpenCourseWare https://ocw.mit.edu 6.006 Introduction to Algorithms Spring 2020 For information about citing these materials or our Terms of Use, visit: https://ocw.mit.edu/terms Introduction to Algorithms: 6.006 Massachusetts Institute of Technology Instructors: Erik Demaine, Jason Ku, and Justin Solomon Lecture 11: Weighted Shortest Paths Lecture 11: Weighted Shortest Paths Review • Single-Source Shortest Paths with BFS in O(|V | + |E|) time (return distance per vertex) • Single-Source Reachability with BFS or DFS in O(|E|) time (return only reachable vertices) • Connected components with Full-BFS or Full-DFS in O(|V | + |E|) time • Topological Sort of a DAG with Full-DFS in O(|V | + |E|) time • Previously: distance = number of edges in path Today: generalize meaning of distance Weighted Graphs • A weighted graph is a graph G = (V, E) together with a weight function w : E → Z • i.e., assigns each edge e = (u, v) ∈ E an integer weight: w(e) = w(u, v) • Many applications for edge weights in a graph: – distances in road network – latency in network connections – strength of a relationship in a social network • Two common ways to represent weights computationally: – Inside graph representation: store edge weight with each vertex in adjacency lists – Store separate Set data structure mapping each edge to its weight • We assume a representation that allows querying the weight of an edge in O(1) time Examples G1 G2 a e b f c g d h 6 8 −2 5 9 −5 7 3 2 −1 −4 1 4 a e b f c g d h 6 8 −2 5 9 −5 7 3 2 −1 −4 1 4 2 Restrictions SSSP Algorithm Graph Weights Name Running Time O(·) Lecture General Unweighted BFS |V | + |E| L09 L11 (Today!) L12 L13 DAG Any DAG Relaxation |V | + |E| General Any Bellman-Ford Dijkstra |V | · |E| General Non-negative Lecture 11: Weighted Shortest Paths Weighted Paths • The weight w(π) of a path π in a weighted graph is the sum of weights of edges in the path • The (weighted) shortest path from s ∈ V to t ∈ V is path of minimum weight from s to t • δ(s, t) = inf{w(π) | path π from s to t} is the shortest-path weight from s to t • (Often use “distance” for shortest-path weight in weighted graphs, not number of edges) • As with unweighted graphs: – δ(s, t) = ∞ if no path from s to t – Subpaths of shortest paths are shortest paths (or else could splice in a shorter path) • Why inﬁmum not minimum? Possible that no ﬁnite-length minimum-weight path exists • When? Can occur if there is a negative-weight cycle in the graph, Ex: (b, f, g, c, b) in G1 • A negative-weight cycle is a path π starting and ending at same vertex with w(π) < 0 • δ(s, t) = −∞ if there is a path from s to t through a vertex on a negative-weight cycle • If this occurs, don’t want a shortest path, but may want the negative-weight cycle Weighted Shortest Paths Algorithms • Next four lectures: algorithms to ﬁnd shortest-path weights in weighted graphs • (No parent pointers: can reconstruct shortest paths tree in linear time after. Next page!) • Already know one algorithm: Breadth-First Search! Runs in O(|V | + |E|) time when, e.g.: – graph has positive weights, and all weights are the same – graph has positive weights, and sum of all weights at most O(|V | + |E|) • For general weighted graphs, we don’t know how to solve SSSP in O(|V | + |E|) time • But if your graph is a Directed Acyclic Graph you can! |V | log |V | + |E| 3 Lecture 11: Weighted Shortest Paths Shortest-Paths Tree • For BFS, we kept track of parent pointers during search. Alternatively, compute them after! • If know δ(s, v) for all vertices v ∈ V , can construct shortest-path tree in O(|V | + |E|) time • For weighted shortest paths from s, only need parent pointers for vertices v with ﬁnite δ(s, v) • Initialize empty P and set P (s) = None • For each vertex u ∈ V where δ(s, v) is ﬁnite: – For each outgoing neighbor v ∈ Adj+(u): ∗ If P (v) not assigned and δ(s, v) = δ(s, u) + w(u, v): · There exists a shortest path through edge (u, v), so set P (v) = u • Parent pointers may traverse cycles of zero weight. Mark each vertex in such a cycle. • For each unmarked vertex u ∈ V (including vertices later unmarked): – For each v ∈ Adj+(u) where v is marked and δ(s, v) = δ(s, u) + w(u, v): ∗ Unmark vertices in cycle containing v by traversing parent pointers from v ∗ Set P (v) = u, breaking the cycle • Exercise: Prove this algorithm correctly computes parent pointers in linear time • Because we can compute parent pointers afterward, we focus on computing distances DAG Relaxation • Idea! Maintain a distance estimate d(s, v) (initially ∞) for each vertex v ∈ V , that always upper bounds true distance δ(s, v), then gradually lowers until d(s, v) = δ(s, v) • When do we lower? When an edge violates the triangle inequality! • Triangle Inequality: the shortest-path weight from u to v cannot be greater than the shortest path from u to v through another vertex x, i.e., δ(u, v) ≤ δ(u, x)+ δ(x, v) for all u, v, x ∈ V • If d(s, v) > d(s, u) + w(u, v) for some edge (u, v), then triangle inequality is violated :( • Fix by lowering d(s, v) to d(s, u) + w(u, v), i.e., relax (u, v) to satisfy violated constraint • Claim: Relaxation is safe: maintains that each d(s, v) is weight of a path to v (or ∞) ∀v ∈ V • Proof: Assume d(s, v0) is weight of a path (or ∞) for all v0 ∈ V . Relaxing some edge (u, v) sets d(s, v) to d(s, u) + w(u, v), which is the weight of a path from s to v through u. 4 Lecture 11: Weighted Shortest Paths • Set d(s, v) = ∞ for all v ∈ V , then set d(s, s) = 0 • Process each vertex u in a topological sort order of G: – For each outgoing neighbor v ∈ Adj+(u): ∗ If d(s, v) > d(s, u) + w(u, v): · relax edge (u, v), i.e., set d(s, v) = d(s, u) + w(u, v) • Example: Run DAG Relaxation from vertex a in G2 Correctness • Claim: At end of DAG Relaxation: d(s, v) = δ(s, v) for all v ∈ V • Proof: Induct on k: d(s, v) = δ(s, v) for all v in ﬁrst k vertices in topological order – Base case: Vertex s and every vertex before s in topological order satisﬁes claim at start – Inductive step: Assume claim holds for ﬁrst k0 vertices, let v be the (k0 + 1)th – Consider a shortest path from s to v, and let u be the vertex preceding v on path – u occurs before v in topological order, so d(s, u) = δ(s, u) by induction – When processing u, d(s, v) is set to be no larger (≤) than δ(s, u) + w(u, v) = δ(s, v) – But d(s, v) ≥ δ(s, v), since relaxation is safe, so d(s, v) = δ(s, v) • Alternatively: – For any vertex v, DAG relaxation sets d(s, v) = min{d(s, u)+w(u, v) | u ∈ Adj−(v)} – Shortest path to v must pass through some incoming neighbor u of v – So if d(s, u) = δ(s, u) for all u ∈ Adj−(v) by induction, then d(s, v) = δ(s, v) Running Time • Initialization takes O(|V |) time, and Topological Sort takes O(|V | + |E|) time P • Additional work upper bounded by O(1) × deg+(u) = O(|E|) u∈V • Total running time is linear, O(|V | + |E|) MIT OpenCourseWare https://ocw.mit.edu 6.006 Introduction to Algorithms Spring 2020 For information about citing these materials or our Terms of Use, visit: https://ocw.mit.edu/terms Restrictions SSSP Algorithm Graph Weights Name Running Time O(·) Lecture General Unweighted BFS |V | + |E| L09 L11 L12 (Today!) L13 DAG Any DAG Relaxation |V | + |E| General Any Bellman-Ford Dijkstra |V | · |E| General Non-negative Introduction to Algorithms: 6.006 Massachusetts Institute of Technology Instructors: Erik Demaine, Jason Ku, and Justin Solomon Lecture 12: Bellman-Ford Lecture 12: Bellman-Ford Previously • Weighted graphs, shortest-path weight, negative-weight cycles • Finding shortest-path tree from shortest-path weights in O(|V | + |E|) time • DAG Relaxation: algorithm to solve SSSP on a weighted DAG in O(|V | + |E|) time • SSSP for graph with negative weights – Compute δ(s, v) for all v ∈ V (−∞ if v reachable via negative-weight cycle) – If a negative-weight cycle reachable from s, return one Warmups • Exercise 1: Given undirected graph G, return whether G contains a negative-weight cycle • Solution: Return Yes if there is an edge with negative weight in G in O(|E|) time :O • So for this lecture, we restrict our discussion to directed graphs • Exercise 2: Given SSSP algorithm A that runs in O(|V |(|V | + |E|) time, show how to use it to solve SSSP in O(|V ||E|) time • Solution: Run BFS or DFS to ﬁnd the vertices reachable from s in O(|E|) time – Mark each vertex v not reachable from s with δ(s, v) = ∞ in O(|V |) time – Make graph G0 = (V 0, E0) with only vertices reachable from s in O(|V | + |E|) time – Run A from s in G0 . – G0 is connected, so |V 0| = O(|E0|) = O(|E|) so A runs in O(|V ||E|) time • Today, we will ﬁnd a SSSP algorithm with this running time that works for general graphs! |V | log |V | + |E| 2 Lecture 12: Bellman-Ford Simple Shortest Paths • If graph contains cycles and negative weights, might contain negative-weight cycles :( • If graph does not contain negative-weight cycles, shortest paths are simple! • Claim 1: If δ(s, v) is ﬁnite, there exists a shortest path to v that is simple • Proof: By contradiction: – Suppose no simple shortest path; let π be a shortest path with fewest vertices – π not simple, so exists cycle C in π; C has non-negative weight (or else δ(s, v) = −∞) – Removing C from π forms path π0 with fewer vertices and weight w(π0) ≤ w(π) • Since simple paths cannot repeat vertices, ﬁnite shortest paths contain at most |V | − 1 edges Negative Cycle Witness • k-Edge Distance δk(s, v): the minimum weight of any path from s to v using ≤ k edges • Idea! Compute δ|V |−1(s, v) and δ|V |(s, v) for all v ∈ V – If δ(s, v) =6 −∞, δ(s, v) = δ|V |−1(s, v), since a shortest path is simple (or nonexistent) – If δ|V |(s, v) < δ|V |−1(s, v) ∗ there exists a shorter non-simple path to v, so δ|V |(s, v) = −∞ ∗ call v a (negative cycle) witness – However, there may be vertices with −∞ shortest-path weight that are not witnesses • Claim 2: If δ(s, v) = −∞, then v is reachable from a witness • Proof: Sufﬁces to prove: every negative-weight cycle reachable from s contains a witness – Consider a negative-weight cycle C reachable from s P 0 – For v ∈ C, let v0 ∈ C denote v’s predecessor in C, where v∈C w(v , v) < 0 – Then δ|V |(s, v) ≤ δ|V |−1(s, v0)+w(v0, v) (RHS weight of some path on ≤|V | vertices) P P P P – So δ|V |(s, v) ≤ δ|V |−1(s, v0) + w(v0, v) < δ|V |−1(s, v) v∈C v∈C v∈C v∈C – If C contains no witness, δ|V |(s, v) ≥ δ|V |−1(s, v) for all v ∈ C, a contradiction 3 Lecture 12: Bellman-Ford Bellman-Ford • Idea! Use graph duplication: make multiple copies (or levels) of the graph • |V | + 1 levels: vertex vk in level k represents reaching vertex v from s using ≤ k edges • If edges only increase in level, resulting graph is a DAG! • Construct new DAG G0 = (V 0, E0) from G = (V, E): – G0 has |V |(|V | + 1) vertices vk for all v ∈ V and k ∈{0, . . . , |V |} – G0 has |V |(|V | + |E|) edges: ∗|V | edges (vk−1, vk) for k ∈{1, . . . , |V |} of weight zero for each v ∈ V ∗|V | edges (uk−1, vk) for k ∈{1, . . . , |V |} of weight w(u, v) for each (u, v) ∈ E • Run DAG Relaxation on G0 from s0 to compute δ(s0, vk) for all vk ∈ V 0 • For each vertex: set d(s, v) = δ(s0, v|V |−1) • For each witness u ∈ V where δ(s0, u|V |) < δ(s0, u|V |−1): – For each vertex v reachable from u in G: ∗ set d(s, v) = −∞ Example G G0 a b c d 6 3 −5 −1 −4 a0 b0 d0 c0 a1 b1 d1 c1 a2 b2 d2 c2 a3 b3 d3 c3 a4 b4 d4 c4 −5 6 −4 −1 3 0 0 0 0 δ(a0, vk) k \\ v a b c d 0 0 ∞ ∞ ∞ 1 0 −5 6 ∞ 2 0 −5 −9 9 3 0 −5 −9 −6 4 0 −7 −9 −6 δ(a, v) 0 −∞ −∞ −∞ 4 Lecture 12: Bellman-Ford Correctness • Claim 3: δ(s0, vk) = δk(s, v) for all v ∈ V and k ∈{0, . . . , |V |} • Proof: By induction on k: – Base case: true for all v ∈ V when k = 0 (only v0 reachable from s0 is v = s) – Inductive Step: Assume true for all k < k0, prove for k = k0 δ(s0, vk0 ) = min{δ(s0, uk0−1) + w(uk0−1, vk0 ) | uk0−1 ∈ Adj−(vk0 )} = min{{δ(s0, uk0−1) + w(u, v) | u ∈ Adj−(v)} ∪{δ(s0, vk0−1)}} = min{{δk0−1(s, u) + w(u, v) | u ∈ Adj−(v)} ∪{δk0−1(s, v)}} (by induction) = δk0 (s, v) • Claim 4: At the end of Bellman-Ford d(s, v) = δ(s, v) • Proof: Correctly computes δ|V |−1(s, v) and δ|V |(s, v) for all v ∈ V by Claim 3 – If δ(s, v) =6 −∞, correctly sets d(s, v) = δ|V |−1(s, v) = δ(s, v) – Then sets d(s, v) = −∞ for any v reachable from a witness; correct by Claim 2 Running Time • G0 has size O(|V |(|V | + |E|)) and can be constructed in as much time • Running DAG Relaxation on G0 takes linear time in the size of G0 • Does O(1) work for each vertex reachable from a witness • Finding reachability of a witness takes O(|E|) time, with at most O(|V |) witnesses: O(|V ||E|) • (Alternatively, connect super node x to witnesses via 0-weight edges, linear search from x) • Pruning G at start to only subgraph reachable from s yields O(|V ||E|)-time algorithm Extras: Return Negative-Weight Cycle or Space Optimization • Claim 5: Shortest s0 − v|V | path π for any witness v contains a negative-weight cycle in G • Proof: Since π contains |V | + 1 vertices, must contain at least one cycle C in G – C has negative weight (otherwise, remove C to make path π0 with fewer vertices and w(π0) ≤ w(π), contradicting witness v) • Can use just O(|V |) space by storing only δ(s0, vk−1) and δ(s0, vk) for each k from 1 to |V | • Traditionally, Bellman-Ford stores only one value per vertex, attempting to relax every edge in |V | rounds; but estimates do not correspond to k-Edge Distances, so analysis trickier • But these space optimizations don’t return a negative weight cycle MIT OpenCourseWare https://ocw.mit.edu 6.006 Introduction to Algorithms Spring 2020 For information about citing these materials or our Terms of Use, visit: https://ocw.mit.edu/terms Restrictions SSSP Algorithm Graph Weights Name Running Time O(·) Lecture General Unweighted BFS |V | + |E| L09 L11 L12 L13 (Today!) DAG Any DAG Relaxation |V | + |E| General Any Bellman-Ford Dijkstra |V | · |E| General Non-negative Introduction to Algorithms: 6.006 Massachusetts Institute of Technology Instructors: Erik Demaine, Jason Ku, and Justin Solomon Lecture 13: Dijkstra’s Algorithm Lecture 13: Dijkstra’s Algorithm Review • Single-Source Shortest Paths on weighted graphs • Previously: O(|V | + |E|)-time algorithms for small positive weights or DAGs • Last time: Bellman-Ford, O(|V ||E|)-time algorithm for general graphs with negative weights • Today: faster for general graphs with non-negative edge weights, i.e., for e ∈ E, w(e) ≥ 0 |V | log |V | + |E| Non-negative Edge Weights • Idea! Generalize BFS approach to weighted graphs: – Grow a sphere centered at source s – Repeatedly explore closer vertices before further ones – But how to explore closer vertices if you don’t know distances beforehand? :( • Observation 1: If weights non-negative, monotonic distance increase along shortest paths – i.e., if vertex u appears on a shortest path from s to v, then δ(s, u) ≤ δ(s, v) – Let Vx ⊂ V be the subset of vertices reachable within distance ≤ x from s – If v ∈ Vx, then any shortest path from s to v only contains vertices from Vx – Perhaps grow Vx one vertex at a time! (but growing for every x is slow if weights large) • Observation 2: Can solve SSSP fast if given order of vertices in increasing distance from s – Remove edges that go against this order (since cannot participate in shortest paths) – May still have cycles if zero-weight edges: repeatedly collapse into single vertices – Compute δ(s, v) for each v ∈ V using DAG relaxation in O(|V | + |E|) time 2 Lecture 13: Dijkstra’s Algorithm Dijkstra’s Algorithm • Named for famous Dutch computer scientist Edsger Dijkstra (actually D¨ykstra!) • Idea! Relax edges from each vertex in increasing order of distance from source s • Idea! Efﬁciently ﬁnd next vertex in the order using a data structure • Changeable Priority Queue Q on items with keys and unique IDs, supporting operations: Q.build(X) initialize Q with items in iterator X Q.delete min() remove an item with minimum key Q.decrease key(id, k) ﬁnd stored item with ID id and change key to k • Implement by cross-linking a Priority Queue Q0 and a Dictionary D mapping IDs into Q0 • Assume vertex IDs are integers from 0 to |V | − 1 so can use a direct access array for D • For brevity, say item x is the tuple (x.id, x.key) • Set d(s, v) = ∞ for all v ∈ V , then set d(s, s) = 0 • Build changeable priority queue Q with an item (v, d(s, v)) for each vertex v ∈ V • While Q not empty, delete an item (u, d(s, u)) from Q that has minimum d(s, u) – For vertex v in outgoing adjacencies Adj+(u): ∗ If d(s, v) > d(s, u) + w(u, v): · Relax edge (u, v), i.e., set d(s, v) = d(s, u) + w(u, v) · Decrease the key of v in Q to new estimate d(s, v) • Run Dijkstra on example 3 Lecture 13: Dijkstra’s Algorithm Example Delete v from Q s c d a b δ(s, v) s 0 0 Correctness a ∞ 10 7 7 7 d(s, v) b ∞ ∞ 11 10 9 9 c ∞ 3 3 d 2 G ∞ 10 ∞ 1 5 s a b c d 4 7 5 8 3 2 5 • Claim: At end of Dijkstra’s algorithm, d(s, v) = δ(s, v) for all v ∈ V • Proof: – If relaxation sets d(s, v) to δ(s, v), then d(s, v) = δ(s, v) at the end of the algorithm ∗ Relaxation can only decrease estimates d(s, v) ∗ Relaxation is safe, i.e., maintains that each d(s, v) is weight of a path to v (or ∞) – Sufﬁces to show d(s, v) = δ(s, v) when vertex v is removed from Q ∗ Proof by induction on ﬁrst k vertices removed from Q ∗ Base Case (k = 1): s is ﬁrst vertex removed from Q, and d(s, s) = 0 = δ(s, s) ∗ Inductive Step: Assume true for k < k0, consider k0th vertex v0 removed from Q ∗ Consider some shortest path π from s to v0, with w(π) = δ(s, v0) ∗ Let (x, y) be the ﬁrst edge in π where y is not among ﬁrst k0 − 1 (perhaps y = v0) ∗ When x was removed from Q, d(s, x) = δ(s, x) by induction, so: d(s, y) ≤ δ(s, x) + w(x, y) relaxed edge (x, y) when removed x = δ(s, y) subpaths of shortest paths are shortest paths ≤ δ(s, v 0) non-negative edge weights 0) ≤ d(s, v relaxation is safe ≤ d(s, y) v 0 is vertex with minimum d(s, v 0) in Q ∗ So d(s, v0) = δ(s, v0), as desired 4 Lecture 13: Dijkstra’s Algorithm Running Time • Count operations on changeable priority queue Q, assuming it contains n items: Operation Time Occurrences in Dijkstra Q.build(X) (n = |X|) Q.delete min() Q.decrease key(id, k) Bn Mn Dn 1 |V | |E| • Total running time is O(B|V | + |V | · M|V | + |E| · D|V |) • Assume pruned graph to search only vertices reachable from the source, so |V | = O(|E|) Priority Queue Q0 Q Operations O(·) Dijkstra O(·) n = |V | = O(|E|) on n items build(X) delete min() decrease key(id, k) Array n n 1 |V |2 Binary Heap n log n(a) log n |E| log |V | Fibonacci Heap n log n(a) 1(a) |E| + |V | log |V | • If graph is dense, i.e., |E| = Θ(|V |2), using an Array for Q0 yields O(|V |2) time • If graph is sparse, i.e., |E| = Θ(|V |), using a Binary Heap for Q0 yields O(|V | log |V |) time • A Fibonacci Heap is theoretically good in all cases, but is not used much in practice • We won’t discuss Fibonacci Heaps in 6.006 (see 6.854 or CLRS chapter 19 for details) • You should assume Dijkstra runs in O(|E|+|V | log |V |) time when using in theory problems Summary: Weighted Single-Source Shortest Paths Restrictions SSSP Algorithm Graph Weights Name Running Time O(·) General Unweighted BFS |V | + |E| DAG Any DAG Relaxation |V | + |E| General Non-negative Dijkstra Bellman-Ford |V | log |V | + |E| |V | · |E| General Any • What about All-Pairs Shortest Paths? • Doing a SSSP algorithm |V | times is actually pretty good, since output has size O(|V |2) • Can do better than |V | · O(|V | · |E|) for general graphs with negative weights (next time!) MIT OpenCourseWare https://ocw.mit.edu 6.006 Introduction to Algorithms Spring 2020 For information about citing these materials or our Terms of Use, visit: https://ocw.mit.edu/terms Restrictions SSSP Algorithm Graph Weights Name Running Time O(·) General Unweighted BFS |V | + |E| DAG Any DAG Relaxation |V | + |E| General Non-negative Dijkstra Bellman-Ford |V | log |V | + |E| General Any Introduction to Algorithms: 6.006 Massachusetts Institute of Technology Instructors: Erik Demaine, Jason Ku, and Justin Solomon Lecture 14: Johnson’s Algorithm Lecture 14: Johnson’s Algorithm Previously |V | · |E| All-Pairs Shortest Paths (APSP) • Input: directed graph G = (V, E) with weights w : E → Z • Output: δ(u, v) for all u, v ∈ V , or abort if G contains negative-weight cycle • Useful when understanding whole network, e.g., transportation, circuit layout, supply chains... • Just doing a SSSP algorithm |V | times is actually pretty good, since output has size O(|V |2) – |V | · O(|V | + |E|) with BFS if weights positive and bounded by O(|V | + |E|) – |V | · O(|V | + |E|) with DAG Relaxation if acyclic – |V | · O(|V | log |V | + |E|) with Dijkstra if weights non-negative or graph undirected – |V | · O(|V | · |E|) with Bellman-Ford (general) • Today: Solve APSP in any weighted graph in |V | · O(|V | log |V | + |E|) time 2 Lecture 14: Johnson’s Algorithm Approach • Idea: Make all edge weights non-negative while preserving shortest paths! • i.e., reweight G to G0 with no negative weights, where a shortest path in G is shortest in G0 • If non-negative, then just run Dijkstra |V | times to solve APSP • Claim: Can compute distances in G from distances in G0 in O(|V |(|V | + |E|)) time – Compute shortest-path tree from distances, for each s ∈ V 0 in O(|V | + |E|) time (L11) – Also shortest-paths tree in G, so traverse tree with DFS while also computing distances – Takes O(|V | · (|V | + |E|)) time (which is less time than |V | times Dijkstra) • But how to make G0 with non-negative edge weights? Is this even possible?? • Claim: Not possible if G contains a negative-weight cycle • Proof: Shortest paths are simple if no negative weights, but not if negative-weight cycle • Given graph G with negative weights but no negative-weight cycles, can we make edge weights non-negative while preserving shortest paths? Making Weights Non-negative • Idea! Add negative of smallest weight in G to every edge! All weights non-negative! :) • FAIL: Does not preserve shortest paths! Biases toward paths traversing fewer edges :( • Idea! Given vertex v, add h to all outgoing edges and subtract h from all incoming edges • Claim: Shortest paths are preserved under the above reweighting • Proof: – Weight of every path starting at v changes by h – Weight of every path ending at v changes by −h – Weight of a path passing through v does not change (locally) • This is a very general and useful trick to transform a graph while preserving shortest paths! 3 Lecture 14: Johnson’s Algorithm • Even works with multiple vertices! • Deﬁne a potential function h : V → Z mapping each vertex v ∈ V to a potential h(v) • Make graph G0: same as G but edge (u, v) ∈ E has weight w0(u, v) = w(u, v)+h(u)−h(v) • Claim: Shortest paths in G are also shortest paths in G0 • Proof: Pk – Weight of path π = (v0, . . . , vk) in G is w(π) = i=1 w(vi−1, vi) Pk – Weight of π in G0 is: i=1 w(vi−1, vi) + h(vi−1) − h(vi) = w(π) + h(v0) − h(vk) – (Sum of h’s telescope, since there is a positive and negative h(vi) for each interior i) – Every path from v0 to vk changes by the same amount – So any shortest path will still be shortest Making Weights Non-negative • Can we ﬁnd a potential function such that G0 has no negative edge weights? • i.e., is there an h such that w(u, v) + h(u) − h(v) ≥ 0 for every (u, v) ∈ E? • Re-arrange this condition to h(v) ≤ h(u) + w(u, v), looks like triangle inequality! • Idea! Condition would be satisﬁed if h(v) = δ(s, v) and δ(s, v) is ﬁnite for some s • But graph may be disconnected, so may not exist any such vertex s... :( • Idea! Add a new vertex s with a directed 0-weight edge to every v ∈ V ! :) • δ(s, v) ≤ 0 for all v ∈ V , since path exists a path of weight 0 • Claim: If δ(s, v) = −∞ for any v ∈ V , then the original graph has a negative-weight cycle • Proof: – Adding s does not introduce new cycles (s has no incoming edges) – So if reweighted graph has a negative-weight cycle, so does the original graph • Alternatively, if δ(s, v) is ﬁnite for all v ∈ V : – w0(u, v) = w(u, v) + h(u) − h(v) ≥ 0 for every (u, v) ∈ E by triangle inequality! – New weights in G0 are non-negative while preserving shortest paths! 4 Lecture 14: Johnson’s Algorithm Johnson’s Algorithm • Construct Gx from G by adding vertex x connected to each vertex v ∈ V with 0-weight edge • Compute δx(x, v) for every v ∈ V (using Bellman-Ford) • If δx(x, v) = −∞ for any v ∈ V : – Abort (since there is a negative-weight cycle in G) • Else: – Reweight each edge w0(u, v) = w(u, v) + δx(x, u) − δx(x, v) to form graph G0 – For each u ∈ V : ∗ Compute shortest-path distances δ0(u, v) to all v in G0 (using Dijkstra) ∗ Compute δ(u, v) = δ0(u, v) − δx(x, u) + δx(x, v) for all v ∈ V Correctness • Already proved that transformation from G to G0 preserves shortest paths • Rest reduces to correctness of Bellman-Ford and Dijkstra • Reducing from Signed APSP to Non-negative APSP • Reductions save time! No induction today! :) Running Time • O(|V | + |E|) time to construct Gx • O(|V ||E|) time for Bellman-Ford • O(|V | + |E|) time to construct G0 • O(|V | · (|V | log |V | + |E|)) time for |V | runs of Dijkstra • O(|V |2) time to compute distances in G from distances in G0 • O(|V |2 log |V | + |V ||E|) time in total MIT OpenCourseWare https://ocw.mit.edu 6.006 Introduction to Algorithms Spring 2020 For information about citing these materials or our Terms of Use, visit: https://ocw.mit.edu/terms Introduction to Algorithms: 6.006 Massachusetts Institute of Technology Instructors: Erik Demaine, Jason Ku, and Justin Solomon Lecture 15: Recursive Algorithms Lecture 15: Recursive Algorithms How to Solve an Algorithms Problem (Review) • Reduce to a problem you already know (use data structure or algorithm) Search Data Structures Sort Algorithms Graph Algorithms Array Insertion Sort Breadth First Search Linked List Selection Sort DAG Relaxation (DFS + Topo) Dynamic Array Merge Sort Dijkstra Sorted Array Counting Sort Bellman-Ford Direct-Access Array Radix Sort Johnson Hash Table AVL Sort AVL Tree Heap Sort Binary Heap • Design your own recursive algorithm – Constant-sized program to solve arbitrary input – Need looping or recursion, analyze by induction – Recursive function call: vertex in a graph, directed edge from A → B if B calls A – Dependency graph of recursive calls must be acyclic (if can terminate) – Classify based on shape of graph Class Graph Brute Force Decrease & Conquer Divide & Conquer Dynamic Programming Star Chain Tree DAG Greedy/Incremental Subgraph – Hard part is thinking inductively to construct recurrence on subproblems – How to solve a problem recursively (SRT BOT) 1. Subproblem deﬁnition 2. Relate subproblem solutions recursively 3. Topological order on subproblems (⇒ subproblem DAG) 4. Base cases of relation 5. Original problem solution via subproblem(s) 6. Time analysis 2 Lecture 15: Recursive Algorithms Merge Sort in SRT BOT Framework • Merge sorting an array A of n elements can be expressed in SRT BOT as follows: – Subproblems: S(i, j) = sorted array on elements of A[i : j] for 0 ≤ i ≤ j ≤ n – Relation: S(i, j) = merge(S(i, m), S(m, j)) where m = b(i + j)/2c – Topo. order: Increasing j − i – Base cases: S(i, i + 1) = [A[i]] – Original: S(0, n) – Time: T (n) = 2 T (n/2) + O(n) = O(n lg n) • In this case, subproblem DAG is a tree (divide & conquer) Fibonacci Numbers • Suppose we want to compute the nth Fibonacci number Fn • Subproblems: F (i) = the ith Fibonacci number Fi for i ∈{0, 1, . . . , n} • Relation: F (i) = F (i − 1) + F (i − 2) (deﬁnition of Fibonacci numbers) • Topo. order: Increasing i • Base cases: F (0) = 0, F (1) = 1 • Original prob.: F (n) 1 def fib(n): 2 if n < 2: return n # base case 3 return fib(n - 1) + fib(n - 2) # recurrence • Divide and conquer implies a tree of recursive calls (draw tree) • Time: T (n) = T (n − 1) + T (n − 2) + O(1) > 2T (n − 2), T (n) = Ω(2n/2) exponential... :( • Subproblem F (k) computed more than once! (F (n − k) times) • Can we avoid this waste? 3 Lecture 15: Recursive Algorithms Re-using Subproblem Solutions • Draw subproblem dependencies as a DAG • To solve, either: – Top down: record subproblem solutions in a memo and re-use (recursion + memoization) – Bottom up: solve subproblems in topological sort order (usually via loops) • For Fibonacci, n + 1 subproblems (vertices) and < 2n dependencies (edges) • Time to compute is then O(n) additions 1 # recursive solution (top down) 2 def fib(n): 3 memo = {} 4 def F(i): 5 if i < 2: return i # base cases 6 if i not in memo: # check memo 7 memo[i] = F(i - 1) + F(i - 2) # relation 8 return memo[i] 9 return F(n) # original 1 # iterative solution (bottom up) 2 def fib(n): 3 F = {} 4 F[0], F[1] = 0, 1 # base cases 5 for i in range(2, n + 1): # topological order 6 F[i] = F[i - 1] + F[i - 2] # relation 7 return F[n] # original • A subtlety is that Fibonacci numbers grow to Θ(n) bits long, potentially ≫ word size w • Each addition costs O(dn/we) time • So total cost is O(ndn/we) = O(n + n2/w) time 4 Lecture 15: Recursive Algorithms Dynamic Programming • Weird name coined by Richard Bellman – Wanted government funding, needed cool name to disguise doing mathematics! – Updating (dynamic) a plan or schedule (program) • Existence of recursive solution implies decomposable subproblems1 • Recursive algorithm implies a graph of computation • Dynamic programming if subproblem dependencies overlap (DAG, in-degree > 1) • “Recurse but re-use” (Top down: record and lookup subproblem solutions) • “Careful brute force” (Bottom up: do each subproblem in order) • Often useful for counting/optimization problems: almost trivially correct recurrences How to Solve a Problem Recursively (SRT BOT) 1. Subproblem deﬁnition subproblem x ∈ X • Describe the meaning of a subproblem in words, in terms of parameters • Often subsets of input: preﬁxes, sufﬁxes, contiguous substrings of a sequence • Often record partial state: add subproblems by incrementing some auxiliary variables 2. Relate subproblem solutions recursively x(i) = f(x(j), . . .) for one or more j < i 3. Topological order to argue relation is acyclic and subproblems form a DAG 4. Base cases • State solutions for all (reachable) independent subproblems where relation breaks down 5. Original problem • Show how to compute solution to original problem from solutions to subproblem(s) • Possibly use parent pointers to recover actual solution, not just objective function 6. Time analysis P • work(x), or if work(x) = O(W ) for all x ∈ X, then |X| · O(W ) x∈X • work(x) measures nonrecursive work in relation; treat recursions as taking O(1) time 1This property often called optimal substructure. It is a property of recursion, not just dynamic programming 5 Lecture 15: Recursive Algorithms DAG Shortest Paths • Recall the DAG SSSP problem: given a DAG G and vertex s, compute δ(s, v) for all v ∈ V • Subproblems: δ(s, v) for all v ∈ V • Relation: δ(s, v) = min{δ(s, u) + w(u, v) | u ∈ Adj−(v)} ∪ {∞} • Topo. order: Topological order of G • Base cases: δ(s, s) = 0 • Original: All subproblems P • Time: O(1 + | Adj−(v)|) = O(|V | + |E|) v∈V • DAG Relaxation computes the same min values as this dynamic program, just – step-by-step (if new value < min, update min via edge relaxation), and – from the perspective of u and Adj+(u) instead of v and Adj−(v) Bowling • Given n pins labeled 0, 1, . . . , n − 1 • Pin i has value vi • Ball of size similar to pin can hit either – 1 pin i, in which case we get vi points – 2 adjacent pins i and i + 1, in which case we get vi · vi+1 points • Once a pin is hit, it can’t be hit again (removed) • Problem: Throw zero or more balls to maximize total points • Example: [ −1, 1 , 1 , 1 , 9, 9 , 3 , −3, −5 , 2, 2 ] 6 Lecture 15: Recursive Algorithms Bowling Algorithms • Let’s start with a more familiar divide-and-conquer algorithm: – Subproblems: B(i, j) = maximum score starting with just pins i, i + 1, . . . , j − 1, for 0 ≤ i ≤ j ≤ n – Relation: ∗ m = b(i + j)/2c ∗ Either hit m and m + 1 together, or don’t ∗ B(i, j) = max{vm · vm+1 + B(i, m) + B(m + 2, j), B(i, m + 1) + B(m + 1, j)} – Topo. order: Increasing j − i – Base cases: B(i, i) = 0, B(i, i + 1) = max{vi, 0} – Original: B(0, n) – Time: T (n) = 4 T (n/2) + O(1) = O(n2) • This algorithm works but isn’t very fast, and doesn’t generalize well (e.g., to allow for a bigger ball that hits three balls at once) • Dynamic programming algorithm: use sufﬁxes – Subproblems: B(i) = maximum score starting with just pins i, i + 1, . . . , n − 1, for 0 ≤ i ≤ n – Relation: ∗ Locally brute-force what could happen with ﬁrst pin (original pin i): skip pin, hit one pin, hit two pins ∗ Reduce to smaller sufﬁx and recurse, either B(i + 1) or B(i + 2) ∗ B(i) = max{B(i + 1), vi + B(i + 1), vi · vi+1 + B(i + 2)} – Topo. order: Decreasing i (for i = n, n − 1, . . . , 0) – Base cases: B(n) = B(n + 1) = 0 – Original: B(0) – Time: (assuming memoization) ∗ Θ(n) subproblems · Θ(1) work in each ∗ Θ(n) total time • Fast and easy to generalize! • Equivalent to maximum-weight path in Subproblem DAG: B0 B1 B2 B3 · · · Bn max{v0, 0} max{v1, 0} max{v2, 0} v0 · v1 v1 · v2 v2 · v3 7 Lecture 15: Recursive Algorithms Bowling Code • Converting a SRT BOT speciﬁcation into code is automatic/straightforward • Here’s the result for the Bowling Dynamic Program above: 1 # recursive solution (top down) 2 def bowl(v): 3 memo = {} 4 def B(i): 5 if i >= len(v): return 0 # base cases 6 if i not in memo: # check memo 7 memo[i] = max(B(i+1), # relation: skip pin i 8 v[i] + B(i+1), # OR bowl pin i separately 9 v[i] * v[i+1] + B(i+2)) # OR bowl pins i and i+1 together 10 return memo[i] 11 return B(0) # original 1 # iterative solution (bottom up) 2 def bowl(v): 3 B = {} 4 B[len(v)] = 0 # base cases 5 B[len(v)+1] = 0 6 for i in reversed(range(len(v))): # topological order 7 B[i] = max(B[i+1], # relation: skip pin i 8 v[i] + B(i+1), # OR bowl pin i separately 9 v[i] * v[i+1] + B(i+2)) # OR bowl pins i and i+1 together 10 return B[0] # original How to Relate Subproblem Solutions • The general approach we’re following to deﬁne a relation on subproblem solutions: – Identify a question about a subproblem solution that, if you knew the answer to, would reduce to “smaller” subproblem(s) ∗ In case of bowling, the question is “how do we bowl the ﬁrst couple of pins?” – Then locally brute-force the question by trying all possible answers, and taking the best ∗ In case of bowling, we take the max because the problem asks to maximize – Alternatively, we can think of correctly guessing the answer to the question, and di- rectly recursing; but then we actually check all possible guesses, and return the “best” • The key for efﬁciency is for the question to have a small (polynomial) number of possible answers, so brute forcing is not too expensive • Often (but not always) the nonrecursive work to compute the relation is equal to the number of answers we’re trying MIT OpenCourseWare https://ocw.mit.edu 6.006 Introduction to Algorithms Spring 2020 For information about citing these materials or our Terms of Use, visit: https://ocw.mit.edu/terms Introduction to Algorithms: 6.006 Massachusetts Institute of Technology Instructors: Erik Demaine, Jason Ku, and Justin Solomon Lecture 16: Dyn. Prog. Subproblems Lecture 16: Dyn. Prog. Subproblems Dynamic Programming Review • Recursion where subproblem dependencies overlap, forming DAG • “Recurse but re-use” (Top down: record and lookup subproblem solutions) • “Careful brute force” (Bottom up: do each subproblem in order) Dynamic Programming Steps (SRT BOT) 1. Subproblem deﬁnition subproblem x 2 X • Describe the meaning of a subproblem in words, in terms of parameters • Often subsets of input: preﬁxes, sufﬁxes, contiguous substrings of a sequence • Often multiply possible subsets across multiple inputs • Often record partial state: add subproblems by incrementing some auxiliary variables 2. Relate subproblem solutions recursively x(i) = f(x(j), . . .) for one or more j < i • Identify a question about a subproblem solution that, if you knew the answer to, reduces the subproblem to smaller subproblem(s) • Locally brute-force all possible answers to the question 3. Topological order to argue relation is acyclic and subproblems form a DAG 4. Base cases • State solutions for all (reachable) independent subproblems where relation breaks down 5. Original problem • Show how to compute solution to original problem from solutions to subproblem(s) • Possibly use parent pointers to recover actual solution, not just objective function 6. Time analysis P • x2X work(x), or if work(x) = O(W) for all x 2 X, then |X| · O(W) • work(x) measures nonrecursive work in relation; treat recursions as taking O(1) time 2 Lecture 16: Dyn. Prog. Subproblems Longest Common Subsequence (LCS) • Given two strings A and B, ﬁnd a longest (not necessarily contiguous) subsequence of A that is also a subsequence of B. • Example: A = hieroglyphology, B = michaelangelo • Solution: hello or heglo or iello or ieglo, all length 5 • Maximization problem on length of subsequence 1. Subproblems • x(i, j) = length of longest common subsequence of sufﬁxes A[i :] and B[j :] • For 0  i  |A| and 0  j  |B| 2. Relate • Either ﬁrst characters match or they don’t • If ﬁrst characters match, some longest common subsequence will use them • (if no LCS uses ﬁrst matched pair, using it will only improve solution) • (if an LCS uses ﬁrst in A[i] and not ﬁrst in B[j], matching B[j] is also optimal) • If they do not match, they cannot both be in a longest common subsequence • Guess whether A[i] or B[j] is not in LCS ⇢ x(i + 1, j + 1) + 1 if A[i] = B[j] • x(i, j) = max{x(i + 1, j), x(i, j + 1)} otherwise • (draw subset of all rectangular grid dependencies) 3. Topological order • Subproblems x(i, j) depend only on strictly larger i or j or both • Simplest order to state: Decreasing i + j • Nice order for bottom-up code: Decreasing i, then decreasing j 4. Base • x(i, |B|) = x(|A|, j) = 0 (one string is empty) 5. Original problem • Length of longest common subsequence of A and B is x(0, 0) • Store parent pointers to reconstruct subsequence • If the parent pointer increases both indices, add that character to LCS 3 Lecture 16: Dyn. Prog. Subproblems 6. Time • # subproblems: (|A| + 1) · (|B| + 1) • work per subproblem: O(1) • O(|A| · |B|) running time 1 2 3 4 5 6 7 8 9 10 def lcs(A, B): a, b = len(A), len(B) x = [[0] * (b + 1) for _ in range(a + 1)] for i in reversed(range(a)): for j in reversed(range(b)): if A[i] == B[j]: x[i][j] = x[i + 1][j + 1] + 1 else: x[i][j] = max(x[i + 1][j], x[i][j return x[0][0] + 1]) 4 Lecture 16: Dyn. Prog. Subproblems Longest Increasing Subsequence (LIS) • Given a string A, ﬁnd a longest (not necessarily contiguous) subsequence of A that strictly increases (lexicographically). • Example: A = carbohydrate • Solution: abort, of length 5 • Maximization problem on length of subsequence • Attempted solution: – Natural subproblems are preﬁxes or sufﬁxes of A, say sufﬁx A[i :] – Natural question about LIS of A[i :]: is A[i] in the LIS? (2 possible answers) – But then how do we recurse on A[i + 1 :] and guarantee increasing subsequence? – Fix: add constraint to subproblems to give enough structure to achieve increasing property 1. Subproblems • x(i) = length of longest increasing subsequence of sufﬁx A[i :] that includes A[i] • For 0  i  |A| 2. Relate • We’re told that A[i] is in LIS (ﬁrst element) • Next question: what is the second element of LIS? – Could be any A[j] where j > i and A[j] > A[i] (so increasing) – Or A[i] might be the last element of LIS • x(i) = max{1 + x(j) | i < j < |A|, A[j] > A[i]} [ {1} 3. Topological order • Decreasing i 4. Base • No base case necessary, because we consider the possibility that A[i] is last 5. Original problem • What is the ﬁrst element of LIS? Guess! • Length of LIS of A is max{x(i) | 0  i < |A|} • Store parent pointers to reconstruct subsequence 5 Lecture 16: Dyn. Prog. Subproblems 6. Time • # subproblems: |A| • work per subproblem: O(|A|) • O(|A|2) running time • Exercise: speed up to O(|A| log |A|) by doing only O(log |A|) work per subproblem, via AVL tree augmentation 1 def lis(A): 2 a = len(A) 3 x = [1] * a 4 for i in reversed(range(a)): 5 for j in range(i, a): 6 if A[j] > A[i]: 7 x[i] = max(x[i], 1 + x[j]) 8 return max(x) − − − − − − − − 6 Lecture 16: Dyn. Prog. Subproblems Alternating Coin Game • Given sequence of n coins of value v0, v1, . . . , vn 1 • Two players (“me” and “you”) take turns • In a turn, take ﬁrst or last coin among remaining coins • My goal is to maximize total value of my taken coins, where I go ﬁrst • First solution exploits that this is a zero-sum game: I take all coins you don’t 1. Subproblems • Choose subproblems that correspond to the state of the game • For every contiguous subsequence of coins from i to j, 0  i  j < n • x(i, j) = maximum total value I can take starting from coins of values vi, . . . , vj 2. Relate • I must choose either coin i or coin j (Guess!) • Then it’s your turn, so you’ll get value x(i + 1, j) or x(i, j 1), respectively • To ﬁgure out how much value I get, subtract this from total coin values Pj Pj 1 • x(i, j) = max{vi + vk x(i + 1, j), vj + vk x(i, j 1)} k=i+1 k=i 3. Topological order • Increasing j i 4. Base • x(i, i) = vi 5. Original problem • x(0, n 1) • Store parent pointers to reconstruct strategy 6. Time • # subproblems: ⇥(n2) • work per subproblem: ⇥(n) to compute sums • ⇥(n3) running time Pj • Exercise: speed up to ⇥(n2) time by precomputing all sums k=i vk in ⇥(n2) time, via dynamic programming (!) − − − − 7 Lecture 16: Dyn. Prog. Subproblems • Second solution uses subproblem expansion: add subproblems for when you move next 1. Subproblems • Choose subproblems that correspond to the full state of the game • Contiguous subsequence of coins from i to j, and which player p goes next • x(i, j, p) = maximum total value I can take when player p 2 {me, you} starts from coins of values vi, . . . , vj 2. Relate • Player p must choose either coin i or coin j (Guess!) • If p = me, then I get the value; otherwise, I get nothing • Then it’s the other player’s turn • x(i, j, me) = max{vi + x(i + 1, j, you), vj + x(i, j 1, you)} • x(i, j, you) = min{x(i + 1, j, me), x(i, j 1, me)} 3. Topological order • Increasing j i 4. Base • x(i, i, me) = vi • x(i, i, you) = 0 5. Original problem • x(0, n 1, me) • Store parent pointers to reconstruct strategy 6. Time • # subproblems: ⇥(n2) • work per subproblem: ⇥(1) • ⇥(n2) running time 8 Lecture 16: Dyn. Prog. Subproblems Subproblem Constraints and Expansion • We’ve now seen two examples of constraining or expanding subproblems • If you ﬁnd yourself lacking information to check the desired conditions of the problem, or lack the natural subproblem to recurse on, try subproblem constraint/expansion! • More subproblems and constraints give the relation more to work with, so can make DP more feasible • Usually a trade-off between number of subproblems and branching/complexity of relation • More examples next lecture MIT OpenCourseWare https://ocw.mit.edu 6.006 Introduction to Algorithms Spring 2020 For information about citing these materials or our Terms of Use, visit: https://ocw.mit.edu/terms Introduction to Algorithms: 6.006 Massachusetts Institute of Technology Instructors: Erik Demaine, Jason Ku, and Justin Solomon Lecture 17: Dyn. Prog. III Lecture 17: Dyn. Prog. III Dynamic Programming Steps (SRT BOT) 1. Subproblem deﬁnition subproblem x ∈ X • Describe the meaning of a subproblem in words, in terms of parameters • Often subsets of input: preﬁxes, sufﬁxes, contiguous substrings of a sequence • Often multiply possible subsets across multiple inputs • Often record partial state: add subproblems by incrementing some auxiliary variables 2. Relate subproblem solutions recursively x(i) = f(x(j), . . .) for one or more j < i • Identify a question about a subproblem solution that, if you knew the answer to, reduces the subproblem to smaller subproblem(s) • Locally brute-force all possible answers to the question 3. Topological order to argue relation is acyclic and subproblems form a DAG 4. Base cases • State solutions for all (reachable) independent subproblems where relation breaks down 5. Original problem • Show how to compute solution to original problem from solutions to subproblem(s) • Possibly use parent pointers to recover actual solution, not just objective function 6. Time analysis P • work(x), or if work(x) = O(W ) for all x ∈ X, then |X| · O(W ) x∈X • work(x) measures nonrecursive work in relation; treat recursions as taking O(1) time Recall: DAG Shortest Paths [L15] • Subproblems: δ(s, v) for all v ∈ V • Relation: δ(s, v) = min{δ(s, u) + w(u, v) | u ∈ Adj−(v)} ∪ {∞} • Topo. order: Topological order of G 2 Lecture 17: Dyn. Prog. III Single-Source Shortest Paths Revisited 1. Subproblems • Expand subproblems to add information to make acyclic! (an example we’ve already seen of subproblem expansion) • δk(s, v) = weight of shortest path from s to v using at most k edges • For v ∈ V and 0 ≤ k ≤|V | 2. Relate • Guess last edge (u, v) on shortest path from s to v • δk(s, v) = min{δk−1(s, u) + w(u, v) | (u, v) ∈ E} ∪{δk−1(s, v)} 3. Topological order • Increasing k: subproblems depend on subproblems only with strictly smaller k 4. Base • δ0(s, s) = 0 and δ0(s, v) = ∞ for v 6= s (no edges) • (draw subproblem graph) 5. Original problem • If has ﬁnite shortest path, then δ(s, v) = δ|V |−1(s, v) • Otherwise some δ|V |(s, v) < δ|V |−1(s, v), so path contains a negative-weight cycle • Can keep track of parent pointers to subproblem that minimized recurrence 6. Time • # subproblems: |V | × (|V | + 1) • Work for subproblem δk(s, v): O(degin(v)) |V | |V | X X X O(degin(v)) = O(|E|) = O(|V | · |E|) k=0 v∈V k=0 This is just Bellman-Ford! (computed in a slightly different order) 3 Lecture 17: Dyn. Prog. III All-Pairs Shortest Paths: Floyd–Warshall • Could deﬁne subproblems δk(u, v) = minimum weight of path from u to v using at most k edges, as in Bellman–Ford • Resulting running time is |V | times Bellman–Ford, i.e., O(|V |2 · |E|) = O(|V |4) • Know a better algorithm from L14: Johnson achieves O(|V |2 log |V | + |V | · |E|) = O(|V |3) • Can achieve Θ(|V |3) running time (matching Johnson for dense graphs) with a simple dy- namic program, called Floyd–Warshall • Number vertices so that V = {1, 2, . . . , |V |} 1. Subproblems • d(u, v, k) = minimum weight of a path from u to v that only uses vertices from {1, 2, . . . , k} ∪{u, v} • For u, v ∈ V and 1 ≤ k ≤|V | 2. Relate • x(u, v, k) = min{x(u, k, k − 1) + x(k, v, k − 1), x(u, v, k − 1)} • Only constant branching! No longer guessing previous vertex/edge 3. Topological order • Increasing k: relation depends only on smaller k 4. Base • x(u, u, 0) = 0 • x(u, v, 0) = w(u, v) if (u, v) ∈ E • x(u, v, 0) = ∞ if none of the above 5. Original problem • x(u, v, |V |) for all u, v ∈ V 6. Time • O(|V |3) subproblems • Each O(1) work • O(|V |3) in total • Constant number of dependencies per subproblem brings the factor of O(|E|) in the running time down to O(|V |). 4 Lecture 17: Dyn. Prog. III Arithmetic Parenthesization • Input: arithmetic expression a0 ∗ 1 a1 ∗ 2 a2 · · · ∗ n−1 an−1 where each ai is an integer and each ∗ i ∈{+, ×} • Output: Where to place parentheses to maximize the evaluated expression • Example: 7 + 4 × 3 + 5 → ((7) + (4)) × ((3) + (5)) = 88 • Allow negative integers! • Example: 7 + (−4) × 3 + (−5) → ((7) + ((−4) × ((3) + (−5)))) = 15 1. Subproblems • Sufﬁcient to maximize each subarray? No! (−3) × (−3) = 9 > (−2) × (−2) = 4 • x(i, j, opt) = opt value obtainable by parenthesizing ai ∗ i+1 · · · ∗ j−1 aj−1 • For 0 ≤ i < j ≤ n and opt ∈{min, max} 2. Relate • Guess location of outermost parentheses / last operation evaluated • x(i, j, opt) = opt {x(i, k, opt0) ∗ k x(k, j, opt00)) | i < k < j; opt0 , opt00 ∈{min, max}} 3. Topological order • Increasing j − i: subproblem x(i, j, opt) depends only on strictly smaller j − i 4. Base • x(i, i + 1, opt) = ai, only one number, no operations left! 5. Original problem • X(0, n, max) • Store parent pointers (two!) to ﬁnd parenthesization (forms binary tree!) 6. Time 2) • # subproblems: less than n · n · 2 = O(n • work per subproblem O(n) · 2 · 2 = O(n) • O(n3) running time 5 Lecture 17: Dyn. Prog. III Piano Fingering • Given sequence t0, t1, . . . , tn−1 of n single notes to play with right hand (will generalize to multiple notes and hands later) • Performer has right-hand ﬁngers 1, 2, . . . , F (F = 5 for most humans) • Given metric d(t, f, t0, f 0) of difﬁculty of transitioning from note t with ﬁnger f to note t0 with ﬁnger f 0 – Typically a sum of penalties for various difﬁculties, e.g.: – 1 < f < f 0 and t > t0 is uncomfortable – Legato (smooth) play requires t =6 t0 (else inﬁnite penalty) – Weak-ﬁnger rule: prefer to avoid f 0 ∈{4, 5} – {f, f 0} = {3, 4} is annoying • Goal: Assign ﬁngers to notes to minimize total difﬁculty • First attempt: 1. Subproblems • x(i) = minimum total difﬁculty for playing notes ti, ti+1, . . . , tn−1 2. Relate • Guess ﬁrst ﬁnger: assignment f for ti • x(i) = min{x(i + 1) + d(ti, f, ti+1, ?) | 1 ≤ f ≤ F } • Not enough information to ﬁll in ? • Need to know which ﬁnger at the start of x(i + 1) • But different starting ﬁngers could hurt/help both x(i + 1) and d(ti, f, ti+1, ?) • Need a table mapping start ﬁngers to optimal solutions for x(i + 1) • I.e., need to expand subproblems with start condition 6 Lecture 17: Dyn. Prog. III • Solution: 1. Subproblems • x(i, f) = minimum total difﬁculty for playing notes ti, ti+1, . . . , tn−1 starting with ﬁn- ger f on note ti • For 0 ≤ i < n and 1 ≤ f ≤ F 2. Relate • Guess next ﬁnger: assignment f 0 for ti+1 • x(i, f) = min{x(i + 1, f 0) + d(ti, f, ti+1, f 0) | 1 ≤ f 0 ≤ F } 3. Topological order • Decreasing i (any f order) 4. Base • x(n − 1, f) = 0 (no transitions) 5. Original problem • min{x(0, f) | 1 ≤ f ≤ F } 6. Time • Θ(n · F ) subproblems • Θ(F ) work per subproblem • Θ(n · F 2) • No dependence on the number of different notes! 7 Lecture 17: Dyn. Prog. III Guitar Fingering • Up to S = number of strings different ways to play the same note • Redeﬁne “ﬁnger” to be tuple (ﬁnger playing note, string playing note) • Throughout algorithm, F gets replaced by F · S • Running time is thus Θ(n · F 2 · S2) Multiple Notes at Once • Now suppose ti is a set of notes to play at time i • Given a bigger transition difﬁculty function d(t, f, t0, f 0) • Goal: ﬁngering fi : ti →{1, 2, . . . , F } specifying how to ﬁnger each note (including which P n−1 string for guitar) to minimize i=1 d(ti−1, fi−1, ti, fi) • At most T F choices for each ﬁngering fi, where T = maxi |ti| – T ≤ F = 10 for normal piano (but there are exceptions) – T ≤ S for guitar • Θ(n · T F ) subproblems • Θ(T F ) work per subproblem • Θ(n · T 2F ) time • Θ(n) time for T, F ≤ 10 Video Game Appliactions • Guitar Hero / Rock Band – F = 4 (and 5 different notes) • Dance Dance Revolution – F = 2 feet – T = 2 (at most two notes at once) – Exercise: handle sustained notes, using “where each foot is” (on an arrow or in the middle) as added state for sufﬁx subproblems MIT OpenCourseWare https://ocw.mit.edu 6.006 Introduction to Algorithms Spring 2020 For information about citing these materials or our Terms of Use, visit: https://ocw.mit.edu/terms Introduction to Algorithms: 6.006 Massachusetts Institute of Technology Instructors: Erik Demaine, Jason Ku, and Justin Solomon Lecture 18: Pseudopolynomial Lecture 18: Pseudopolynomial Dynamic Programming Steps (SRT BOT) 1. Subproblem deﬁnition subproblem x 2 X • Describe the meaning of a subproblem in words, in terms of parameters • Often subsets of input: preﬁxes, sufﬁxes, contiguous substrings of a sequence • Often multiply possible subsets across multiple inputs • Often record partial state: add subproblems by incrementing some auxiliary variables • Often smaller integers than a given integer (today’s focus) 2. Relate subproblem solutions recursively x(i) = f(x(j), . . .) for one or more j < i • Identify a question about a subproblem solution that, if you knew the answer to, reduces the subproblem to smaller subproblem(s) • Locally brute-force all possible answers to the question 3. Topological order to argue relation is acyclic and subproblems form a DAG 4. Base cases • State solutions for all (reachable) independent subproblems where relation breaks down 5. Original problem • Show how to compute solution to original problem from solutions to subproblem(s) • Possibly use parent pointers to recover actual solution, not just objective function 6. Time analysis P • x2X work(x), or if work(x) = O(W) for all x 2 X, then |X| · O(W) • work(x) measures nonrecursive work in relation; treat recursions as taking O(1) time − − 2 Lecture 18: Pseudopolynomial Rod Cutting • Given a rod of length L and value v(`) of rod of length ` for all ` 2 {1, 2, . . . , L} • Goal: Cut the rod to maximize the value of cut rod pieces • Example: L = 7, v = [0, 1, 10, 13, 18, 20, 31, 32] ` = 0 1 2 3 4 5 6 7 • Maybe greedily take most valuable per unit length? • Nope! arg max` v[`]/` = 6, and partitioning [6, 1] yields 32 which is not optimal! • Solution: v[2] + v[2] + v[3] = 10 + 10 + 13 = 33 • Maximization problem on value of partition 1. Subproblems • x(`): maximum value obtainable by cutting rod of length ` • For ` 2 {0, 1, . . . , L} 2. Relate • First piece has some length p (Guess!) • x(`) = max{v(p) + x(` p) | p 2 {1, . . . , `}} • (draw dependency graph) 3. Topological order • Increasing `: Subproblems x(`) depend only on strictly smaller `, so acyclic 4. Base • x(0) = 0 (length-zero rod has no value!) 5. Original problem • Maximum value obtainable by cutting rod of length L is x(L) • Store choices to reconstruct cuts • If current rod length ` and optimal choice is `0, remainder is piece p = ` `0 • (maximum-weight path in subproblem DAG!) 6. Time • # subproblems: L + 1 • work per subproblem: O(`) = O(L) • O(L2) running time 3 Lecture 18: Pseudopolynomial Is This Polynomial Time? • (Strongly) polynomial time means that the running time is bounded above by a constant- degree polynomial in the input size measured in words • In Rod Cutting, input size is L + 1 words (one integer L and L integers in v) • O(L2) is a constant-degree polynomial in L + 1, so YES: (strongly) polynomial time 1 # recursive 2 x = {} 3 def cut_rod(l, v): 4 if l < 1: return 0 # base case 5 if l not in x: # check memo 6 for piece in range(1, l + 1): # try piece 7 x_ = v[piece] + cut_rod(l - piece, v) # recurrence 8 if (l not in x) or (x[l] < x_): # update memo 9 x[l] = x_ 10 return x[l] 1 # iterative 2 def cut_rod(L, v): 3 x = [0] * (L + 1) # base case 4 for l in range(L + 1): # topological order 5 for piece in range(1, l + 1): # try piece 6 x_ = v[piece] + x[l - piece] # recurrence 7 if x[l] < x_: # update memo 8 x[l] = x_ 9 return x[L] 1 # iterative with parent pointers 2 def cut_rod_pieces(L, v): 3 x = [0] * (L + 1) # base case 4 parent = [None] * (L + 1) # parent pointers 5 for l in range(1, L + 1): # topological order 6 for piece in range(1, l + 1): # try piece 7 x_ = v[piece] + x[l - piece] # recurrence 8 if x[l] < x_: # update memo 9 x[l] = x_ 10 parent[l] = l - piece # update parent 11 l, pieces = L, [] 12 while parent[l] is not None: # walk back through parents 13 piece = l - parent[l] 14 pieces.append(piece) 15 l = parent[l] 16 return pieces − ≥ − ≥ 4 Lecture 18: Pseudopolynomial Subset Sum • Input: Sequence of n positive integers A = {a0, a1, . . . , an} P • Output: Is there a subset of A that sums exactly to T? (i.e., 9A0 ✓ A s.t. = T?) a2A0 a • Example: A = (1, 3, 4, 12, 19, 21, 22), T = 47 allows A0 = {3, 4, 19, 21} • Optimization problem? Decision problem! Answer is YES or NO, TRUE or FALSE • In example, answer is YES. However, answer is NO for some T, e.g., 2, 6, 9, 10, 11, . . . 1. Subproblems • x(i, t) = does any subset of A[i :] sum to t? • For i 2 {0, 1, . . . , n}, t 2 {0, 1, . . . , T} 2. Relate • Idea: Is ﬁrst item ai in a valid subset A0? (Guess!) • If yes, then try to sum to t ai 0 using remaining items • If no, then try to sum to t using remaining items ⇢ x(i + 1, t A[i]) if t A[i] • x(i, t) = OR x(i + 1, t) always 3. Topological order • Subproblems x(i, t) only depend on strictly larger i, so acyclic • Solve in order of decreasing i 4. Base • x(i, 0) = YES for i 2 {0, . . . , n} (space packed exactly!) • x(0, t) = NO for j 2 {1, . . . , T} (no more items available to pack) 5. Original problem • Original problem given by x(0, T) • Example: A = (3, 4, 3, 1), T = 6 solution: A0 = (3, 3) • Bottom up: Solve all subproblems (Example has 35) 5 Lecture 18: Pseudopolynomial • Top down: Solve only reachable subproblems (Example, only 14!) 6. Time • # subproblems: O(nT ), O(1) work per subproblem, O(nT ) time ≥ ≥ 6 Lecture 18: Pseudopolynomial Is This Polynomial? • Input size is n + 1: one integer T and n integers in A • Is O(nT ) bounded above by a polynomial in n + 1? NO, not necessarily • On w-bit word RAM, T  2w and w lg(n + 1), but we don’t have an upper bound on w • E.g., w = n is not unreasonable, but then running time is O(n2n), which is exponential Pseudopolynomial • Algorithm has pseudopolynomial time: running time is bounded above by a constant- degree polynomial in input size and input integers • Such algorithms are polynomial in the case that integers are polynomially bounded in input size, i.e., nO(1) (same case that Radix Sort runs in O(n) time) • Counting sort O(n + u), radix sort O(n logn u), direct-access array build O(n + u), and Fibonacci O(n) are all pseudopolynomial algorithms we’ve seen already • Radix sort is actually weakly polynomial (a notion in between strongly polynomial and pseudopolynomial): bounded above by a constant-degree polynomial in the input size mea- sured in bits, i.e., in the logarithm of the input integers • Contrast with Rod Cutting, which was polynomial – Had pseudopolynomial dependence on L – But luckily had L input integers too – If only given subset of sellable rod lengths (Knapsack Problem, which generalizes Rod Cutting and Subset Sum — see recitation), then algorithm would have been only pseudopolynomial Complexity • Is Subset Sum solvable in polynomial time when integers are not polynomially bounded? • No if P 6= NP. What does that mean? Next lecture! 7 Lecture 18: Pseudopolynomial Main Features of Dynamic Programs • Review of examples from lecture • Subproblems: – Preﬁx/sufﬁxes: Bowling, LCS, LIS, Floyd–Warshall, Rod Cutting (coincidentally, re- ally Integer subproblems), Subset Sum – Substrings: Alternating Coin Game, Arithmetic Parenthesization – Multiple sequences: LCS – Integers: Fibonacci, Rod Cutting, Subset Sum ⇤ Pseudopolynomial: Fibonacci, Subset Sum – Vertices: DAG shortest paths, Bellman–Ford, Floyd–Warshall • Subproblem constraints/expansion: – Nonexpansive constraint: LIS (include ﬁrst item) – 2⇥ expansion: Alternating Coin Game (who goes ﬁrst?), Arithmetic Parenthesization (min/max) – ⇥(1)⇥ expansion: Piano Fingering (ﬁrst ﬁnger assignment) – ⇥(n)⇥ expansion: Bellman–Ford (# edges) • Relation: – Branching = # dependant subproblems in each subproblem – ⇥(1) branching: Fibonacci, Bowling, LCS, Alternating Coin Game, Floyd–Warshall, Subset Sum – ⇥(degree) branching (source of |E| in running time): DAG shortest paths, Bellman– Ford – ⇥(n) branching: LIS, Arithmetic Parenthesization, Rod Cutting – Combine multiple solutions (not path in subproblem DAG): Fibonacci, Floyd– Warshall, Arithmetic Parenthesization • Original problem: – Combine multiple subproblems: DAG shortest paths, Bellman–Ford, Floyd–Warshall, LIS, Piano Fingering MIT OpenCourseWare https://ocw.mit.edu 6.006 Introduction to Algorithms Spring 2020 For information about citing these materials or our Terms of Use, visit: https://ocw.mit.edu/terms Introduction to Algorithms: 6.006 Massachusetts Institute of Technology Instructors: Erik Demaine, Jason Ku, and Justin Solomon Lecture 19: Complexity Lecture 19: Complexity Decision Problems • Decision problem: assignment of inputs to YES (1) or NO (0) • Inputs are either NO inputs or YES inputs Problem s-t Shortest Path Does a given G contain a path from s to t with weight at most d? Negative Cycle Does a given G contain a negative weight cycle? Longest Simple Path Does a given G contain a simple path with weight at least d? Subset Sum Does a given set of integers A contain a subset with sum S? Tetris Can you survive a given sequence of pieces in given board? Chess Can a player force a win from a given board? Halting problem Does a given computer program terminate for a given input? Decision • Algorithm/Program: constant-length code (working on a word-RAM with Ω(log n)-bit words) to solve a problem, i.e., it produces correct output for every input and the length of the code is independent of the instance size • Problem is decidable if there exists a program to solve the problem in ﬁnite time Decidability • Program is ﬁnite (constant) string of bits, i.e., a nonnegative integer ∈ N. Problem is function p : N →{0, 1}, i.e., inﬁnite string of bits. • (# of programs |N|, countably inﬁnite) ≪ (# of problems |R|, uncountably inﬁnite) • (Proof by Cantor’s diagonalization argument, probably covered in 6.042) • Proves that most decision problems not solvable by any program (undecidable) • E.g., the Halting problem is undecidable (many awesome proofs in 6.045) • Fortunately most problems we think of are algorithmic in structure and are decidable Decidable Decision Problems R problems decidable in ﬁnite time (‘R’ comes from recursive languages) EXP problems decidable in exponential time 2nO(1) (most problems we think of are here) P problems decidable in polynomial time nO(1) (efﬁcient algorithms, the focus of this class) • These sets are distinct, i.e., P $ EXP $ R (via time hierarchy theorems, see 6.045) • E.g., Chess is in EXP \\ P 2 Lecture 19: Complexity Nondeterministic Polynomial Time (NP) • P is the set of decision problems for which there is an algorithm A such that, for every input I of size n, A on I runs in poly(n) time and solves I correctly • NP is the set of decision problems for which there is a veriﬁcation algorithm V that takes as input an input I of the problem and a certiﬁcate bit string of length polynomial in the size of I, so that: – V always runs in time polynomial in the size of I; – if I is a YES input, then there is some certiﬁcate c so that V outputs YES on input (I, c); and – if I is a NO input, then no matter what certiﬁcate c we choose, V always output NO on input (I, c). • You can think of the certiﬁcate as a proof that I is a YES input. If I is actually a NO input, then no proof should work. Problem Certiﬁcate Veriﬁer s-t Shortest Path Negative Cycle Longest Simple Path Subset Sum Tetris A path P from s to t A cycle C A path P A set of items A0 Sequence of moves Adds the weights on P and checks whether ≤ d Adds the weights on C and checks whether < 0 Checks whether P is a simple path with weight ≥ d Checks whether A0 ∈ A has sum S Checks that the moves allow survival • P ⊆ NP: The veriﬁer V just solves the instance ignoring any certiﬁcate • NP ⊆ EXP: Try all possible certiﬁcates! At most 2nO(1) of them, run veriﬁer V on all • Open: Does P = NP? NP = EXP? • Most people think P $ NP ($ EXP), i.e., generating solutions harder than checking • If you prove either way, people will give you lots of money ($1M Millennium Prize) • Why do we care? If can show a problem is hardest problem in NP, then problem cannot be solved in polynomial time if P 6= NP • How do we relate difﬁculty of problems? Reductions! 3 Lecture 19: Complexity Reductions • Suppose you want to solve problem A • One way to solve is to convert A into a problem B you know how to solve • Solve using an algorithm for B and use it to compute solution to A • This is called a reduction from problem A to problem B (A → B) • Because B can be used to solve A, B is at least as hard as A (A ≤ B) • General algorithmic strategy: reduce to a problem you know how to solve A Conversion B Unweighted Shortest Path Integer-weighted Shortest Path Longest Path Give equal weights Subdivide edges Negate weights Weighted Shortest Path Unweighted Shortest Path Shortest Path • Problem A is NP-hard if every problem in NP is polynomially reducible to A • i.e., A is at least as hard as (can be used to solve) every problem in NP (X ≤ A for X ∈ NP) • NP-complete = NP ∩ NP-hard • All NP-complete problems are equivalent, i.e., reducible to each other • First NP-complete problem? Every decision problem reducible to satisfying a logical circuit, a problem called “Circuit SAT”. • Longest Simple Path and Tetris are NP-complete, so if any problem is in NP \\ P, these are • Chess is EXP-complete: in EXP and reducible from every problem in EXP (so ∈/ P) 4 Lecture 19: Complexity Examples of NP-complete Problems • Subset Sum from L18 (“weakly NP-complete” which is what allows a pseudopolynomial- time algorithm, but no polynomial algorithm unless P = NP) • 3-Partition: given n integers, can you divide them into triples of equal sum? (“strongly NP-complete”: no pseudopolynomial-time algorithm unless P = NP) • Rectangle Packing: given n rectangles and a target rectangle whose area is the sum of the n rectangle areas, pack without overlap – Reduction from 3-Partition to Rectangle Packing: transform integer ai into 1 × ai rect- P angle; set target rectangle to n/3 × ( i ai) /3 • Jigsaw puzzles: given n pieces with possibly ambiguous tabs/pockets, ﬁt the pieces together – Reduction from Rectangle Packing: use uniquely matching tabs/pockets to force build- ing rectangles and rectangular boundary; use one ambiguous tab/pocket for all other boundaries • Longest common subsequence of n strings • Longest simple path in a graph • Traveling Salesman Problem: shortest path that visits all vertices of a given graph (or deci- sion version: is minimum weight ≤ d) • Shortest path amidst obstacles in 3D • 3-coloring given graph (but 2-coloring ∈ P) • Largest clique in a given graph • SAT: given a Boolean formula (made with AND, OR, NOT), is it every true? E.g., x AND NOT x is a NO input • Minesweeper, Sudoku, and most puzzles • Super Mario Bros., Legend of Zelda, Pok´emon, and most video games are NP-hard (many are harder) MIT OpenCourseWare https://ocw.mit.edu 6.006 Introduction to Algorithms Spring 2020 For information about citing these materials or our Terms of Use, visit: https://ocw.mit.edu/terms Introduction to Algorithms: 6.006 Massachusetts Institute of Technology Instructors: Erik Demaine, Jason Ku, and Justin Solomon Lecture 20: Course Review Lecture 20: Course Review 6.006: Introduction to Algorithms • Goals: 1. Solve hard computational problems (with non-constant-sized inputs) 2. Argue an algorithm is correct (Induction, Recursion) 3. Argue an algorithm is “good” (Asymptotics, Model of Computation) – (effectively communicate all three above, to human or computer) • Do there always exist “good” algorithms? – Most problems are not solvable efﬁciently, but many we think of are! – Polynomial means polynomial in size of input – Pseudopolynomial means polynomial in size of input AND size of numbers in input – NP: Nondeterministic Polynomial time, polynomially checkable certiﬁcates – NP-hard: set of problems that can be used to solve any problem in NP in poly-time – NP-complete: intersection of NP-hard and NP How to solve an algorithms problem? • Reduce to a problem you know how to solve – Search/Sort (Q1) ∗ Search: Extrinsic (Sequence) and Intrinsic (Set) Data Structures ∗ Sort: Comparison Model, Stability, In-place – Graphs (Q2) ∗ Reachability, Connected Components, Cycle Detection, Topological Sort ∗ Single-Source / All-Pairs Shortest Paths • Design a new recursive algorithm – Brute Force – Divide & Conquer – Dynamic Programming (Q3) – Greedy/Incremental 2 Lecture 20: Course Review Next Steps • (U) 6.046: Design & Analysis of Algorithms • (G) 6.851: Advanced Data Structures • (G) 6.854: Advanced Algorithms 6.046 • Extension of 6.006 – Data Structures: Union-Find, Amortization via potential analysis – Graphs: Minimum Spanning Trees, Network Flows/Cuts – Algorithm Design (Paradigms): Divide & Conquer, Dynamic Programming, Greedy – Complexity: Reductions • Relax Problem (change deﬁnition of correct/efﬁcient) – Randomized Algorithms ∗ 6.006 mostly deterministic (hashing) ∗ Las Vegas: always correct, probably fast (like hashing) ∗ Monte Carlo: always fast, probably correct ∗ Can generally get faster randomized algorithms on structured data – Numerical Algorithms/Continuous Optimization ∗ 6.006 only deals with integers ∗ Approximate real numbers! Pay time for precision – Approximation Algorithms ∗ Input optimization problem (min/max over weighted outputs) ∗ Many optimization problems NP-hard ∗ How close can we get to an optimal solution in polynomial time? • Change Model of Computation – Cache Models (memory hierarchy cost model) – Quantum Computer (exploiting quantum properties) – Parallel Processors (use multiple CPUs instead of just one) ∗ Multicore, large shared memory ∗ Distributed cores, message passing 3 Lecture 20: Course Review Future Courses Model Application • Computation / Complexity (6.045, 6.840, 6.841) • Biology (6.047) • Randomness (6.842) • Game Theory (6.853) • Quantum (6.845) • Cryptography (6.875) • Distributed / message passing (6.852) • Vision (6.819) • Multicore / shared memory (6.816, 6.846) • Graphics (6.837) • Graph and Matrix (6.890) • Geometry (6.850) • Constant Factors / Performance (6.172) • Folding (6.849) MIT OpenCourseWare https://ocw.mit.edu 6.006 Introduction to Algorithms Spring 2020 For information about citing these materials or our Terms of Use, visit: https://ocw.mit.edu/terms"
            },
            {
                "role": "user",
                "content": "Can you summarize the key topics covered in this lecture?"
            },
            {
                "role": "assistant",
                "content": "This lecture covers foundational algorithmic concepts including problem formulation, correctness proof using induction, efficiency analysis using asymptotic notation, data structures (arrays, linked lists, BSTs, heaps), and sorting methods (selection sort, merge sort, counting sort). Additionally, it introduces the Word-RAM computational model, providing a theoretical framework for measuring algorithm efficiency."
            }
        ]
    },
    {
        "system": "Hello! I am Kiran, an AI assistant from Tampere University, dedicated to providing you with accurate and insightful assistance. My goal is to help you understand concepts, summarize lectures, and answer questions effectively.",
        "conversations": [
            {
                "role": "host",
                "content": "Introduction to Algorithms: 6.006 Massachusetts Institute of Technology Instructors: Erik Demaine, Jason Ku, and Justin Solomon Recitation 1 Recitation 1 Algorithms The study of algorithms searches for efﬁcient procedures to solve problems. The goal of this class is to not only teach you how to solve problems, but to teach you to communicate to others that a solution to a problem is both correct and efﬁcient. • A problem is a binary relation connecting problem inputs to correct outputs. • A (deterministic) algorithm is a procedure that maps inputs to single outputs. • An algorithm solves a problem if for every problem input it returns a correct output. While a problem input may have more than one correct output, an algorithm should only return one output for a given input (it is a function). As an example, consider the problem of ﬁnding another student in your recitation who shares the same birthday. Problem: Given the students in your recitation, return either the names of two students who share the same birthday and year, or state that no such pair exists. This problem relates one input (your recitation) to one or more outputs comprising birthday- matching pairs of students or one negative result. A problem input is sometimes called an instance of the problem. One algorithm that solves this problem is the following. Algorithm: Maintain an initially empty record of student names and birthdays. Go around the room and ask each student their name and birthday. After interviewing each student, check to see whether their birthday already exists in the record. If yes, return the names of the two students found. Otherwise, add their name and birthday to the record. If after interviewing all students no satisfying pair is found, return that no matching pair exists. Of course, our algorithm solves a much more general problem than the one proposed above. The same algorithm can search for a birthday-matching pair in any set of students, not just the students in your recitation. In this class, we try to solve problems which generalize to inputs that may be arbitrarily large. The birthday matching algorithm can be applied to a recitation of any size. But how can we determine whether the algorithm is correct and efﬁcient? 2 Recitation 1 Correctness Any computer program you write will have ﬁnite size, while an input it acts on may be arbitrarily large. Thus every algorithm we discuss in this class will need to repeat commands in the algorithm via loops or recursion, and we will be able to prove correctness of the algorithm via induction. Let’s prove that the birthday algorithm is correct. Proof. Induct on the ﬁrst k students interviewed. Base case: for k = 0, there is no matching pair, and the algorithm returns that there is no matching pair. Alternatively, assume for induction that the algorithm returns correctly for the ﬁrst k students. If the ﬁrst k students contain a matching pair, than so does the ﬁrst k + 1 students and the algorithm already returned a matching pair. Otherwise the ﬁrst k students do not contain a matching pair, so if the k+1 students contain a match, the match includes student k+1, and the algorithm checks whether the student k + 1 has the same birthday as someone already processed. Efﬁciency What makes a computer program efﬁcient? One program is said to be more efﬁcient than another if it can solve the same problem input using fewer resources. We expect that a larger input might take more time to solve than another input having smaller size. In addition, the resources used by a program, e.g. storage space or running time, will depend on both the algorithm used and the ma- chine on which the algorithm is implemented. We expect that an algorithm implemented on a fast machine will run faster than the same algorithm on a slower machine, even for the same input. We would like to be able to compare algorithms, without having to worry about how fast our machine is. So in this class, we compare algorithms based on their asymptotic performance relative to problem input size, in order to ignore constant factor differences in hardware performance. 3 Recitation 1 Asymptotic Notation We can use asymptotic notation to ignore constants that do not change with the size of the problem input. O(f(n)) represents the set of functions with domain over the natural numbers satisfying the following property. O Notation: Non-negative function g(n) is in O(f(n)) if and only if there exists a positive real number c and positive integer n0 such that g(n) ≤ c · f(n) for all n ≥ n0. This deﬁnition upper bounds the asymptotic growth of a function for sufﬁciently large n, i.e., the bound on growth is true even if we were to scale or shift our function by a constant amount. By convention, it is more common for people to say that a function g(n) is O(f(n)) or equal to O(f(n)), but what they really mean is set containment, i.e., g(n) ∈ O(f(n)). So since our problem’s input size is cn for some constant c, we can forget about c and say the input size is O(n) (order n). A similar notation can be used for lower bounds. Ω Notation: Non-negative function g(n) is in Ω(f(n)) if and only if there exists a positive real number c and positive integer n0 such that c · f(n) ≤ g(n) for all n ≥ n0. When one function both asymptotically upper bounds and asymptotically lower bounds another function, we use Θ notation. When g(n) = Θ(f(n)), we say that f(n) represents a tight bound on g(n). Θ Notation: Non-negative g(n) is in Θ(f(n)) if and only if g(n) ∈ O(f(n)) ∩ Ω(f(n)). We often use shorthand to characterize the asymptotic growth (i.e., asymptotic complexity) of common functions, such as those shown in the table below1. Here we assume c ∈ Θ(1). Shorthand Constant Logarithmic Linear Quadratic Polynomial Exponential1 Θ(f(n)) Θ(1) Θ(log n) Θ(n) 2) Θ(n c) Θ(n 2Θ(nc) Linear time is often necessary to solve problems where the entire input must be read in order to solve the problem. However, if the input is already accessible in memory, many problems can be solved in sub-linear time. For example, the problem of ﬁnding a value in a sorted array (that has already been loaded into memory) can be solved in logarithmic time via binary search. We focus on polynomial time algorithms in this class, typically for small values of c. There’s a big difference between logarithmic, linear, and exponential. If n = 1000, log n ≈ 101, n ≈ 103 , while ≈ 10300 2n . For comparison, the number of atoms in the universe is estimated around 1080 . It is common to use the variable ‘n’ to represent a parameter that is linear in the problem input size, though this is not always the case. For example, when talking about graph algorithms later in the term, a problem input will be a graph parameterized by vertex set V and edge set E, so a natural input size will be Θ(|V | + |E|). Alternatively, when talking about matrix algorithms, it is common to let n be the width of a square matrix, where a problem input will have size Θ(n2), specifying each element of the n × n matrix. 1Note that exponential 2Θ(n c) is a convenient abuse of notation meaning {2p | p ∈ Θ(nc)}. 4 Recitation 1 Model of Computation In order to precisely calculate the resources used by an algorithm, we need to model how long a computer takes to perform basic operations. Specifying such a set of operations provides a model of computation upon which we can base our analysis. In this class, we will use the w-bit Word- RAM model of computation, which models a computer as a random access array of machine words called memory, together with a processor that can perform operations on the memory. A machine word is a sequence of w bits representing an integer from the set {0, . . . , 2w − 1}. A Word-RAM processor can perform basic binary operations on two machine words in constant time, including addition, subtraction, multiplication, integer division, modulo, bitwise operations, and binary comparisons. In addition, given a word a, the processor can read or write the word in memory located at address a in constant time. If a machine word contains only w bits, the processor will only be able to read and write from at most 2w addresses in memory2. So when solving a problem on an input stored in n machine words, we will always assume our Word-RAM has a word size of at least w > log2 n bits, or else the machine would not be able to access all of the input in memory. To put this limitation in perspective, a Word-RAM model of a byte-addressable 64-bit machine allows inputs up to ∼ 1010 GB in size. Data Structure The running time of our birthday matching algorithm depends on how we store the record of names and birthdays. A data structure is a way to store a non-constant amount of data, supporting a set of operations to interact with that data. The set of operations supported by a data structure is called an interface. Many data structures might support the same interface, but could provide different performance for each operation. Many problems can be solved trivially by storing data in an appropriate choice of data structure. For our example, we will use the most primitive data structure native to the Word-RAM: the static array. A static array is simply a contiguous sequence of words reserved in memory, supporting a static sequence interface: • StaticArray(n): allocate a new static array of size n initialized to 0 in Θ(n) time • StaticArray.get at(i): return the word stored at array index i in Θ(1) time • StaticArray.set at(i, x): write the word x to array index i in Θ(1) time The get at(i) and set at(i, x) operations run in constant time because each item in the array has the same size: one machine word. To store larger objects at an array index, we can interpret the machine word at the index as a memory address to a larger piece of memory. A Python tuple is like a static array without set at(i, x). A Python list implements a dynamic array (see L02). 2For example, on a typical 32-bit machine, each byte (8-bits) is addressable (for historical reasons), so the size of the machine’s random-access memory (RAM) is limited to (8-bits)×(232) ≈ 4 GB. 5 Recitation 1 1 class StaticArray: 2 def __init__(self, n): 3 self.data = [None] * n 4 def get_at(self, i): 5 if not (0 <= i < len(self.data)): raise IndexError 6 return self.data[i] 7 def set_at(self, i, x): 8 if not (0 <= i < len(self.data)): raise IndexError 9 self.data[i] = x 10 11 def birthday_match(students): ’’’ 12 13 Find a pair of students with the same birthday 14 Input: tuple of student (name, bday) tuples 15 Output: tuple of student names or None ’’’ 16 17 n = len(students) # O(1) 18 record = StaticArray(n) # O(n) 19 for k in range(n): # n 20 (name1, bday1) = students[k] # O(1) 21 for i in range(k): # k Check if in record 22 (name2, bday2) = record.get_at(i) # O(1) 23 if bday1 == bday2: # O(1) 24 return (name1, name2) # O(1) 25 record.set_at(k, (name1, bday1)) # O(1) 26 return None # O(1) Running Time Analysis Now let’s analyze the running time of our birthday matching algorithm on a recitation containing n students. We will assume that each name and birthday ﬁts into a constant number of machine words so that a single student’s information can be collected and manipulated in constant time3 . We step through the algorithm line by line. All the lines take constant time except for lines 8, 9, and 11. Line 8 takes Θ(n) time to initialize the static array record; line 9 loops at most n times; and line 11 loops through the k items existing in the record. Thus the running time for this algorithm is at most: n−1 X 2) O(n) + (O(1) + k · O(1)) = O(n k=0 This is quadratic in n, which is polynomial! Is this efﬁcient? No! We can do better by using a different data structure for our record. We will spend the ﬁrst half of this class studying elementary data structures, where each data structure will be tailored to support a different set of operations efﬁciently. 3This is a reasonable restriction, which allows names and birthdays to contain O(w) characters from a constant sized alphabet. Since w > log2 n, this restriction still allows each student’s information to be distinct. 6 Recitation 1 Asymptotics Exercises 1. Have students generate 10 functions and order them based on asymptotic growth. \u0000\u0001 n 2. Find a simple, tight asymptotic bound for 6006 . Solution: Deﬁnition yields n(n − 1) . . . (n − 6005) in the numerator (a degree 6006 poly- \u0000\u0001 n 6006). nomial) and 6006! in the denominator (constant with respect to n). So 6006 = Θ(n \u0010\u0000 \u0011 \u0000 √ \u0001\u00012 n 3. Find a simple, tight asymptotic bound for log6006 log n . Solution: Recall exponent and logarithm rules: log ab = log a + log b, log (ab) = b log a, and loga b = log b/ log a. \u0012\u0010 \u0010 √ \u0011\u00112 \u0013 2 \u0000√ \u0001 log n n = log n log n log6006 log 6006 = Θ(log n 1/2 + log log n) = Θ(log n) 4. Show that 2n+1 ∈ Θ(2n), but that 22n+1 6∈ O(22n ). Solution: In the ﬁrst case, 2n+1 = 2 · 2n , which is a constant factor larger than 2n . In the \u0000\u00012 22n second case, 22n+1 = , which is deﬁnitely more than a constant factor larger than 22n . 5. Show that (log n)a = O(nb) for all positive constants a and b. Solution: It’s enough to show nb/(log n)a limits to ∞ as n →∞, and this is equivalent to arguing that the log of this expression approaches ∞: \u0012 b \u0013 n lim log = lim (b log n − a log log n) = lim (bx − a log x) = ∞, n→∞ (log n)a n→∞ x→∞ as desired. a Note: for the same reasons, n = O(cn) for any c > 1. 6. Show that (log n)log n = Ω(n). m Solution: Note that m = Ω(2m), so setting n = 2m completes the proof. 7. Show that (6n)! 6∈ Θ(n!), but that log ((6n)!) ∈ Θ(log(n!)). Solution: We invoke Sterling’s approximation, \u0012 \u0012 \u0013\u0013 √ \u0010 n \u0011n 1 n! = 2πn 1 + Θ . e n Substituting in 6n gives an expression that is at least 66n larger than the original. But taking the logarithm of Sterling’s gives log(n!) = Θ(n log n), and substituting in 6n yields only constant additional factors. MIT OpenCourseWare https://ocw.mit.edu 6.006 Introduction to Algorithms Spring 2020 For information about citing these materials or our Terms of Use, visit: https://ocw.mit.edu/terms Introduction to Algorithms: 6.006 Massachusetts Institute of Technology Instructors: Erik Demaine, Jason Ku, and Justin Solomon Recitation 2 Recitation 2 Sequence Interface (L02, L07) Sequences maintain a collection of items in an extrinsic order, where each item stored has a rank in the sequence, including a ﬁrst item and a last item. By extrinsic, we mean that the ﬁrst item is ‘ﬁrst’, not because of what the item is, but because some external party put it there. Sequences are generalizations of stacks and queues, which support a subset of sequence operations. Container build(X) len() given an iterable X, build sequence from items in X return the number of stored items Static iter seq() get at(i) set at(i, x) return the stored items one-by-one in sequence order return the ith item replace the ith item with x Dynamic insert at(i, x) delete at(i) insert first(x) delete first() insert last(x) delete last() add x as the ith item remove and return the ith item add x as the ﬁrst item remove and return the ﬁrst item add x as the last item remove and return the last item (Note that insert / delete operations change the rank of all items after the modiﬁed item.) Set Interface (L03-L08) By contrast, Sets maintain a collection of items based on an intrinsic property involving what the items are, usually based on a unique key, x.key, associated with each item x. Sets are generaliza- tions of dictionaries and other intrinsic query databases. Container build(X) len() given an iterable X, build set from items in X return the number of stored items Static find(k) return the stored item with key k Dynamic insert(x) delete(k) add x to set (replace item with key x.key if one already exists) remove and return the stored item with key k Order iter ord() find min() find max() find next(k) find prev(k) return the stored items one-by-one in key order return the stored item with smallest key return the stored item with largest key return the stored item with smallest key larger than k return the stored item with largest key smaller than k (Note that find operations return None if no qualifying item exists.) 2 Recitation 2 Sequence Implementations Here, we will discuss three data structures to implement the sequence interface. In Problem Set 1, you will extend both Linked Lists and Dynamic arrays to make both ﬁrst and last dynamic operations O(1) time for each. Notice that none of these data structures support dynamic operations at arbitrary index in sub-linear time. We will learn how to improve this operation in Lecture 7. Data Operation, Worst Case O(·) Container Static Dynamic Structure build(X) get at(i) set at(i,x) insert first(x) delete first() insert last(x) delete last() insert at(i, x) delete at(i) Array n 1 n n n Linked List n n 1 n n Dynamic Array n 1 n 1(a) n Array Sequence Computer memory is a ﬁnite resource. On modern computers many processes may share the same main memory store, so an operating system will assign a ﬁxed chunk of memory addresses to each active process. The amount of memory assigned depends on the needs of the process and the availability of free memory. For example, when a computer program makes a request to store a variable, the program must tell the operating system how much memory (i.e. how many bits) will be required to store it. To fulﬁll the request, the operating system will ﬁnd the available memory in the process’s assigned memory address space and reserve it (i.e. allocate it) for that purpose until it is no longer needed. Memory management and allocation is a detail that is abstracted away by many high level languages including Python, but know that whenever you ask Python to store something, Python makes a request to the operating system behind-the-scenes, for a ﬁxed amount of memory in which to store it. Now suppose a computer program wants to store two arrays, each storing ten 64-bit words. The program makes separate requests for two chunks of memory (640 bits each), and the operating system fulﬁlls the request by, for example, reserving the ﬁrst ten words of the process’s assigned address space to the ﬁrst array A, and the second ten words of the address space to the second array B. Now suppose that as the computer program progresses, an eleventh word w needs to be added to array A. It would seem that there is no space near A to store the new word: the beginning of the process’s assigned address space is to the left of A and array B is stored on the right. Then how can we add w to A? One solution could be to shift B right to make room for w, but tons of data may already be reserved next to B, which you would also have to move. Better would be to simply request eleven new words of memory, copy A to the beginning of the new memory allocation, store w at the end, and free the ﬁrst ten words of the process’s address space for future memory requests. A ﬁxed-length array is the data structure that is the underlying foundation of our model of com- putation (you can think of your computer’s memory as a big ﬁxed-length array that your operating 3 Recitation 2 system allocates from). Implementing a sequence using an array, where index i in the array cor- responds to item i in the sequence allows get at and set at to be O(1) time because of our random access machine. However, when deleting or inserting into the sequence, we need to move items and resize the array, meaning these operations could take linear-time in the worst case. Below is a full Python implementation of an array sequence. 1 class Array_Seq: 2 def __init__(self): # O(1) 3 self.A = [] 4 self.size = 0 5 6 def __len__(self): return self.size # O(1) 7 def __iter__(self): yield from self.A # O(n) iter_seq 8 9 def build(self, X): # O(n) 10 self.A = [a for a in X] # pretend this builds a static array 11 self.size = len(self.A) 12 13 def get_at(self, i): return self.A[i] # O(1) 14 def set_at(self, i, x): self.A[i] = x # O(1) 15 16 def _copy_forward(self, i, n, A, j): # O(n) 17 for k in range(n): 18 A[j + k] = self.A[i + k] 19 20 def _copy_backward(self, i, n, A, j): # O(n) 21 for k in range(n - 1, -1, -1): 22 A[j + k] = self.A[i + k] 23 24 def insert_at(self, i, x): # O(n) 25 n = len(self) 26 A = [None] * (n + 1) 27 self._copy_forward(0, i, A, 0) 28 A[i] = x 29 self._copy_forward(i, n - i, A, i + 1) 30 self.build(A) 31 32 def delete_at(self, i): # O(n) 33 n = len(self) 34 A = [None] * (n - 1) 35 self._copy_forward(0, i, A, 0) 36 x = self.A[i] 37 self._copy_forward(i + 1, n - i - 1, A, i) 38 self.build(A) 39 return x 40 # O(n) 41 def insert_first(self, x): self.insert_at(0, x) 42 def delete_first(self): return self.delete_at(0) 43 def insert_last(self, x): self.insert_at(len(self), x) 44 def delete_last(self): return self.delete_at(len(self) - 1) 4 Recitation 2 Linked List Sequence A linked list is a different type of data structure entirely. Instead of allocating a contiguous chunk of memory in which to store items, a linked list stores each item in a node, node, a constant-sized container with two properties: node.item storing the item, and node.next storing the memory address of the node containing the next item in the sequence. 1 class Linked_List_Node: 2 def __init__(self, x): # O(1) 3 self.item = x 4 self.next = None 5 6 def later_node(self, i): # O(i) 7 if i == 0: return self 8 assert self.next 9 return self.next.later_node(i - 1) Such data structures are sometimes called pointer-based or linked and are much more ﬂexible than array-based data structures because their constituent items can be stored anywhere in memory. A linked list stores the address of the node storing the ﬁrst element of the list called the head of the list, along with the linked list’s size, the number of items stored in the linked list. It is easy to add an item after another item in the list, simply by changing some addresses (i.e. relinking pointers). In particular, adding a new item at the front (head) of the list takes O(1) time. However, the only way to ﬁnd the ith item in the sequence is to step through the items one-by-one, leading to worst- case linear time for get at and set at operations. Below is a Python implementation of a full linked list sequence. 1 class Linked_List_Seq: 2 def __init__(self): 3 self.head = None 4 self.size = 0 5 6 def __len__(self): return self.size 7 8 def __iter__(self): 9 node = self.head 10 while node: 11 yield node.item 12 node = node.next 13 14 def build(self, X): 15 for a in reversed(X): 16 self.insert_first(a) 17 18 def get_at(self, i): 19 node = self.head.later_node(i) 20 return node.item 21 # O(1) # O(1) # O(n) iter_seq # O(n) # O(i) 5 Recitation 2 22 def set_at(self, i, x): # O(i) 23 node = self.head.later_node(i) 24 node.item = x 25 26 def insert_first(self, x): # O(1) 27 new_node = Linked_List_Node(x) 28 new_node.next = self.head 29 self.head = new_node 30 self.size += 1 31 32 def delete_first(self): # O(1) 33 x = self.head.item 34 self.head = self.head.next 35 self.size -= 1 36 return x 37 38 def insert_at(self, i, x): # O(i) 39 if i == 0: 40 self.insert_first(x) 41 return 42 new_node = Linked_List_Node(x) 43 node = self.head.later_node(i - 1) 44 new_node.next = node.next 45 node.next = new_node 46 self.size += 1 47 48 def delete_at(self, i): # O(i) 49 if i == 0: 50 return self.delete_first() 51 node = self.head.later_node(i - 1) 52 x = node.next.item 53 node.next = node.next.next 54 self.size -= 1 55 return x 56 # O(n) 57 def insert_last(self, x): self.insert_at(len(self), x) 58 def delete_last(self): return self.delete_at(len(self) - 1) Dynamic Array Sequence The array’s dynamic sequence operations require linear time with respect to the length of array A. Is there another way to add elements to an array without paying a linear overhead transfer cost each time you add an element? One straight-forward way to support faster insertion would be to over-allocate additional space when you request space for the array. Then, inserting an item would be as simple as copying over the new value into the next empty slot. This compromise trades a little extra space in exchange for constant time insertion. Sounds like a good deal, but any additional allocation will be bounded; eventually repeated insertions will ﬁll the additional space, and the array will again need to be reallocated and copied over. Further, any additional space you reserve will mean less space is available for other parts of your program. 6 Recitation 2 Then how does Python support appending to the end of a length n Python List in worst-case O(1) time? The answer is simple: it doesn’t. Sometimes appending to the end of a Python List requires O(n) time to transfer the array to a larger allocation in memory, so sometimes appending to a Python List takes linear time. However, allocating additional space in the right way can guarantee that any sequence of n insertions only takes at most O(n) time (i.e. such linear time transfer oper- ations do not occur often), so insertion will take O(1) time per insertion on average. We call this asymptotic running time amortized constant time, because the cost of the operation is amortized (distributed) across many applications of the operation. To achieve an amortized constant running time for insertion into an array, our strategy will be to allocate extra space in proportion to the size of the array being stored. Allocating O(n) additional space ensures that a linear number of insertions must occur before an insertion will overﬂow the allocation. A typical implementation of a dynamic array will allocate double the amount of space needed to store the current array, sometimes referred to as table doubling. However, allocating any constant fraction of additional space will achieve the amortized bound. Python Lists allocate additional space according to the following formula (from the Python source code written in C): 1 new_allocated = (newsize >> 3) + (newsize < 9 ? 3 : 6); Here, the additional allocation is modest, roughly one eighth of the size of the array being appended (bit shifting the size to the right by 3 is equivalent to ﬂoored division by 8). But the additional al- location is still linear in the size of the array, so on average, n/8 insertions will be performed for every linear time allocation of the array, i.e. amortized constant time. What if we also want to remove items from the end of the array? Popping the last item can occur in constant time, simply by decrementing a stored length of the array (which Python does). However, if a large number of items are removed from a large list, the unused additional allocation could occupy a signiﬁcant amount of wasted memory that will not available for other purposes. When the length of the array becomes sufﬁciently small, we can transfer the contents of the array to a new, smaller memory allocation so that the larger memory allocation can be freed. How big should this new allocation be? If we allocate the size of the array without any additional allocation, an immediate insertion could trigger another allocation. To achieve constant amortized running time for any sequence of n appends or pops, we need to make sure there remains a linear fraction of unused allocated space when we rebuild to a smaller array, which guarantees that at least Ω(n) sequential dynamic operations must occur before the next time we need to reallocate memory. Below is a Python implementation of a dynamic array sequence, including operationsinsert last (i.e., Python list append) and delete last (i.e., Python list pop), using table doubling propor- tions. When attempting to append past the end of the allocation, the contents of the array are transferred to an allocation that is twice as large. When removing down to one fourth of the alloca- tion, the contents of the array are transferred to an allocation that is half as large. Of course Python Lists already support dynamic operations using these techniques; this code is provided to help you understand how amortized constant append and pop could be implemented. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 7 Recitation 2 class Dynamic_Array_Seq(Array_Seq): def __init__(self, r = 2): # O(1) super().__init__() self.size = 0 self.r = r self._compute_bounds() self._resize(0) def __len__(self): return self.size # O(1) def __iter__(self): # O(n) for i in range(len(self)): yield self.A[i] def build(self, X): # O(n) for a in X: self.insert_last(a) def _compute_bounds(self): # O(1) self.upper = len(self.A) self.lower = len(self.A) // (self.r * self.r) def _resize(self, n): # O(1) or O(n) if (self.lower < n < self.upper): return m = max(n, 1) * self.r A = [None] * m self._copy_forward(0, self.size, A, 0) self.A = A self._compute_bounds() def insert_last(self, x): # O(1)a self._resize(self.size + 1) self.A[self.size] = x self.size += 1 def delete_last(self): # O(1)a self.A[self.size - 1] = None self.size -= 1 self._resize(self.size) def insert_at(self, i, x): # O(n) self.insert_last(None) self._copy_backward(i, self.size - (i + 1), self.A, i + 1) self.A[i] = x def delete_at(self, i): # O(n) x = self.A[i] self._copy_forward(i + 1, self.size - (i + 1), self.A, i) self.delete_last() return x # O(n) def insert_first(self, x): self.insert_at(0, x) def delete_first(self): return self.delete_at(0) 8 Recitation 2 Exercises: • Suppose the next pointer of the last node of a linked list points to an earlier node in the list, creating a cycle. Given a pointer to the head of the list (without knowing its size), describe a linear-time algorithm to ﬁnd the number of nodes in the cycle. Can you do this while using only constant additional space outside of the original linked list? Solution: Begin with two pointers pointing at the head of the linked list: one slow pointer and one fast pointer. The pointers take turns traversing the nodes of the linked list, starting with the fast pointer. On the slow pointer’s turn, the slow pointer simply moves to the next node in the list; while on the fast pointer’s turn, the fast pointer initially moves to the next node, but then moves on to the next node’s next node before ending its turn. Every time the fast pointer visits a node, it checks to see whether it’s the same node that the slow pointer is pointing to. If they are the same, then the fast pointer must have made a full loop around the cycle, to meet the slow pointer at some node v on the cycle. Now to ﬁnd the length of the cycle, simply have the fast pointer continue traversing the list until returning back to v, counting the number of nodes visited along the way. To see that this algorithm runs in linear time, clearly the last step of traversing the cycle takes at most linear time, as v is the only node visited twice while traversing the cycle. Further, we claim the slow pointer makes at most one move per node. Suppose for contradiction the slow pointer moves twice away from some node u before being at the same node as the fast pointer, meaning that u is on the cycle. In the same time the slow pointer takes to traverse the cycle from u back to u, the fast pointer will have traveled around the cycle twice, meaning that both pointers must have existed at the same node prior to the slow pointer leaving u, a contradiction. • Given a data structure implementing the Sequence interface, show how to use it to implement the Set interface. (Your implementation does not need to be efﬁcient.) Solution: 1 def Set_from_Seq(seq): 2 class set_from_seq: 3 def __init__(self): self.S = seq() 4 def __len__(self): return len(self.S) 5 def __iter__(self): yield from self.S 6 7 def build(self, A): 8 self.S.build(A) 9 10 def insert(self, x): 11 for i in range(len(self.S)): 12 if self.S.get_at(i).key == x.key: 13 self.S.set_at(i, x) 14 return 15 self.S.insert_last(x) 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 9 Recitation 2 def delete(self, k): for i in range(len(self.S)): if self.S.get_at(i).key == k: return self.S.delete_at(i) def find(self, k): for x in self: if x.key == k: return x return None def find_min(self): out = None for x in self: if (out is None) or (x.key < out.key): out = x return out def find_max(self): out = None for x in self: if (out is None) or (x.key > out.key): out = x return out def find_next(self, k): out = None for x in self: if x.key > k: if (out is None) or (x.key < out.key): out = x return out def find_prev(self, k): out = None for x in self: if x.key < k: if (out is None) or (x.key > out.key): out = x return out def iter_ord(self): x = self.find_min() while x: yield x x = self.find_next(x.key) return set_from_seq MIT OpenCourseWare https://ocw.mit.edu 6.006 Introduction to Algorithms Spring 2020 For information about citing these materials or our Terms of Use, visit: https://ocw.mit.edu/terms Introduction to Algorithms: 6.006 Massachusetts Institute of Technology Instructors: Erik Demaine, Jason Ku, and Justin Solomon Recitation 3 Recitation 3 Recall that in Recitation 2 we reduced the Set interface to the Sequence Interface (we simulated one with the other). This directly provides a Set data structure from an array (albeit a poor one). Data Structure Array Container build(X) n Operations O(·) Static Dynamic Order find(k) insert(x) find min() find prev(k) delete(k) find max() find next(k) n n n n We would like to do better, and we will spend the next ﬁve lectures/recitations trying to do exactly that! One of the simplest ways to get a faster Set is to store our items in a sorted array, where the item with the smallest key appears ﬁrst (at index 0), and the item with the largest key appears last. Then we can simply binary search to ﬁnd keys and support Order operations! This is still not great for dynamic operations (items still need to be shifted when inserting or removing from the middle of the array), but ﬁnding items by their key is much faster! But how do we get a sorted array in the ﬁrst place? Data Structure Sorted Array Container build(X) ? Operations O(·) Static Dynamic Order find(k) insert(x) find min() find prev(k) delete(k) find max() find next(k) log n n 1 log n 1 class Sorted_Array_Set: 2 def __init__(self): self.A = Array_Seq() # O(1) 3 def __len__(self): return len(self.A) # O(1) 4 def __iter__(self): yield from self.A # O(n) 5 def iter_order(self): yield from self # O(n) 6 7 def build(self, X): # O(?) 8 self.A.build(X) 9 self._sort() 10 11 def _sort(self): # O(?) 12 ?? 13 14 def _binary_search(self, k, i, j): # O(log n) 15 if i >= j: return i 16 m = (i + j) // 2 17 x = self.A.get_at(m) 18 if x.key > k: return self._binary_search(k, i, m - 1) 19 if x.key < k: return self._binary_search(k, m + 1, j) 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 2 Recitation 3 return m def find_min(self): # O(1) if len(self) > 0: return self.A.get_at(0) else: return None def find_max(self): # O(1) if len(self) > 0: return self.A.get_at(len(self) - 1) else: return None def find(self, k): # O(log n) if len(self) == 0: return None i = self._binary_search(k, 0, len(self) - 1) x = self.A.get_at(i) if x.key == k: return x else: return None def find_next(self, k): # O(log n) if len(self) == 0: return None i = self._binary_search(k, 0, len(self) - 1) x = self.A.get_at(i) if x.key > k: return x if i + 1 < len(self): return self.A.get_at(i + 1) else: return None def find_prev(self, k): # O(log n) if len(self) == 0: return None i = self._binary_search(k, 0, len(self) - 1) x = self.A.get_at(i) if x.key < k: return x if i > 0: return self.A.get_at(i - 1) else: return None def insert(self, x): # O(n) if len(self.A) == 0: self.A.insert_first(x) else: i = self._binary_search(x.key, 0, len(self.A) - 1) k = self.A.get_at(i).key if k == x.key: self.A.set_at(i, x) return False if k > x.key: self.A.insert_at(i, x) else: self.A.insert_at(i + 1, x) return True def delete(self, k): # O(n) i = self._binary_search(k, 0, len(self.A) - 1) assert self.A.get_at(i).key == k return self.A.delete_at(i) 3 Recitation 3 Sorting Sorting an array A of comparable items into increasing order is a common subtask of many com- putational problems. Insertion sort and selection sort are common sorting algorithms for sorting small numbers of items because they are easy to understand and implement. Both algorithms are incremental in that they maintain and grow a sorted subset of the items until all items are sorted. The difference between them is subtle: • Selection sort maintains and grows a subset the largest i items in sorted order. • Insertion sort maintains and grows a subset of the ﬁrst i input items in sorted order. Selection Sort Here is a Python implementation of selection sort. Having already sorted the largest items into sub-array A[i+1:], the algorithm repeatedly scans the array for the largest item not yet sorted and swaps it with item A[i]. As can be seen from the code, selection sort can require Ω(n2) comparisons, but will perform at most O(n) swaps in the worst case. 1 def selection_sort(A): # Selection sort array A 2 for i in range(len(A) - 1, 0, -1): # O(n) loop over array 3 m = i # O(1) initial index of max 4 for j in range(i): # O(i) search for max in A[:i] 5 if A[m] < A[j]: # O(1) check for larger value 6 m = j # O(1) new max found 7 A[m], A[i] = A[i], A[m] # O(1) swap Insertion Sort Here is a Python implementation of insertion sort. Having already sorted sub-array A[:i], the algorithm repeatedly swaps item A[i] with the item to its left until the left item is no larger than A[i]. As can be seen from the code, insertion sort can require Ω(n2) comparisons and Ω(n2) swaps in the worst case. 1 def insertion_sort(A): # Insertion sort array A 2 for i in range(1, len(A)): # O(n) loop over array 3 j = i # O(1) initialize pointer 4 while j > 0 and A[j] < A[j - 1]: # O(i) loop over prefix 5 A[j - 1], A[j] = A[j], A[j - 1] # O(1) swap 6 j = j - 1 # O(1) decrement j In-place and Stability Both insertion sort and selection sort are in-place algorithms, meaning they can each be imple- mented using at most a constant amount of additional space. The only operations performed on the array are comparisons and swaps between pairs of elements. Insertion sort is stable, meaning that items having the same value will appear in the sort in the same order as they appeared in the input array. By comparison, this implementation of selection sort is not stable. For example, the input (2, 1, 10) would produce the output (10 , 1, 2). 4 Recitation 3 Merge Sort In lecture, we introduced merge sort, an asymptotically faster algorithm for sorting large numbers of items. The algorithm recursively sorts the left and right half of the array, and then merges the two halves in linear time. The recurrence relation for merge sort is then T (n) = 2T (n/2) + Θ(n), which solves to T (n) = Θ(n log n). An Θ(n log n) asymptotic growth rate is much closer to linear than quadratic, as log n grows exponentially slower than n. In particular, log n grows slower than any polynomial nε for ε > 0. 1 def merge_sort(A, a = 0, b = None): # Sort sub-array A[a:b] 2 if b is None: # O(1) initialize 3 b = len(A) # O(1) 4 if 1 < b - a: # O(1) size k = b - a 5 c = (a + b + 1) // 2 # O(1) compute center 6 merge_sort(A, a, c) # T(k/2) recursively sort left 7 merge_sort(A, c, b) # T(k/2) recursively sort right 8 L, R = A[a:c], A[c:b] # O(k) copy 9 i, j = 0, 0 # O(1) initialize pointers 10 while a < b: # O(n) 11 if (j >= len(R)) or (i < len(L) and L[i] < R[j]): # O(1) check side 12 A[a] = L[i] # O(1) merge from left 13 i = i + 1 # O(1) decrement left pointer 14 else: 15 A[a] = R[j] # O(1) merge from right 16 j = j + 1 # O(1) decrement right pointer 17 a = a + 1 # O(1) decrement merge pointer Merge sort uses a linear amount of temporary storage (temp) when combining the two halves, so it is not in-place. While there exist algorithms that perform merging using no additional space, such implementations are substantially more complicated than the merge sort algorithm. Whether merge sort is stable depends on how an implementation breaks ties when merging. The above implementation is not stable, but it can be made stable with only a small modiﬁcation. Can you modify the implementation to make it stable? We’ve made CoffeeScript visualizers for the merge step of this algorithm, as well as one showing the recursive call structure. You can ﬁnd them here: https://codepen.io/mit6006/pen/wEXOOq https://codepen.io/mit6006/pen/RYJdOG Build a Sorted Array With an algorithm to sort our array in Θ(n log n), we can now complete our table! We sacriﬁce some time in building the data structure to speed up order queries. This is a common technique called preprocessing. Operations O(·) Container Static Dynamic Order Data Structure build(X) find(k) insert(x) delete(k) find min() find max() find prev(k) find next(k) Array n n n n n Sorted Array n log n log n n 1 log n 5 Recitation 3 Recurrences There are three primary methods for solving recurrences: • Substitution: Guess a solution and substitute to show the recurrence holds. • Recursion Tree: Draw a tree representing the recurrence and sum computation at nodes. This is a very general method, and is the one we’ve used in lecture so far. • Master Theorem: A general formula to solve a large class of recurrences. It is useful, but can also be hard to remember. Master Theorem The Master Theorem provides a way to solve recurrence relations in which recursive calls de- crease problem size by a constant factor. Given a recurrence relation of the form T (n) = aT (n/b)+ f(n) and T (1) = Θ(1), with branching factor a ≥ 1, problem size reduction factor b > 1, and asymptotically non-negative function f(n), the Master Theorem gives the solution to the recur- logb n logb a rence by comparing f(n) to a = n , the number of leaves at the bottom of the recursion logb a tree. When f(n) grows asymptotically faster than n , the work done at each level decreases geometrically so the work at the root dominates; alternatively, when f(n) grows slower, the work done at each level increases geometrically and the work at the leaves dominates. When their growth rates are comparable, the work is evenly spread over the tree’s O(log n) levels. n n b n bi 1 alogb n × f(1) ai × f( n bi ) a × f( n b ) 1 × f(n) case solution conditions 1 logb a) T (n) = Θ(n f(n) = O(nlogb a−ε) for some constant ε > 0 2 T (n) = Θ(nlogb a logk+1 n) f(n) = Θ(nlogb a logk n) for some constant k ≥ 0 3 T (n) = Θ(f(n)) f(n) = Ω(nlogb a+ε) for some constant ε > 0 and af(n/b) < cf(n) for some constant 0 < c < 1 The Master Theorem takes on a simpler form when f(n) is a polynomial, such that the recurrence has the from T (n) = aT (n/b) + Θ(nc) for some constant c ≥ 0. 6 Recitation 3 case solution conditions intuition 1 logb a) T (n) = Θ(n c < logb a Work done at leaves dominates 2 T (n) = Θ(nc log n) c = logb a Work balanced across the tree 3 T (n) = Θ(nc) c > logb a Work done at root dominates This special case is straight-forward to prove by substitution (this can be done in recitation). To apply the Master Theorem (or this simpler special case), you should state which case applies, and show that your recurrence relation satisﬁes all conditions required by the relevant case. There are even stronger (more general) formulas1 to solve recurrences, but we will not use them in this class. Exercises 1. Write a recurrence for binary search and solve it. Solution: T (n) = T (n/2) + O(1) so T (n) = O(log n) by case 2 of Master Theorem. 2. T (n) = T (n − 1) + O(1) Solution: T (n) = O(n), length n chain, O(1) work per node. 3. T (n) = T (n − 1) + O(n) Solution: T (n) = O(n2), length n chain, O(k) work per node at height k. 4. T (n) = 2T (n − 1) + O(1) Solution: T (n) = O(2n), height n binary tree, O(1) work per node. 5. T (n) = T (2n/3) + O(1) Solution: T (n) = O(log n), length log3/2(n) chain, O(1) work per node. 6. T (n) = 2T (n/2) + O(1) Solution: T (n) = O(n), height log2 n binary tree, O(1) work per node. 7. T (n) = T (n/2) + O(n) Solution: T (n) = O(n), length log2 n chain, O(2k) work per node at height k. 8. T (n) = 2T (n/2) + O(n log n) Solution: T (n) = O(n log2 n) (special case of Master Theorem does not apply because n log n is not polynomial), height log2 n binary tree, O(k · 2k) work per node at height k. 9. T (n) = 4T (n/2) + O(n) Solution: T (n) = O(n2), height log2 n degree-4 tree, O(2k) work per node at height k. 1http://en.wikipedia.org/wiki/Akra-Bazzi_method MIT OpenCourseWare https://ocw.mit.edu 6.006 Introduction to Algorithms Spring 2020 For information about citing these materials or our Terms of Use, visit: https://ocw.mit.edu/terms Introduction to Algorithms: 6.006 Massachusetts Institute of Technology Instructors: Erik Demaine, Jason Ku, and Justin Solomon Recitation 4 Recitation 4 Data Structure Operations O(·) Container Static Dynamic Order build(X) find(k) insert(x) delete(k) find min() find max() find prev(k) find next(k) Array n n n n n Sorted Array n log n log n n 1 log n We’ve learned how to implement a set interface using a sorted array, where query operations are efﬁcient but whose dynamic operations are lacking. Recalling that Θ(log n) growth is much closer to Θ(1) than Θ(n), a sorted array provides really good performance! But one of the most common operations you will do in programming is to search for something you’re storing, i.e., find(k). Is it possible to find faster than Θ(log n)? It turns out that if the only thing we can do to items is to compare their relative order, then the answer is no! Comparison Model The comparison model of computation acts on a set of comparable objects. The objects can be thought of as black boxes, supporting only a set of binary boolean operations called comparisons (namely <, ≤, >, ≥, =, and =6 ). Each operation takes as input two objects and outputs a Boolean value, either True or False, depending on the relative ordering of the elements. A search algorithm operating on a set of n items will return a stored item with a key equal to the input key, or return no item if no such item exists. In this section, we assume that each item has a unique key. If binary comparisons are the only way to distinguish between stored items and a search key, a deterministic comparison search algorithm can be thought of as a ﬁxed binary decision tree rep- resenting all possible executions of the algorithm, where each node represents a comparison per- formed by the algorithm. During execution, the algorithm walks down the tree along a path from the root. For any given input, a comparison sorting algorithm will make some comparison ﬁrst, the comparison at the root of the tree. Depending on the outcome of this comparison, the computation will then proceed with a comparison at one of its two children. The algorithm repeatedly makes comparisons until a leaf is reached, at which point the algorithm terminates, returning an output to the algorithm. There must be a leaf for each possible output to the algorithm. For search, there are n + 1 possible outputs, the n items and the result where no item is found, so there must be at least n + 1 leaves in the decision tree. Then the worst-case number of comparisons that must be made by any comparison search algorithm will be the height of the algorithm’s decision tree, i.e., the length of any longest root to leaf path. 2 Recitation 4 Exercise: Prove that the smallest height for any tree on n nodes is dlg(n + 1)e − 1 = Ω(log n). Solution: We show that the maximum number of nodes in any binary tree with height h is n ≤ T (h) = 2h+1 − 1, so h ≥ (lg(n + 1)) − 1. Proof by induction on h. The only tree of height zero has one node, so T (0) = 1, a base case satisfying the claim. The maximum number of nodes in a height-h tree must also have the maximum number of nodes in its two subtrees, so T (h) = 2T (h − 1) + 1. Substituting T (h) yields 2h+1 − 1 = 2(2h − 1) + 1, proving the claim. A tree with n + 1 leaves has more than n nodes, so its height is at least Ω(log n). Thus the min- imum number of comparisons needed to distinguish between the n items is at least Ω(log n), and the worst-case running time of any deterministic comparison search algorithm is at least Ω(log n)! So sorted arrays and balanced BSTs are able to support find(k) asymptotically optimally, in a comparison model of computation. Comparisons are very limiting because each operation performed can lead to at most constant branching factor in the decision tree. It doesn’t matter that comparisons have branching factor two; any ﬁxed constant branching factor will lead to a decision tree with at least Ω(log n) height. If we were not limited to comparisons, it opens up the possibility of faster-than-O(log n) search. More speciﬁcally, if we can use an operation that allows for asymptotically larger than constant ω(1) branching factor, then our decision tree could be shallower, leading to a faster algorithm. Direct Access Arrays Most operations within a computer only allow for constant logical branching, like if statements in your code. However, one operation on your computer allows for non-constant branching factor: speciﬁcally the ability to randomly access any memory address in constant time. This special oper- ation allows an algorithm’s decision tree to branch with large branching factor, as large as there is space in your computer. To exploit this operation, we deﬁne a data structure called a direct access array, which is a normal static array that associates a semantic meaning with each array index location: speciﬁcally that any item x with key k will be stored at array index k. This statement only makes sense when item keys are integers. Fortunately, in a computer, any thing in memory can be associated with an integer—for example, its value as a sequence of bits or its address in memory—so from now on we will only consider integer keys. Now suppose we want to store a set of n items, each associated with a unique integer key in the bounded range from 0 to some large number u − 1. We can store the items in a length u direct access array, where each array slot i contains an item associated with integer key i, if it exists. To ﬁnd an item having integer key i, a search algorithm can simply look in array slot i to respond to the search query in worst-case constant time! However, order operations on this data structure will be very slow: we have no guarantee on where the ﬁrst, last, or next element is in the direct access array, so we may have to spend u time for order operations. 3 Recitation 4 Worst-case constant time search comes at the cost of storage space: a direct access array must have a slot available for every possible key in range. When u is very large compared to the number of items being stored, storing a direct access array can be wasteful, or even impossible on modern machines. For example, suppose you wanted to support the set find(k) operation on ten-letter names using a direct access array. The space of possible names would be u ≈ 2610 ≈ 9.5 × 1013; even storing a bit array of that length would require 17.6 Terabytes of storage space. How can we overcome this obstacle? The answer is hashing! 1 class DirectAccessArray: 2 def __init__(self, u): self.A = [None] * u # O(u) 3 def find(self, k): return self.A[k] # O(1) 4 def insert(self, x): self.A[x.key] = x # O(1) 5 def delete(self, k): self.A[k] = None # O(1) 6 def find_next(self, k): 7 for i in range(k, len(self.A)): # O(u) 8 if A[i] is not None: 9 return A[i] 10 def find_max(self): 11 for i in range(len(self.A) - 1, -1, -1): # O(u) 12 if A[i] is not None: 13 return A[i] 14 def delete_max(self): 15 for i in range(len(self.A) - 1, -1, -1): # O(u) 16 x = A[i] 17 if x is not None: 18 A[i] = None 19 return x Hashing Is it possible to get the performance beneﬁts of a direct access array while using only linear O(n) space when n ≪ u? A possible solution could be to store the items in a smaller dynamic direct access array, with m = O(n) slots instead of u, which grows and shrinks like a dynamic array depending on the number of items stored. But to make this work, we need a function that maps item keys to different slots of the direct access array, h(k) : {0, . . . , u − 1} →{0, . . . , m − 1}. We call such a function a hash function or a hash map, while the smaller direct access array is called a hash table, and h(k) is the hash of integer key k. If the hash function happens to be injective over the n keys you are storing, i.e. no two keys map to the same direct access array index, then we will be able to support worst-case constant time search, as the hash table simply acts as a direct access array over the smaller domain m. 4 Recitation 4 Unfortunately, if the space of possible keys is larger than the number of array indices, i.e. m < u, then any hash function mapping u possible keys to m indices must map multiple keys to the same array index, by the pigeonhole principle. If two items associated with keys k1 and k2 hash to the same index, i.e. h(k1) = h(k2), we say that the hashes of k1 and k2 collide. If you don’t know in advance what keys will be stored, it is extremely unlikely that your choice of hash function will avoid collisions entirely1. If the smaller direct access array hash table can only store one item at each index, when collisions occur, where do we store the colliding items? Either we store collisions somewhere else in the same direct access array, or we store collisions somewhere else. The ﬁrst strategy is called open addressing, which is the way most hash tables are actually implemented, but such schemes can be difﬁcult to analyze. We will adopt the second strategy called chaining. Chaining Chaining is a collision resolution strategy where colliding keys are stored separately from the orig- inal hash table. Each hash table index holds a pointer to a chain, a separate data structure that sup- ports the dynamic set interface, speciﬁcally operations find(k), insert(x), and delete(k). It is common to implement a chain using a linked list or dynamic array, but any implementation will do, as long as each operation takes no more than linear time. Then to insert item x into the hash table, simply insert x into the chain at index h(x.key); and to find or delete a key k from the hash table, simply ﬁnd or delete k from the chain at index h(k). Ideally, we want chains to be small, because if our chains only hold a constant number of items, the dynamic set operations will run in constant time. But suppose we are unlucky in our choice of hash function, and all the keys we want to store has all of them to the same index location, into the same chain. Then the chain will have linear size, meaning the dynamic set operations could take linear time. A good hash function will try to minimize the frequency of such collisions in order to minimize the maximum size of any chain. So what’s a good hash function? Hash Functions Division Method (bad): The simplest mapping from an integer key domain of size u to a smaller one of size m is simply to divide the key by m and take the remainder: h(k) = (k mod m), or in Python, k % m. If the keys you are storing are uniformly distributed over the domain, the division method will distribute items roughly evenly among hashed indices, so we expect chains to have small size providing good performance. However, if all items happen to have keys with the same remainder when divided by m, then this hash function will be terrible. Ideally, the performance of our data structure would be independent of the keys we choose to store. 1If you know all of the keys you will want to store in advance, it is possible to design a hashing scheme that will always avoid collisions between those keys. This idea, called perfect hashing, follows from the Birthday Paradox. 5 Recitation 4 Universal Hashing (good): For a large enough key domain u, every hash function will be bad for some set of n inputs2. However, we can achieve good expected bounds on hash table performance by choosing our hash function randomly from a large family of hash functions. Here the expecta- tion is over our choice of hash function, which is independent of the input. This is not expectation over the domain of possible input keys. One family of hash functions that performs well is: n o H(m, p) = hab(k) = (((ak + b) mod p) mod m) a, b ∈{0, . . . , p − 1} and a 6= 0 , where p is a prime that is larger than the key domain u. A single hash function from this family is speciﬁed by choosing concrete values for a and b. This family of hash functions is universal3: for any two keys, the probability that their hashes will collide when hashed using a hash function chosen uniformly at random from the universal family, is no greater than 1/m, i.e. Pr {h(ki) = h(kj )} ≤ 1/m, ∀ki =6 kj ∈{0, . . . , u − 1}. h∈H If we know that a family of hash functions is universal, then we can upper bound the expected size of any chain, in expectation over our choice of hash function from the family. Let Xij be the indicator random variable representing the value 1 if keys ki and kj collide for a chosen hash function, and 0 otherwise. Then the random variable representing the number of items hashed to P index h(ki) will be the sum Xi = Xij over all keys kj from the set of n keys {k0, . . . , kn−1} j stored in the hash table. Then the expected number of keys hashed to the chain at index h(ki) is: ⎧ ⎫ ⎨ ⎬ X X X E {Xi} = E Xij ⎭ = E {Xij } = 1 + E {Xij } h∈H h∈H ⎩ h∈H h∈H j j j= 6 i X = 1 + (1) Pr {h(ki) = h(kj )} + (0) Pr {h(ki) =6 h(kj )} h∈H h∈H j6=i X ≤ 1 + 1/m = 1 + (n − 1)/m. j6=i If the size of the hash table is at least linear in the number of items stored, i.e. m = Ω(n), then the expected size of any chain will be 1 + (n − 1)/Ω(n) = O(1), a constant! Thus a hash table where collisions are resolved using chaining, implemented using a randomly chosen hash function from a universal family, will perform dynamic set operations in expected constant time, where the expectation is taken over the random choice of hash function, independent from the input keys! Note that in order to maintain m = O(n), insertion and deletion operations may require you to rebuild the direct access array to a different size, choose a new hash function, and reinsert all the items back into the hash table. This can be done in the same way as in dynamic arrays, leading to amortized bounds for dynamic operations. 2If u > nm, every hash function from u to m maps some n keys to the same hash, by the pigeonhole principle. 3The proof that this family is universal is beyond the scope of 6.006, though it is usually derived in 6.046. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 6 Recitation 4 class Hash_Table_Set: def __init__(self, r = 200): # O(1) self.chain_set = Set_from_Seq(Linked_List_Seq) self.A = [] self.size = 0 self.r = r # 100/self.r = fill ratio self.p = 2**31 - 1 self.a = randint(1, self.p - 1) self._compute_bounds() self._resize(0) def __len__(self): return self.size # O(1) def __iter__(self): # O(n) for X in self.A: yield from X def build(self, X): # O(n)e for x in X: self.insert(x) def _hash(self, k, m): # O(1) return ((self.a * k) % self.p) % m def _compute_bounds(self): # O(1) self.upper = len(self.A) self.lower = len(self.A) * 100*100 // (self.r*self.r) def _resize(self, n): # O(n) if (self.lower >= n) or (n >= self.upper): f = self.r // 100 if self.r % 100: f += 1 # f = ceil(r / 100) m = max(n, 1) * f A = [self.chain_set() for _ in range(m)] for x in self: h = self._hash(x.key, m) A[h].insert(x) self.A = A self._compute_bounds() def find(self, k): # O(1)e h = self._hash(k, len(self.A)) return self.A[h].find(k) def insert(self, x): # O(1)ae self._resize(self.size + 1) h = self._hash(x.key, len(self.A)) added = self.A[h].insert(x) if added: self.size += 1 return added 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 7 Recitation 4 def delete(self, k): # O(1)ae assert len(self) > 0 h = self._hash(k, len(self.A)) x = self.A[h].delete(k) self.size -= 1 self._resize(self.size) return x def find_min(self): # O(n) out = None for x in self: if (out is None) or (x.key < out.key): out = x return out def find_max(self): # O(n) out = None for x in self: if (out is None) or (x.key > out.key): out = x return out def find_next(self, k): # O(n) out = None for x in self: if x.key > k: if (out is None) or (x.key < out.key): out = x return out def find_prev(self, k): # O(n) out = None for x in self: if x.key < k: if (out is None) or (x.key > out.key): out = x return out def iter_order(self): # O(nˆ2) x = self.find_min() while x: yield x x = self.find_next(x.key) 8 Recitation 4 Data Structure Operations O(·) Container Static Dynamic Order build(X) find(k) insert(x) delete(k) find min() find max() find prev(k) find next(k) Array n n n n n Sorted Array n log n log n n 1 log n Direct Access Array u 1 1 u u Hash Table n(e) 1(e) 1(a)(e) n n Exercise Given an unsorted array A = [a0, . . . , an−1] containing n positive integers, the DUPLICATES prob- lem asks whether two integers in the array have the same value. 1) Describe a brute-force worst-case O(n2)-time algorithm to solve DUPLICATES. \u0000 \u0001 Solution: Loop through all n 2 = O(n2) pairs of integers from the array and check if they are equal in O(1) time. 2) Describe a worst-case O(n log n)-time algorithm to solve DUPLICATES. Solution: Sort the array in worst-case O(n log n) time (e.g. using merge sort), and then scan through the sorted array, returning if any of the O(n) adjacent pairs have the same value. 3) Describe an expected O(n)-time algorithm to solve DUPLICATES. Solution: Hash each of the n integers into a hash table (implemented using chaining and a hash function chosen randomly from a universal hash family4), with insertion taking expected O(1) time. When inserting an integer into a chain, check it against the other integers already in the chain, and return if another integer in the chain has the same value. Since each chain has expected O(1) size, this check takes expected O(1) time, so the algorithm runs in expected O(n) time. 4) If k < n and ai ≤ k for all ai ∈ A, describe a worst-case O(1)-time algorithm to solve DUPLICATES. Solution: If k < n, a duplicate always exists, by the pigeonhole principle. 5) If n ≤ k and ai ≤ k for all ai ∈ A, describe a worst-case O(k)-time algorithm to solve DUPLICATES. Solution: Insert each of the n integers into a direct access array of length k, which will take worst-case O(k) time to instantiate, and worst-case O(1) time per insert operation. If an integer already exists at an array index when trying to insert, then return that a duplicate exists. 4In 6.006, you do not have to specify these details when answering problems. You may simply quote that hash tables can achieve the expected/amortized bounds for operations described in class. MIT OpenCourseWare https://ocw.mit.edu 6.006 Introduction to Algorithms Spring 2020 For information about citing these materials or our Terms of Use, visit: https://ocw.mit.edu/terms Introduction to Algorithms: 6.006 Massachusetts Institute of Technology Instructors: Erik Demaine, Jason Ku, and Justin Solomon Recitation 5 Recitation 5 Comparison Sorting Last time we discussed a lower bound on search in a comparison model. We can use a similar analysis to lower bound the worst-case running time of any sorting algorithm that only uses com- parisons. There are n! possible outputs to a sorting algorithm: the n! permutations of the items. Then the decision tree for any deterministic sorting algorithm that uses only comparisons must have at least n! leaves, and thus (by the same analysis as the search decision tree) must have height that is at least Ω(log(n!)) = Ω(n log n) height1, leading to a running time of at least Ω(n log n). Direct Access Array Sort Just as with search, if we are not limited to comparison operations, it is possible to beat the Ω(n log n) bound. If the items to be sorted have unique keys from a bounded positive range {0, . . . , u − 1} (so n ≤ u), we can sort them simply by using a direct access array. Construct a direct access array with size u and insert each item x into index x.key. Then simply read through the direct access array from left to right returning items as they are found. Inserting takes time Θ(n) time while initializing and scanning the direct access array takes Θ(u) time, so this sorting algorithm runs in Θ(n + u) time. If u = O(n), then this algorithm is linear! Unfortunately, this sorting algorithm has two drawbacks: ﬁrst, it cannot handle duplicate keys, and second, it cannot handle large key ranges. 1 def direct_access_sort(A): 2 \"Sort A assuming items have distinct non-negative keys\" 3 u = 1 + max([x.key for x in A]) # O(n) find maximum key 4 D = [None] * u # O(u) direct access array 5 for x in A: # O(n) insert items 6 D[x.key] = x 7 i = 0 8 for key in range(u): # O(u) read out items in order 9 if D[key] is not None: 10 A[i] = D[key] 11 i += 1 1We can prove this directly via Stirling’s approximation, n! ≈ √ 2πn(n/e)n , or by observing that n! > (n/2)n/2 . 2 Recitation 5 Counting Sort To solve the ﬁrst problem, we simply link a chain to each direct access array index, just like in hashing. When multiple items have the same key, we store them both in the chain associated with their key. Later, it will be important that this algorithm be stable: that items with duplicate keys appear in the same order in the output as the input. Thus, we choose chains that will support a sequence queue interface to keep items in order, inserting to the end of the queue, and then returning items back in the order that they were inserted. 1 2 3 4 5 6 7 def counting_sort(A): \"Sort A assuming items have u = 1 + max([x.key for x in D = [[] for i in range(u)] for x in A: D[x.key].append(x) i = 0 non-negative keys\" A]) # O(n) # O(u) # O(n) find maximum key direct access array of chains insert into chain at x.key 8 9 for chain for x in in D: chain: # O(u) read out items in order 10 11 A[i] i += = 1 x Counting sort takes O(u) time to initialize the chains of the direct access array, O(n) time to insert all the elements, and then O(u) time to scan back through the direct access array to return the items; so the algorithm runs in O(n + u) time. Again, when u = O(n), then counting sort runs in linear time, but this time allowing duplicate keys. There’s another implementation of counting sort which just keeps track of how many of each key map to each index, and then moves each item only once, rather the implementation above which moves each item into a chain and then back into place. The implementation below computes the ﬁnal index location of each item via cumulative sums. 1 2 3 4 5 6 7 8 9 10 11 def counting_sort(A): \"Sort A assuming items have u = 1 + max([x.key for x in D = [0] * u for x in A: D[x.key] += 1 for k in range(1, u): D[k] += D[k - 1] for x in list(reversed(A)): A[D[x.key] - 1] = x D[x.key] -= 1 non-negative keys\" A]) # O(n) # O(u) # O(n) # O(u) # O(n) find maximum key direct access array count keys cumulative sums move items into place Now what if we want to sort keys from a larger integer range? Our strategy will be to break up integer keys into parts, and then sort each part! In order to do that, we will need a sorting strategy to sort tuples, i.e. multiple parts. 3 Recitation 5 Tuple Sort Suppose we want to sort tuples, each containing many different keys (e.g. x.k1, x.k2, x.k3, . . .), so that the sort is lexicographic with respect to some ordering of the keys (e.g. that key k1 is more important than key k2 is more important than key k3, etc.). Then tuple sort uses a stable sorting algorithm as a subroutine to repeatedly sort the objects, ﬁrst according to the least important key, then the second least important key, all the way up to most important key, thus lexicographically sorting the objects. Tuple sort is similar to how one might sort on multiple rows of a spreadsheet by different columns. However, tuple sort will only be correct if the sorting from previous rounds are maintained in future rounds. In particular, tuple sort requires the subroutine sorting algorithms be stable. Radix Sort Now, to increase the range of integer sets that we can sort in linear time, we break each integer up into its multiples of powers of n, representing each item key its sequence of digits when represented in base n. If the integers are non-negative and the largest integer in the set is u, then this base n number will have dlogn ue digits. We can think of these digit representations as tuples and sort them with tuple sort by sorting on each digit in order from least signiﬁcant to most signiﬁcant digit using counting sort. This combination of tuple sort and counting sort is called radix sort. If the c largest integer in the set u ≤ n , then radix sort runs in O(nc) time. Thus, if c is constant, then radix sort also runs in linear time! 1 def radix_sort(A): 2 \"Sort A assuming items have non-negative keys\" 3 n = len(A) 4 u = 1 + max([x.key for x in A]) # O(n) find maximum key 5 c = 1 + (u.bit_length() // n.bit_length()) 6 class Obj: pass 7 D = [Obj() for a in A] 8 for i in range(n): # O(nc) make digit tuples 9 D[i].digits = [] 10 D[i].item = A[i] 11 high = A[i].key 12 for j in range(c): # O(c) make digit tuple 13 high, low = divmod(high, n) 14 D[i].digits.append(low) 15 for i in range(c): # O(nc) sort each digit 16 for j in range(n): # O(n) assign key i to tuples 17 D[j].key = D[j].digits[i] 18 counting_sort(D) # O(n) sort on digit i 19 for i in range(n): # O(n) output to A 20 A[i] = D[i].item We’ve made a CoffeeScript Counting/Radix sort visualizer which you can ﬁnd here: https://codepen.io/mit6006/pen/LgZgrd 4 Recitation 5 Exercises 1) Sort the following integers using a base-10 radix sort. (329, 457, 657, 839, 436, 720, 355) −→ (329, 355, 436, 457, 657, 720, 839) 2) Describe a linear time algorithm to sort n integers from the range [−n2 , . . . , n3]. Solution: Add n2 to each number so integers are all positive, apply Radix sort, and then subtract n2 from each element of the output. 3) Describe a linear time algorithm to sort a set n of strings, each having k English characters. Solution: Use tuple sort to repeatedly sort the strings by each character from right to left with counting sort, using the integers {0, . . . , 25} to represent the English alphabet. There are k rounds of counting sort, and each round takes Θ(n + 26) = Θ(n) time, thus the algorithm runs in Θ(nk) time. This running time is linear because the input size is Θ(nk). MIT OpenCourseWare https://ocw.mit.edu 6.006 Introduction to Algorithms Spring 2020 For information about citing these materials or our Terms of Use, visit: https://ocw.mit.edu/terms Introduction to Algorithms: 6.006 Massachusetts Institute of Technology Instructors: Erik Demaine, Jason Ku, and Justin Solomon Recitation 6 Recitation 6 Binary Trees A binary tree is a tree (a connected graph with no cycles) of binary nodes: a linked node con- tainer, similar to a linked list node, having a constant number of ﬁelds: • a pointer to an item stored at the node, • a pointer to a parent node (possibly None), • a pointer to a left child node (possibly None), and • a pointer to a right child node (possibly None). 1 class Binary_Node: 2 def __init__(A, x): # O(1) 3 A.item = x 4 A.left = None 5 A.right = None 6 A.parent = None 7 # A.subtree_update() # wait for R07! Why is a binary node called “binary”? In actuality, a binary node can be connected to three other nodes (its parent, left child, and right child), not just two. However, we will differentiate a node’s parent from it’s children, and so we call the node “binary” based on the number of children the node has. A binary tree has one node that that is the root of the tree: the only node in the tree lacking a parent. All other nodes in the tree can reach the root of the tree containing them by traversing parent pointers. The set of nodes passed when traversing parent pointers from node <X> back to the root are called the ancestors for <X> in the tree. The depth of a node <X> in the subtree rooted at <R> is the length of the path from <X> back to <R>. The height of node <X> is the maximum depth of any node in the subtree rooted at <X>. If a node has no children, it is called a leaf. Why would we want to store items in a binary tree? The difﬁculty with a linked list is that many linked-list nodes can be O(n) pointer hops away from the head of the list, so it may take O(n) time to reach them. By contrast, as we’ve seen in earlier recitations, it is possible to construct a binary tree on n nodes such that no node is more than O(log n) pointer hops away from the root, i.e., there exist binary trees with logarithmic height. The power of a binary tree structure is if we can keep the height h of the tree low, i.e., O(log n), and only perform operations on the tree that run in time on the order of the height of the tree, then these operations will run in O(h) = O(log n) time (which is much closer to O(1) than to O(n)). 2 Recitation 6 Traversal Order The nodes in a binary tree have a natural order based on the fact that we distinguish one child to be left and one child to be right. We deﬁne a binary tree’s traversal order based on the following implicit characterization: • every node in the left subtree of node <X> comes before <X> in the traversal order; and • every node in the right subtree of node <X> comes after <X> in the traversal order. Given a binary node <A>, we can list the nodes in <A>’s subtree by recursively listing the nodes in <A>’s left subtree, listing <A> itself, and then recursively listing the nodes in <A>’s right subtree. This algorithm runs in O(n) time because every node is recursed on once doing constant work. 1 def subtree_iter(A): # O(n) 2 if A.left: yield from A.left.subtree_iter() 3 yield A 4 if A.right: yield from A.right.subtree_iter() Right now, there is no semantic connection between the items being stored and the traversal order of the tree. Next time, we will provide two different semantic meanings to the traversal order (one of which will lead to an efﬁcient implementation of the Sequence interface, and the other will lead to an efﬁcient implementation of the Set interface), but for now, we will just want to preserve the traversal order as we manipulate the tree. Tree Navigation Given a binary tree, it will be useful to be able to navigate the nodes in their traversal order efﬁ- ciently. Probably the most straight forward operation is to ﬁnd the node in a given node’s subtree that appears ﬁrst (or last) in traversal order. To ﬁnd the ﬁrst node, simply walk left if a left child exists. This operation takes O(h) time because each step of the recursion moves down the tree. Find the last node in a subtree is symmetric. 1 def subtree_first(A): # O(h) 2 if A.left: return A.left.subtree_first() 3 else: return A 4 5 def subtree_last(A): # O(h) 6 if A.right: return A.right.subtree_last() 7 else: return A Given a node in a binary tree, it would also be useful too ﬁnd the next node in the traversal order, i.e., the node’s successor, or the previous node in the traversal order, i.e., the node’s predecessor. To ﬁnd the successor of a node <A>, if <A> has a right child, then <A>’s successor will be the ﬁrst node in the right child’s subtree. Otherwise, <A>’s successor cannot exist in <A>’s subtree, so we walk up the tree to ﬁnd the lowest ancestor of <A> such that <A> is in the ancestor’s left subtree. 3 Recitation 6 In the ﬁrst case, the algorithm only walks down the tree to ﬁnd the successor, so it runs in O(h) time. Alternatively in the second case, the algorithm only walks up the tree to ﬁnd the successor, so it also runs in O(h) time. The predecessor algorithm is symmetric. 1 def successor(A): # O(h) 2 if A.right: return A.right.subtree_first() 3 while A.parent and (A is A.parent.right): 4 A = A.parent 5 return A.parent 6 7 def predecessor(A): # O(h) 8 if A.left: return A.left.subtree_last() 9 while A.parent and (A is A.parent.left): 10 A = A.parent 11 return A.parent Dynamic Operations If we want to add or remove items in a binary tree, we must take care to preserve the traversal order of the other items in the tree. To insert a node <B> before a given node <A> in the traversal order, either node <A> has a left child or not. If <A> does not have a left child, than we can simply add <B> as the left child of <A>. Otherwise, if <A> has a left child, we can add <B> as the right child of the last node in <A>’s left subtree (which cannot have a right child). In either case, the algorithm walks down the tree at each step, so the algorithm runs in O(h) time. Inserting after is symmetric. 1 def subtree_insert_before(A, B): # O(h) 2 if A.left: 3 A = A.left.subtree_last() 4 A.right, B.parent = B, A 5 else: 6 A.left, B.parent = B, A 7 # A.maintain() # wait for R07! 8 9 def subtree_insert_after(A, B): # O(h) 10 if A.right: 11 A = A.right.subtree_first() 12 A.left, B.parent = B, A 13 else: 14 A.right, B.parent = B, A 15 # A.maintain() # wait for R07! To delete the item contained in a given node from its binary tree, there are two cases based on whether the node storing the item is a leaf. If the node is a leaf, then we can simply clear the child pointer from the node’s parent and return the node. Alternatively, if the node is not a leaf, we can swap the node’s item with the item in the node’s successor or predecessor down the tree until the item is in a leaf which can be removed. Since swapping only occurs down the tree, again this operation runs in O(h) time. 4 Recitation 6 1 def subtree_delete(A): # O(h) 2 if A.left or A.right: # A is not a leaf 3 if A.left: B = A.predecessor() 4 else: B = A.successor() 5 A.item, B.item = B.item, A.item 6 return B.subtree_delete() 7 if A.parent: # A is a leaf 8 if A.parent.left is A: A.parent.left = None 9 else: A.parent.right = None 10 # A.parent.maintain() # wait for R07! 11 return A Binary Node Full Implementation 1 class Binary_Node: 2 def __init__(A, x): # O(1) 3 A.item = x 4 A.left = None 5 A.right = None 6 A.parent = None 7 # A.subtree_update() # wait for R07! 8 9 def subtree_iter(A): # O(n) 10 if A.left: yield from A.left.subtree_iter() 11 yield A 12 if A.right: yield from A.right.subtree_iter() 13 14 def subtree_first(A): # O(h) 15 if A.left: return A.left.subtree_first() 16 else: return A 17 18 def subtree_last(A): # O(h) 19 if A.right: return A.right.subtree_last() 20 else: return A 21 22 def successor(A): # O(h) 23 if A.right: return A.right.subtree_first() 24 while A.parent and (A is A.parent.right): 25 A = A.parent 26 return A.parent 27 28 def predecessor(A): # O(h) 29 if A.left: return A.left.subtree_last() 30 while A.parent and (A is A.parent.left): 31 A = A.parent 32 return A.parent 33 5 Recitation 6 34 def subtree_insert_before(A, B): # O(h) 35 if A.left: 36 A = A.left.subtree_last() 37 A.right, B.parent = B, A 38 else: 39 A.left, B.parent = B, A 40 # A.maintain() # wait for R07! 41 42 def subtree_insert_after(A, B): # O(h) 43 if A.right: 44 A = A.right.subtree_first() 45 A.left, B.parent = B, A 46 else: 47 A.right, B.parent = B, A 48 # A.maintain() # wait for R07! 49 50 def subtree_delete(A): # O(h) 51 if A.left or A.right: 52 if A.left: B = A.predecessor() 53 else: B = A.successor() 54 A.item, B.item = B.item, A.item 55 return B.subtree_delete() 56 if A.parent: 57 if A.parent.left is A: A.parent.left = None 58 else: A.parent.right = None 59 # A.parent.maintain() # wait for R07! 60 return A Top-Level Data Structure All of the operations we have deﬁned so far have been within the Binary Tree class, so that they apply to any subtree. Now we can ﬁnally deﬁne a general Binary Tree data structure that stores a pointer to its root, and the number of items it stores. We can implement the same operations with a little extra work to keep track of the root and size. 1 class Binary_Tree: 2 def __init__(T, Node_Type = Binary_Node): 3 T.root = None 4 T.size = 0 5 T.Node_Type = Node_Type 6 7 def __len__(T): return T.size 8 def __iter__(T): 9 if T.root: 10 for A in T.root.subtree_iter(): 11 yield A.item 6 Recitation 6 Exercise: Given an array of items A = (a0, . . . , an−1), describe a O(n)-time algorithm to con- struct a binary tree T containing the items in A such that (1) the item stored in the ith node of T ’s traversal order is item ai, and (2) T has height O(log n). Solution: Build T by storing the middle item in a root node, and then recursively building the remaining left and right halves in left and right subtrees. This algorithm satisﬁes property (1) by deﬁnition of traversal order, and property (2) because the height roughly follows the recurrence H(n) = 1 + H(n/2). The algorithm runs in O(n) time because every node is recursed on once doing constant work. 1 2 3 4 5 6 def build(X): A = [x for x in X] def build_subtree(A, i, j): c = (i + j) // 2 root = self.Node_Type(A[c]) if i < c: # needs to store more items in left subtree 7 8 9 10 11 12 root.left = build_subtree(A, i, c - 1) root.left.parent = root if c < j: # needs to store more root.right = build_subtree(A, c + 1, j) root.right.parent = root return root items in right subtree 13 self.root = build_subtree(A, 0, len(A)-1) Exercise: Argue that the following iterative procedure to return the nodes of a tree in traversal order takes O(n) time. 1 2 3 def tree_iter(T): node = T.subtree_first() while node: 4 5 yield node node = node.successor() Solution: This procedure walks around the tree traversing each edge of the tree twice: once going down the tree, and once going back up. Then because the number of edges in a tree is one fewer than the number of nodes, the traversal takes O(n) time. 7 Recitation 6 Application: Set To use a Binary Tree to implement a Set interface, we use the traversal order of the tree to store the items sorted in increasing key order. This property is often called the Binary Search Tree Prop- erty, where keys in a node’s left subtree are less than the key stored at the node, and keys in the node’s right subtree are greater than the key stored at the node. Then ﬁnding the node containing a query key (or determining that no node contains the key) can be done by walking down the tree, recursing on the appropriate side. Exercise: Make a Set Binary Tree (Binary Search Tree) by inserting student-chosen items one by one, then searching and/or deleting student-chosen keys one by one. 1 class BST_Node(Binary_Node): 2 def subtree_find(A, k): # O(h) 3 if k < A.item.key: 4 if A.left: return A.left.subtree_find(k) 5 elif k > A.item.key: 6 if A.right: return A.right.subtree_find(k) 7 else: return A 8 return None 9 10 def subtree_find_next(A, k): # O(h) 11 if A.item.key <= k: 12 if A.right: return A.right.subtree_find_next(k) 13 else: return None 14 elif A.left: 15 B = A.left.subtree_find_next(k) 16 if B: return B 17 return A 18 19 def subtree_find_prev(A, k): # O(h) 20 if A.item.key >= k: 21 if A.left: return A.left.subtree_find_prev(k) 22 else: return None 23 elif A.right: 24 B = A.right.subtree_find_prev(k) 25 if B: return B 26 return A 27 28 def subtree_insert(A, B): # O(h) 29 if B.item.key < A.item.key: 30 if A.left: A.left.subtree_insert(B) 31 else: A.subtree_insert_before(B) 32 elif B.item.key > A.item.key: 33 if A.right: A.right.subtree_insert(B) 34 else: A.subtree_insert_after(B) 35 else: A.item = B.item 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 8 Recitation 6 class Set_Binary_Tree(Binary_Tree): # Binary Search Tree def __init__(self): super().__init__(BST_Node) def iter_order(self): yield from self def build(self, X): for x in X: self.insert(x) def find_min(self): if self.root: def find_max(self): if self.root: def find(self, k): if self.root: return self.root.subtree_first().item return self.root.subtree_last().item node = self.root.subtree_find(k) if node: return node.item def find_next(self, k): if self.root: node = self.root.subtree_find_next(k) if node: return node.item def find_prev(self, k): if self.root: node = self.root.subtree_find_prev(k) if node: return node.item def insert(self, x): new_node = self.Node_Type(x) if self.root: self.root.subtree_insert(new_node) if new_node.parent is None: return False else: self.root = new_node self.size += 1 return True def delete(self, k): assert self.root node = self.root.subtree_find(k) assert node ext = node.subtree_delete() if ext.parent is None: self.root = None self.size -= 1 return ext.item MIT OpenCourseWare https://ocw.mit.edu 6.006 Introduction to Algorithms Spring 2020 For information about citing these materials or our Terms of Use, visit: https://ocw.mit.edu/terms Introduction to Algorithms: 6.006 Massachusetts Institute of Technology Instructors: Erik Demaine, Jason Ku, and Justin Solomon Recitation 7 Recitation 7 Balanced Binary Trees Previously, we discussed binary trees as a general data structure for storing items, without bound- ing the maximum height of the tree. The ultimate goal will be to keep our tree balanced: a tree on n nodes is balanced if its height is O(log n). Then all the O(h)-time operations we talked about last time will only take O(log n) time. There are many ways to keep a binary tree balanced under insertions and deletions (Red-Black Trees, B-Trees, 2-3 Trees, Splay Trees, etc.). The oldest (and perhaps simplest) method is called an AVL Tree. Every node of an AVL Tree is height-balanced (i.e., satisﬁes the AVL Property) where the left and right subtrees of a height-balanced node differ in height by at most 1. To put it a different way, deﬁne the skew of a node to be the height of its right subtree minus the height of its left subtree (where the height of an empty subtree is −1. Then a node is height-balanced if it’s skew is either −1, 0, or 1. A tree is height-balanced if every node in the tree is height-balanced. Height-balance is good because it implies balance! Exercise: A height-balanced tree is balanced. Solution: Balanced means that h = O(log n). Equivalently, balanced means that log n is lower bounded by Ω(h) so that n = 2Ω(h). So if we can show the minimum number of nodes in a height- balanced tree is at least exponential in h, then it must also be balanced. Let F (h) denote the fewest nodes in any height-balanced tree of height h. Then F (h) satisﬁes the recurrence: F (h) = 1 + F (h − 1) + F (h − 2) ≥ 2F (h − 2), since the subtrees of the root’s children should also contain the fewest nodes. As base cases, the fewest nodes in a height-balanced tree of height 0 is one, i.e., F (0) = 1, while the fewest nodes in a height-balanced tree of height 1 is two, i.e., F (1) = 2. Then this recurrence is lower bounded by F (h) ≥ 2h/2 = 2Ω(h) as desired. Recitation 7 2 Rotations As we add or remove nodes to our tree, it is possible that our tree will become imbalanced. We will want to change the structure of the tree without changing its traversal order, in the hopes that we can make the tree’s structure more balanced. We can change the structure of a tree using a local operation called a rotation. A rotation takes a subtree that locally looks like one the following two conﬁgurations and modiﬁes the connections between nodes in O(1) time to transform it into the other conﬁguration. 1 _____<D>__ rotate_right(<D>) __<B>_____ 2 __<B>__ <E> => <A> __<D>__ 3 <A> <C> / \\ / \\ <C> <E> 4 / \\ / \\ /___\\ <= /___\\ / \\ / \\ 5 /___\\ /___\\ rotate_left(<B>) /___\\ /___\\ This operation preserves the traversal order of the tree while changing the depth of the nodes in subtrees <A> and <E>. Next time, we will use rotations to enforce that a balanced tree stays balanced after inserting or deleting a node. 1 def subtree_rotate_right(D): def subtree_rotate_left(B): # O(1) 2 assert D.left assert B.right 3 B, E = D.left, D.right A, D = B.left, B.right 4 A, C = B.left, B.right C, E = D.left, D.right 5 D, B = B, D B, D = D, B 6 D.item, B.item = B.item, D.item B.item, D.item = D.item, B.item 7 B.left, B.right = A, D D.left, D.right = B, E 8 D.left, D.right = C, E B.left, B.right = A, C 9 if A: A.parent = B if A: A.parent = B 10 if E: E.parent = D if E: E.parent = D 11 # B.subtree_update() # B.subtree_update() # wait for R07! 12 # D.subtree_update() # D.subtree_update() # wait for R07! Maintaining Height-Balance Suppose we have a height-balanced AVL tree, and we perform a single insertion or deletion by adding or removing a leaf. Either the resulting tree is also height-balanced, or the change in leaf has made at least one node in the tree have magnitude of skew greater than 1. In particular, the only nodes in the tree whose subtrees have changed after the leaf modiﬁcation are ancestors of that leaf (at most O(h) of them), so these are the only nodes whose skew could have changed and they could have changed by at most 1 to have magnitude at most 2. As shown in lecture via a brief case analysis, given a subtree whose root has skew is 2 and every other node in its subtree is height-balanced, we can restore balance to the subtree in at most two rotations. Thus to rebalance the entire tree, it sufﬁces to walk from the leaf to the root, rebalancing each node along the way, performing at most O(log n) rotations in total. A detailed proof is outlined in the lecture notes and is not repeated here; but the proof may be reviewed in recitation if students would like to see the 3 Recitation 7 full argument. Below is code to implement the rebalancing algorithm presented in lecture. 1 def skew(A): # O(?) 2 return height(A.right) - height(A.left) 3 4 def rebalance(A): # O(?) 5 if A.skew() == 2: 6 if A.right.skew() < 0: 7 A.right.subtree_rotate_right() 8 A.subtree_rotate_left() 9 elif A.skew() == -2: 10 if A.left.skew() > 0: 11 A.left.subtree_rotate_left() 12 A.subtree_rotate_right() 13 14 def maintain(A): # O(h) 15 A.rebalance() 16 A.subtree_update() 17 if A.parent: A.parent.maintain() Unfortunately, it’s not clear how to efﬁciently evaluate the skew of a a node to determine whether or not we need to perform rotations, because computing a node’s height naively takes time linear in the size of the subtree. The code below to compute height recurses on every node in <A>’s subtree, so takes at least Ω(n) time. 1 def height(A): # Omega(n) 2 if A is None: return -1 3 return 1 + max(height(A.left), height(A.right)) Rebalancing requires us to check at least Ω(log n) heights in the worst-case, so if we want rebal- ancing the tree to take at most O(log n) time, we need to be able to evaluate the height of a node in O(1) time. Instead of computing the height of a node every time we need it, we will speed up computation via augmentation: in particular each node stores and maintains the value of its own subtree height. Then when we’re at a node, evaluating its height is a simple as reading its stored value in O(1) time. However, when the structure of the tree changes, we will need to update and recompute the height at nodes whose height has changed. 1 def height(A): 2 if A: return A.height 3 else: return -1 1 def subtree_update(A): # O(1) 2 A.height = 1 + max(height(A.left), height(A.right)) In the dynamic operations presented in R06, we put commented code to call update on every node whose subtree changed during insertions, deletions, or rotations. A rebalancing insertion or dele- tion operation only calls subtree update on at most O(log n) nodes, so as long as updating a 4 Recitation 7 node takes at most O(1) time to recompute augmentations based on the stored augmentations of the node’s children, then the augmentations can be maintained during rebalancing in O(log n) time. In general, the idea behind augmentation is to store additional information at each node so that information can be queried quickly in the future. You’ve done some augmentation already in PS1, where you augmented a singly-linked list with back pointers to make it faster to evaluate a node’s predecessor. To augment the nodes of a binary tree with a subtree property P(<X>), you need to: • clearly deﬁne what property of <X>’s subtree corresponds to P(<X>), and • show how to compute P(<X>) in O(1) time from the augmentations of <X>’s children. If you can do that, then you will be able to store and maintain that property at each node without affecting the O(log n) running time of rebalancing insertions and deletions. We’ve shown how to traverse around a binary tree and perform insertions and deletions, each in O(h) time while also maintaining height-balance so that h = O(log n). Now we are ﬁnally ready to implement an efﬁcient Sequence and Set. Binary Node Implementation with AVL Balancing 1 def height(A): 2 if A: return A.height 3 else: return -1 4 5 class Binary_Node: 6 def __init__(A, x): # O(1) 7 A.item = x 8 A.left = None 9 A.right = None 10 A.parent = None 11 A.subtree_update() 12 13 def subtree_update(A): # O(1) 14 A.height = 1 + max(height(A.left), height(A.right)) 15 16 def skew(A): # O(1) 17 return height(A.right) - height(A.left) 18 19 def subtree_iter(A): # O(n) 20 if A.left: yield from A.left.subtree_iter() 21 yield A 22 if A.right: yield from A.right.subtree_iter() 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 5 Recitation 7 def subtree_first(A): # O(log n) if A.left: return A.left.subtree_first() else: return A def subtree_last(A): # O(log n) if A.right: return A.right.subtree_last() else: return A def successor(A): # O(log n) if A.right: return A.right.subtree_first() while A.parent and (A is A.parent.right): A = A.parent return A.parent def predecessor(A): # O(log n) if A.left: return A.left.subtree_last() while A.parent and (A is A.parent.left): A = A.parent return A.parent def subtree_insert_before(A, B): # O(log n) if A.left: A = A.left.subtree_last() A.right, B.parent = B, A else: A.left, B.parent = B, A A.maintain() def subtree_insert_after(A, B): # O(log n) if A.right: A = A.right.subtree_first() A.left, B.parent = B, A else: A.right, B.parent = B, A A.maintain() def subtree_delete(A): # O(log n) if A.left or A.right: if A.left: B = A.predecessor() else: B = A.successor() A.item, B.item = B.item, A.item return B.subtree_delete() if A.parent: if A.parent.left is A: A.parent.left = None else: A.parent.right = None A.parent.maintain() return A 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 6 Recitation 7 def subtree_rotate_right(D): # O(1) assert D.left B, E = D.left, D.right A, C = B.left, B.right D, B = B, D D.item, B.item = B.item, D.item B.left, B.right = A, D D.left, D.right = C, E if A: A.parent = B if E: E.parent = D B.subtree_update() D.subtree_update() def subtree_rotate_left(B): # O(1) assert B.right A, D = B.left, B.right C, E = D.left, D.right B, D = D, B B.item, D.item = D.item, B.item D.left, D.right = B, E B.left, B.right = A, C if A: A.parent = B if E: E.parent = D B.subtree_update() D.subtree_update() def rebalance(A): # O(1) if A.skew() == 2: if A.right.skew() < 0: A.right.subtree_rotate_right() A.subtree_rotate_left() elif A.skew() == -2: if A.left.skew() > 0: A.left.subtree_rotate_left() A.subtree_rotate_right() def maintain(A): # O(log n) A.rebalance() A.subtree_update() if A.parent: A.parent.maintain() 7 Recitation 7 Application: Set Using our new deﬁnition of Binary Node that maintains balance, the implementation presented in R06 of the Binary Tree Set immediately supports all operations in h = O(log n) time, except build(X) and iter() which run in O(n log n) and O(n) time respectively. This data structure is what’s normally called an AVL tree, but what we will call a Set AVL. Application: Sequence To use a Binary Tree to implement a Sequence interface, we use the traversal order of the tree to store the items in Sequence order. Now we need a fast way to ﬁnd the ith item in the sequence because traversal would take O(n) time. If we knew how many items were stored in our left subtree, we could compare that size to the index we are looking for and recurse on the appropriate side. In order to evaluate subtree size efﬁciently, we augment each node in the tree with the size of its subtree. A node’s size can be computed in constant time given the sizes of its children by summing them and adding 1. 1 class Size_Node(Binary_Node): 2 def subtree_update(A): # O(1) 3 super().subtree_update() 4 A.size = 1 5 if A.left: A.size += A.left.size 6 if A.right: A.size += A.right.size 7 8 def subtree_at(A, i): # O(h) 9 assert 0 <= i 10 if A.left: L_size = A.left.size 11 else: L_size = 0 12 if i < L_size: return A.left.subtree_at(i) 13 elif i > L_size: return A.right.subtree_at(i - L_size - 1) 14 else: return A Once we are able to ﬁnd the ith node in a balanced binary tree in O(log n) time, the remainder of the Sequence interface operations can be implemented directly using binary tree operations. Fur- ther, via the ﬁrst exercise in R06, we can build such a tree from an input sequence in O(n) time. We call this data structure a Sequence AVL. Implementations of both the Sequence and Set interfaces can be found on the following pages. We’ve made a CoffeeScript Balanced Binary Search Tree visualizer which you can ﬁnd here: https://codepen.io/mit6006/pen/NOWddZ 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 8 Recitation 7 class Seq_Binary_Tree(Binary_Tree): def __init__(self): super().__init__(Size_Node) def build(self, X): def build_subtree(X, i, j): c = (i + j) // 2 root = self.Node_Type(A[c]) if i < c: root.left = build_subtree(X, i, c - 1) root.left.parent = root if c < j: root.right = build_subtree(X, c + 1, j) root.right.parent = root root.subtree_update() return root self.root = build_subtree(X, 0, len(X) - 1) self.size = self.root.size def get_at(self, i): assert self.root return self.root.subtree_at(i).item def set_at(self, i, x): assert self.root self.root.subtree_at(i).item = x def insert_at(self, i, x): new_node = self.Node_Type(x) if i == 0: if self.root: node = self.root.subtree_first() node.subtree_insert_before(new_node) else: self.root = new_node else: node = self.root.subtree_at(i - 1) node.subtree_insert_after(new_node) self.size += 1 def delete_at(self, i): assert self.root node = self.root.subtree_at(i) ext = node.subtree_delete() if ext.parent is None: self.root = None self.size -= 1 return ext.item def insert_first(self, x): self.insert_at(0, x) def delete_first(self): return self.delete_at(0) def insert_last(self, x): self.insert_at(len(self), x) def delete_last(self): return self.delete_at(len(self) - 1) 9 Recitation 7 Exercise: Make a Sequence AVL Tree or Set AVL Tree (Balanced Binary Search Tree) by inserting student chosen items one by one. If any node becomes height-imbalanced, rebalance its ancestors going up the tree. Here’s a Sequence AVL Tree example that may be instructive (remember to update subtree heights and sizes as you modify the tree!). 1 T = Seq_Binary_Tree() 2 T.build([10,6,8,5,1,3]) 3 T.get_at(4) 4 T.set_at(4, -4) 5 T.insert_at(4, 18) 6 T.insert_at(4, 12) 7 T.delete_at(2) Solution: 1 2 3 4 5 6 Line # Result 1 None | 2,3 | 4 | 5 | | | | ___8__ | ___8___ | ___8_____ | 10_ _1_ | 10_ _-4_ | 10_ ___-4_ | 6 5 3 | 6 5 3 | 6 5__ 3 | | | 18 | 6 | | ___8 | 10_ | 6 | _______ ____-4_ _12__ 3 5 18 | | | | | | 7 __1 __6_ 10 5 2____ __-4_ 18 3 7 8 Also labeled with subtree height H, size #: 9 10 None 11 12 13 ___________8H2#6__________ 10H1#2_____ _____1H1#3_____ 6H0#1 5H0#1 3H0#1 14 15 16 17 ___________8H2#6__________ 10H1#2_____ _____1H1#3_____ 6H0#1 5H0#1 3H0#1 18 19 20 21 ___________8H2#6___________ 10H1#2_____ _____-4H1#3_____ 6H0#1 5H0#1 3H0#1 22 23 24 25 26 ___________8H3#7_________________ 10H1#2_____ ___________-4H2#4_____ 6H0#1 5H1#2______ 3H0#1 18H0#1 27 28 29 30 31 ___________8H3#8_______________________ 10H1#2_____ ____________-4H2#5_____ 6H0#1 _____12H1#3______ 3H0#1 5H0#1 18H0#1 32 33 34 35 __________12H2#7____________ ______6H1#3_____ ______-4H1#3_____ 10H0#1 5H0#1 18H0#1 3H0#1 10 Recitation 7 Exercise: Maintain a sequence of n bits that supports two operations, each in O(log n) time: • flip(i): ﬂip the bit at index i • count ones upto(i): return the number of bits in the preﬁx up to index i that are one Solution: Maintain a Sequence Tree storing the bits as items, augmenting each node A with A.subtree ones, the number of 1 bits in its subtree. We can maintain this augmentation in O(1) time from the augmentations stored at its children. 1 2 3 4 5 6 def update(A): A.subtree_ones = A.item if A.left: A.subtree_ones += A.left.subtree_ones if A.right: A.subtree_ones += A.right.subtree_ones To implement flip(i), ﬁnd the ith node A using subtree node at(i) and ﬂip the bit stored at A.item. Then update the augmentation at A and every ancestor of A by walking up the tree in O(log n) time. To implement count ones upto(i), we will ﬁrst deﬁne the subtree-based recursive function subtree count ones upto(A, i) which returns the number of 1 bits in the subtree of node A that are at most index i within A’s subtree. Then count ones upto(i) is symantically equiv- ilent to subtree count ones upto(T.root, i). Since each recursive call makes at most one recursive call on a child, operation takes O(log n) time. 1 2 3 4 5 6 7 8 9 10 11 12 13 def subtree_count_ones_upto(A, i): assert 0 <= i < A.size out = 0 if A.left: if i < A.left.size: return subtree_count_ones_upto(A.left, i) out += A.left.subtree_ones i -= A.left.size out += A.item if i > 0: assert A.right out += subtree_count_ones_upto(A.right, i - 1) return out MIT OpenCourseWare https://ocw.mit.edu 6.006 Introduction to Algorithms Spring 2020 For information about citing these materials or our Terms of Use, visit: https://ocw.mit.edu/terms Introduction to Algorithms: 6.006 Massachusetts Institute of Technology Instructors: Erik Demaine, Jason Ku, and Justin Solomon Recitation 8 Recitation 8 Priority Queues Priority queues provide a general framework for at least three sorting algorithms, which differ only in the data structure used in the implementation. algorithm data structure insertion extraction total Selection Sort Insertion Sort Heap Sort Array Sorted Array Binary Heap O(1) O(n) O(log n) O(n) O(1) O(log n) O(n2) 2) O(n O(n log n) Let’s look at Python code that implements these priority queues. We start with an abstract base class that has the interface of a priority queue, maintains an internal array A of items, and trivially implements insert(x) and delete max() (the latter being incorrect on its own, but useful for subclasses). 1 class PriorityQueue: 2 def __init__(self): 3 self.A = [] 4 5 def insert(self, x): 6 self.A.append(x) 7 8 def delete_max(self): 9 if len(self.A) < 1: 10 raise IndexError(’pop from empty priority queue’) 11 return self.A.pop() # NOT correct on its own! 12 13 @classmethod 14 def sort(Queue, A): 15 pq = Queue() # make empty priority queue 16 for x in A: # n x T_insert 17 pq.insert(x) 18 out = [pq.delete_max() for _ in A] # n x T_delete_max 19 out.reverse() 20 return out Shared across all implementations is a method for sorting, given implementations of insert and delete max. Sorting simply makes two loops over the array: one to insert all the elements, and another to populate the output array with successive maxima in reverse order. 2 Recitation 8 Array Heaps We showed implementations of selection sort and merge sort previously in recitation. Here are implementations from the perspective of priority queues. If you were to unroll the organization of this code, you would have essentially the same code as we presented before. 1 class PQ_Array(PriorityQueue): 2 # PriorityQueue.insert already correct: appends to end of self.A 3 def delete_max(self): # O(n) 4 n, A, m = len(self.A), self.A, 0 5 for i in range(1, n): 6 if A[m].key < A[i].key: 7 m = i 8 A[m], A[n] = A[n], A[m] # swap max with end of array 9 return super().delete_max() # pop from end of array 1 class PQ_SortedArray(PriorityQueue): 2 # PriorityQueue.delete_max already correct: pop from end of self.A 3 def insert(self, *args): # O(n) 4 super().insert(*args) # append to end of array 5 i, A = len(self.A) - 1, self.A # restore array ordering 6 while 0 < i and A[i + 1].key < A[i].key: 7 A[i + 1], A[i] = A[i], A[i + 1] 8 i -= 1 We use *args to allow insert to take one argument (as makes sense now) or zero arguments; we will need the latter functionality when making the priority queues in-place. 3 Recitation 8 Binary Heaps The next implementation is based on a binary heap, which takes advantage of the logarithmic height of a complete binary tree to improve performance. The bulk of the work done by these functions are encapsulated by max heapify up and max heapify down below. 1 class PQ_Heap(PriorityQueue): 2 def insert(self, *args): # O(log n) 3 super().insert(*args) # append to end of array 4 n, A = self.n, self.A 5 max_heapify_up(A, n, n - 1) 6 7 def delete_max(self): # O(log n) 8 n, A = self.n, self.A 9 A[0], A[n] = A[n], A[0] 10 max_heapify_down(A, n, 0) 11 return super().delete_max() # pop from end of array Before we deﬁne max heapify operations, we need functions to compute parent and child indices given an index representing a node in a tree whose root is the ﬁrst element of the array. In this implementation, if the computed index lies outside the bounds of the array, we return the input index. Always returning a valid array index instead of throwing an error helps to simplify future code. 1 def parent(i): 2 p = (i - 1) // 2 3 return p if 0 < i else i 4 5 def left(i, n): 6 l = 2 i + 1 * 7 return l if l < n else i 8 9 def right(i, n): 10 r = 2 i + 2 * 11 return r if r < n else i 4 Recitation 8 Here is the meat of the work done by a max heap. Assuming all nodes in A[:n] satisfy the Max-Heap Property except for node A[i] makes it easy for these functions to maintain the Node Max-Heap Property locally. 1 def max_heapify_up(A, n, c): # T(c) = O(log c) 2 p = parent(c) # O(1) index of parent (or c) 3 if A[p].key < A[c].key: # O(1) compare 4 A[c], A[p] = A[p], A[c] # O(1) swap parent 5 max_heapify_up(A, n, p) # T(p) = T(c/2) recursive call on parent 1 def max_heapify_down(A, n, p): # T(p) = O(log n - log p) 2 l, r = left(p, n), right(p, n) # O(1) indices of children (or p) 3 c = l if A[r].key < A[l].key else r # O(1) index of largest child 4 if A[p].key < A[c].key: # O(1) compare 5 A[c], A[p] = A[p], A[c] # O(1) swap child 6 max_heapify_down(A, n, c) # T(c) recursive call on child O(n) Build Heap P n Recall that repeated insertion using a max heap priority queue takes time i=0 log i = log n! = O(n log n). We can build a max heap in linear time if the whole array is accessible to you. The idea is to construct the heap in reverse level order, from the leaves to the root, all the while maintaining that all nodes processed so far maintain the Max-Heap Property by running max heapify down at each node. As an optimization, we note that the nodes in the last half of the array are all leaves, so we do not need to run max heapify down on them. 1 def build_max_heap(A): 2 n = len(A) 3 for i in range(n // 2, -1, -1): # O(n) loop backward over array 4 max_heapify_down(A, n, i) # O(log n - log i)) fix max heap To see that this procedure takes O(n) instead of O(n log n) time, we compute an upper bound √ explicitly using summation. In the derivation, we use Stirling’s approximation: n! = Θ( n(n/e)n). n \u0012 \u0013 \u0012 \u0012 \u0013\u0013 X n n n n T (n) < (log n − log i) = log = O log √ n! n(n/e)n i=0 √ √ = O(log(e n/ n)) = O(n log e − log n) = O(n) Note that using this linear-time procedure to build a max heap does not affect the asymptotic efﬁciency of heap sort, because each of n delete max still takes O(log n) time each. But it is practically more efﬁcient procedure to initially insert n items into an empty heap. 5 Recitation 8 In-Place Heaps To make heap sort in place1 (as well as restoring the in-place property of selection sort and inser- tion sort), we can modify the base class PriorityQueue to take an entire array A of elements, and maintain the queue itself in the preﬁx of the ﬁrst n elements of A (where n <= len(A)). The insert function is no longer given a value to insert; instead, it inserts the item already stored in A[n], and incorporates it into the now-larger queue. Similarly, delete max does not return a value; it merely deposits its output into A[n] before decreasing its size. This approach only works in the case where all n insert operations come before all n delete max operations, as in priority queue sort. 1 class PriorityQueue: 2 def __init__(self, A): 3 self.n, self.A = 0, A 4 5 def insert(self): # absorb element A[n] into the queue 6 if not self.n < len(self.A): 7 raise IndexError(’insert into full priority queue’) 8 self.n += 1 9 10 def delete_max(self): # remove element A[n - 1] from the queue 11 if self.n < 1: 12 raise IndexError(’pop from empty priority queue’) 13 self.n -= 1 # NOT correct on its own! 14 15 @classmethod 16 def sort(Queue, A): 17 pq = Queue(A) # make empty priority queue 18 for i in range(len(A)): # n x T_insert 19 pq.insert() 20 for i in range(len(A)): # n x T_delete_max 21 pq.delete_max() 22 return pq.A This new base class works for sorting via any of the subclasses: PQ Array, PQ SortedArray, PQ Heap. The ﬁrst two sorting algorithms are even closer to the original selection sort and inser- tion sort, and the ﬁnal algorithm is what is normally referred to as heap sort. We’ve made a CoffeeScript heap visualizer which you can ﬁnd here: https://codepen.io/mit6006/pen/KxOpep 1Recall that an in-place sort only uses O(1) additional space during execution, so only a constant number of array elements can exist outside the array at any given time. 6 Recitation 8 Exercises 1. Draw the complete binary tree associated with the sub-array array A[:8]. Turn it into a max heap via linear time bottom-up heap-iﬁcation. Run insert twice, and then delete max twice. 1 A = [7, 3, 5, 6, 2, 0, 3, 1, 9, 4] 2. How would you ﬁnd the minimum element contained in a max heap? Solution: A max heap has no guarantees on the location of its minimum element, except that it may not have any children. Therefore, one must search over all n/2 leaves of the binary tree which takes Ω(n) time. 3. How long would it take to convert a max heap to a min heap? Solution: Run a modiﬁed build max heap on the original heap, enforcing a Min-Heap Property instead of a Max-Heap Property. This takes linear time. The fact that the original heap was a max heap does not improve the running time. 7 Recitation 8 4. Proximate Sorting: An array of distinct integers is k-proximate if every integer of the array is at most k places away from its place in the array after being sorted, i.e., if the ith integer of the unsorted input array is the jth largest integer contained in the array, then |i − j| ≤ k. In this problem, we will show how to sort a k-proximate array faster than Θ(n log n). (a) Prove that insertion sort (as presented in this class, without any changes) will sort a k-proximate array in O(nk) time. Solution: To prove O(nk), we show that each of the n insertion sort rounds swap an item left by at most O(k). In the original ordering, entries that are ≥ 2k slots apart must already be ordered correctly: indeed, if A[s] > A[t] but t − s ≥ 2k, there is no way to reverse the order of these two items while moving each at most k slots. This means that for each entry A[i] in the original order, fewer than 2k of the items A[0], . . . , A[i − 1] are greater than A[i]. Thus, on round i of insertion sort when A[i] is swapped into place, fewer than 2k swaps are required, so round i requires O(k) time. It’s possible to prove a stronger bound: that ai = A[i] is swapped at most k times in round i (instead of 2k). This is a bit subtle: the ﬁnal sorted index of ai is at most k slots away from i by the k-proximate assumption, but ai might not move to its ﬁnal position immediately, but may move past its ﬁnal sorted position and then be bumped to the right in future rounds. Suppose for contradiction a loop swaps the pth largest item A[i] to the left by more than k to position p0 < i − k, past at least k items larger than A[i]. Since A is k-proximate, i − p ≤ k, i.e. i − k ≤ p, so p0 < p. Thus at least one item less than A[i] must exist to the right of A[i]. Let A[j] be the smallest such item, the qth largest item in sorted order. A[j] is smaller than k + 1 items to the left of A[j], and no item to the right of A[j] is smaller than A[j], so q ≤ j − (k + 1), i.e. j − q ≥ k + 1. But A is k-proximate, so j − q ≤ k, a contradiction. (b) Θ(nk) is asymptotically faster than Θ(n2) when k = o(n), but is not asymptotically faster than Θ(n log n) when k = ω(log n). Describe an algorithm to sort a k-proximate array in O(n log k) time, which can be faster (but no slower) than Θ(n log n). Solution: We perform a variant of heap sort, where the heap only stores k + 1 items at a time. Build a min-heap H out of A[0], . . . , A[k − 1]. Then, repeatedly, insert the next item from A into H, and then store H.delete min() as the next entry in sorted order. So we ﬁrst call H.insert(A[k]) followed by B[0] = H.delete min(); the next iteration calls H.insert(A[k+1]) and B[1] = H.delete min(); and so on. (When there are no more entries to insert into H, do only the delete min step.) B is the sorted answer. This algorithm works because the ith smallest entry in array A must be one of A[0], A[1], . . . , A[i +k] by the k-proximate assumption, and by the time we’re about to write B[i], all of these entries have already been inserted into H (and some also deleted). Assuming entries B[0], . . . , B[i − 1] are correct (by induction), this means the ith smallest value is still in H while all smaller values have already been removed, so this ith smallest value is in fact H.delete min(), and B[i] gets ﬁlled correctly. Each heap operation takes time O(log k) because there are at most k + 1 items in the heap, so the n insertions and n deletions take O(n log k) total. MIT OpenCourseWare https://ocw.mit.edu 6.006 Introduction to Algorithms Spring 2020 For information about citing these materials or our Terms of Use, visit: https://ocw.mit.edu/terms Introduction to Algorithms: 6.006 Massachusetts Institute of Technology Instructors: Erik Demaine, Jason Ku, and Justin Solomon Recitation 9 Recitation 9 Graphs A graph G = (V, E) is a mathematical object comprising a set of vertices V (also called nodes) and a set of edges E, where each edge in E is a two-element subset of vertices from V . A vertex and edge are incident or adjacent if the edge contains the vertex. Let u and v be vertices. An edge is directed if its subset pair is ordered, e.g., (u, v), and undirected if its subset pair is unordered, e.g., {u, v} or alternatively both (u, v) and (v, u). A directed edge e = (u, v) extends from vertex u (e’s tail) to vertex v (e’s head), with e an incoming edge of v and an outgoing edge of u. In an undirected graph, every edge is incoming and outgoing. The in-degree and out-degree of a vertex v denotes the number of incoming and outgoing edges connected to v respectively. Unless otherwise speciﬁed, when we talk about degree, we generally mean out-degree. As their name suggest, graphs are often depicted graphically, with vertices drawn as points, and edges drawn as lines connecting the points. If an edge is directed, its corresponding line typically includes an indication of the direction of the edge, for example via an arrowhead near the edge’s head. Below are examples of a directed graph G1 and an undirected graph G2. G1 = (V1, E1) V1 = {0, 1, 2, 3, 4} E1 = {(0, 1), (1, 2), (2, 0), (3, 4)} G2 = (V2, E2) V2 = {0, 1, 2, 3, 4} E2 = {{0, 1}, {0, 3}, {0, 4}, {2, 3}} 0 1 2 3 4 0 1 2 3 4 A path1 in a graph is a sequence of vertices (v0, . . . , vk) such that for every ordered pair of vertices (vi, vi+1), there exists an outgoing edge in the graph from vi to vi+1. The length of a path is the number of edges in the path, or one less than the number of vertices. A graph is called strongly connected if there is a path from every node to every other node in the graph. Note that every connected undirected graph is also strongly connected because every undirected edge incident to a vertex is also outgoing. Of the two connected components of directed graph G1, only one of them is strongly connected. 1These are “walks” in 6.042. A “path” in 6.042 does not repeat vertices, which we would call a simple path. 2 Recitation 9 Graph Representations There are many ways to represent a graph in code. The most common way is to store a Set data structure Adj mapping each vertex u to another data structure Adj(u) storing the adjacencies of v, i.e., the set of vertices that are accessible from v via a single outgoing edge. This inner data structure is called an adjacency list. Note that we don’t store the edge pairs explicitly; we store only the out-going neighbor vertices for each vertex. When vertices are uniquely labeled from 0 to |V | − 1, it is common to store the top-level Set Adj within a direct access array of length |V |, where array slot i points to the adjacency list of the vertex labeled i. Otherwise, if the vertices are not labeled in this way, it is also common to use a hash table to map each u ∈ V to Adj(u). Then, it is common to store each adjacency list Adj(u) as a simple unordered array of the outgoing adjacencies. For example, the following are adjacency list representations of G1 and G2, using a direct access array for the top-level Set and an array for each adjacency list. 1 A1 = [[1], A2 = [[1, 4, 3], # 0 2 [2], [0], # 1 3 [0], [3], # 2 4 [4], [0, 2], # 3 5 []] [0]] # 4 Using an array for an adjacency list is a perfectly good data structures if all you need to do is loop over the edges incident to a vertex (which will be the case for all algorithms we will discuss in this class, so will be our default implementation). Each edge appears in any adjacency list at most twice, so the size of an adjacency list representation implemented using arrays is Θ(|V | + |E|). A drawback of this representation is that determining whether your graph contains a given edge (u, v) might require Ω(|V |) time to step through the array representing the adjacency list of u or v. We can overcome this obstacle by storing adjacency lists using hash tables instead of regular un- sorted arrays, which will support edge checking in expected O(1) time, still using only Θ(|V |+|E|) space. However, we won’t need this operation for our algorithms, so we will assume the simpler unsorted-array-based adjacency list representation. Below are representations of G1 and G2 that use a hash table for both the outer Adj Set and the inner adjacency lists Adj(u), using Python dictionaries: 1 S1 = {0: {1}, S2 = {0: {1, 3, 4}, # 0 2 1: {2}, 1: {0}, # 1 3 2: {0}, 2: {3}, # 2 4 3: {4}} 3: {0, 2}, # 3 5 4: {0}} # 4 Recitation 9 3 Breadth-First Search Given a graph, a common query is to ﬁnd the vertices reachable by a path from a queried vertex s. A breadth-ﬁrst search (BFS) from s discovers the level sets of s: level Li is the set of ver- tices reachable from s via a shortest path of length i (not reachable via a path of shorter length). Breadth-ﬁrst search discovers levels in increasing order starting with i = 0, where L0 = {s} since the only vertex reachable from s via a path of length i = 0 is s itself. Then any vertex reach- able from s via a shortest path of length i + 1 must have an incoming edge from a vertex whose shortest path from s has length i, so it is contained in level Li. So to compute level Li+1, include every vertex with an incoming edge from a vertex in Li, that has not already been assigned a level. By computing each level from the preceding level, a growing frontier of vertices will be explored according to their shortest path length from s. Below is Python code implementing breadth-ﬁrst search for a graph represented using index- labeled adjacency lists, returning a parent label for each vertex in the direction of a shortest path back to s. Parent labels (pointers) together determine a BFS tree from vertex s, containing some shortest path from s to every other vertex in the graph. 1 def bfs(Adj, s): # Adj: adjacency list, s: starting vertex 2 parent = [None for v in Adj] # O(V) (use hash if unlabeled) 3 parent[s] = s # O(1) root 4 level = [[s]] # O(1) initialize levels 5 while 0 < len(level[-1]): # O(?) last level contains vertices 6 level.append([]) # O(1) amortized, make new level 7 for u in level[-2]: # O(?) loop over last full level 8 for v in Adj[u]: # O(Adj[u]) loop over neighbors 9 if parent[v] is None: # O(1) parent not yet assigned 10 parent[v] = u # O(1) assign parent from level[-2] 11 level[-1].append(v) # O(1) amortized, add to border 12 return parent How fast is breadth-ﬁrst search? In particular, how many times can the inner loop on lines 9–11 be executed? A vertex is added to any level at most once in line 11, so the loop in line 7 processes each vertex v at most once. The loop in line 8 cycles through all deg(v) outgoing edges from P vertex v. Thus the inner loop is repeated at most O( deg(v)) = O(|E|) times. Because the v∈V parent array returned has length |V |, breadth-ﬁrst search runs in O(|V | + |E|) time. 4 Recitation 9 Exercise: For graphs G1 and G2, conducting a breadth-ﬁrst search from vertex v0 yields the parent labels and level sets below. 1 P1 = [0, L1 = [[0], P2 = [0, L2 = [[0], # 0 2 0, [1], 0, [1,3,4], # 1 3 1, [2], 3, [2], # 2 4 None, []] 0, []] # 3 5 None] 0] # 4 We can use parent labels returned by a breadth-ﬁrst search to construct a shortest path from a vertex s to vertex t, following parent pointers from t backward through the graph to s. Below is Python code to compute the shortest path from s to t which also runs in worst-case O(|V | + |E|) time. 1 def unweighted_shortest_path(Adj, s, t): 2 parent = bfs(Adj, s) # O(V + E) BFS tree from s 3 if parent[t] is None: # O(1) t reachable from s? 4 return None # O(1) no path 5 i = t # O(1) label of current vertex 6 path = [t] # O(1) initialize path 7 while i != s: # O(V) walk back to s 8 i = parent[i] # O(1) move to parent 9 path.append(i) # O(1) amortized add to path 10 return path[::-1] # O(V) return reversed path Exercise: Given an unweighted graph G = (V, E), ﬁnd a shortest path from s to t having an odd number of edges. Solution: Construct a new graph G0 = (V 0, E0). For every vertex u in V , construct two vertices uE and uO in V 0: these represent reaching the vertex u through an even and odd number of edges, respectively. For every edge (u, v) in E, construct the edges (uE , vO) and (uO, vE ) in E0 . Run breadth-ﬁrst search on G0 from sE to ﬁnd the shortest path from sE to tO. Because G0 is bipartite between even and odd vertices, even paths from sE will always end at even vertices, and odd paths will end at odd vertices, so ﬁnding a shortest path from sE to tO will represent a path of odd length in the original graph. Because G0 has 2|V | vertices and 2|E| edges, constructing G0 and running breadth-ﬁrst search from sE each take O(|V | + |E|) time. MIT OpenCourseWare https://ocw.mit.edu 6.006 Introduction to Algorithms Spring 2020 For information about citing these materials or our Terms of Use, visit: https://ocw.mit.edu/terms Introduction to Algorithms: 6.006 Massachusetts Institute of Technology Instructors: Erik Demaine, Jason Ku, and Justin Solomon Recitation 10 Recitation 10 Depth-First Search A breadth-ﬁrst search discovers vertices reachable from a queried vertex s level-by-level outward from s. A depth-ﬁrst search (DFS) also ﬁnds all vertices reachable from s, but does so by search- ing undiscovered vertices as deep as possible before exploring other branches. Instead of exploring all neighbors of s one after another as in a breadth-ﬁrst search, depth-ﬁrst searches as far as possi- ble from the ﬁrst neighbor of s before searching any other neighbor of s. Just as with breadth-ﬁrst search, depth-ﬁrst search returns a set of parent pointers for vertices reachable from s in the order the search discovered them, together forming a DFS tree. However, unlike a BFS tree, a DFS tree will not represent shortest paths in an unweighted graph. (Additionally, DFS returns an order on vertices discovered which will be discussed later.) Below is Python code implementing a recursive depth-ﬁrst search for a graph represented using index-labeled adjacency lists. 1 def dfs(Adj, s, parent = None, order = None): # Adj: adjacency list, s: start 2 if parent is None: # O(1) initialize parent list 3 parent = [None for v in Adj] # O(V) (use hash if unlabeled) 4 parent[s] = s # O(1) root 5 order = [] # O(1) initialize order array 6 for v in Adj[s]: # O(Adj[s]) loop over neighbors 7 if parent[v] is None: # O(1) parent not yet assigned 8 parent[v] = s # O(1) assign parent 9 dfs(Adj, v, parent, order) # Recursive call 10 order.append(s) # O(1) amortized 11 return parent, order How fast is depth-ﬁrst search? A recursive dfs call is performed only when a vertex does not have a parent pointer, and is given a parent pointer immediately before the recursive call. Thus dfs is called on each vertex at most once. Further, the amount of work done by each recursive search from vertex v is proportional to the out-degree deg(v) of v. Thus, the amount of work done by P depth-ﬁrst search is O( deg(v)) = O(|E|). Because the parent array returned has length |V |, v∈V depth-ﬁrst search runs in O(|V | + |E|) time. Exercise: Describe a graph on n vertices for which BFS and DFS would ﬁrst visit vertices in the same order. Solution: Many possible solutions. Two solutions are a chain of vertices from v, or a star graph with an edge from v to every other vertex. 2 Recitation 10 Full Graph Exploration Of course not all vertices in a graph may be reachable from a query vertex s. To search all ver- tices in a graph, one can use depth-ﬁrst search (or breadth-ﬁrst search) to explore each connected component in the graph by performing a search from each vertex in the graph that has not yet been discovered by the search. Such a search is conceptually equivalent to adding an auxiliary vertex with an outgoing edge to every vertex in the graph and then running breadth-ﬁrst or depth-ﬁrst search from the added vertex. Python code searching an entire graph via depth-ﬁrst search is given below. 1 def full_dfs(Adj): # Adj: adjacency list 2 parent = [None for v in Adj] # O(V) (use hash if unlabeled) 3 order = [] # O(1) initialize order list 4 for v in range(len(Adj)): # O(V) loop over vertices 5 if parent[v] is None: # O(1) parent not yet assigned 6 parent[v] = v # O(1) assign self as parent (a root) 7 dfs(Adj, v, parent, order) # DFS from v (BFS can also be used) 8 return parent, order For historical reasons (primarily for its connection to topological sorting as discussed later) depth- ﬁrst search is often used to refer to both a method to search a graph from a speciﬁc vertex, and as a method to search an entire (as in graph explore). You may do the same when answering problems in this class. DFS Edge Classiﬁcation To help prove things about depth-ﬁrst search, it can be useful to classify the edges of a graph in relation to a depth-ﬁrst search tree. Consider a graph edge from vertex u to v. We call the edge a tree edge if the edge is part of the DFS tree (i.e. parent[v] = u). Otherwise, the edge from u to v is not a tree edge, and is either a back edge, forward edge, or cross edge depending respectively on whether: u is a descendant of v, v is a descendant of u, or neither are descendants of each other, in the DFS tree. Exercise: Draw a graph, run DFS from a vertex, and classify each edge relative to the DFS tree. Show that forward and cross edges cannot occur when running DFS on an undirected graph. Exercise: How can you identify back edges computationally? Solution: While performing a depth-ﬁrst search, keep track of the set of ancestors of each vertex in the DFS tree during the search (in a direct access array or a hash table). When processing neighbor v of s in dfs(Adj, s), if v is an ancestor of s, then (s, v) is a back edge, and certiﬁes a cycle in the graph. 3 Recitation 10 Topological Sort A directed graph containing no directed cycle is called a directed acyclic graph or a DAG. A topological sort of a directed acyclic graph G = (V, E) is a linear ordering of the vertices such that for each edge (u, v) in E, vertex u appears before vertex v in the ordering. In the dfs func- tion, vertices are added to the order list in the order in which their recursive DFS call ﬁnishes. If the graph is acyclic, the order returned by dfs (or graph search) is the reverse of a topolog- ical sort order. Proof by cases. One of dfs(u) or dfs(v) is called ﬁrst. If dfs(u) was called before dfs(v), dfs(v) will start and end before dfs(u) completes, so v will appear before u in order. Alternatively, if dfs(v) was called before dfs(u), dfs(u) cannot be called before dfs(v) completes, or else a path from v to u would exist, contradicting that the graph is acyclic; so v will be added to order before vertex u. Reversing the order returned by DFS will then repre- sent a topological sort order on the vertices. Exercise: A high school contains many student organization, each with its own hierarchical struc- ture. For example, the school’s newspaper has an editor-in-chief who oversees all students con- tributing to the newspaper, including a food-editor who oversees only students writing about school food. The high school’s principal needs to line students up to receive diplomas at graduation, and wants to recognize student leaders by giving a diploma to student a before student b whenever a oversees b in any student organization. Help the principal determine an order to give out diplomas that respects student organization hierarchy, or prove to the principal that no such order exists. Solution: Construct a graph with one vertex per student, and a directed edge from student a to b if student a oversees student b in some student organization. If this graph contains a cycle, the princi- pal is out of luck. Otherwise, a topological sort of the students according to this graph will satisfy the principal’s request. Run DFS on the graph (exploring the whole graph as in graph explore) to obtain an order of DFS vertex ﬁnishing times in O(|V | + |E|) time. While performing the DFS, keep track of the ancestors of each vertex in the DFS tree, and evaluate if each new edge processed is a back edge. If a back edge is found from vertex u to v, follow parent pointers back to v from u to obtain a directed cycle in the graph to prove to the principal that no such order exists. Otherwise, if no cycle is found, the graph is acyclic and the order returned by DFS is the reverse of a topological sort, which may then be returned to the principal. We’ve made a CoffeeScript graph search visualizer which you can ﬁnd here: https://codepen.io/mit6006/pen/dgeKEN MIT OpenCourseWare https://ocw.mit.edu 6.006 Introduction to Algorithms Spring 2020 For information about citing these materials or our Terms of Use, visit: https://ocw.mit.edu/terms Introduction to Algorithms: 6.006 Massachusetts Institute of Technology Instructors: Erik Demaine, Jason Ku, and Justin Solomon Recitation 11 Recitation 11 Weighted Graphs For many applications, it is useful to associate a numerical weight to edges in a graph. For example, a graph modeling a road network might weight each edge with the length of a road corresponding to the edge, or a graph modeling an online dating network might contain edges from one user to another weighted by directed attraction. A weighted graph is then a graph G = (V, E) together with a weight function w : E → R, mapping edges to real-valued weights. In practice, edge weights will often not be represented by a separate function at all, preferring instead to store each weight as a value in an adjacency matrix, or inside an edge object stored in an adjacency list or set. For example, below are randomly weighted adjacency set representations of the graphs from Recitation 11. A function to extract such weights might be: def w(u,v): return W[u][v]. 1 W1 = [0: {1: -2}, W2 = {0: {1: 1, 3: 2, 4: -1}, # 0 2 1: {2: 0}, 1: {0: 1}, # 1 3 2: {0: 1}, 2: {3: 0}, # 2 4 3: {4: 3}} 3: {0: 2, 2: 0}, # 3 5 4: {0: -1}} # 4 Now that you have an idea of how weights could be stored, for the remainder of this class you may simply assume that a weight function w can be stored using O(|E|) space, and can return the weight of an edge in constant time1. When referencing the weight of an edge e = (u, v), we will often use the notation w(u, v) interchangeably with w(e) to refer to the weight of an edge. Exercise: Represent graphs W1 and W2 as adjacency matrices. How could you store weights in an adjacency list representation? Weighted Shortest Paths A weighted path is simply a path in a weighted graph as deﬁned in Recitation 11, where the weight of the path is the sum of the weights from edges in the path. Again, we will often abuse our nota- Pk−1 tion: if π = (v1, . . . , vk) is a weighted path, we let w(π) denote the path’s weight i=1 w(vi, vi+1). The (single source) weighted shortest paths problem asks for a lowest weight path to every vertex v in a graph from an input source vertex s, or an indication that no lowest weight path exists from s to v. We already know how to solve the weighted shortest paths problem on graphs for which all edge weights are positive and are equal to each other: simply run breadth-ﬁrst search from s to minimize the number of edges traversed, thus minimizing path weight. But when edges have different and/or non-positive weights, breadth-ﬁrst search cannot be applied directly. 1We will typically only be picky with the distinction between worst-case and expected bounds when we want to test your understanding of data structures. Hash tables perform well in practice, so use them! 2 Restrictions SSSP Algorithm Graph Weights Name Running Time O(·) General Unweighted BFS |V | + |E| DAG Any DAG Relaxation |V | + |E| General Any Bellman-Ford Dijkstra |V | · |E| General Non-negative Recitation 11 In fact, when a graph contains a cycle (a path starting and ending at the same vertex) that has negative weight, then some shortest paths might not even exist, because for any path containing a vertex from the negative weight cycle, a shorter path can be found by adding a tour around the cycle. If any path from s to some vertex v contains a vertex from a negative weight cycle, we will say the shortest path from s to v is undeﬁned, with weight −∞. If no path exists from s to v, then we will say the shortest path from s to v is undeﬁned, with weight +∞. In addition to breadth-ﬁrst search, we will present three additional algorithms to compute single source shortest paths that cater to different types of weighted graphs. Weighted Single Source Shortest Path Algorithms |V | log |V | + |E| Relaxation We’ve shown you one view of relaxation in lecture. Below is another framework by which you can view DAG relaxation. As a general algorithmic paradigm, a relaxation algorithm searches for a solution to an optimization problem by starting with a solution that is not optimal, then iteratively improves the solution until it becomes an optimal solution to the original problem. In the single source shortest paths problem, we would like to ﬁnd the weight δ(s, v) of a shortest path from source s to each vertex v in a graph. As a starting point, for each vertex v we will initialize an upper bound estimate d(v) on the shortest path weight from s to v, +∞ for all d(s, v) except d(s, s) = 0. During the relaxation algorithm, we will repeatedly relax some path estimate d(s, v), decreasing it toward the true shortest path weight δ(s, v). If ever d(s, v) = δ(s, v), we say that estimate d(s, v) is fully relaxed. When all shortest path estimates are fully relaxed, we will have solved the original problem. Then an algorithm to ﬁnd shortest paths could take the following form: 1 def general_relax(Adj, w, s): # Adj: adjacency list, w: weights, s: start 2 d = [float(’inf’) for _ in Adj] # shortest path estimates d(s, v) 3 parent = [None for _ in Adj] # initialize parent pointers 4 d[s], parent[s] = 0, s # initialize source 5 while True: # repeat forever! 6 relax some d[v] ?? # relax a shortest path estimate d(s, v) 7 return d, parent # return weights, paths via parents There are a number of problems with this algorithm, not least of which is that it never terminates! But if we can repeatedly decrease each shortest path estimates to fully relax each d(s, v), we will have found shortest paths. How do we ‘relax’ vertices and when do we stop relaxing? 3 Recitation 11 To relax a shortest path estimate d(s, v), we will relax an incoming edge to v, from another vertex u. If we maintain that d(s, u) always upper bounds the shortest path from s to u for all u ∈ V , then the true shortest path weight δ(s, v) can’t be larger than d(s, u) + w(u, v) or else going to u along a shortest path and traversing the edge (u, v) would be a shorter path2. Thus, if at any time d(s, u) + w(u, v) < d(s, v), we can relax the edge by setting d(s, v) = d(s, u) + w(u, v), strictly improving our shortest path estimate. 1 def try_to_relax(Adj, w, d, parent, u, v): 2 if d[v] > d[u] + w(u, v): # better path through vertex u 3 d[v] = d[u] + w(u, v) # relax edge with shorter path found 4 parent[v] = u If we only change shortest path estimates via relaxation, than we can prove that the shortest path estimates will never become smaller than true shortest paths. Safety Lemma: Relaxing an edge maintains d(s, v) ≥ δ(s, v) for all v ∈ V . Proof. We prove a stronger statement, that for all v ∈ V , d(s, v) is either inﬁnite or the weight of some path from s to v (so cannot be larger than a shortest path). This is true at initialization: each d(s, v) is +∞, except for d(s) = 0 corresponding to the zero-length path. Now suppose at some other time the claim is true, and we relax edge (u, v). Relaxing the edge decreases d(s, v) to a ﬁnite value d(s, u) + w(u, v), which by induction is a length of a path from s to v: a path from s to u and the edge (u, v). If ever we arrive at an assignment of all shortest path estimates such that no edge in the graph can be relaxed, then we can prove that shortest path estimates are in fact shortest path distances. Termination Lemma: If no edge can be relaxed, then d(s, v) ≤ δ(s, v) for all v ∈ V . Proof. Suppose for contradiction δ(s, v) < d(s, v) so that there is a shorter path π from s to v. Let (a, b) be ther ﬁrst edge of π such that d(b) > δ(s, b). Then edge (a, b) can be relaxed, a contradiction. So, we can change lines 5-6 of the general relaxation algorithm to repeatedly relax edges from the graph until no edge can be further relaxed. 1 while some_edge_relaxable(Adj, w, d): 2 (u, v) = get_relaxable_edge(Adj, w, d) 3 try_to_relax(Adj, w, d, parent, u, v) It remains to analyze the running time of this algorithm, which cannot be determined unless we provide detail for how this algorithm chooses edges to relax. If there exists a negative weight cycle in the graph reachable from s, this algorithm will never terminate as edges along the cycle could be relaxed forever. But even for acyclic graphs, this algorithm could take exponential time. 2This is a special case of the triangle inequality: δ(a, c) ≤ δ(a, b) + δ(b, c) for all a, b, c ∈ V . 4 Recitation 11 Exponential Relaxation How many modifying edge relaxations could occur in an acyclic graph before all edges are fully relaxed? Below is a weighted directed graph on 2n + 1 vertices and 3n edges for which the relaxation framework could perform an exponential number of modifying relaxations, if edges are relaxed in a bad order. top left right This graph contains n sections, with section i containing three edges, (v2i, v2i+1), (v2i, v2i+2), and (v2i+1, v2i+2), each with weight 2n−i; we will call these edges within a section, left, top, and right respectively. In this construction, the lowest weight path from v0 to vi is achieved by traversing top edges until vi’s section is reached. Shortest paths from v0 can easily by found by performing only a linear number of modifying edge relaxations: relax the top and left edges of each successive section. However, a bad relaxation order might result in many more modifying edge relaxations. To demonstrate a bad relaxation order, initialize all minimum path weight estimates to ∞, except d(s, s) = 0 for source s = v0. First relax the left edge, then the right edge of section 0, updating 2n+1 the shortest path estimate at v2 to d(s, v2) = 2n + 2n = . In actuality, the shortest path from v0 to v2 is via the top edge, i.e., δ(s, v2) = 2n . But before relaxing the top edge of section 0, recursively apply this procedure to fully relax the remainder of the graph, from section 1 to n − 1, computing shortest path estimates based on the incorrect value of d(s, v2) = 2n+1 . Only then relax the top edge of section 0, after which d(s, v2) is modiﬁed to its correct value 2n . Lastly, fully relax sections 1 through n − 1 one more time recursively, to their correct and ﬁnal values. How many modifying edge relaxations are performed by this edge relaxation ordering? Let T (n) represent the number of modifying edge relaxations performed by the procedure on a graph con- taining n sections, with recurrence relation given by T (n) = 3 + 2T (n − 2). The solution to this recurrence is T (n) = O(2n/2), exponential in the size of the graph. Perhaps there exists some edge relaxation order requiring only a polynomial number of modifying edge relaxations? DAG Relaxation In a directed acyclic graph (DAG), there can be no negative weight cycles, so eventually relaxation must terminate. It turns out that relaxing each outgoing edge from every vertex exactly once in a topological sort order of the vertices, correctly computes shortest paths. This shortest paths algorithm is sometimes called DAG Relaxation. Recitation 11 5 1 def DAG_Relaxation(Adj, w, s): # Adj: adjacency list, w: weights, s: start 2 _, order = dfs(Adj, s) # run depth-first search on graph 3 order.reverse() # reverse returned order 4 d = [float(’inf’) for _ in Adj] # shortest path estimates d(s, v) 5 parent = [None for _ in Adj] # initialize parent pointers 6 d[s], parent[s] = 0, s # initialize source 7 for u in order: # loop through vertices in topo sort 8 for v in Adj[u]: # loop through out-going edges of u 9 try_to_relax(Adj, w, d, parent, u, v) # try to relax edge from u to v 10 return d, parent # return weights, paths via parents Claim: The DAG Relaxation algorithm computes shortest paths in a directed acyclic graph. Proof. We prove that at termination, d(s, v) = δ(s, v) for all v ∈ V . First observe that Safety ensures that a vertex not reachable from s will retain d(s, v) = +∞ at termination. Alterna- tively, consider any shortest path π = (v1, . . . , vm) from v1 = s to any vertex vm = v reach- able from s. The topological sort order ensures that edges of the path are relaxed in the order in which they appear in the path. Assume for induction that before edge (vi, vi+1) ∈ π is relaxed, d(s, vi) = δ(s, vi). Setting d(s, s) = 0 at the start provides a base case. Then relaxing edge (vi, vi+1) sets d(s, vi+1) = δ(s, vi) + w(vi, vi+1) = δ(s, vi+1), as sub-paths of shortest paths are also shortest paths. Thus the procedure constructs shortest path weights as desired. Since depth- ﬁrst search runs in linear time and the loops relax each edge exactly once, this algorithm takes O(|V | + |E|) time. Exercise: You have been recruited by MIT to take part in a new part time student initiative where you will take only one class per term. You don’t care about graduating; all you really want to do is to take 19.854, Advanced Quantum Machine Learning on the Blockchain: Neural Interfaces, but are concerned because of its formidable set of prerequisites. MIT professors will allow you take any class as long as you have taken at least one of the class’s prerequisites prior to taking the class. But passing a class without all the prerequsites is difﬁcult. From a survey of your peers, you know for each class and prerequisite pair, how many hours of stress the class will demand. Given a list of classes, prerequisites, and surveyed stress values, describe a linear time algorithm to ﬁnd a sequence of classes that minimizes the amount of stress required to take 19.854, never taking more than one prerequisite for any class. You may assume that every class is offered every semester. Solution: Build a graph with a vertex for every class and a directed edge from class a to class b if b is a prerequisite of a, weighted by the stress of taking class a after having taken class b as a prerequisite. Use topological sort relaxation to ﬁnd the shortest path from class 19.854 to every other class. From the classes containing no prerequisites (sinks of the DAG), ﬁnd one with minimum total stress to 19.854, and return its reversed shortest path. MIT OpenCourseWare https://ocw.mit.edu 6.006 Introduction to Algorithms Spring 2020 For information about citing these materials or our Terms of Use, visit: https://ocw.mit.edu/terms Introduction to Algorithms: 6.006 Massachusetts Institute of Technology Instructors: Erik Demaine, Jason Ku, and Justin Solomon Recitation 12 Recitation 12 Bellman-Ford In lecture, we presented a version of Bellman-Ford1 based on graph duplication and DAG Re- laxation that solves SSSPs in O(|V ||E|) time and space, and can return a negative-weight cycle reachable on a path from s to v, for any vertex v with δ(s, v) = −∞. The original Bellman-Ford algorithm is easier to state but is a little less powerful. It solves SSSPs in the same time using only O(|V |) space, but only detects whether a negative-weight cycle exists (will not return such a negative weight cycle). It is based on the relaxation framework discussed in R11. The algorithm is straight-forward: initialize distance estimates, and then relax every edge in the graph in |V |−1 rounds. The claim is that: if the graph does not contain negative-weight cycles, d(s, v) = δ(s, v) for all v ∈ V at termination; otherwise if any edge still relaxable (i.e., still violates the triangle inequality), the graph contains a negative weight cycle. A Python implementation of the Bellman-Ford algorithm is given below. 1 def bellman_ford(Adj, w, s): # Adj: adjacency list, w: weights, s: start 2 # initialization 3 infinity = float(’inf’) # number greater than sum of all + weights 4 d = [infinity for _ in Adj] # shortest path estimates d(s, v) 5 parent = [None for _ in Adj] # initialize parent pointers 6 d[s], parent[s] = 0, s # initialize source 7 # construct shortest paths in rounds 8 V = len(Adj) # number of vertices 9 for k in range(V - 1): # relax all edges in (V - 1) rounds 10 for u in range(V): # loop over all edges (u, v) 11 for v in Adj[u]: # relax edge from u to v 12 try_to_relax(Adj, w, d, parent, u, v) 13 # check for negative weight cycles accessible from s 14 for u in range(V): # Loop over all edges (u, v) 15 for v in Adj[u]: 16 if d[v] > d[u] + w(u,v): # If edge relax-able, report cycle 17 raise Exception(’Ack! There is a negative weight cycle!’) 18 return d, parent This algorithm has the same overall structure as the general relaxation paradigm, but limits the order in which edges can be processed. In particular, the algorithm relaxes every edge of the graph (lines 10-12), in a series of |V | − 1 rounds (line 9). The following lemma establishes correctness of the algorithm. 1This algorithm is called Bellman-Ford after two researchers who independently proposed the same algorithm in different contexts. 2 Recitation 12 Lemma 1 At the end of relaxation round i of Bellman-Ford, d(s, v) = δ(s, v) for any vertex v that has a shortest path from s to v which traverses at most i edges. Proof. Proof by induction on round i. At the start of the algorithm (at end of round 0), the only vertex with shortest path from s traversing at most 0 edges is vertex s, and Bellman-Ford correctly sets d(s, s) = 0 = δ(s, s). Now suppose the claim is true at the end of round i − 1. Let v be a vertex containing a shortest path from s traversing at most i edges. If v has a shortest path from s traversing at most i−1 edges, d(s, v) = δ(s, v) prior to round i, and will continue to hold at the end of round i by the upper-bound property2 Alternatively, d(s, v) 6= δ(s, v) prior to round i, and let u be the second to last vertex visited along some shortest path from s to v which traverses exactly i edges. Some shortest path from s to u traverses at most i − 1 edges, so d(s, u) = δ(s, u) prior to round i. Then after the edge from u to v is relaxed during round i, d(s, v) = δ(s, v) as desired. If the graph does not contain negative weight cycles, some shortest path is simple, and contains at most |V |−1 edges as it traverses any vertex of the graph at most once. Thus after |V |−1 rounds of Bellman-Ford, d(s, v) = δ(s, v) for every vertex with a simple shortest path from s to v. However, if after |V | − 1 rounds of relaxation, some edge (u, v) still violates the triangle inequality (lines 14-17), then there exists a path from s to v using |V | edges which has lower weight than all paths using fewer edges. Such a path cannot be simple, so it must contain a negative weight cycle. This algorithm runs |V | rounds, where each round performs a constant amount of work for each edge in the graph, so Bellman-Ford runs in O(|V ||E|) time. Note that lines 10-11 actually take O(|V | + |E|) time to loop over the entire adjacency list structure, even for vertices adjacent to no edge. If the graph contains isolated vertices that are not S, we can just remove them from Adj to ensure that |V | = O(|E|). Note that if edges are processed in a topological sort order with respect to a shortest path tree from s, then Bellman-Ford will correctly compute shortest paths from s after its ﬁrst round; of course, it is not easy to ﬁnd such an order. However, for many graphs, signiﬁcant savings can be obtained by stopping Bellman-Ford after any round for which no edge relaxation is modifying. Note that this algorithm is different than the one presented in lecture in two important ways: • The original Bellman-Ford only keeps track of one ‘layer’ of d(s, v) estimates in each round, while the lecture version keeps track of dk(s, v) for k ∈{0, . . . , |V |}, which can be then used to construct negative-weight cycles. • A distance estimate d(s, v) in round k of original Bellman-Ford does not necessarily equal dk(s, v), the k-edge distance to v computed in the lecture version. This is because the original Bellman-Ford may relax multiple edges along a shortest path to v in a single round, while the lecture version relaxes at most one in each level. In other words, distance estimate d(s, v) in round k of original Bellman-Ford is never larger than dk(s, v), but it may be much smaller and converge to a solution quicker than the lecture version, so may be faster in practice. 2Recall that the Safety Lemma from Recitation 11 ensures that relaxation maintains δ(s, v) ≤ d(s, v) for all v. 3 Recitation 12 Exercise: Alice, Bob, and Casey are best friends who live in different corners of a rural school district. During the summer, they decide to meet every Saturday at some intersection in the district to play tee-ball. Each child will bike to the meeting location from their home along dirt roads. Each dirt road between road intersections has a level of fun associated with biking along it in a certain direction, depending on the incline and quality of the road, the number of animals passed, etc. Road fun-ness may be positive, but could also be be negative, e.g. when a road is difﬁcult to traverse in a given direction, or passes by a scary dog, etc. The children would like to: choose a road intersection to meet and play tee-ball that maximizes the total fun of all three children in reaching their chosen meeting location; or alternatively, abandon tee-ball altogether in favor of biking, if a loop of roads exists in their district along which they can bike all day with ever increasing fun. Help the children organize their Saturday routine by ﬁnding a tee-ball location, or determining that there exists a continuously fun bike loop in their district (for now, you do not have to ﬁnd such a loop). You may assume that each child can reach any road in the district by bike. Solution: Construct a graph on road intersections within the district, as well as the locations a, b, and c of the homes of the three children, with a directed edge from one vertex to another if there is a road between them traversable in that direction by bike, weighted by negative fun-ness of the road. If a negative weight cycle exists in this graph, such a cycle would represent a continuously fun bike loop. To check for the existence of any negative weight cycle in the graph, run Bellman- Ford from vertex a. If Bellman-Ford detects a negative weight cycle by ﬁnding an edge (u, v) that can be relaxed in round |V |, return that a continuously fun bike loop exists. Alternatively, if no negative weight cycle exists, minimal weighted paths correspond to bike routes that maximize fun. Running Bellman-Ford from vertex a then computes shortest paths d(s, v) from a to each vertex v in the graph. Run Bellman-Ford two more times, once from vertex b and once from vertex c, computing shortest paths values d(b, v) and d(c, v) respectively for each vertex v in the graph. Then for each vertex v, compute the sum d(a, v) + d(b, v) + d(c, v). A vertex that minimizes this sum will correspond to a road intersection that maximizes total fun of all three children in reaching it. This algorithm runs Bellman-Ford three times and then compares a constant sized sum at each vertex, so this algorithm runs in O(|V ||E|) time. MIT OpenCourseWare https://ocw.mit.edu 6.006 Introduction to Algorithms Spring 2020 For information about citing these materials or our Terms of Use, visit: https://ocw.mit.edu/terms Introduction to Algorithms: 6.006 Massachusetts Institute of Technology Instructors: Erik Demaine, Jason Ku, and Justin Solomon Recitation 13 Recitation 13 Dijkstra’s Algorithm Dijkstra is possibly the most commonly used weighted shortest paths algorithm; it is asymptoti- cally faster than Bellman-Ford, but only applies to graphs containing non-negative edge weights, which appear often in many applications. The algorithm is fairly intuitive, though its implemen- tation can be more complicated than that of other shortest path algorithms. Think of a weighted graph as a network of pipes, each with non-negative length (weight). Then turn on a water faucet at a source vertex s. Assuming the water ﬂowing from the faucet traverses each pipe at the same rate, the water will reach each pipe intersection vertex in the order of their shortest distance from the source. Dijkstra’s algorithm discretizes this continuous process by repeatedly relaxing edges from a vertex whose minimum weight path estimate is smallest among vertices whose out-going edges have not yet been relaxed. In order to efﬁciently ﬁnd the smallest minimum weight path estimate, Dijkstra’s algorithm is often presented in terms of a minimum priority queue data structure. Dijk- stra’s running time then depends on how efﬁciently the priority queue can perform its supported operations. Below is Python code for Dijkstra’s algorithm in terms of priority queue operations. 1 def dijkstra(Adj, w, s): 2 d = [float(’inf’) for _ in Adj] # shortest path estimates d(s, v) 3 parent = [None for _ in Adj] # initialize parent pointers 4 d[s], parent[s] = 0, s # initialize source 5 Q = PriorityQueue() # initialize empty priority queue 6 V = len(Adj) # number of vertices 7 for v in range(V): # loop through vertices 8 Q.insert(v, d[v]) # insert vertex-estimate pair 9 for _ in range(V): # main loop 10 u = Q.extract_min() # extract vertex with min estimate 11 for v in Adj[u]: # loop through out-going edges 12 try_to_relax(Adj, w, d, parent, u, v) 13 Q.decrease_key(v, d[v]) # update key of vertex 14 return d, parent This algorithm follows the same structure as the general relaxation framework. Lines 2-4 initialize shortest path weight estimates and parent pointers. Lines 5-7 initialize a priority queue with all vertices from the graph. Lines 8-12 comprise the main loop. Each time the loop is executed, line 9 removes a vertex from the queue, so the queue will be empty at the end of the loop. The vertex u processed in some iteration of the loop is a vertex from the queue whose shortest path weight estimate is smallest, from among all vertices not yet removed from the queue. Then, lines 10-11 relax the out-going edges from u as usual. However, since relaxation may reduce the shortest path weight estimate d(s, v), vertex v’s key in the queue must be updated (if it still exists in the queue); line 12 accomplishes this update. 2 Recitation 13 Why does Dijkstra’s algorithm compute shortest paths for a graph with non-negative edge weights? The key observation is that shortest path weight estimate of vertex u equals its actual shortest path weight d(s, u) = δ(s, u) when u is removed from the priority queue. Then by the upper-bound property, d(s, u) = δ(s, u) will still hold at termination of the algorithm. A proof of correctness is described in the lecture notes, and will not be repeated here. Instead, we will focus on analyzing running time for Dijkstra implemented using different priority queues. Exercise: Construct a weighted graph with non-negative edge weights, and apply Dijkstra’s algo- rithm to ﬁnd shortest paths. Speciﬁcally list the key-value pairs stored in the priority queue after each iteration of the main loop, and highlight edges corresponding to constructed parent pointers. Priority Queues An important aspect of Dijkstra’s algorithm is the use of a priority queue. The priority queue interface used here differs slightly from our presentation of priority queues earlier in the term. Here, a priority queue maintains a set of key-value pairs, where vertex v is a value and d(s, v) is its key. Aside from empty initialization, the priority queue supports three operations: insert(val, key) adds a key-value pair to the queue, extract min() removes and returns a value from the queue whose key is minimum, and decrease key(val, new key) which reduces the key of a given value stored in the queue to the provided new key. The running time of Dijkstra depends on the running times of these operations. Speciﬁcally, if Ti, Te, and Td are the respective running times for inserting a key-value pair, extracting a value with minimum key, and decreasing the key of a value, the running time of Dijkstra will be: TDijkstra = O(|V | · Ti + |V | · Te + |E| · Td). There are many different ways to implement a priority queue, achieving different running times for each operation. Probably the simplest implementation is to store all the vertices and their current shortest path estimate in a dictionary. A hash table of size O(|V |) can support expected constant time O(1) insertion and decrease-key operations, though to ﬁnd and extract the vertex with minimum key takes linear time O(|V |). If the vertices are indices into the vertex set with a linear range, then we can alternatively use a direct access array, leading to worst case O(1) time insertion and decrease-key, while remaining linear O(|V |) to ﬁnd and extract the vertex with minimum key. In either case, the running time for Dijkstra simpliﬁes to: TDict = O(|V |2 + |E|). This is actually quite good! If the graph is dense, |E| = Ω(|V |2), this implementation is linear in the size of the input! Below is a Python implementation of Dijkstra using a direct access array to implement the priority queue. 3 Recitation 13 1 class PriorityQueue: # Hash Table Implementation 2 def __init__(self): # stores keys with unique labels 3 self.A = {} 4 5 def insert(self, label, key): # insert labeled key 6 self.A[label] = key 7 8 def extract_min(self): # return a label with minimum key 9 min_label = None 10 for label in self.A: 11 if (min_label is None) or (self.A[label] < self.A[min_label].key): 12 min_label = label 13 del self.A[min_label] 14 return min_label 15 16 def decrease_key(self, label, key): # decrease key of a given label 17 if (label in self.A) and (key < self.A[label]): 18 self.A[label] = key If the graph is sparse, |E| = O(|V |), we can speed things up with more sophisticated priority queue implementations. We’ve seen that a binary min heap can implement insertion and extract-min in O(log n) time. However, decreasing the key of a value stored in a priority queue requires ﬁnding the value in the heap in order to change its key, which naively could take linear time. However, this difﬁculty is easily addressed: each vertex can maintain a pointer to its stored location within the heap, or the heap can maintain a mapping from values (vertices) to locations within the heap (you were asked to do this in Problem Set 5). Either solution can support ﬁnding a given value in the heap in constant time. Then, after decreasing the value’s key, one can restore the min heap property in logarithmic time by re-heapifying the tree. Since a binary heap can support each of the three operations in O(log |V |) time, the running time of Dijkstra will be: THeap = O((|V | + |E|) log |V |). For sparse graphs, that’s O(|V | log |V |)! For graphs in between sparse and dense, there is an even more sophisticated priority queue implementation using a data structure called a Fibonacci Heap, which supports amortized O(1) time insertion and decrease-key operations, along with O(log n) minimum extraction. Thus using a Fibonacci Heap to implement the Dijkstra priority queue leads to the following worst-case running time: TF ibHeap = O(|V | log |V | + |E|). We won’t be talking much about Fibonacci Heaps in this class, but they’re theoretically useful for speeding up Dijkstra on graphs that have a number of edges asymptotically in between linear and quadratic in the number of graph vertices. You may quote the Fibonacci Heap running time bound whenever you need to argue the running time of Dijkstra when solving theory questions. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 4 Recitation 13 class Item: def __init__(self, label, key): self.label, self.key = label, key class PriorityQueue: # Binary Heap Implementation def __init__(self): # stores keys with unique labels self.A = [] self.label2idx = {} def min_heapify_up(self, c): if c == 0: return p = (c - 1) // 2 if self.A[p].key > self.A[c].key: self.A[c], self.A[p] = self.A[p], self.A[c] self.label2idx[self.A[c].label] = c self.label2idx[self.A[p].label] = p self.min_heapify_up(p) def min_heapify_down(self, p): if p >= len(self.A): return l = 2 * p + 1 r = 2 * p + 2 if l >= len(self.A): l = p if r >= len(self.A): r = p c = l if self.A[r].key > self.A[l].key else r if self.A[p].key > self.A[c].key: self.A[c], self.A[p] = self.A[p], self.A[c] self.label2idx[self.A[c].label] = c self.label2idx[self.A[p].label] = p self.min_heapify_down(c) def insert(self, label, key): # insert labeled key self.A.append(Item(label, key)) idx = len(self.A) - 1 self.label2idx[self.A[idx].label] = idx self.min_heapify_up(idx) def extract_min(self): # remove a label with minimum key self.A[0], self.A[-1] = self.A[-1], self.A[0] self.label2idx[self.A[0].label] = 0 del self.label2idx[self.A[-1].label] min_label = self.A.pop().label self.min_heapify_down(0) return min_label def decrease_key(self, label, key): # decrease key of a given label if label in self.label2idx: idx = self.label2idx[label] if key < self.A[idx].key: self.A[idx].key = key self.min_heapify_up(idx) 5 Recitation 13 Fibonacci Heaps are not actually used very often in practice as it is more complex to implement, and results in larger constant factor overhead than the other two implementations described above. When the number of edges in the graph is known to be at most linear (e.g., planar or bounded degree graphs) or at least quadratic (e.g. complete graphs) in the number of vertices, then using a binary heap or dictionary respectively will perform as well asymptotically as a Fibonacci Heap. We’ve made a JavaScript Dijkstra visualizer which you can ﬁnd here: https://codepen.io/mit6006/pen/BqgXWM Exercise: CIA ofﬁcer Mary Cathison needs to drive to meet with an informant across an unwel- come city. Some roads in the city are equipped with government surveillance cameras, and Mary will be detained if cameras from more than one road observe her car on the way to her informant. Mary has a map describing the length of each road and the locations and ranges of surveillance cameras. Help Mary ﬁnd the shortest drive to reach her informant, being seen by at most one surveillance camera along the way. Solution: Construct a graph having two vertices (v, 0) and (v, 1) for every road intersection v within the city. Vertex (v, i) represents arriving at intersection v having already been spotted by exactly i camera(s). For each road from intersection u to v: add two directed edges from (u, 0) to (v, 0) and from (u, 1) to (v, 1) if traveling on the road will not be visible by a camera; and add one directed edge from (u, 0) to (v, 1) if traveling on the road will be visible. If s is Mary’s start location and t is the location of the informant, any path from (s, 0) to (t, 0) or (t, 1) in the constructed graph will be a path visible by at most one camera. Let n be the number of road intersections and m be the number of roads in the network. Assuming lengths of roads are positive, use Dijkstra’s algorithm to ﬁnd the shortest such path in O(m + n log n) time using a Fibonacci Heap for Dijkstra’s priority queue. Alternatively, since the road network is likely planar and/or bounded degree, it may be safe to assume that m = O(n), so a binary heap could be used instead to ﬁnd a shortest path in O(n log n) time. MIT OpenCourseWare https://ocw.mit.edu 6.006 Introduction to Algorithms Spring 2020 For information about citing these materials or our Terms of Use, visit: https://ocw.mit.edu/terms Restrictions SSSP Algorithm Graph Weights Name Running Time O(·) General Unweighted BFS |V | + |E| DAG Any DAG Relaxation |V | + |E| General Non-negative Dijkstra Bellman-Ford |V | log |V | + |E| General Any Introduction to Algorithms: 6.006 Massachusetts Institute of Technology Instructors: Erik Demaine, Jason Ku, and Justin Solomon Recitation 14 Recitation 14 Single Source Shortest Paths Review We’ve learned four algorithms to solve the single source shortest paths (SSSP) problem; they are listed in the table below. Then, to solve shortest paths problems, you must ﬁrst deﬁne or construct a graph related to your problem, and then running an SSSP algorithm on that graph in a way that solves your problem. Generally, you will want to use the fastest SSSP algorithm that solves your problem. Bellman-Ford applies to any weighted graph but is the slowest of the four, so we prefer the other algorithms whenever they are applicable. |V | · |E| We presented these algorithms with respect to the SSSP problem, but along the way, we also showed how to use these algorithms to solve other problems. For example, we can also count connected components in a graph using Full-DFS or Full-BFS, topologically sort vertices in a DAG using DFS, and detect negative weight cycles using Bellman-Ford. All Pairs Shortest Paths Given a weighted graph G = (V, E, w), the (weighted) All Pairs Shortest Paths (APSP) problem asks for the minimum weight δ(u, v) of any path from u to v for every pair of vertices u, v ∈ V . To make the problem a little easier, if there exists a negative weight cycle in G, our algorithm is not required to return any output. A straight-forward way to solve this problem is to reduce to solving an SSSP problem |V | times, once from each vertex in V . This strategy is actually quite good for special types of graphs! For example, suppose we want to solve ASPS on an unweighted graph that is sparse (i.e. |E| = O(|V |)). Running BFS from each vertex takes O(|V |2) time. Since we need to return a value δ(u, v) for each pair of vertices, any ASPS algorithm requires at least Ω(V 2) time, so this algorithm is optimal for graphs that are unweighted and sparse. However, for general graphs, possibly containing negative weight edges, running Bellman-Ford |V | times is quite slow, O(|V |2|E|), a factor of |E| larger than the output. By contrast, if we have a graph that only has non-negative weights, applying Dijkstra |V | times takes O(|V |2 log |V | + |V ||E|) time. On a sparse graph, running Dijkstra |V | times is only a log |V | factor larger than the output, while |V | times Bellman-Ford is a linear |V | factor larger. Is it possible to solve the APSP problem on general weighted graphs faster than O(|V |2|E|)? Recitation 14 2 Johnson’s Algorithm The idea behind Johnson’s Algorithm is to reduce the ASPS problem on a graph with arbitrary edge weights to the ASPS problem on a graph with non-negative edge weights. The algorithm does this by re-weighting the edges in the original graph to non-negative values in such a way so that shortest paths in the re-weighted graph are also shortest paths in the original graph. Then ﬁnd- ing shortest paths in the re-weighted graph using |V | times Dijkstra will solve the original problem. How can we re-weight edges in a way that preserves shortest paths? Johnson’s clever idea is to assign each vertex v a real number h(v), and change the weight of each edge (a, b) from w(a, b) to w0(a, b) = w(a, b) + h(a) − h(b), to form a new weight graph G0 = (V, E, w0). Claim: A shortest path (v1, v2, . . . , vk) in G0 is also a shortest path in G from v1 to vk. Pk−1 Proof. Let w(π) = i=1 w(vi, vi+1) be the weight of path π in G. Then weight of π in G0 is: k−1 k−1 X X w 0(vi, vi+1) = w(vi, vi+1) + h(vi) − h(vi+1) i=1 i=1 ! ! ! k−1 k−1 k−1 X X X = w(vi, vi+1) + h(vi) − h(vi+1) = w(π) + h(v1) − h(vk). i=1 i=1 i=1 So, since each path from v1 to vk is increased by the same number h(v1) − h(vk), shortest paths remain shortest. It remains to ﬁnd a vertex assignment function h, for which all edge weights w0(a, b) in the modi- ﬁed graph are non-negative. Johnson’s deﬁnes h in the following way: add a new node x to G with a directed edge from x to v for each vertex v ∈ V to construct graph G∗ , letting h(v) = δ(x, v). This assignment of h ensures that w0(a, b) ≥ 0 for every edge (a, b). Claim: If h(v) = δ(x, v) and h(v) is ﬁnite, then w0(a, b) = w(a, b) + h(a) − h(b) ≥ 0 for every edge (a, b) ∈ E. Proof. The claim is equivalent to claiming δ(x, b) ≤ w(a, b)+ δ(x, a) for every edge (a, b) ∈ E, i.e. the minimum weight of any path from x to b in G∗ is not greater than the minimum weight of any path from x to a than traversing the edge from a to b, which is true by deﬁnition of minimum weight. (This is simply a restatement of the triangle inequality.) Johnson’s algorithm computes h(v) = δ(x, v), negative minimum weight distances from the added node x, using Bellman-Ford. If δ(x, v) = −∞ for any vertex v, then there must be a negative weight cycle in the graph, and Johnson’s can terminate as no output is required. Otherwise, John- son’s can re-weight the edges of G to w0(a, b) = w(a, b)+ h(a) − h(b) ≥ 0 into G0 containing only positive edge weights. Since shortest paths in G0 are shortest paths in G, we can run Dijkstra |V | times on G0 to ﬁnd a single source shortest paths distances δ0(u, v) from each vertex u in G0 . Then we can compute each δ(u, v) by setting it to δ0(u, v)−δ(x, u)+δ(x, y). Johnson’s takes O(|V ||E|) time to run Bellman-Ford, and O(|V |(|V | log |V | + |E|)) time to run Dijkstra |V | times, so this algorithm runs in O(|V |2 log |V | + |V ||E|) time, asymptotically better than O(|V |2|E|). MIT OpenCourseWare https://ocw.mit.edu 6.006 Introduction to Algorithms Spring 2020 For information about citing these materials or our Terms of Use, visit: https://ocw.mit.edu/terms Introduction to Algorithms: 6.006 Massachusetts Institute of Technology Instructors: Erik Demaine, Jason Ku, and Justin Solomon Recitation 15 Recitation 15 Dynamic Programming Dynamic Programming generalizes Divide and Conquer type recurrences when subproblem de- pendencies form a directed acyclic graph instead of a tree. Dynamic Programming often applies to optimization problems, where you are maximizing or minimizing a single scalar value, or counting problems, where you have to count all possibilities. To solve a problem using dynamic program- ming, we follow the following steps as part of a recursive problem solving framework. How to Solve a Problem Recursively (SRT BOT) 1. Subproblem deﬁnition subproblem x ∈ X • Describe the meaning of a subproblem in words, in terms of parameters • Often subsets of input: preﬁxes, sufﬁxes, contiguous subsequences • Often record partial state: add subproblems by incrementing some auxiliary variables 2. Relate subproblem solutions recursively x(i) = f(x(j), . . .) for one or more j < i 3. Topological order to argue relation is acyclic and subproblems form a DAG 4. Base cases • State solutions for all (reachable) independent subproblems where relation doesn’t ap- ply/work 5. Original problem • Show how to compute solution to original problem from solutions to subproblems • Possibly use parent pointers to recover actual solution, not just objective function 6. Time analysis P • work(x), or if work(x) = O(W ) for all x ∈ X, then |X| · O(W ) x∈X • work(x) measures nonrecursive work in relation; treat recursions as taking O(1) time 2 Recitation 15 Implementation Once subproblems are chosen and a DAG of dependencies is found, there are two primary methods for solving the problem, which are functionally equivalent but are implemented differently. • A top down approach evaluates the recursion starting from roots (vertices incident to no incoming edges). At the end of each recursive call the calculated solution to a subproblem is recorded into a memo, while at the start of each recursive call, the memo is checked to see if that subproblem has already been solved. • A bottom up approach calculates each subproblem according to a topological sort order of the DAG of subproblem dependencies, also recording each subproblem solution in a memo so it can be used to solve later subproblems. Usually subproblems are constructed so that a topological sort order is obvious, especially when subproblems only depend on subproblems having smaller parameters, so performing a DFS to ﬁnd this ordering is usually unnecessary. Top down is a recursive view, while Bottom up unrolls the recursion. Both implementations are valid and often used. Memoization is used in both implementations to remember computation from previous subproblems. While it is typical to memoize all evaluated subproblems, it is often possi- ble to remember (memoize) fewer subproblems, especially when subproblems occur in ‘rounds’. Often we don’t just want the value that is optimized, but we would also like to return a path of subproblems that resulted in the optimized value. To reconstruct the answer, we need to maintain auxiliary information in addition to the value we are optimizing. Along with the value we are optimizing, we can maintain parent pointers to the subproblem or subproblems upon which a solution to the current subproblem depends. This is analogous to maintaining parent pointers in shortest path problems. Exercise: Simpliﬁed Blackjack We deﬁne a simpliﬁed version of the game blackjack between one player and a dealer. A deck of cards is an ordered sequence of n cards D = (c1, . . . , cn), where each card ci is an integer between 1 and 10 inclusive (unlike in real blackjack, aces will always have value 1). Blackjack is played in rounds. In one round, the dealer will draw the top two cards from the deck (initially c1 and c2), then the player will draw the next two cards (initially c3 and c4), and then the player may either choose to draw or not draw one additional card (a hit). The player wins the round if the value of the player’s hand (i.e., the sum of cards drawn by the player in the round) is ≤ 21 and exceeds the value of the dealer’s hand; otherwise, the player loses the round. The game ends when a round ends with fewer than 5 cards remaining in the deck. Given a deck of n cards with a known order, describe an O(n)-time algorithm to determine the maximum number of rounds the player can win by playing simpliﬁed blackjack with the deck. 3 Recitation 15 Solution: 1. Subproblems • Choose sufﬁxes • x(i) : maximum rounds player can win by playing blackjack using cards (ci, . . . , cn) 2. Relate • Guess whether the player hits or not • Dealer’s hand always has value ci + ci+1 • Player’s hand will have value either: – ci+2 + ci+3 (no hit, 4 cards used in round), or – ci+2 + ci+3 + ci+4 (hit, 5 cards used in round) • Let w(d, p) be the round result given hand values d and p (dealer and player) – player win: w(d, p) = 1 if d < p ≤ 21 – player loss: w(d, p) = 0 otherwise (if p ≤ d or 21 < p) • x(i) = max{w(ci +ci+1, ci+2 +ci+3)+x(i+4), w(ci +ci+1, ci+2 +ci+3 +ci+4)+x(i+5)} • (for n − (i − 1) ≥ 5, i.e., i ≤ n − 4) 3. Topo • Subproblems x(i) only depend on strictly larger i, so acyclic 4. Base • x(n − 3) = x(n − 2) = x(n − 1) = x(n) = x(n + 1) = 0 • (not enough cards for another round) 5. Original • Solve x(i) for i ∈{1, . . . , n + 1}, via recursive top down or iterative bottom up • x(1): the maximum rounds player can win by playing blackjack with the full deck 6. Time • # subproblems: n + 1 • work per subproblem: O(1) • O(n) running time 4 Recitation 15 Exercise: Text Justiﬁcation Text Justiﬁcation is the problem of ﬁtting a sequence of n space separated words into a column of lines with constant width s, to minimize the amount of white-space between words. Each word can be represented by its width wi < s. A good way to minimize white space in a line is to minimize badness of a line. Assuming a line contains words from wi to wj, the badness of the line is deﬁned as b(i, j) = (s − (wi + . . . + wj ))3 if s > (wi + . . . + wj ), and b(i, j) = ∞ otherwise. A good text justiﬁcation would then partition words into lines to minimize the sum total of badness over all lines containing words. The cubic power heavily penalizes large white space in a line. Microsoft Word uses a greedy algorithm to justify text that puts as many words into a line as it can before moving to the next line. This algorithm can lead to some really bad lines. LATEX on the other hand formats text to minimize this measure of white space using a dynamic program. Describe an O(n2) algorithm to ﬁt n words into a column of width s that minimizes the sum of badness over all lines. Solution: 1. Subproblems • Choose sufﬁxes as subproblems • x(i): minimum badness sum of formatting the words from wi to wn−1 2. Relate • The ﬁrst line must break at some word, so try all possibilities • x(i) = min{b(i, j) + x(j + 1) | i ≤ j < n} 3. Topo • Subproblems x(i) only depend on strictly larger i, so acyclic 4. Base • x(n) = 0 badness of justifying zero words is zero 5. Original • Solve subproblems via recursive top down or iterative bottom up • Solution to original problem is x(0) • Store parent pointers to reconstruct line breaks 6. Time • # subproblems: O(n) 2) • work per subproblem: O(n 5 Recitation 15 • O(n3) running time • Can we do even better? Optimization • Computing badness b(i, j) could take linear time! • If we could pre-compute and remember each b(i, j) in O(1) time, then: • work per subproblem: O(n) • O(n2) running time Pre-compute all b(i, j) in O(n2), also using dynamic programming! 1. Subproblems • x(i, j): sum of word lengths wi to wj 2. Relate P • x(i, j) = k wk takes O(j − i) time to compute, slow! • x(i, j) = x(i, j − 1) + wj takes O(1) time to compute, faster! 3. Topo • Subproblems x(i, j) only depend on strictly smaller j − i, so acyclic 4. Base • x(i, i) = wi for all 0 ≤ i < n, just one word 5. Original • Solve subproblems via recursive top down or iterative bottom up • Compute each b(i, j) = (s − x(i, j))3 in O(1) time 6. Time 2) • # subproblems: O(n • work per subproblem: O(1) • O(n2) running time MIT OpenCourseWare https://ocw.mit.edu 6.006 Introduction to Algorithms Spring 2020 For information about citing these materials or our Terms of Use, visit: https://ocw.mit.edu/terms Introduction to Algorithms: 6.006 Massachusetts Institute of Technology Instructors: Erik Demaine, Jason Ku, and Justin Solomon Recitation 16 Recitation 16 Dynamic Programming Exercises Max Subarray Sum Given an array A of n integers, what is the largest sum of any nonempty subarray? (in this class, subarray always means a contiguous sequence of elements) Example: A = [-9, 1, -5, 4, 3, -6, 7, 8, -2], largest subsum is 16. Solution: We could brute force in O(n3) by computing the sum of each of the O(n2) subarrays in O(n) time. We can get a faster algorithm by noticing that the subarray with maximum sum must end somewhere. Finding the maximum subarray ending at a particular location k can be computed in O(n) time by scanning to the left from k, keeping track of a rolling sum, and remembering the maximum along the way; since there are n ending locations, this algorithm runs in O(n2) time. We can do even faster by recognizing that each successive scan to the left is redoing work that has already been done in earlier scans. Let’s use dynamic programming to reuse this work! 1. Subproblems • x(k): the max subarray sum ending at A[k] • Preﬁx subproblem, but with condition, like Longest Increase Subsequence • (Exercise: reformulate in terms of sufﬁxes instead of preﬁxes) 2. Relate • Maximizing subarray ending at k either uses item k − 1 or it doesn’t • If it doesn’t, then subarray is just A[k] • Otherwise, k − 1 is used, and we should include the maximum subarray ending at k − 1 • x(k) = max{A[k], A[k] + x(k − 1)} 3. Topo. Order • Subproblems x(k) only depend on strictly smaller k, so acyclic 4. Base • x(0) = A[0] (since subarray must be nonempty) 5. Original • Solve subproblems via recursive top down or iterative bottom up 2 Recitation 16 • Solution to original is max of all subproblems, i.e., max{x(k) | k ∈{0, . . . , n − 1}} • Subproblems are used twice: when computing the next larger, and in the ﬁnal max 6. Time • # subproblems: O(n) • work per subproblem: O(1) • time to solve original problem: O(n) • O(n) time in total 1 # bottom up implementation 2 def max_subarray_sum(A): 3 x = [None for _ in A] # memo 4 x[0] = A[0] # base case 5 for k in range(1, len(A)): # iteration 6 x[k] = max(A[k], A[k] + x[k - 1]) # relation 7 return max(x) # original Edit Distance A plagiarism detector needs to detect the similarity between two texts, string A and string B. One measure of similarity is called edit distance, the minimum number of edits that will transform string A into string B. An edit may be one of three operations: delete a character of A, replace a character of A with another letter, and insert a character between two characters of A. Describe a O(|A||B|) time algorithm to compute the edit distance between A and B. Solution: 1. Subproblems • Approach will be to modify A until its last character matches B • x(i, j): minimum number of edits to transform preﬁx up to A(i) to preﬁx up to B(j) • (Exercise: reformulate in terms of sufﬁxes instead of preﬁxes) 2. Relate • If A(i) = B(j), then match! • Otherwise, need to edit to make last element of A equal to B(j) • Edit is either an insertion, replace, or deletion (Guess!) • Deletion removes A(i) • Insertion adds B(j) to end of A, then removes it and B(j) 3 Recitation 16 • Replace changes A(i) to B(j) and removes both A(i) and B(j) \u001a x(i − 1, j − 1) if A(i) = B(i) • x(i, j) = 1 + min(x(i − 1, j), x(i, j − 1), x(i − 1, j − 1)) otherwise 3. Topo. Order • Subproblems x(i, j) only depend on strictly smaller i and j, so acyclic 4. Base • x(i, 0) = i, x(0, j) = j (need many insertions or deletions) 5. Original • Solve subproblems via recursive top down or iterative bottom up • Solution to original problem is x(|A|, |B|) • (Can store parent pointers to reconstruct edits transforming A to B) 6. Time • # subproblems: O(n2) • work per subproblem: O(1) • O(n2) running time 1 def edit_distance(A, B): 2 x = [[None] * len(A) for _ in range(len(B))] # memo 3 x[0][0] = 0 # base cases 4 for i in range(1, len(A)): 5 x[i][0] = x[i - 1][0] + 1 # delete A[i] 6 for j in range(1, len(B)): 7 x[0][j] = x[0][j - 1] + 1 # insert B[j] into A 8 for i in range(1, len(A)): # dynamic program 9 for j in range(1, len(B)): 10 if A[i] == B[j]: 11 x[i][j] = x[i - 1][j - 1] # matched! no edit needed 12 else: # edit needed! 13 ed_del = 1 + x[i - 1][j] # delete A[i] 14 ed_ins = 1 + x[i][j - 1] # insert B[j] after A[i] 15 ed_rep = 1 + x[i - 1][j - 1] # replace A[i] with B[j] 16 x[i][j] = min(ed_del, ed_ins, ed_rep) 17 return x[len(A) - 1][len(B) - 1] Exercise: Modify the code above to return a minimal sequence of edits to transform string A into string B. (Note, the base cases in the above code are computed individually to make reconstructing a solution easier.) MIT OpenCourseWare https://ocw.mit.edu 6.006 Introduction to Algorithms Spring 2020 For information about citing these materials or our Terms of Use, visit: https://ocw.mit.edu/terms Introduction to Algorithms: 6.006 Massachusetts Institute of Technology Instructors: Erik Demaine, Jason Ku, and Justin Solomon Recitation 17 Recitation 17 Treasureship! The new boardgame Treasureship is played by placing 2 × 1 ships within a 2 × n rectangular grid. Just as in regular battleship, each 2 × 1 ship can be placed either horizontally or vertically, occupy- ing exactly 2 grid squares, and each grid square may only be occupied by a single ship. Each grid square has a positive or negative integer value, representing how much treasure may be acquired or lost at that square. You may place as many ships on the board as you like, with the score of a placement of ships being the value sum of all grid squares covered by ships. Design an efﬁcient dynamic-programming algorithm to determine a placement of ships that will maximize your total score. Solution: 1. Subproblems • The game board has n columns of height 2 (alternatively 2 rows of width n) • Let v(x, y) denote the grid value at row y column x, for y ∈{1, 2} and x ∈{1, . . . , n} • Guess how to cover the right-most square(s) in an optimal placement • Can either: – not cover, – place a ship to cover vertically, or – place a ship to cover horizontally. • After choosing an option, the remainder of the board may not be a rectangle • Right side of board looks like one of the following cases: 1 (0) ##### (+1) ##### (-1) #### (+2) ##### (-2) ### row 2 2 ##### #### ##### ### ##### row 1 • Exists optimal placement where no two ships aligned horizontally on top of each other • Proof: cover instead by two vertical ships next to each other! • So actually only need ﬁrst three cases above: 0, +1, −1 • Let s(i, j) represent game board subset containing columns 1 to i of row 1, and columns 1 to i + j of row 2, for j ∈{0, +1, −1} • x(i, j): maximum score, only placing ships on board subset s(i, j) • for i ∈{0, . . . , n}, j ∈{0, +1, −1} 2 Recitation 17 2. Relate • If j = +1, can cover right-most square with horizontal ship or leave empty • If j = −1, can cover right-most square with horizontal ship or leave empty • If j = 0, can cover column i with vertical ship or not cover one of right-most squares ⎧ ⎨ max{v(i, 1) + v(i − 1, 1) + x(i − 2, +1), x(i − 1, 0)} if j = −1 • x(i, j) = max{v(i + 1, 2) + v(i, 2) + x(i, −1), x(i, 0)} if j = +1 ⎩ max{v(i, 1) + v(i, 2) + x(i − 1, 0), x(i, −1), x(i − 1, +1)} if j = 0 3. Topo • Subproblems x(i, j) only depend on strictly smaller 2i + j, so acyclic. 4. Base • s(i, j) contains 2i + j grid squares • x(i, j) = 0 if 2i + j < 2 (can’t place a ship if fewer than 2 squares!) 5. Original • Solution is x(n, 0), the maximum considering all grid squares. • Store parent pointers to reconstruct ship locations 6. Time • # subproblems: O(n) • work per subproblem O(1) • O(n) running time 3 Recitation 17 Wafer Power A start-up is working on a new electronic circuit design for highly-parallel computing. Evenly- spaced along the perimeter of a circular wafer sits n ports for either a power source or a computing unit. Each computing unit needs energy from a power source, transferred between ports via a wire etched into the top surface of the wafer. However, if a computing unit is connected to a power source that is too close, the power can overload and destroy the circuit. Further, no two etched wires may cross each other. The circuit designer needs an automated way to evaluate the effectiveness of different designs, and has asked you for help. Given an arrangement of power sources and computing units plugged into the n ports, describe an O(n3)-time dynamic programming algorithm to match computing units to power sources by etching non-crossing wires between them onto the surface of the wafer, in order to maximize the number of powered computing units, where wires may not connect two adjacent ports along the perimeter. Below is an example wafer, with non- crossing wires connecting computing units (white) to power sources (black). Solution: 1. Subproblems • Let (a1, . . . , an) be the ports cyclically ordered counter-clockwise around the wafer, where ports a1 and an are adjacent • Let ai be True if the port is a computing unit, and False if it is a power source • Want to match opposite ports connected by non-crossing wires • If match across the wafer, need to independently match ports on either side (substrings!) • x(i, j): maximum number of matchings, restricting to ports ak for all k ∈{i, . . . , j} • for i ∈{1, . . . , n}, j ∈{i − 1, . . . , n} • j − i + 1 is number of ports in substring (allow j = i − 1 as an empty substring) 4 Recitation 17 2. Relate • Guess what port to match with ﬁrst port in substring. Either: – ﬁrst port does not match with anything, try to match the rest; – ﬁrst port matches a port in the middle, try to match each side independently. • Non-adjacency condition restricts possible matchings between i and some port t: – if (i, j) = (1, n), can’t match i with last port n or 2, so try t ∈{3, . . . , n − 1} – otherwise, just can’t match i with i + 1, so try t ∈{i + 2, . . . , j} • Let m(i, j) = 1 if ai 6= aj and m(i, j) = 0 otherwise (ports of opposite type match) • x(1, n) = max{x(2, n)} ∪{m(1, t) + x(2, t − 1) + x(t + 1, n) | t ∈{3, . . . , n − 1}} • x(i, j) = max{x(i+1, j)}∪{m(i, t)+x(i+1, t−1)+x(t+1, j) | t ∈{i+2, . . . , j}} 3. Topo • Subproblems x(i, j) only depend on strictly smaller j − i, so acyclic 4. Base • x(i, j) = 0 for any j − i + 1 ∈{0, 1, 2} (no match within 0, 1, or 2 adjacent ports) 5. Original • Solve subproblems via recursive top down or iterative bottom up. • Solution to original problem is x(1, n). • Store parent pointers to reconstruct matching (e.g., choice of t or no match at each step) 6. Time • # subproblems: O(n2) • work per subproblem: O(n) • O(n3) running time MIT OpenCourseWare https://ocw.mit.edu 6.006 Introduction to Algorithms Spring 2020 For information about citing these materials or our Terms of Use, visit: https://ocw.mit.edu/terms Introduction to Algorithms: 6.006 Massachusetts Institute of Technology Instructors: Erik Demaine, Jason Ku, and Justin Solomon Recitation 18: Subset Sum Variants Recitation 18: Subset Sum Variants Subset Sum Review • Input: Set of n positive integers A[i] P • Output: Is there subset A0 ⊂ A such that a∈A0 a = S? • Can solve with dynamic programming in O(nS) time Subset Sum 1. Subproblems • Here we’ll try 1-indexed preﬁxes for comparison • x(i, j): True if can make sum j using items 1 to i, False otherwise 2. Relate • Is last item i in a valid subset? (Guess!) • If yes, then try to sum to j − A[i] ≥ 0 using remaining items • If no, then try to sum to j using remaining items ( ) x(i − 1, j − A[i]) if j ≥ A[i] • x(i, j) = OR x(i − 1, j) always • for i ∈{0, . . . , n}, j ∈{0, . . . , S} 3. Topo • Subproblems x(i, j) only depend on strictly smaller i, so acyclic 4. Base • x(i, 0) = True for i ∈{0, . . . , n} (trivial to make zero sum!) • x(0, j) = False for j ∈{1, . . . , S} (impossible to make positive sum from empty set) 5. Original • Solve subproblems via recursive top down or iterative bottom up • Maximum evaluated expression is given by x(n, S) 6. Time • (# subproblems: O(nS)) × (work per subproblem O(1)) = O(nS) running time. 2 Recitation 18: Subset Sum Variants Exercise: Partition - Given a set of n positive integers A, describe an algorithm to determine whether A can be partitioned into two non-intersecting subsets A1 and A2 of equal sum, i.e. P P A1 ∩ A2 = ∅ and A1 ∪ A2 = A such that a∈A1 a = a∈A2 a. Example: A = {1, 4, 3, 12, 19, 21, 22} has partition A1 = {1, 19, 21}, A2 = {3, 4, 12, 22}. 1 P Solution: Run subset sum dynamic program with same A and S = a. 2 a∈A Exercise: Close Partition - Given a set of n positive integers A, describe an algorithm to ﬁnd a partition of A into two non-intersecting subsets A1 and A2 such that the difference between their respective sums are minimized. P Solution: Run subset sum dynamic program as above, but evaluate for every S0 ∈{0, . . . , 1 a}, 2 a∈A and return the largest S0 such that the subset sum dynamic program returns true. Note that this still only takes O(nS) time: O(nS) to compute all subproblems, and then O(nS) time again to loop over the subproblems to ﬁnd the max true S0 . Exercise: Can you adapt subset sum to work with negative integers? Solution: Same as subset sum (see L19), but we allow calling subproblems with larger j. But now instead of solving x(i, j) only in the range i ∈{0, . . . , n}, j ∈{0, . . . , S} as in positive subset P P sum, we allow j to range from jmin = a (smallest possible j) to jmax = a a∈A,a<0 a∈A,a>0 (largest possible j). x(i, j) = OR {x(i − 1, j − A[i]), x(i − 1, j)} (note jmin ≤ j − A[i] ≤ jmax is always true) Subproblem dependencies are still acyclic because x(i, j) only depend on strictly smaller i. Base cases are x(0, 0) = True and x(0, j) = False if j 6= 0. Running time is then proportional to number of constant work subproblems, O(n(jmax − jmin)). Alternatively, you can convert to an equivalent instance of positive subset sum and solve that. P Choose large number Q > max(|S|, |a|). Add 2Q to each integer in A to form A0 , and a∈A append the value 2Q, n − 1 times to the end of A0 . Every element of A0 is now positive, so solve positive subset sum with S0 = S + n(2Q). Because (2n − 1)Q < S0 < (2n + 1)Q, any satisfying subset will contain exactly n integers from A0 since the sum of any fewer would have sum no P greater than (n − 1)2Q + |a| < (2n − 1)Q, and sum of any more would have sum no smaller P a∈A than (n + 1)2Q − |a| > (2n + 1)Q. Further, at least one integer in a satisfying subset of A0 a∈A corresponds to an integer of A since S0 is not divisible by 2Q. If A0 has a subset B0 summing to S0, then the items in A corresponding to integers in B0 will comprise a nonempty subset that sums to S. Conversely, if A has a subset B that sums to S, choosing the k elements of A0 corresponding the integers in B and n − k of the added 2Q values in A0 will comprise a subset B0 that sums to S0 . This is an example of a reduction: we show how to use a black-box to solve positive subset sum to solve general subset sum. However, this reduction does lead to a weaker pseudopolynomial time bound of O(n(S + 2nQ)) than the modiﬁed algorithm presented above. 3 vi Recitation 18: Subset Sum Variants 0-1 Knapsack • Input: Knapsack with size S, want to ﬁll with items each item i has size si and value vi. P P • Output: A subset of items (may take 0 or 1 of each) with si ≤ S maximizing value • (Subset sum same as 0-1 Knapsack when each vi = si, deciding if total value S achievable) • Example: Items {(si, vi)} = {(6, 6), (9, 9), (10, 12)}, S = 15 • Solution: Subset with max value is all items except the last one (greedy fails) 1. Subproblems • Idea: Is last item in an optimal knapsack? (Guess!) • If yes, get value vi and pack remaining space S − si using remaining items • If no, then try to sum to S using remaining items • x(i, j): maximum value by packing knapsack of size j using items 1 to i 2. Relate ( ) vi + x(i − 1, j − si) if j ≥ si • x(i, j) = max x(i − 1, j) always • for i ∈{0, . . . , n}, j ∈{0, . . . , S} 3. Topo • Subproblems x(i, j) only depend on strictly smaller i, so acyclic 4. Base • x(i, 0) = 0 for i ∈{0, . . . , n} (zero value possible if no more space) • x(0, j) = 0 for j ∈{1, . . . , S} (zero value possible if no more items) 5. Original • Solve subproblems via recursive top down or iterative bottom up • Maximum evaluated expression is given by x(n, S) • Store parent pointers to reconstruct items to put in knapsack 6. Time • # subproblems: O(nS) • work per subproblem O(1) • O(nS) running time 4 Recitation 18: Subset Sum Variants Exercise: Close Partition (Alternative solution) 1 P Solution: Given integers A, solve a 0-1 Knapsack instance with si = vi = A[i] and S = a, 2 a∈A where the subset returned will be one half of a closest partition. Exercise: Unbounded Knapsack - Same problem as 0-1 Knapsack, except that you may take as many of any item as you like. Solution: The O-1 Knapsack formulation works directly except for a small change in relation, where i will not be decreased if it is taken once, where the topological order strictly decreases i + j with each recursive call. ( ) vi + x(i, j − si) if j ≥ si x(i, j) = max x(i − 1, j) always An equivalent formulation reduces subproblems to expand work done per subproblem: 1. Subproblems: • x(j): maximum value by packing knapsack of size j using the provided items 2. Relate: • x(j) = max{vi + x(j − si) | i ∈{1, . . . , n} and si ≤ j} ∪{0}, for j ∈{0, . . . , S} 3. Topo • Subproblems x(j) only depend on strictly smaller j, so acyclic 4. Base • x(0) = 0 (no space to pack!) 5. Original • Solve subproblems via recursive top down or iterative bottom up • Maximum evaluated expression is given by x(S) • Store parent pointers to reconstruct items to put in knapsack 6. Time • # subproblems: O(S) • work per subproblem O(n) • O(nS) running time We’ve made CoffeeScript visualizers solving subset sum and 0-1 Knapsack: https://codepen.io/mit6006/pen/JeBvKe https://codepen.io/mit6006/pen/VVEPod MIT OpenCourseWare https://ocw.mit.edu 6.006 Introduction to Algorithms Spring 2020 For information about citing these materials or our Terms of Use, visit: https://ocw.mit.edu/terms Introduction to Algorithms: 6.006 Massachusetts Institute of Technology Instructors: Erik Demaine, Jason Ku, and Justin Solomon Recitation 19: Complexity Recitation 19: Complexity 0-1 Knapsack Revisited • 0-1 Knapsack – Input: Knapsack with volume S, want to ﬁll with items: item i has size si and value vi. P P – Output: A subset of items (may take 0 or 1 of each) with si ≤ S maximizing vi – Solvable in O(nS) time via dynamic programming • How does running time compare to input? – What is size of input? If numbers written in binary, input has size O(n log S) bits – Then O(nS) runs in exponential time compared to the input – If numbers polynomially bounded, S = nO(1), then dynamic program is polynomial – This is called a pseudopolynomial time algorithm • Is 0-1 Knapsack solvable in polynomial time when numbers not polynomially bounded? • No if P 6= NP. What does this mean? (More Computational Complexity in 6.045 and 6.046) Decision Problems • Decision problem: assignment of inputs to No (0) or Yes (1) • Inputs are either No instances or Yes instances (i.e. satisfying instances) Problem s-t Shortest Path Does a given G contain a path from s to t with weight at most d? Negative Cycle Does a given G contain a negative weight cycle? Longest Path Does a given G contain a simple path with weight at least d? Subset Sum Does a given set of integers A contain a subset with sum S? Tetris Can you survive a given sequence of pieces? Chess Can a player force a win from a given board? Halting problem Does a given computer program terminate for a given input? Decision • Algorithm/Program: constant length code (working on a word-RAM with Ω(log n)-bit words) to solve a problem, i.e., it produces correct output for every input and the length of the code is independent of the instance size • Problem is decidable if there exists a program to solve the problem in ﬁnite time 2 Recitation 20: Complexity Decidability • Program is ﬁnite string of bits, problem is function p : N →{0, 1}, i.e. inﬁnite string of bits • (# of programs |N|, countably inﬁnite) ≪ (# of problems |R|, uncountably inﬁnite) • (Proof by Cantor’s diagonal argument, probably covered in 6.042) • Proves that most decision problems not solvable by any program (undecidable) • e.g. the Halting problem is undecidable (many awesome proofs in 6.045) • Fortunately most problems we think of are algorithmic in structure and are decidable Decidable Problem Classes R problems decidable in ﬁnite time ‘R’ comes from recursive languages EXP problems decidable in exponential time 2nO(1) most problems we think of are here P problems decidable in polynomial time nO(1) efﬁcient algorithms, the focus of this class • These sets are distinct, i.e. P ( EXP ( R (via time hierarchy theorems, see 6.045) Nondeterministic Polynomial Time (NP) • P is the set of decision problems for which there is an algorithm A such that for every instance I of size n, A on I runs in poly(n) time and solves I correctly • NP is the set of decision problems for which there is an algorithm V , a “veriﬁer”, that takes as input an instance I of the problem, and a “certiﬁcate” bit string of length polynomial in the size of I, so that: – V always runs in time polynomial in the size of I, – if I is a YES-instance, then there is some certiﬁcate c so that V on input (I, c) returns YES, and – if I is a NO-instance, then no matter what c is given to V together with I, V will always output NO on (I, c). • You can think of the certiﬁcate as a proof that I is a YES-instance. If I is actually a NO- instance then no proof should work. 3 Recitation 20: Complexity Problem Certiﬁcate Veriﬁer s-t Shortest Path Negative Cycle Longest Path Subset Sum Tetris A path P from s to t A cycle C A path P A set of items A0 Sequence of moves Adds the weights on P and checks if ≤ d Adds the weights on C and checks if < 0 Checks if P is a simple path with weight at least d Checks if A0 ∈ A has sum S Checks that the moves allow survival • P ⊂ NP (if you can solve the problem, the solution is a certiﬁcate) • Open: Does P = NP? NP = EXP? • Most people think P ( NP (( EXP), i.e.,t generating solutions harder than checking • If you prove either way, people will give you lots of money. ($1M Millennium Prize) • Why do we care? If can show a problem is hardest problem in NP, then problem cannot be solved in polynomial time if P 6= NP • How do we relate difﬁculty of problems? Reductions! Reductions • Suppose you want to solve problem A • One way to solve is to convert A into a problem B you know how to solve • Solve using an algorithm for B and use it to compute solution to A • This is called a reduction from problem A to problem B (A → B) • Because B can be used to solve A, B is at least as hard (A ≤ B) • General algorithmic strategy: reduce to a problem you know how to solve A Conversion B Unweighted Shortest Path Product Weighted Shortest Path Sum Weighted Shortest Path Give equal weights Logarithms Exponents Weighted Shortest Path Sum Weighted Shortest Path Product Weighted Shortest Path • Problem A is NP-Hard if every problem in NP is polynomially reducible to A • i.e. A is at least as hard as (can be used to solve) every problem in NP (X ≤ A for X ∈ NP) • NP-Complete = NP ∩ NP-Hard 4 Recitation 20: Complexity • All NP-Complete problems are equivalent, i.e. reducible to each other • First NP-Complete? Every decision problem reducible to satisfying a logical circuit. • Longest Path, Tetris are NP-Complete, Chess is EXP-Complete EXP-Complete Problem Difficulty (informal) P NP EXP R NP-Hard EXP-Hard NP-Complete 0-1 Knapsack is NP-Hard • Reduce known NP-Hard Problem to 0-1 Knapsack: Partition – Input: List of n numbers ai – Output: Does there exist a partition into two sets with equal sum? P • Reduction: si = vi = ai, S = 2 1 ai • 0-1 Knapsack at least as hard as Partition, so since Partition is NP-Hard, so is 0-1 Knapsack • 0-1 Knapsack in NP, so also NP-Complete MIT OpenCourseWare https://ocw.mit.edu 6.006 Introduction to Algorithms Spring 2020 For information about citing these materials or our Terms of Use, visit: https://ocw.mit.edu/terms Introduction to Algorithms: 6.006 Massachusetts Institute of Technology Instructors: Erik Demaine, Jason Ku, and Justin Solomon Quiz 1 Review Quiz 1 Review High Level • Need to solve large problems n with constant-sized code, correctly and efﬁciently • Analyzing running time: How to count? – Asymptotics – Recurrences (substitution, tree method, Master Theorem) – Model of computation: Word-RAM, Comparison • How to solve an algorithms problem – Reduce to a problem you know how to solve ∗ Use an algorithm you know (e.g. sort) ∗ Use a data structure you know (e.g. search) – Design a new recursive algorithm (harder, mostly in 6.046) ∗ Brute Force ∗ Decrease & Conquer ∗ Divide & Conquer (like merge sort) ∗ Dynamic Programming (later in 6.006!) ∗ Greedy/Incremental Algorithm: Sorting Reduce your problem to a problem you already know how to solve using known algorithms. You should know how each of these sorting algorithms are implemented, as well as be able to choose the right algorithm for a given task. Algorithm Time O(·) In-place? Stable? Comments Insertion Sort 2 n Y Y O(nk) for k-proximate Selection Sort 2 n Y N O(n) swaps Merge Sort n log n N Y stable, optimal comparison AVL Sort n log n N Y good if also need dynamic Heap Sort n log n Y N low space, optimal comparison Counting Sort n + u N Y O(n) when u = O(n) Radix Sort n + n log u n N Y O(n) when u = O(nc) 2 Quiz 1 Review Data Structures Reduce your problem to using a data structure storing a set of items, supporting certain search and dynamic operations efﬁciently. You should know how each of these data structures implement the operations they support, as well as be able to choose the right data structure for a given task. Sequence data structures support extrinsic operations that maintain, query, and modify an exter- nally imposed order on items. Sequence Operations O(·) Container Static Dynamic Data Structure build(X) get at(i) set at(i,x) insert first(x) delete first() insert last(x) delete last() insert at(i,x) delete at(i) Array n 1 n n n Linked List n n 1 n n Dynamic Array n 1 n 1(a) n Sequence AVL n log n log n log n log n Set data structures support intrinsic operations that maintain, query, and modify a set of items based on what the items are, i.e., based on the unique key associated with each item. Set Data Structure Operations O(·) Container Static Dynamic Order build(X) find(k) insert(x) delete(k) find min() find max() find prev(k) find next(k) Array n n n n n Sorted Array n log n log n n 1 log n Direct Access u 1 1 u u Hash Table n(e) 1(e) 1(a)(e) n n Set AVL n log n log n log n log n log n Priority Queues support a limited number of Set operations. Priority Queue Operations O(·) Data Structure build(X) insert(x) delete max() find max() Dynamic Array n 1(a) n n Sorted Dyn. Array n log n n 1(a) 1 Set AVL n log n log n log n log n Binary Heap n log n(a) log n(a) 1 3 Quiz 1 Review Problem Solving Testing Strategies • Read every problem ﬁrst, rank them in the order of your conﬁdence • For most problems, you can receive ≥ 50% of points in two sentences or less • Probably better to do half the problems well than all the problems poorly Types of problems Type Internals Externals Tests understanding of: Mechanical Y N how core material works Reduction N Y how to apply core material Modiﬁcation Y Y how to adapt core material (augmentation, divide & conquer, amortization, etc.) Questions to ask: • Is this a Mechanical, Reduction, or Modiﬁcation type problem? • Is this problem about data structures? sorting? both? • If data structures, do you need to support Sequence ops? Set ops? both? • If stuck, is there an easy way to get a correct but inefﬁcient algorithm? Question yourself if you are: • Trying to compute decimals, rationals, or real numbers • Using Radix sort for every answer • Augmenting a binary tree with something other than a subtree property Data Structures Problems • First solve using Sorting or Set/Sequence interfaces, choose algorithm/data structure after • Describe all data structure(s) used (including what data they store) and their invariants • Implement every operation we ask for in terms of your data structures • Separate and label parts of your solution! 4 Quiz 1 Review Problem 1. Restaurant Lineup (S19 Q1) Popular restaurant Criminal Seafood does not take reservations, but maintains a wait list where customers who have been on the wait list longer are seated earlier. Sometimes customers decide to eat somewhere else, so the restaurant must remove them from the wait list. Assume each customer has a different name, and no two customers are added to the wait list at the exact same time. Design a database to help Criminal Seafood maintain its wait list supporting the following operations, each in O(1) time. State whether each operation running time is worst-case, amortized, and/or expected. build() initialize an empty database add name(x) add name x to the back of the wait list remove name(x) remove name x from the wait list seat() remove and return the name of the customer from the front of the wait list Problem 2. Rainy Research (S19 Q1) Mether Wan is a scientist who studies global rainfall. Mether often receives data measurements from a large set of deployed sensors. Each collected data measurement is a triple of integers (r, `, t), where r is a positive amount of rainfall measured at latitude ` at time t. The peak rainfall at latitude ` since time t is the maximum rainfall of any measurement at latitude ` measured at a time greater than or equal to t (or zero if no such measurement exists). Describe a database that can store Mether’s sensor data and support the following operations, each in worst-case O(log n) time where n is the number of measurements in the database at the time of the operation. build() initialize an empty database record data(r, `, t) add a rainfall measurement r at latitude ` at time t peak rainfall(`, t) return the peak rainfall at latitude ` since time t MIT OpenCourseWare https://ocw.mit.edu 6.006 Introduction to Algorithms Spring 2020 For information about citing these materials or our Terms of Use, visit: https://ocw.mit.edu/terms Introduction to Algorithms: 6.006 Massachusetts Institute of Technology Instructors: Erik Demaine, Jason Ku, and Justin Solomon Quiz 1 Review Quiz 1 Review High Level • Need to solve large problems n with constant-sized code, correctly and efﬁciently • Analyzing running time: How to count? – Asymptotics – Recurrences (substitution, tree method, Master Theorem) – Model of computation: Word-RAM, Comparison • How to solve an algorithms problem – Reduce to a problem you know how to solve ∗ Use an algorithm you know (e.g. sort) ∗ Use a data structure you know (e.g. search) – Design a new recursive algorithm (harder, mostly in 6.046) ∗ Brute Force ∗ Decrease & Conquer ∗ Divide & Conquer (like merge sort) ∗ Dynamic Programming (later in 6.006!) ∗ Greedy/Incremental Algorithm: Sorting Reduce your problem to a problem you already know how to solve using known algorithms. You should know how each of these sorting algorithms are implemented, as well as be able to choose the right algorithm for a given task. Algorithm Time O(·) In-place? Stable? Comments Insertion Sort 2 n Y Y O(nk) for k-proximate Selection Sort 2 n Y N O(n) swaps Merge Sort n log n N Y stable, optimal comparison AVL Sort n log n N Y good if also need dynamic Heap Sort n log n Y N low space, optimal comparison Counting Sort n + u N Y O(n) when u = O(n) Radix Sort n + n log u n N Y O(n) when u = O(nc) 2 Quiz 1 Review Data Structures Reduce your problem to using a data structure storing a set of items, supporting certain search and dynamic operations efﬁciently. You should know how each of these data structures implement the operations they support, as well as be able to choose the right data structure for a given task. Sequence data structures support extrinsic operations that maintain, query, and modify an exter- nally imposed order on items. Sequence Operations O(·) Container Static Dynamic Data Structure build(X) get at(i) set at(i,x) insert first(x) delete first() insert last(x) delete last() insert at(i,x) delete at(i) Array n 1 n n n Linked List n n 1 n n Dynamic Array n 1 n 1(a) n Sequence AVL n log n log n log n log n Set data structures support intrinsic operations that maintain, query, and modify a set of items based on what the items are, i.e., based on the unique key associated with each item. Set Data Structure Operations O(·) Container Static Dynamic Order build(X) find(k) insert(x) delete(k) find min() find max() find prev(k) find next(k) Array n n n n n Sorted Array n log n log n n 1 log n Direct Access u 1 1 u u Hash Table n(e) 1(e) 1(a)(e) n n Set AVL n log n log n log n log n log n Priority Queues support a limited number of Set operations. Priority Queue Operations O(·) Data Structure build(X) insert(x) delete max() find max() Dynamic Array n 1(a) n n Sorted Dyn. Array n log n n 1(a) 1 Set AVL n log n log n log n log n Binary Heap n log n(a) log n(a) 1 3 Quiz 1 Review Problem Solving Testing Strategies • Read every problem ﬁrst, rank them in the order of your conﬁdence • For most problems, you can receive ≥ 50% of points in two sentences or less • Probably better to do half the problems well than all the problems poorly Types of problems Type Internals Externals Tests understanding of: Mechanical Y N how core material works Reduction N Y how to apply core material Modiﬁcation Y Y how to adapt core material (augmentation, divide & conquer, amortization, etc.) Questions to ask: • Is this a Mechanical, Reduction, or Modiﬁcation type problem? • Is this problem about data structures? sorting? both? • If data structures, do you need to support Sequence ops? Set ops? both? • If stuck, is there an easy way to get a correct but inefﬁcient algorithm? Question yourself if you are: • Trying to compute decimals, rationals, or real numbers • Using Radix sort for every answer • Augmenting a binary tree with something other than a subtree property Data Structures Problems • First solve using Sorting or Set/Sequence interfaces, choose algorithm/data structure after • Describe all data structure(s) used (including what data they store) and their invariants • Implement every operation we ask for in terms of your data structures • Separate and label parts of your solution! 4 Quiz 1 Review Problem 1. Restaurant Lineup (S19 Q1) Popular restaurant Criminal Seafood does not take reservations, but maintains a wait list where customers who have been on the wait list longer are seated earlier. Sometimes customers decide to eat somewhere else, so the restaurant must remove them from the wait list. Assume each customer has a different name, and no two customers are added to the wait list at the exact same time. Design a database to help Criminal Seafood maintain its wait list supporting the following operations, each in O(1) time. State whether each operation running time is worst-case, amortized, and/or expected. build() initialize an empty database add name(x) add name x to the back of the wait list remove name(x) remove name x from the wait list seat() remove and return the name of the customer from the front of the wait list Solution: Maintain a doubly-linked list containing customers on the wait list in order, maintaining a pointer to the front of the linked list corresponding to the front of the wait list, and a pointer to the back of the linked list corresponding to the back of the wait list. Also maintain a hash table mapping each customer name to the linked list node containing that customer. To implement add name(x), create a new linked list node containing name x and add it to the back of the linked list in worst-case O(1) time. Then add name x to the hash table pointing to the newly created node in amortized expected O(1) time. To implement remove name(x), lookup name x in the hash table in and remove the mapped node from the linked list in expected O(1) time. Lastly, to implement seat(), remove the node from the front of the linked list containing name x, remove name x from the hash table, and then return x, in amortized expected O(1) time. 5 Quiz 1 Review Problem 2. Rainy Research (S19 Q1) Mether Wan is a scientist who studies global rainfall. Mether often receives data measurements from a large set of deployed sensors. Each collected data measurement is a triple of integers (r, `, t), where r is a positive amount of rainfall measured at latitude ` at time t. The peak rainfall at latitude ` since time t is the maximum rainfall of any measurement at latitude ` measured at a time greater than or equal to t (or zero if no such measurement exists). Describe a database that can store Mether’s sensor data and support the following operations, each in worst-case O(log n) time where n is the number of measurements in the database at the time of the operation. build() initialize an empty database record data(r, `, t) add a rainfall measurement r at latitude ` at time t peak rainfall(`, t) return the peak rainfall at latitude ` since time t Solution: Maintain a Set AVL tree L storing distinct measurement latitudes, where each latitude ` maps to a rainfall Set AVL tree R(`) containing all measurement triples with latitude `, keyed by time. We only store nodes associated with measurements, so the height of each Set AVL tree is bounded by O(log n). For each rainfall tree, augment each node p with the maximum rainfall p.m of any measurement within p’s subtree. This augmentation can be maintained in constant time at a node p by taking the maximum of the rainfall at p and the augmented maximums of p’s left and right children (if they exist); thus this augmentation can be maintained without effecting the asymptotic running time of standard AVL tree operations. To implement record data(r, `, t), search L for latitude ` in worst-case O(log n) time. If ` does not exist in L, insert a new node corresponding to ` mapping to a new empty rainfall Set AVL tree, also in O(log n) time. In either case, insert the measurement triple to R(`), for a total running time of worst-case O(log n). To implement peak rainfall(`, t), search L for latitude ` in worst-case O(log n) time. If ` does not exist, return zero. Otherwise, perform a one-sided range query on R(`) to ﬁnd the peak rainfall at latitude ` since time t. Speciﬁcally, let peak(v, t) be the maximum rainfall of any measurement in node v’s subtree measured at time ≥ t (or zero if v is not a node): ( max {v.item.r, v.right.m, peak(v.left, t)} if v.t ≥ t peak(v, t) = . peak(v.right, t) if v.t < t Then peak rainfall is simply peak(v, t) with v being the root of the tree, which can be computed using at most O(log n) recursive calls. So this operation runs in worst-case O(log n) time. Note, this problem can also be solved where each latitude AVL tree is keyed by rainfall, augment- ing nodes with maximum time in subtree. We leave this as an exercise to the reader. MIT OpenCourseWare https://ocw.mit.edu 6.006 Introduction to Algorithms Spring 2020 For information about citing these materials or our Terms of Use, visit: https://ocw.mit.edu/terms Introduction to Algorithms: 6.006 Massachusetts Institute of Technology Instructors: Erik Demaine, Jason Ku, and Justin Solomon Quiz 2 Review Quiz 2 Review Scope • Quiz 1 material fair game but explicitly not emphasized • 6 lectures on graphs, L09-L14, 2 Problem Sets, PS5-PS6 Graph Problems • Graph reachability by BFS or DFS in O(|E|) time • Graph exploration/connected components via Full-BFS or Full-DFS • Topological sort / Cycle detection via DFS • Negative-weight cycle detection via Bellman-Ford • Single Source Shortest Paths (SSSP) Restrictions SSSP Algorithm Graph Weights Name Running Time O(·) DAG Any DAG Relaxation |V | + |E| General Unweighted BFS |V | + |E| General Non-negative Dijkstra Bellman-Ford |V | log |V | + |E| |V | · |E| General Any • All Pairs Shortest Paths (APSP) – Run a SSSP algorithm |V | times – Johnson’s solves APSP with negative weights in O(|V |2 log |V | + |V ||E|) Graph Problem Strategies • Be sure to explicitly describe a graph in terms of problem parameters • Convert problem into ﬁnding a shortest path, cycle, topo. sort, conn. comps., etc. • May help to duplicate graph vertices to encode additional information • May help to add auxiliary vertices/edges to graph • May help to pre-process the graph (e.g., to remove part of the graph) 2 Quiz 2 Review Graph Problem Common Mistakes • Deﬁne your graphs! Specify vertices, edges, and weights clearly (and count them!) – (e.g., construct graph G = (V, E) with a vertex for each... and a directed edge (u, v) with weight w for each...) • State the problem you are solving, not just the algorithm you use to solve it – (e.g., solve SSSP from s by running DAG Relaxation...) • Connect the graph problem you solve back to the original problem – (e.g., the weight of a path from s to t in G corresponds to the sum of tolls paid along a driving route, so a path of minimum weight corresponds to a route minimizing tolls) Problem 1. Counting Blobs (S18 Quiz 2) An image is a 2D grid of black and white square pixels where each white pixel is contained in a blob. Two white pixels are in the same blob if they share an edge of the grid. Black pixels are not contained in blobs. Given an n × m array representing an image, describe an O(nm)-time algorithm to count the number of blobs in the image. Problem 2. Unicycles (S18 Quiz 2) Given a connected undirected graph G = (V, E) with strictly positive weights w : E → Z+ where |E| = |V |, describe an O(|V |)-time algorithm to determine a path from vertex s to vertex t of minimum weight. Problem 3. Doh!-nut (S18 Quiz 2) Momer has just ﬁnished work at the FingSprield power plant at location p, and needs to drive to his home at location h. But along the way, if his driving route ever comes within driving distance k of a doughnut shop, he will stop and eat doughnuts, and his wife, Harge, will be angry. Momer knows the layout of FingSprield, which can be modeled as a set of n locations, with two-way roads of known driving distance connecting some pairs of locations (you may assume that no location is incident to more than ﬁve roads), as well as the locations of the d doughnut shops in the city. Describe an O(n log n)-time algorithm to ﬁnd the shortest driving route from the power plant back home that avoids driving within driving distance k of a doughnut shop (or determine no such route exists). Problem 4. Long Shortest Paths Given directed graph G = (V, E) having arbitrary edge weights w : E → Z and two vertices s, t ∈ V , describe an O(|V |3)-time algorithm to ﬁnd the minimum weight of any path from s to t containing at least |V | edges. MIT OpenCourseWare https://ocw.mit.edu 6.006 Introduction to Algorithms Spring 2020 For information about citing these materials or our Terms of Use, visit: https://ocw.mit.edu/terms Introduction to Algorithms: 6.006 Massachusetts Institute of Technology Instructors: Erik Demaine, Jason Ku, and Justin Solomon Quiz 2 Review Quiz 2 Review Scope • Quiz 1 material fair game but explicitly not emphasized • 6 lectures on graphs, L09-L14, 2 Problem Sets, PS5-PS6 Graph Problems • Graph reachability by BFS or DFS in O(|E|) time • Graph exploration/connected components via Full-BFS or Full-DFS • Topological sort / Cycle detection via DFS • Negative-weight cycle detection via Bellman-Ford • Single Source Shortest Paths (SSSP) Restrictions SSSP Algorithm Graph Weights Name Running Time O(·) DAG Any DAG Relaxation |V | + |E| General Unweighted BFS |V | + |E| General Non-negative Dijkstra Bellman-Ford |V | log |V | + |E| |V | · |E| General Any • All Pairs Shortest Paths (APSP) – Run a SSSP algorithm |V | times – Johnson’s solves APSP with negative weights in O(|V |2 log |V | + |V ||E|) Graph Problem Strategies • Be sure to explicitly describe a graph in terms of problem parameters • Convert problem into ﬁnding a shortest path, cycle, topo. sort, conn. comps., etc. • May help to duplicate graph vertices to encode additional information • May help to add auxiliary vertices/edges to graph • May help to pre-process the graph (e.g., to remove part of the graph) 2 Quiz 2 Review Graph Problem Common Mistakes • Deﬁne your graphs! Specify vertices, edges, and weights clearly (and count them!) – (e.g., construct graph G = (V, E) with a vertex for each... and a directed edge (u, v) with weight w for each...) • State the problem you are solving, not just the algorithm you use to solve it – (e.g., solve SSSP from s by running DAG Relaxation...) • Connect the graph problem you solve back to the original problem – (e.g., the weight of a path from s to t in G corresponds to the sum of tolls paid along a driving route, so a path of minimum weight corresponds to a route minimizing tolls) Problem 1. Counting Blobs (S18 Quiz 2) An image is a 2D grid of black and white square pixels where each white pixel is contained in a blob. Two white pixels are in the same blob if they share an edge of the grid. Black pixels are not contained in blobs. Given an n × m array representing an image, describe an O(nm)-time algorithm to count the number of blobs in the image. Solution: Construct a graph G with a vertex per white pixel, with an undirected edge between two vertices if the pixels associated with them are both white and share an edge of the grid. This graph has size at most O(nm) vertices and at most O(nm) edges (as pixels share edges with at most four other pixels), so can be constructed in O(nm) time. Each connected component of this graph corresponds to a blob, so run Full-BFS or Full-DFS to count the number of connected components in G in O(nm) time. Problem 2. Unicycles (S18 Quiz 2) Given a connected undirected graph G = (V, E) with strictly positive weights w : E → Z+ where |E| = |V |, describe an O(|V |)-time algorithm to determine a path from vertex s to vertex t of minimum weight. Solution: Given two vertices in a weighted tree containing only positive weight edges, there is a unique simple path between them which is also the minimum weight path. A depth-ﬁrst search from any source vertex s in the tree results in a directed DFS tree in O(|V |) time (since |E| = |V | − 1). Then relaxing edges in topological sort order of the directed DFS tree computes minimum weight paths from s in O(|V |) time. Since G has one cycle, our strategy will be to break the cycle by removing an edge, and then compute the minimum weight path from s to t in the resultant tree. First, we ﬁnd the vertex v closest to s on the cycle by running depth-ﬁrst search from s in O(|V |) time (since |E| = |V |). One edge e1 of the cycle will not be in the tree returned by DFS (a back edge to v), with the other edge of the cycle incident to v being a single outgoing DFS tree edge e2. If s is on the cycle, v = s; otherwise the unique path from s to v does not contain e1 or e2. 3 Quiz 2 Review A shortest path from s to t cannot traverse both edges e1 and e2, or else the path would visit v at least twice, traversing a cycle of positive weight. Removing either e1 or e2 results in a tree, at least one of which contains the minimum weight path from s to t. Thus, ﬁnd the minimum weight path from s to t in each tree using the algorithm described above, returning the minimum of the two in O(|V |) time. Problem 3. Doh!-nut (S18 Quiz 2) Momer has just ﬁnished work at the FingSprield power plant at location p, and needs to drive to his home at location h. But along the way, if his driving route ever comes within driving distance k of a doughnut shop, he will stop and eat doughnuts, and his wife, Harge, will be angry. Momer knows the layout of FingSprield, which can be modeled as a set of n locations, with two-way roads of known driving distance connecting some pairs of locations (you may assume that no location is incident to more than ﬁve roads), as well as the locations of the d doughnut shops in the city. Describe an O(n log n)-time algorithm to ﬁnd the shortest driving route from the power plant back home that avoids driving within driving distance k of a doughnut shop (or determine no such route exists). Solution: Construct a graph G with a vertex for each of the n city locations, and an undirected edge between two locations if there is a road connecting them, with each edge weighted by the positive length of its corresponding road. The degree of each vertex is bounded by a constant (i.e., 5), so the number of edges in G is O(n). First, we identify vertices that are within driving distance k of a doughnut shop location: create an auxiliary vertex x with a 0-weight outgoing edge from x to every doughnut shop location, and run Dijkstra from x. Remove every vertex from the graph whose shortest path from x is less than or equal to k, resulting in graph G0 ⊂ G. If either p or h are not in G0, then no route exists. Otherwise, run Dijkstra from p in G0 . If no path exists to h, then no valid route exists. Otherwise, Dijkstra ﬁnds a shortest path from p to h, so return it (via parent pointers). This algorithm runs Dijkstra twice. Since the size of either graph is O(|V |), Dijkstra runs in O(|V | log |V |) = O(n log n) time (e.g. using a binary heap to implement a priority queue). Problem 4. Long Shortest Paths Given directed graph G = (V, E) having arbitrary edge weights w : E → Z and two vertices s, t ∈ V , describe an O(|V |3)-time algorithm to ﬁnd the minimum weight of any path from s to t containing at least |V | edges. Solution: Our strategy will compute intermediate values for each vertex v ∈ V : 1. the minimum weight w1(v) of any path from s to v using exactly |V | edges, and then 2. the minimum weight w2(v) of any path from v to t using any number of edges. First, to compute (1), we make a duplicated graph similar to Bellman-Ford, but without edges corresponding to remaining at a vertex. Speciﬁcally, construct a graph G1 with • |V | + 1 vertices for each vertex v ∈ V : vertex vk for k ∈{0, . . . , |V |} representing reaching v from s along a path containing k edges; and • |V | edges for each edge (u, v) ∈ E: edge (uk−1, vk) of the same weight for k ∈{1, . . . , |V |}. 4 Quiz 2 Review Now a path in G1 from s0 to v|V | for any v ∈ V corresponds to a path from s to v in G through exactly |V | edges. So solve SSSPs in G1 from s0 to compute the minimum weight of paths to each vertex traversing exactly |V | edges. This graph is acyclic, and has size O(|V |(|V | + |E|)) = O(|V |3), so we can solve SSSP on G1 via DAG relaxation in O(|V |3) time. Second, to compute (2), we make a new graph G2 from G where every edge is reversed. Then every path to t in G corresponds to a path in G2 from t, so compute SSSPs from t in G2 to ﬁnd the minimum weight of any path from v to t in G using any number of edges, which can be done in O(|V ||E|) = O(|V |3) time using Bellman-Ford. Once computed, ﬁnding the minimum sum of w1(v) + w2(v) over all vertices v ∈ V will provide the minimum weight of any path from s to t containing at least |V | edges, since every such path can be decomposed into its ﬁrst |V | edges and then the remainder. This loop takes O(|V |) time, so the algorithm runs in O(|V |3) time in total. MIT OpenCourseWare https://ocw.mit.edu 6.006 Introduction to Algorithms Spring 2020 For information about citing these materials or our Terms of Use, visit: https://ocw.mit.edu/terms Introduction to Algorithms: 6.006 Massachusetts Institute of Technology Instructors: Erik Demaine, Jason Ku, and Justin Solomon Quiz 3 Review Quiz 3 Review Scope • Quiz 1 & Quiz 2 material fair game but explicitly not emphasized • 4 lectures on dynamic programming, L15-L18, 2 Problem Sets, PS7-PS8 • Recursive framework (SRTBOT) • Dynamic Programming: subproblem dependencies overlap, forming a DAG Recursive Framework (SRT BOT) 1. Subproblem deﬁnition • Describe meaning of subproblem in words, in terms of parameters x ∈ X • Often subsets of input: preﬁxes, sufﬁxes, contiguous substrings of a sequence • Often multiply possible subsets across multiple inputs • Often “remember” information by maintaining some auxiliary state (expansion) • Often expand based on state of integers in problem space (pseudopolynomial?) 2. Relate recursively • Relate subproblem solutions recursively x(i) = f(x(j), . . .) for one or more j < i • Identify question about subproblem solution whose answer would let you reduce to smaller subproblem(s), then locally brute-force all possible answers to the question 3. Topological order • Argue relation is acyclic (deﬁnes “smaller” subproblems), subproblems form a DAG 4. Base cases • State solutions for all (reachable) independent subproblems where relation breaks down 5. Original problem • Show how to compute solution to original problem from solutions to subproblem(s) • Possibly use parent pointers to recover actual solution, not just objective function 6. Time analysis P • work(x), or if work(x) = O(W ) for all x ∈ X, then |X| · O(W ) x∈X • work(x) measures nonrecursive work in relation; treat recursions as taking O(1) time 2 Quiz 3 Review Problem 1. Future Investing (adapted from S18 PS9) Tiffany Bannen stumbles upon a lottery chart dropped by a time traveler from the future, which lists winning lottery numbers and positive integer cash payouts for the next n days. Tiffany wants to use this information to make money, but is worried if she plays winning numbers every day, lottery organizers will get suspicious. As such, she decides to play the lottery infrequently: at most twice in any seven day period. Describe a O(n)-time algorithm to determine the maximum amount of lottery winnings Tiff can win in the next n days by playing the lottery infrequently. Problem 2. Oh Charlie, Where Art Thou? (adapted from S18 PS9) A wealthy family, Alice, Bob, and their young son Charlie are sailing around the world when they encounter a massive storm. Charlie is thrown overboard, presumed drowned. Twenty years later, a man comes to Alice and Bob claiming to be Charlie. Alice and Bob are excited, but skeptical. Alice and Bob order a DNA matching test from the genetic testing company 46AndThee. Given three length-n DNA sequences from Alice, Bob, and Charlie, the testing center will determine ancestry as follows: if Charlie’s DNA can be partitioned into two (not necessarily contiguous) subsequences of equal length, where one is a subsequence of Alice’s DNA, and the other is a subsequence of Bob’s DNA, the Charlie is their son. For example, suppose Alice’s DNA is AATT and Bob’s DNA is CCGG. If Charlie’s DNA were CATG, he would be matched as their son, since CATG can be partitioned into disjoint subsequences CG and AT which are subsequences of Alice and Bob’s DNA respectively. However, Charlie would be found to be an imposter if his DNA were AGTC. Describe an O(n4)-time algorithm to determine whether Charlie is a fraud. Problem 3. Sweet Tapas (adapted from S18 Final) Obert Ratkins is having dinner at an upscale tapas bar, where he will order many small plates. There are n plates of food on the menu, where information for plate i is given by a triple of non- negative integers (vi, ci, si): the plate’s volume vi, calories ci, and sweetness si ∈{0, 1} (the plate is sweet if si = 1 and not sweet if si = 0). Obert is on a diet: he wants to eat no more than k calories during his meal, but wants to ﬁll his stomach as much as possible. He also wants to order exactly s < n sweet plates, without purchasing the same dish twice. Describe an O(nks)-time algorithm to ﬁnd the maximum volume of food Obert can eat given his diet. Problem 4. Gokemon Po (adapted from S18 Final) Kash Etchum wants to play Gokemon Po, a new augmented reality game. The goal is to catch a set of n monsters who reside at speciﬁc locations in her town. Monsters must be obtained in a speciﬁc order: before Kash can obtain monster mi, she must have already obtained all monsters mj for j < i. To obtain monster mi, Kash may either purchase the monster in-game for positive integer ci dollars, or she may catch mi for free from that monster’s location. If Kash is not at the monster’s location, she will have to pay a ride share service to drive her there. The minimum possible cost to transport from the location of monster mi to the location of monster mj via ride sharing is denoted by the positive integer s(i, j). Given the price lists of in-game purchases and ride sharing trips between all pairs of monsters, describe an O(n2)-time algorithm to determine the minimum amount of money Kash must spend in order to catch all n monsters, assuming that she starts at the location of monster m1. MIT OpenCourseWare https://ocw.mit.edu 6.006 Introduction to Algorithms Spring 2020 For information about citing these materials or our Terms of Use, visit: https://ocw.mit.edu/terms Introduction to Algorithms: 6.006 Massachusetts Institute of Technology Instructors: Erik Demaine, Jason Ku, and Justin Solomon Quiz 3 Review Quiz 3 Review Scope • Quiz 1 & Quiz 2 material fair game but explicitly not emphasized • 4 lectures on dynamic programming, L15-L18, 2 Problem Sets, PS7-PS8 • Recursive framework (SRTBOT) • Dynamic Programming: subproblem dependencies overlap, forming a DAG Recursive Framework (SRT BOT) 1. Subproblem deﬁnition • Describe meaning of subproblem in words, in terms of parameters x ∈ X • Often subsets of input: preﬁxes, sufﬁxes, contiguous substrings of a sequence • Often multiply possible subsets across multiple inputs • Often “remember” information by maintaining some auxiliary state (expansion) • Often expand based on state of integers in problem space (pseudopolynomial?) 2. Relate recursively • Relate subproblem solutions recursively x(i) = f(x(j), . . .) for one or more j < i • Identify question about subproblem solution whose answer would let you reduce to smaller subproblem(s), then locally brute-force all possible answers to the question 3. Topological order • Argue relation is acyclic (deﬁnes “smaller” subproblems), subproblems form a DAG 4. Base cases • State solutions for all (reachable) independent subproblems where relation breaks down 5. Original problem • Show how to compute solution to original problem from solutions to subproblem(s) • Possibly use parent pointers to recover actual solution, not just objective function 6. Time analysis P • work(x), or if work(x) = O(W ) for all x ∈ X, then |X| · O(W ) x∈X • work(x) measures nonrecursive work in relation; treat recursions as taking O(1) time 2 Quiz 3 Review Problem 1. Future Investing (adapted from S18 PS9) Tiffany Bannen stumbles upon a lottery chart dropped by a time traveler from the future, which lists winning lottery numbers and positive integer cash payouts for the next n days. Tiffany wants to use this information to make money, but is worried if she plays winning numbers every day, lottery organizers will get suspicious. As such, she decides to play the lottery infrequently: at most twice in any seven day period. Describe a O(n)-time algorithm to determine the maximum amount of lottery winnings Tiff can win in the next n days by playing the lottery infrequently. Solution: 1. Subproblems • Let L(i) be the cash payout of playing the lottery on day i ∈{1, . . . , n} • Need to keep track of most recent two plays (or equivalently, restrictions on future plays) • x(i, j): maximum lottery winnings playing on sufﬁx of days from i to n, assuming play on day i and next allowable play is on day i + j • for i ∈{1, . . . , n} and j ∈{1, . . . , 6} 2. Relate • Tiffany will play again on some day in future. Guess! • It is never optimal to go 11 days without playing the lottery, as playing on the 6th day would be valid and strictly increase winnings • The next play can be on day i + k for k ∈{j, . . . , 11} • If next play on i + k for k ∈{1, . . . , 6}, next allowable play is on day i + 7 • If next play on i + k for k ∈{7, . . . , 11}, next allowable play is on day i + k + 1 • x(i, j) = L(i) + max{x(i + k, max{1, 7 − k}) | k ∈{i, . . . , 11} and i + k ≤ n} 3. Topo • Subproblems x(i, j) only depend on strictly larger i, so acyclic 4. Base • x(n, j) = L(i) for all j ∈{1, . . . , 6} 5. Original • Solve subproblems via recursive top down or iterative bottom up • Guess ﬁrst play (within ﬁrst seven days) • Solution to original problem is max{x(i, 1) | i ∈{1, . . . , 7}} • (Can store parent pointers to reconstruct days played) 6. Time • # subproblems: 6n • work per subproblem: O(1) • work for original: O(1) • O(n) running time ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ 3 Quiz 3 Review Problem 2. Oh Charlie, Where Art Thou? (adapted from S18 PS9) A wealthy family, Alice, Bob, and their young son Charlie are sailing around the world when they encounter a massive storm. Charlie is thrown overboard, presumed drowned. Twenty years later, a man comes to Alice and Bob claiming to be Charlie. Alice and Bob are excited, but skeptical. Alice and Bob order a DNA matching test from the genetic testing company 46AndThee. Given three length-n DNA sequences from Alice, Bob, and Charlie, the testing center will determine ancestry as follows: if Charlie’s DNA can be partitioned into two (not necessarily contiguous) subsequences of equal length, where one is a subsequence of Alice’s DNA, and the other is a subsequence of Bob’s DNA, the Charlie is their son. For example, suppose Alice’s DNA is AATT and Bob’s DNA is CCGG. If Charlie’s DNA were CATG, he would be matched as their son, since CATG can be partitioned into disjoint subsequences CG and AT which are subsequences of Alice and Bob’s DNA respectively. However, Charlie would be found to be an imposter if his DNA were AGTC. Describe an O(n4)-time algorithm to determine whether Charlie is a fraud. Solution: 1. Subproblems • Let A, B, and C be the relevant length-n DNA sequences from Alice, Bob, and Charlie. • Want to match some characters of A and B to all characters of C • x(i, j, ki, kj): true if can match a length-ki subsequence of sufﬁx A[i :] and a length-kj characters from preﬁx B[j :] to all characters in sufﬁx C[(n − ki − kj ) :] (the sufﬁx containing the last ki + kj characters), and false otherwise. • for i, j ∈{0, . . . , n} and ki, kj ∈{0, . . . , n/2} (assume n is even) 2. Relate • Must match character C[i]; if A[i] = C[i] or B[i] = C[i] recurse on remainder • Alternatively, do not use either A[i] or B[i] ⎧ ⎫ x(i + 1, j, ki + 1, kj ) if A[i] = C[n − ki − kj ] and ki > 0, ⎪ ⎪ ⎨ ⎬ x(i, j + 1, ki, kj + 1) if B[j] = C[n − ki − kj ] and kj > 0, • x(i, j, ki, kj) = OR x(i + 1, j, ki, kj ) if i < n, ⎪ ⎪ ⎩ ⎭ x(i, j + 1, ki, kj ) if j < n 3. Topo • Subproblem x(i, j, ki, kj) only depends on strictly smaller i + j, so acyclic 4. Base • x(n, n, 0, 0) is true (all matched!) • x(n, j, ki, kj ) false if ki > 0 (no more characters in A) • x(i, n, ki, kj ) false if kj > 0 (no more characters in B) 5. Original • Solve subproblems via recursive top down or iterative bottom up • Solution to original problem is x(n, n, n/2, n/2) 4 Quiz 3 Review 6. Time • # subproblems: O(n4) • work per subproblem: O(1) • O(n4) running time Problem 3. Sweet Tapas (adapted from S18 Final) Obert Ratkins is having dinner at an upscale tapas bar, where he will order many small plates. There are n plates of food on the menu, where information for plate i is given by a triple of non- negative integers (vi, ci, si): the plate’s volume vi, calories ci, and sweetness si ∈{0, 1} (the plate is sweet if si = 1 and not sweet if si = 0). Obert is on a diet: he wants to eat no more than k calories during his meal, but wants to ﬁll his stomach as much as possible. He also wants to order exactly s < n sweet plates, without purchasing the same dish twice. Describe an O(nks)-time algorithm to ﬁnd the maximum volume of food Obert can eat given his diet. Solution: 1. Subproblems • x(i, j, s0): maximum volume of food possible purchasing a sufﬁx of plates pi to pn−1, using at most j calories, ordering exactly s0 sweet plates • for i ∈{0, . . . , n}, j ∈{0, . . . , k}, s0 ∈{0, . . . , s} 2. Relate • Either order plate pi or not. Guess! • If order pi, get vi in volume but use ci calories • If pi is sweet, need to order one fewer sweet plate ( ) vi + x(i + 1, j − ci, s0 − si) if ci ≤ j and si ≤ s0 , • x(i, j, s0) = max x(i + 1, j, s0) always 3. Topo • Subproblems x(i, j, s0) only depend on strictly larger i, so acyclic 4. Base • x(n, j, 0) = 0 and any j (no more plates to eat) • x(n, j, s0) = −∞ for s0 > 0 and any j (no more plates, but still need to eat sweet) 5. Original • Solution given by x(0, k, s) 6. Time • # subproblems: O(nks) • work per subproblem: O(1) • O(nks) running time 5 Quiz 3 Review Problem 4. Gokemon Po (adapted from S18 Final) Kash Etchum wants to play Gokemon Po, a new augmented reality game. The goal is to catch a set of n monsters who reside at speciﬁc locations in her town. Monsters must be obtained in a speciﬁc order: before Kash can obtain monster mi, she must have already obtained all monsters mj for j < i. To obtain monster mi, Kash may either purchase the monster in-game for positive integer ci dollars, or she may catch mi for free from that monster’s location. If Kash is not at the monster’s location, she will have to pay a ride share service to drive her there. The minimum possible cost to transport from the location of monster mi to the location of monster mj via ride sharing is denoted by the positive integer s(i, j). Given the price lists of in-game purchases and ride sharing trips between all pairs of monsters, describe an O(n2)-time algorithm to determine the minimum amount of money Kash must spend in order to catch all n monsters, assuming that she starts at the location of monster m1. Solution: 1. Subproblems • x(i, j): min cost of catching monsters mi to mn starting at location mj for j ≤ i 2. Relate • If at location of monster, catch it for free! • Otherwise, either acquire monster mi by purchasing or ride-sharing to location. Guess! • If purchase spend ci dollars, else need to ride share to mi from mj ( x(i + 1, j) if j = i • x(i, j) = min{ci + x(i + 1, j), s(j, i) + x(i, i)} otherwise 3. Topo • Subproblems x(i, j) only depend on strictly larger i + j so acyclic 4. Base • x(n + 1, j) = 0 for any j (no cost to collect no monsters) 5. Original • Solution given by x(1, 1) 6. Time • # subproblems: O(n2) • work per subproblem: O(1) • O(n2) running time MIT OpenCourseWare https://ocw.mit.edu 6.006 Introduction to Algorithms Spring 2020 For information about citing these materials or our Terms of Use, visit: https://ocw.mit.edu/terms"
            },
            {
                "role": "user",
                "content": "Can you summarize the key topics covered in this lecture?"
            },
            {
                "role": "assistant",
                "content": "This lecture covers foundational algorithmic concepts including problem formulation, correctness proof using induction, efficiency analysis using asymptotic notation, data structures (arrays, linked lists, BSTs, heaps), and sorting methods (selection sort, merge sort, counting sort). Additionally, it introduces the Word-RAM computational model, providing a theoretical framework for measuring algorithm efficiency."
            }
        ]
    },
    {
        "system": "Hello! I am Kiran, an AI assistant from Tampere University, dedicated to providing you with accurate and insightful assistance. My goal is to help you understand concepts, summarize lectures, and answer questions effectively.",
        "conversations": [
            {
                "role": "host",
                "content": "0.3. Abstract Data Types — BM40A1500 Data Structures and Algorithms Username Password Forgot your password? Register Username: Password Confirm Password Email: BM40A1500 Data Structures and Algorithms Chapter 0 Introduction for Data Structures and Algorithms Show Source | | About « 0.2. Problems, Algorithms, and Programs :: Contents :: 1.1. Chapter Introduction » 0.3. Abstract Data Types¶ This module presents terminology and definitions related to techniques for managing the tremendous complexity of computer programs. It also presents working definitions for the fundamental but somewhat slippery terms “data item” and “data structure”. We begin with the basic elements on which data structures are built. A type is a collection of values. For example, the Boolean type consists of the values true and false. The integers also form a type. An integer is a simple type because its values contain no subparts. A bank account record will typically contain several pieces of information such as name, address, account number, and account balance. Such a record is an example of an aggregate type or composite type. A data item is a piece of information or a record whose value is drawn from a type. A data item is said to be a member of a type. A data type is a type together with a collection of operations to manipulate the type. For example, an integer variable is a member of the integer data type. Addition is an example of an operation on the integer data type. A distinction should be made between the logical concept of a data type and its physical implementation in a computer program. For example, there are two traditional implementations for the list data type: the linked list and the array-based list. The list data type can therefore be implemented using a linked list or an array. But we don’t need to know how the list is implemented when we wish to use a list to help in a more complex design. For example, a list might be used to help implement a graph data structure. As another example, the term “array” could refer either to a data type or an implementation. “Array” is commonly used in computer programming to mean a contiguous block of memory locations, where each memory location stores one fixed-length data item. By this meaning, an array is a physical data structure. However, array can also mean a logical data type composed of a (typically homogeneous) collection of data items, with each data item identified by an index number. It is possible to implement arrays in many different ways besides as a block of contiguous memory locations. The sparse matrix refers to a large, two-dimensional array that stores only a relatively few non-zero values. This is often implemented with a linked structure, or possibly using a hash table. But it could be implemented with an interface that uses traditional row and column indices, thus appearing to the user in the same way that it would if it had been implemented as a block of contiguous memory locations. An abstract data type (ADT) is the specification of a data type within some language, independent of an implementation. The interface for the ADT is defined in terms of a type and a set of operations on that type. The behavior of each operation is determined by its inputs and outputs. An ADT does not specify how the data type is implemented. These implementation details are hidden from the user of the ADT and protected from outside access, a concept referred to as encapsulation. A data structure is the implementation for an ADT. In an object-oriented language, an ADT and its implementation together make up a class. Each operation associated with the ADT is implemented by a member function or method. The variables that define the space required by a data item are referred to as data members. An object is an instance of a class, that is, something that is created and takes up storage during the execution of a computer program. The term data structure often refers to data stored in a computer’s main memory. The related term file structure often refers to the organization of data on peripheral storage, such as a disk drive or CD. Example 0.3.1 The mathematical concept of an integer, along with operations that manipulate integers, form a data type. The int variable type is a physical representation of the abstract integer. The int variable type, along with the operations that act on an int variable, form an ADT. Unfortunately, the int implementation is not completely true to the abstract integer, as there are limitations on the range of values an int variable can store. If these limitations prove unacceptable, then some other representation for the ADT “integer” must be devised, and a new implementation must be used for the associated operations. Example 0.3.2 An ADT for a list of integers might specify the following operations: Insert a new integer at a particular position in the list. Return True if the list is empty. Reinitialize the list. Return the number of integers currently in the list. Retrieve the integer at a particular position in the list. Delete the integer at a particular position in the list. From this description, the input and output of each operation should be clear, but the implementation for lists has not been specified. Given two applications that make use of an ADT, one might use particular operations more frequently than the other, or they might have different time constraints for the various operations. Fortunately, an ADT can accomodate these difference in requirements by providing differing implementations. Example 0.3.3 Two popular implementations for large disk-based database applications are hashing and the B-tree. Both support efficient insertion and deletion of records, and both support exact-match queries. However, hashing is more efficient than the B-tree for exact-match queries. On the other hand, the B-tree can perform range queries efficiently, while hashing is hopelessly inefficient for range queries. Thus, if the database application limits searches to exact-match queries, hashing is preferred. On the other hand, if the application requires support for range queries, the B-tree is preferred. Despite these performance issues, both implementations solve versions of the same problem: updating and searching a large collection of records. The concept of an ADT can help us to focus on key issues even in non-computing applications. Example 0.3.4 When operating a car, the primary activities are steering, accelerating, and braking. On nearly all passenger cars, you steer by turning the steering wheel, accelerate by pushing the gas pedal, and brake by pushing the brake pedal. This design for cars can be viewed as an ADT with operations “steer”, “accelerate”, and “brake”. Two cars might implement these operations in radically different ways, say with different types of engine, or front- versus rear-wheel drive. Yet, most drivers can operate many different cars because the ADT presents a uniform method of operation that does not require the driver to understand the specifics of any particular engine or drive design. These differences are deliberately hidden. The concept of an ADT is one instance of an important principle that must be understood by any successful computer scientist: managing complexity through abstraction. A central theme of computer science is complexity and techniques for handling it. Humans deal with complexity by assigning a label to an assembly of objects or concepts and then manipulating the label in place of the assembly. Cognitive psychologists call such a label a metaphor. A particular label might be related to other pieces of information or other labels. This collection can in turn be given a label, forming a hierarchy of concepts and labels. This hierarchy of labels allows us to focus on important issues while ignoring unnecessary details. Example 0.3.5 We apply the label “hard drive” to a collection of hardware that manipulates data on a particular type of storage device, and we apply the label “CPU” to the hardware that controls execution of computer instructions. These and other labels are gathered together under the label “computer”. Because even the smallest home computers today have millions of components, some form of abstraction is necessary to comprehend how a computer operates. Consider how you might go about the process of designing a complex computer program that implements and manipulates an ADT. The ADT is implemented in one part of the program by a particular data structure. While designing those parts of the program that use the ADT, you can think in terms of operations on the data type without concern for the data structure’s implementation. Without this ability to simplify your thinking about a complex program, you would have no hope of understanding or implementing it. Example 0.3.6 Consider the design for a relatively simple database system stored on disk. Typically, records on disk in such a program are accessed through a buffer pool rather than directly. Variable length records might use a memory manager to find an appropriate location within the disk file to place the record. Multiple index structures will typically be used to support access to a collection of records using multiple search keys. Thus, we have a chain of classes, each with its own responsibilities and access privileges. A database query from a user is implemented by searching an index structure. This index requests access to the record by means of a request to the buffer pool. If a record is being inserted or deleted, such a request goes through the memory manager, which in turn interacts with the buffer pool to gain access to the disk file. A program such as this is far too complex for nearly any human programmer to keep all of the details in their head at once. The only way to design and implement such a program is through proper use of abstraction and metaphors. In object-oriented programming, such abstraction is handled using classes. Data types have both a logical form and a physical form. The definition of the data type in terms of an ADT is its logical form. The implementation of the data type as a data structure is its physical form. Sometimes you might see the term concrete implementation, but the word concrete is redundant. The figure below illustrates this relationship between logical and physical forms for data types. When you implement an ADT, you are dealing with the physical form of the associated data type. When you use an ADT elsewhere in your program, you are concerned with the associated data type’s logical form. Some sections of this book focus on physical implementations for a given data structure. Other sections use the logical ADT for the data structure in the context of a higher-level task. Figure 0.3.1: The relationship between data items, abstract data types, and data structures. The ADT defines the logical form of the data type. The data structure implements the physical form of the data type. Users of an ADT are typically programmers working in the same language as the implementer of the ADT. Typically, these programmers want to use the ADT as a component in another application. The interface to an ADT is also commonly referred to as the Application Programmer Interface, or API, for the ADT. The interface becomes a form of communication between the two programmers. Example 0.3.7 A particular programming environment might provide a library that includes a list class. The logical form of the list is defined by the public functions, their inputs, and their outputs that define the class. This might be all that you know about the list class implementation, and this should be all you need to know. Within the class, a variety of physical implementations for lists is possible. Contact Us || Privacy | | License « 0.2. Problems, Algorithms, and Programs :: Contents :: 1.1. Chapter Introduction » Contact Us | | Report a bug © Copyright 2011-2023 by OpenDSA Project Contributors and distributed under an MIT license. Last updated on Aug 13, 2024. Created using Sphinx 4.4.0. Summary*: Operating system*: Windows Mac OS Linux iOS Android Other Browser*: Chrome Safari Internet Explorer Opera Other Description*: Attach a screenshot (optional):"
            },
            {
                "role": "user",
                "content": "Can you summarize the key topics covered in this lecture?"
            },
            {
                "role": "assistant",
                "content": "This lecture covers foundational algorithmic concepts including problem formulation, correctness proof using induction, efficiency analysis using asymptotic notation, data structures (arrays, linked lists, BSTs, heaps), and sorting methods (selection sort, merge sort, counting sort). Additionally, it introduces the Word-RAM computational model, providing a theoretical framework for measuring algorithm efficiency."
            }
        ]
    },
    {
        "system": "Hello! I am Kiran, an AI assistant from Tampere University, dedicated to providing you with accurate and insightful assistance. My goal is to help you understand concepts, summarize lectures, and answer questions effectively.",
        "conversations": [
            {
                "role": "host",
                "content": "5.1. Algorithm Design Principles — BM40A1500 Data Structures and Algorithms Username Password Forgot your password? Register Username: Password Confirm Password Email: BM40A1500 Data Structures and Algorithms Chapter 5 Algorithm Design Principles Show Source | | About « 4.12. Heapsort :: Contents :: 5.2. The Greedy Approach » 5.1. Algorithm Design Principles¶ In this chapter we will go through some useful principles for designing algorithms. These should be seen as possible tools or ways to approach a new problem instead strict guidelines for developing algorithms. It is often beneficial to combine different principles to solve a problem. The algorithm design principles presented here are: The greedy approach: algorithms that make locally optimal choices at each step. Divide and conquer: a technique for designing algorithms where a solution is found by breaking the problem into smaller (similar) subproblems, solving the subproblems, then combining the subproblem solutions to form the solution to the original problem. Good examples of algorithms that use the divide and conquer design principle are the sorting algorithms Mergesort and Quicksort. Backtracking: a heuristic for brute-force search of a solution space. It is essentially a depth-first search of the solution space. This can be improved using a branch-and-bounds algorithm. Dynamic programming: an approach to designing algorithms that works by storing a table of results for subproblems. Probabilistic algorithms: algorithms that utilize randomness in some way to, for example, speed up the algorithm. Contact Us || Privacy | | License « 4.12. Heapsort :: Contents :: 5.2. The Greedy Approach » Contact Us | | Report a bug © Copyright 2011-2023 by OpenDSA Project Contributors and distributed under an MIT license. Last updated on Aug 13, 2024. Created using Sphinx 4.4.0. Summary*: Operating system*: Windows Mac OS Linux iOS Android Other Browser*: Chrome Safari Internet Explorer Opera Other Description*: Attach a screenshot (optional):"
            },
            {
                "role": "user",
                "content": "Can you summarize the key topics covered in this lecture?"
            },
            {
                "role": "assistant",
                "content": "This lecture covers foundational algorithmic concepts including problem formulation, correctness proof using induction, efficiency analysis using asymptotic notation, data structures (arrays, linked lists, BSTs, heaps), and sorting methods (selection sort, merge sort, counting sort). Additionally, it introduces the Word-RAM computational model, providing a theoretical framework for measuring algorithm efficiency."
            }
        ]
    },
    {
        "system": "Hello! I am Kiran, an AI assistant from Tampere University, dedicated to providing you with accurate and insightful assistance. My goal is to help you understand concepts, summarize lectures, and answer questions effectively.",
        "conversations": [
            {
                "role": "host",
                "content": "1.5. Asymptotic Analysis and Upper Bounds — BM40A1500 Data Structures and Algorithms Username Password Forgot your password? Register Username: Password Confirm Password Email: BM40A1500 Data Structures and Algorithms Chapter 1 Algorithm Analysis Show Source | | About « 1.4. Faster Computer, or Faster Algorithm? :: Contents :: 1.6. Lower Bounds and \\(\\Theta\\) Notation » 1.5. Asymptotic Analysis and Upper Bounds¶ Figure 1.5.2: Two views of a graph illustrating the growth rates for six equations. The bottom view shows in detail the lower-left portion of the top view. The horizontal axis represents input size. The vertical axis can represent time, space, or any other measure of cost. Despite the larger constant for the curve labeled \\(10 n\\) in the figure above, \\(2 n^2\\) crosses it at the relatively small value of \\(n = 5\\). What if we double the value of the constant in front of the linear equation? As shown in the graph, \\(20 n\\) is surpassed by \\(2 n^2\\) once \\(n = 10\\). The additional factor of two for the linear growth rate does not much matter. It only doubles the \\(x\\)-coordinate for the intersection point. In general, changes to a constant factor in either equation only shift where the two curves cross, not whether the two curves cross. When you buy a faster computer or a faster compiler, the new problem size that can be run in a given amount of time for a given growth rate is larger by the same factor, regardless of the constant on the running-time equation. The time curves for two algorithms with different growth rates still cross, regardless of their running-time equation constants. For these reasons, we usually ignore the constants when we want an estimate of the growth rate for the running time or other resource requirements of an algorithm. This simplifies the analysis and keeps us thinking about the most important aspect: the growth rate. This is called asymptotic algorithm analysis. To be precise, asymptotic analysis refers to the study of an algorithm as the input size “gets big” or reaches a limit (in the calculus sense). However, it has proved to be so useful to ignore all constant factors that asymptotic analysis is used for most algorithm comparisons. In rare situations, it is not reasonable to ignore the constants. When comparing algorithms meant to run on small values of \\(n\\), the constant can have a large effect. For example, if the problem requires you to sort many collections of exactly five records, then a sorting algorithm designed for sorting thousands of records is probably not appropriate, even if its asymptotic analysis indicates good performance. There are rare cases where the constants for two algorithms under comparison can differ by a factor of 1000 or more, making the one with lower growth rate impractical for typical problem sizes due to its large constant. Asymptotic analysis is a form of “back of the envelope” estimation for algorithm resource consumption. It provides a simplified model of the running time or other resource needs of an algorithm. This simplification usually helps you understand the behavior of your algorithms. Just be aware of the limitations to asymptotic analysis in the rare situation where the constant is important. 1.5.1. Upper Bounds¶ Several terms are used to describe the running-time equation for an algorithm. These terms—and their associated symbols—indicate precisely what aspect of the algorithm’s behavior is being described. One is the upper bound for the growth of the algorithm’s running time. It indicates the upper or highest growth rate that the algorithm can have. Because the phrase “has an upper bound to its growth rate of \\(f(n)\\)” is long and often used when discussing algorithms, we adopt a special notation, called big-Oh notation. If the upper bound for an algorithm’s growth rate (for, say, the worst case) is (f(n)), then we would write that this algorithm is “in the set \\(O(f(n))\\) in the worst case” (or just “in \\(O(f(n))\\) in the worst case”). For example, if \\(n^2\\) grows as fast as \\(\\mathbf{T}(n)\\) (the running time of our algorithm) for the worst-case input, we would say the algorithm is “in \\(O(n^2)\\) in the worst case”. The following is a precise definition for an upper bound. \\(\\mathbf{T}(n)\\) represents the true running time of the algorithm. \\(f(n)\\) is some expression for the upper bound. For \\(\\mathbf{T}(n)\\) a non-negatively valued function, \\(\\mathbf{T}(n)\\) is in set \\(O(f(n))\\) if there exist two positive constants \\(c\\) and \\(n_0\\) such that \\(\\mathbf{T}(n) \\leq cf(n)\\) for all \\(n > n_0\\). Constant \\(n_0\\) is the smallest value of \\(n\\) for which the claim of an upper bound holds true. Usually \\(n_0\\) is small, such as 1, but does not need to be. You must also be able to pick some constant \\(c\\), but it is irrelevant what the value for \\(c\\) actually is. In other words, the definition says that for all inputs of the type in question (such as the worst case for all inputs of size \\(n\\)) that are large enough (i.e., \\(n > n_0\\)), the algorithm always executes in less than or equal to \\(cf(n)\\) steps for some constant \\(c\\). Example 1.5.1 Consider the sequential search algorithm for finding a specified value in an array of integers. If visiting and examining one value in the array requires \\(c_s\\) steps where \\(c_s\\) is a positive number, and if the value we search for has equal probability of appearing in any position in the array, then in the average case \\(\\mathbf{T}(n) = c_s n/2\\). For all values of \\(n > 1\\), \\(c_s n/2 \\leq c_s n\\). Therefore, by the definition, \\(\\mathbf{T}(n)\\) is in \\(O(n)\\) for \\(n_0 = 1\\) and \\(c = c_s\\). Example 1.5.2 For a particular algorithm, \\(\\mathbf{T}(n) = c_1 n^2 + c_2 n\\) in the average case where \\(c_1\\) and \\(c_2\\) are positive numbers. Then, \\[c_1 n^2 + c_2 n \\leq c_1 n^2 + c_2 n^2 \\leq (c_1 + c_2)n^2\\] for all \\(n > 1\\). So, \\(\\mathbf{T}(n) \\leq c n^2\\) for \\(c = c_1 + c_2\\), and \\(n_0 = 1\\). Therefore, \\(\\mathbf{T}(n)\\) is in \\(O(n^2)\\) by the second definition. Example 1.5.3 Assigning the value from the first position of an array to a variable takes constant time regardless of the size of the array. Thus, \\(\\mathbf{T}(n) = c\\) (for the best, worst, and average cases). We could say in this case that \\(\\mathbf{T}(n)\\) is in \\(O(c)\\). However, it is traditional to say that an algorithm whose running time has a constant upper bound is in \\(O(1)\\). If someone asked you out of the blue “Who is the best?” your natural reaction should be to reply “Best at what?” In the same way, if you are asked “What is the growth rate of this algorithm”, you would need to ask “When? Best case? Average case? Or worst case?” Some algorithms have the same behavior no matter which input instance of a given size that they receive. An example is finding the maximum in an array of integers. But for many algorithms, it makes a big difference which particular input of a given size is involved, such as when searching an unsorted array for a particular value. So any statement about the upper bound of an algorithm must be in the context of some specific class of inputs of size \\(n\\). We measure this upper bound nearly always on the best-case, average-case, or worst-case inputs. Thus, we cannot say, “this algorithm has an upper bound to its growth rate of \\(n^2\\)” because that is an incomplete statement. We must say something like, “this algorithm has an upper bound to its growth rate of \\(n^2\\) in the average case”. Knowing that something is in \\(O(f(n))\\) says only how bad things can be. Perhaps things are not nearly so bad. Because sequential search is in \\(O(n)\\) in the worst case, it is also true to say that sequential search is in \\(O(n^2)\\). But sequential search is practical for large \\(n\\) in a way that is not true for some other algorithms in \\(O(n^2)\\). We always seek to define the running time of an algorithm with the tightest (lowest) possible upper bound. Thus, we prefer to say that sequential search is in \\(O(n)\\). This also explains why the phrase “is in \\(O(f(n))\\)” or the notation “\\(\\in O(f(n))\\)” is used instead of “is \\(O(f(n))\\)” or “\\(= O(f(n))\\)”. There is no strict equality to the use of big-Oh notation. \\(O(n)\\) is in \\(O(n^2)\\), but \\(O(n^2)\\) is not in \\(O(n)\\). 1.5.2. Simplifying Rules¶ Once you determine the running-time equation for an algorithm, it really is a simple matter to derive the big-Oh expressions from the equation. You do not need to resort to the formal definitions of asymptotic analysis. Instead, you can use the following rules to determine the simplest form. If \\(f(n)\\) is in \\(O(g(n))\\) and \\(g(n)\\) is in \\(O(h(n))\\), then \\(f(n)\\) is in \\(O(h(n))\\). If \\(f(n)\\) is in \\(O(k g(n))\\) for any constant \\(k > 0\\), then \\(f(n)\\) is in \\(O(g(n))\\). If \\(f_1(n)\\) is in \\(O(g_1(n))\\) and \\(f_2(n)\\) is in \\(O(g_2(n))\\), then \\(f_1(n) + f_2(n)\\) is in \\(O(\\max(g_1(n), g_2(n)))\\). If \\(f_1(n)\\) is in \\(O(g_1(n))\\) and \\(f_2(n)\\) is in \\(O(g_2(n))\\), then \\(f_1(n) f_2(n)\\) is in \\(O(g_1(n) g_2(n))\\). The first rule says that if some function \\(g(n)\\) is an upper bound for your cost function, then any upper bound for \\(g(n)\\) is also an upper bound for your cost function. The significance of rule (2) is that you can ignore any multiplicative constants in your equations when using big-Oh notation. Rule (3) says that given two parts of a program run in sequence (whether two statements or two sections of code), you need consider only the more expensive part. Rule (4) is used to analyze simple loops in programs. If some action is repeated some number of times, and each repetition has the same cost, then the total cost is the cost of the action multiplied by the number of times that the action takes place. Taking the first three rules collectively, you can ignore all constants and all lower-order terms to determine the asymptotic growth rate for any cost function. The advantages and dangers of ignoring constants were discussed near the beginning of this section. Ignoring lower-order terms is reasonable when performing an asymptotic analysis. The higher-order terms soon swamp the lower-order terms in their contribution to the total cost as (n) becomes larger. Thus, if \\(\\mathbf{T}(n) = 3 n^4 + 5 n^2\\), then \\(\\mathbf{T}(n)\\) is in \\(O(n^4)\\). The \\(n^2\\) term contributes relatively little to the total cost for large \\(n\\). From now on, we will use these simplifying rules when discussing the cost for a program or algorithm. 1.5.3. Summary¶ Settings Saving... Server Error Resubmit Contact Us || Privacy | | License « 1.4. Faster Computer, or Faster Algorithm? :: Contents :: 1.6. Lower Bounds and \\(\\Theta\\) Notation » Contact Us | | Report a bug © Copyright 2011-2023 by OpenDSA Project Contributors and distributed under an MIT license. Last updated on Aug 13, 2024. Created using Sphinx 4.4.0. Summary*: Operating system*: Windows Mac OS Linux iOS Android Other Browser*: Chrome Safari Internet Explorer Opera Other Description*: Attach a screenshot (optional):"
            },
            {
                "role": "user",
                "content": "Can you summarize the key topics covered in this lecture?"
            },
            {
                "role": "assistant",
                "content": "This lecture covers foundational algorithmic concepts including problem formulation, correctness proof using induction, efficiency analysis using asymptotic notation, data structures (arrays, linked lists, BSTs, heaps), and sorting methods (selection sort, merge sort, counting sort). Additionally, it introduces the Word-RAM computational model, providing a theoretical framework for measuring algorithm efficiency."
            }
        ]
    },
    {
        "system": "Hello! I am Kiran, an AI assistant from Tampere University, dedicated to providing you with accurate and insightful assistance. My goal is to help you understand concepts, summarize lectures, and answer questions effectively.",
        "conversations": [
            {
                "role": "host",
                "content": "1.3. Best, Worst, and Average Cases — BM40A1500 Data Structures and Algorithms Username Password Forgot your password? Register Username: Password Confirm Password Email: BM40A1500 Data Structures and Algorithms Chapter 1 Algorithm Analysis Show Source | | About « 1.2. Comparing Algorithms :: Contents :: 1.4. Faster Computer, or Faster Algorithm? » 1.3. Best, Worst, and Average Cases¶ Settings Saving... Server Error Resubmit Settings Saving... Server Error Resubmit When analyzing an algorithm, should we study the best, worst, or average case? Normally we are not interested in the best case, because this might happen only rarely and generally is too optimistic for a fair characterization of the algorithm’s running time. In other words, analysis based on the best case is not likely to be representative of the behavior of the algorithm. However, there are rare instances where a best-case analysis is useful—in particular, when the best case has high probability of occurring. The Shellsort and Quicksort algorithms both can take advantage of the best-case running time of Insertion Sort to become more efficient. How about the worst case? The advantage to analyzing the worst case is that you know for certain that the algorithm must perform at least that well. This is especially important for real-time applications, such as for the computers that monitor an air traffic control system. Here, it would not be acceptable to use an algorithm that can handle \\(n\\) airplanes quickly enough most of the time, but which fails to perform quickly enough when all \\(n\\) airplanes are coming from the same direction. For other applications—particularly when we wish to aggregate the cost of running the program many times on many different inputs—worst-case analysis might not be a representative measure of the algorithm’s performance. Often we prefer to know the average-case running time. This means that we would like to know the typical behavior of the algorithm on inputs of size \\(n\\). Unfortunately, average-case analysis is not always possible. Average-case analysis first requires that we understand how the actual inputs to the program (and their costs) are distributed with respect to the set of all possible inputs to the program. For example, it was stated previously that the sequential search algorithm on average examines half of the array values. This is only true if the element with value \\(K\\) is equally likely to appear in any position in the array. If this assumption is not correct, then the algorithm does not necessarily examine half of the array values in the average case. The characteristics of a data distribution have a significant effect on many search algorithms, such as those based on hashing and search trees such as the BST. Incorrect assumptions about data distribution can have disastrous consequences on a program’s space or time performance. Unusual data distributions can also be used to advantage, such as is done by self-organizing lists. In summary, for real-time applications we are likely to prefer a worst-case analysis of an algorithm. Otherwise, we often desire an average-case analysis if we know enough about the distribution of our input to compute the average case. If not, then we must resort to worst-case analysis. Contact Us || Privacy | | License « 1.2. Comparing Algorithms :: Contents :: 1.4. Faster Computer, or Faster Algorithm? » Contact Us | | Report a bug © Copyright 2011-2023 by OpenDSA Project Contributors and distributed under an MIT license. Last updated on Aug 13, 2024. Created using Sphinx 4.4.0. Summary*: Operating system*: Windows Mac OS Linux iOS Android Other Browser*: Chrome Safari Internet Explorer Opera Other Description*: Attach a screenshot (optional):"
            },
            {
                "role": "user",
                "content": "Can you summarize the key topics covered in this lecture?"
            },
            {
                "role": "assistant",
                "content": "This lecture covers foundational algorithmic concepts including problem formulation, correctness proof using induction, efficiency analysis using asymptotic notation, data structures (arrays, linked lists, BSTs, heaps), and sorting methods (selection sort, merge sort, counting sort). Additionally, it introduces the Word-RAM computational model, providing a theoretical framework for measuring algorithm efficiency."
            }
        ]
    },
    {
        "system": "Hello! I am Kiran, an AI assistant from Tampere University, dedicated to providing you with accurate and insightful assistance. My goal is to help you understand concepts, summarize lectures, and answer questions effectively.",
        "conversations": [
            {
                "role": "host",
                "content": "1.1. Chapter Introduction — BM40A1500 Data Structures and Algorithms Username Password Forgot your password? Register Username: Password Confirm Password Email: BM40A1500 Data Structures and Algorithms Chapter 1 Algorithm Analysis Show Source | | About « 0.3. Abstract Data Types :: Contents :: 1.2. Comparing Algorithms » 1.1. Chapter Introduction¶ How long will it take to process the company payroll once we complete our planned merger? Should I buy a new payroll program from vendor X or vendor Y? If a particular program is slow, is it badly implemented or is it solving a hard problem? Questions like these ask us to consider the difficulty of a problem, or the relative efficiency of two or more approaches to solving a problem. This chapter introduces the motivation, basic notation, and fundamental techniques of algorithm analysis. We focus on a methodology known as asymptotic algorithm analysis, or simply asymptotic analysis. Asymptotic analysis attempts to estimate the resource consumption of an algorithm. It allows us to compare the relative costs of two or more algorithms for solving the same problem. Asymptotic analysis also gives algorithm designers a tool for estimating whether a proposed solution is likely to meet the resource constraints for a problem before they implement an actual program. After reading this chapter, you should understand the concept of a growth rate, the rate at which the cost of an algorithm grows as the size of its input grows; the concept of an upper bound and lower bound for a growth rate, and how to estimate these bounds for a simple program, algorithm, or problem; and the difference between the cost of an algorithm (or program) and the cost of a problem. The chapter concludes with a brief discussion of the practical difficulties encountered when empirically measuring the cost of a program, and some principles for code tuning to improve program efficiency. Contact Us || Privacy | | License « 0.3. Abstract Data Types :: Contents :: 1.2. Comparing Algorithms » Contact Us | | Report a bug © Copyright 2011-2023 by OpenDSA Project Contributors and distributed under an MIT license. Last updated on Aug 13, 2024. Created using Sphinx 4.4.0. Summary*: Operating system*: Windows Mac OS Linux iOS Android Other Browser*: Chrome Safari Internet Explorer Opera Other Description*: Attach a screenshot (optional):"
            },
            {
                "role": "user",
                "content": "Can you summarize the key topics covered in this lecture?"
            },
            {
                "role": "assistant",
                "content": "This lecture covers foundational algorithmic concepts including problem formulation, correctness proof using induction, efficiency analysis using asymptotic notation, data structures (arrays, linked lists, BSTs, heaps), and sorting methods (selection sort, merge sort, counting sort). Additionally, it introduces the Word-RAM computational model, providing a theoretical framework for measuring algorithm efficiency."
            }
        ]
    },
    {
        "system": "Hello! I am Kiran, an AI assistant from Tampere University, dedicated to providing you with accurate and insightful assistance. My goal is to help you understand concepts, summarize lectures, and answer questions effectively.",
        "conversations": [
            {
                "role": "host",
                "content": "1.4. Faster Computer, or Faster Algorithm? — BM40A1500 Data Structures and Algorithms Username Password Forgot your password? Register Username: Password Confirm Password Email: BM40A1500 Data Structures and Algorithms Chapter 1 Algorithm Analysis Show Source | | About « 1.3. Best, Worst, and Average Cases :: Contents :: 1.5. Asymptotic Analysis and Upper Bounds » 1.4. Faster Computer, or Faster Algorithm?¶ Imagine that you have a problem to solve, and you know of an algorithm whose running time is proportional to \\(n^2\\) where \\(n\\) is a measure of the input size. Unfortunately, the resulting program takes ten times too long to run. If you replace your current computer with a new one that is ten times faster, will the \\(n^2\\) algorithm become acceptable? If the problem size remains the same, then perhaps the faster computer will allow you to get your work done quickly enough even with an algorithm having a high growth rate. But a funny thing happens to most people who get a faster computer. They don’t run the same problem faster. They run a bigger problem! Say that on your old computer you were content to sort 10,000 records because that could be done by the computer during your lunch break. On your new computer you might hope to sort 100,000 records in the same time. You won’t be back from lunch any sooner, so you are better off solving a larger problem. And because the new machine is ten times faster, you would like to sort ten times as many records. If your algorithm’s growth rate is linear (i.e., if the equation that describes the running time on input size \\(n\\) is \\(\\mathbf{T}(n) = cn\\) for some constant \\(c\\)), then 100,000 records on the new machine will be sorted in the same time as 10,000 records on the old machine. If the algorithm’s growth rate is greater than \\(cn\\), such as \\(c_1n^2\\), then you will not be able to do a problem ten times the size in the same amount of time on a machine that is ten times faster. How much larger a problem can be solved in a given amount of time by a faster computer? Assume that the new machine is ten times faster than the old. Say that the old machine could solve a problem of size \\(n\\) in an hour. What is the largest problem that the new machine can solve in one hour? The following table shows how large a problem can be solved on the two machines for five running-time functions. Table 1.4.1 The increase in problem size that can be run in a fixed period of time on a computer that is ten times faster. The first column lists the right-hand sides for five growth rate equations. For the purpose of this example, arbitrarily assume that the old machine can run 10,000 basic operations in one hour. The second column shows the maximum value for \\(n\\) that can be run in 10,000 basic operations on the old machine. The third column shows the value for \\(n'\\), the new maximum size for the problem that can be run in the same time on the new machine that is ten times faster. Variable \\(n'\\) is the greatest size for the problem that can run in 100,000 basic operations. The fourth column shows how the size of \\(n\\) changed to become \\(n'\\) on the new machine. The fifth column shows the increase in the problem size as the ratio of \\(n'\\) to \\(n\\). \\[\\begin{split}\\begin{array} {l|r|r|l|r} \\mathbf{f(n)} & \\mathbf{n} & \\mathbf{n'} & \\mathbf{Change} & \\mathbf{n'/n}\\\\ \\hline 10n & 1000 & 10,000 & n' = 10n & 10\\\\ 20n & 500 & 5000 & n' = 10n & 10\\\\ 5 n \\log n & 250 & 1842 & \\sqrt{10} n < n' < 10n & 7.37\\\\ 2 n^2 & 70 & 223 & n' = \\sqrt{10} n & 3.16\\\\ 2^n & 13 & 16 & n' = n + 3 & --\\\\ \\end{array}\\end{split}\\] This table illustrates many important points. The first two equations are both linear; only the value of the constant factor has changed. In both cases, the machine that is ten times faster gives an increase in problem size by a factor of ten. In other words, while the value of the constant does affect the absolute size of the problem that can be solved in a fixed amount of time, it does not affect the improvement in problem size (as a proportion to the original size) gained by a faster computer. This relationship holds true regardless of the algorithm’s growth rate: Constant factors never affect the relative improvement gained by a faster computer. An algorithm with time equation \\(\\mathbf{T}(n) = 2n^2\\) does not receive nearly as great an improvement from the faster machine as an algorithm with linear growth rate. Instead of an improvement by a factor of ten, the improvement is only the square root of that: \\(\\sqrt{10} \\approx 3.16\\). Thus, the algorithm with higher growth rate not only solves a smaller problem in a given time in the first place, it also receives less of a speedup from a faster computer. As computers get ever faster, the disparity in problem sizes becomes ever greater. The algorithm with growth rate \\(\\mathbf{T}(n) = 5 n \\log n\\) improves by a greater amount than the one with quadratic growth rate, but not by as great an amount as the algorithms with linear growth rates. Note that something special happens in the case of the algorithm whose running time grows exponentially. If you look at its plot on a graph, the curve for the algorithm whose time is proportional to \\(2^n\\) goes up very quickly as \\(n\\) grows. The increase in problem size on the machine ten times as fast is about \\(n + 3\\) (to be precise, it is \\(n + \\log_2 10\\)). The increase in problem size for an algorithm with exponential growth rate is by a constant addition, not by a multiplicative factor. Because the old value of \\(n\\) was 13, the new problem size is 16. If next year you buy another computer ten times faster yet, then the new computer (100 times faster than the original computer) will only run a problem of size 19. If you had a second program whose growth rate is \\(2^n\\) and for which the original computer could run a problem of size 1000 in an hour, than a machine ten times faster can run a problem only of size 1003 in an hour! Thus, an exponential growth rate is radically different than the other growth rates shown in the table. The significance of this difference is an important topic in computational complexity theory. Instead of buying a faster computer, consider what happens if you replace an algorithm whose running time is proportional to \\(n^2\\) with a new algorithm whose running time is proportional to \\(n \\log n\\). In a graph relating growth rate functions to input size, a fixed amount of time would appear as a horizontal line. If the line for the amount of time available to solve your problem is above the point at which the curves for the two growth rates in question meet, then the algorithm whose running time grows less quickly is faster. An algorithm with running time \\(\\mathbf{T}n=n^2\\) requires \\(1024 \\times 1024 = 1,048,576\\) time steps for an input of size \\(n=1024\\). An algorithm with running time \\(\\mathbf{T}(n) = n \\log n\\) requires \\(1024 \\times 10 = 10,240\\) time steps for an input of size \\(n = 1024\\), which is an improvement of much more than a factor of ten when compared to the algorithm with running time \\(\\mathbf{T}(n) = n^2\\). Because \\(n^2 > 10 n \\log n\\) whenever \\(n > 58\\), if the typical problem size is larger than 58 for this example, then you would be much better off changing algorithms instead of buying a computer ten times faster. Furthermore, when you do buy a faster computer, an algorithm with a slower growth rate provides a greater benefit in terms of larger problem size that can run in a certain time on the new computer. Contact Us || Privacy | | License « 1.3. Best, Worst, and Average Cases :: Contents :: 1.5. Asymptotic Analysis and Upper Bounds » Contact Us | | Report a bug © Copyright 2011-2023 by OpenDSA Project Contributors and distributed under an MIT license. Last updated on Aug 13, 2024. Created using Sphinx 4.4.0. Summary*: Operating system*: Windows Mac OS Linux iOS Android Other Browser*: Chrome Safari Internet Explorer Opera Other Description*: Attach a screenshot (optional):"
            },
            {
                "role": "user",
                "content": "Can you summarize the key topics covered in this lecture?"
            },
            {
                "role": "assistant",
                "content": "This lecture covers foundational algorithmic concepts including problem formulation, correctness proof using induction, efficiency analysis using asymptotic notation, data structures (arrays, linked lists, BSTs, heaps), and sorting methods (selection sort, merge sort, counting sort). Additionally, it introduces the Word-RAM computational model, providing a theoretical framework for measuring algorithm efficiency."
            }
        ]
    },
    {
        "system": "Hello! I am Kiran, an AI assistant from Tampere University, dedicated to providing you with accurate and insightful assistance. My goal is to help you understand concepts, summarize lectures, and answer questions effectively.",
        "conversations": [
            {
                "role": "host",
                "content": "1.2. Comparing Algorithms — BM40A1500 Data Structures and Algorithms Username Password Forgot your password? Register Username: Password Confirm Password Email: BM40A1500 Data Structures and Algorithms Chapter 1 Algorithm Analysis Show Source | | About « 1.1. Chapter Introduction :: Contents :: 1.3. Best, Worst, and Average Cases » 1.2. Comparing Algorithms¶ 1.2.1. Introduction¶ How do you compare two algorithms for solving some problem in terms of efficiency? We could implement both algorithms as computer programs and then run them on a suitable range of inputs, measuring how much of the resources in question each program uses. This approach is often unsatisfactory for four reasons. First, there is the effort involved in programming and testing two algorithms when at best you want to keep only one. Second, when empirically comparing two algorithms there is always the chance that one of the programs was “better written” than the other, and therefore the relative qualities of the underlying algorithms are not truly represented by their implementations. This can easily occur when the programmer has a bias regarding the algorithms. Third, the choice of empirical test cases might unfairly favor one algorithm. Fourth, you could find that even the better of the two algorithms does not fall within your resource budget. In that case you must begin the entire process again with yet another program implementing a new algorithm. But, how would you know if any algorithm can meet the resource budget? Perhaps the problem is simply too difficult for any implementation to be within budget. These problems can often be avoided by using asymptotic analysis. Asymptotic analysis measures the efficiency of an algorithm, or its implementation as a program, as the input size becomes large. It is actually an estimating technique and does not tell us anything about the relative merits of two programs where one is always “slightly faster” than the other. However, asymptotic analysis has proved useful to computer scientists who must determine if a particular algorithm is worth considering for implementation. The critical resource for a program is most often its running time. However, you cannot pay attention to running time alone. You must also be concerned with other factors such as the space required to run the program (both main memory and disk space). Typically you will analyze the time required for an algorithm (or the instantiation of an algorithm in the form of a program), and the space required for a data structure. Many factors affect the running time of a program. Some relate to the environment in which the program is compiled and run. Such factors include the speed of the computer’s CPU, bus, and peripheral hardware. Competition with other users for the computer’s (or the network’s) resources can make a program slow to a crawl. The programming language and the quality of code generated by a particular compiler can have a significant effect. The “coding efficiency” of the programmer who converts the algorithm to a program can have a tremendous impact as well. If you need to get a program working within time and space constraints on a particular computer, all of these factors can be relevant. Yet, none of these factors address the differences between two algorithms or data structures. To be fair, if you want to compare two programs derived from two algorithms for solving the same problem, they should both be compiled with the same compiler and run on the same computer under the same conditions. As much as possible, the same amount of care should be taken in the programming effort devoted to each program to make the implementations “equally efficient”. In this sense, all of the factors mentioned above should cancel out of the comparison because they apply to both algorithms equally. If you truly wish to understand the running time of an algorithm, there are other factors that are more appropriate to consider than machine speed, programming language, compiler, and so forth. Ideally we would measure the running time of the algorithm under standard benchmark conditions. However, we have no way to calculate the running time reliably other than to run an implementation of the algorithm on some computer. The only alternative is to use some other measure as a surrogate for running time. 1.2.2. Basic Operations and Input Size¶ Of primary consideration when estimating an algorithm’s performance is the number of basic operations required by the algorithm to process an input of a certain size. The terms “basic operations” and “size” are both rather vague and depend on the algorithm being analyzed. Size is often the number of inputs processed. For example, when comparing sorting algorithms the size of the problem is typically measured by the number of records to be sorted. A basic operation must have the property that its time to complete does not depend on the particular values of its operands. Adding or comparing two integer variables are examples of basic operations in most programming languages. Summing the contents of an array containing \\(n\\) integers is not, because the cost depends on the value of \\(n\\) (i.e., the size of the input). Example 1.2.1 Consider a simple algorithm to solve the problem of finding the largest value in an array of \\(n\\) integers. The algorithm looks at each integer in turn, saving the position of the largest value seen so far. This algorithm is called the largest-value sequential search and is illustrated by the following function: # Return position of largest value in integer array A def largest(A): currlarge = 0 # Position of largest element seen for i in range(1, A.length): # For each element if A[currlarge] < A[i]: # if A[i] is larger currlarge = i # remember its position return currlarge # Return largest position Here, the size of the problem is len(A), the number of integers stored in array A. The basic operation is to compare an integer’s value to that of the largest value seen so far. It is reasonable to assume that it takes a fixed amount of time to do one such comparison, regardless of the value of the two integers or their positions in the array. Because the most important factor affecting running time is normally size of the input, for a given input size \\(n\\) we often express the time \\(\\mathbf{T}\\) to run the algorithm as a function of \\(n\\), written as \\(\\mathbf{T}(n)\\). We will always assume \\(\\mathbf{T}(n)\\) is a non-negative value. Let us call \\(c\\) the amount of time required to compare two integers in function largest. We do not care right now what the precise value of \\(c\\) might be. Nor are we concerned with the time required to increment variable \\(i\\) because this must be done for each value in the array, or the time for the actual assignment when a larger value is found, or the little bit of extra time taken to initialize currlarge. We just want a reasonable approximation for the time taken to execute the algorithm. The total time to run largest is therefore approximately \\(cn\\), because we must make \\(n\\) comparisons, with each comparison costing \\(c\\) time. We say that function largest (and by extension, the largest-value sequential search algorithm for any typical implementation) has a running time expressed by the equation \\[\\mathbf{T}(n) = cn.\\] This equation describes the growth rate for the running time of the largest-value sequential search algorithm. Example 1.2.2 The running time of a statement that assigns the first value of an integer array to a variable is simply the time required to copy the value of the first array value. We can assume this assignment takes a constant amount of time regardless of the value. Let us call \\(c_1\\) the amount of time necessary to copy an integer. No matter how large the array on a typical computer (given reasonable conditions for memory and array size), the time to copy the value from the first position of the array is always \\(c_1\\). Thus, the equation for this algorithm is simply \\[\\mathbf{T}(n) = c_1,\\] indicating that the size of the input \\(n\\) has no effect on the running time. This is called a constant running time. Example 1.2.3 Consider the following code: sum = 0 for i in range(n): for j in range(n): sum += 1 What is the running time for this code fragment? Clearly it takes longer to run when \\(n\\) is larger. The basic operation in this example is the increment operation for variable sum. We can assume that incrementing takes constant time; call this time \\(c_2\\). (We can ignore the time required to initialize sum, and to increment the loop counters i and j. In practice, these costs can safely be bundled into time \\(c_2\\).) The total number of increment operations is \\(n^2\\). Thus, we say that the running time is \\(\\mathbf{T}(n) = c_2 n^2\\). 1.2.3. Growth Rates¶ The growth rate for an algorithm is the rate at which the cost of the algorithm grows as the size of its input grows. The following figure shows a graph for six equations, each meant to describe the running time for a particular program or algorithm. A variety of growth rates that are representative of typical algorithms are shown. Figure 1.2.2: Two views of a graph illustrating the growth rates for six equations. The bottom view shows in detail the lower-left portion of the top view. The horizontal axis represents input size. The vertical axis can represent time, space, or any other measure of cost. The two equations labeled \\(10n\\) and \\(20n\\) are graphed by straight lines. A growth rate of \\(cn\\) (for \\(c\\) any positive constant) is often referred to as a linear growth rate or running time. This means that as the value of \\(n\\) grows, the running time of the algorithm grows in the same proportion. Doubling the value of \\(n\\) roughly doubles the running time. An algorithm whose running-time equation has a highest-order term containing a factor of \\(n^2\\) is said to have a quadratic growth rate. In the figure, the line labeled \\(2n^2\\) represents a quadratic growth rate. The line labeled \\(2^n\\) represents an exponential growth rate. This name comes from the fact that \\(n\\) appears in the exponent. The line labeled \\(n!\\) also grows exponentially. As you can see from the figure, the difference between an algorithm whose running time has cost \\(\\mathbf{T}(n) = 10n\\) and another with cost \\(\\mathbf{T}(n) = 2n^2\\) becomes tremendous as \\(n\\) grows. For \\(n > 5\\), the algorithm with running time \\(\\mathbf{T}(n) = 2n^2\\) is already much slower. This is despite the fact that \\(10n\\) has a greater constant factor than \\(2n^2\\). Comparing the two curves marked \\(20n\\) and \\(2n^2\\) shows that changing the constant factor for one of the equations only shifts the point at which the two curves cross. For \\(n>10\\), the algorithm with cost \\(\\mathbf{T}(n) = 2n^2\\) is slower than the algorithm with cost \\(\\mathbf{T}(n) = 20n\\). This graph also shows that the equation \\(\\mathbf{T}(n) = 5 n \\log n\\) grows somewhat more quickly than both \\(\\mathbf{T}(n) = 10 n\\) and \\(\\mathbf{T}(n) = 20 n\\), but not nearly so quickly as the equation \\(\\mathbf{T}(n) = 2n^2\\). For constants \\(a, b > 1, n^a\\) grows faster than either \\(\\log^b n\\) or \\(\\log n^b\\). Finally, algorithms with cost \\(\\mathbf{T}(n) = 2^n\\) or \\(\\mathbf{T}(n) = n!\\) are prohibitively expensive for even modest values of \\(n\\). Note that for constants \\(a, b \\geq 1, a^n\\) grows faster than \\(n^b\\). We can get some further insight into relative growth rates for various algorithms from the following table. Most of the growth rates that appear in typical algorithms are shown, along with some representative input sizes. Once again, we see that the growth rate has a tremendous effect on the resources consumed by an algorithm. Table 1.2.1 Costs for representative growth rates. \\[\\begin{split}\\begin{array}{c|c|c|c|c|c|c|c} \\mathsf{n} & \\mathsf{\\log \\log n} & \\mathsf{\\log n} & \\mathsf{n} & \\mathsf{n \\log n} & \\mathsf{n^2} & \\mathsf{n^3} & \\mathsf{2^n}\\\\ \\hline \\mathsf{16} & \\mathsf{2} & \\mathsf{4} & \\mathsf{2^{4}} & \\mathsf{4 \\cdot 2^{4} = 2^{6}} & \\mathsf{2^{8}} & \\mathsf{2^{12}} & \\mathsf{2^{16}}\\\\ \\mathsf{256} & \\mathsf{3} & \\mathsf{8} & \\mathsf{2^{8}} & \\mathsf{8 \\cdot 2^{8} = 2^{11}} & \\mathsf{2^{16}} & \\mathsf{2^{24}} & \\mathsf{2^{256}}\\\\ \\mathsf{1024} & \\mathsf{\\approx 3.3} & \\mathsf{10} & \\mathsf{2^{10}} & \\mathsf{10 \\cdot 2^{10} \\approx 2^{13}} & \\mathsf{2^{20}} & \\mathsf{2^{30}} & \\mathsf{2^{1024}}\\\\ \\mathsf{64 {\\rm K}} & \\mathsf{4} & \\mathsf{16} & \\mathsf{2^{16}} & \\mathsf{16 \\cdot 2^{16} = 2^{20}} & \\mathsf{2^{32}} & \\mathsf{2^{48}} & \\mathsf{2^{64 {\\rm K}}}\\\\ \\mathsf{1 {\\rm M}} & \\mathsf{\\approx 4.3} & \\mathsf{20} & \\mathsf{2^{20}} & \\mathsf{20 \\cdot 2^{20} \\approx 2^{24}} & \\mathsf{2^{40}} & \\mathsf{2^{60}} & \\mathsf{2^{1 {\\rm M}}}\\\\ \\mathsf{1 {\\rm G}} & \\mathsf{\\approx 4.9} & \\mathsf{30} & \\mathsf{2^{30}} & \\mathsf{30 \\cdot 2^{30} \\approx 2^{35}} & \\mathsf{2^{60}} & \\mathsf{2^{90}} & \\mathsf{2^{1 {\\rm G}}}\\\\ \\end{array}\\end{split}\\] Contact Us || Privacy | | License « 1.1. Chapter Introduction :: Contents :: 1.3. Best, Worst, and Average Cases » Contact Us | | Report a bug © Copyright 2011-2023 by OpenDSA Project Contributors and distributed under an MIT license. Last updated on Aug 13, 2024. Created using Sphinx 4.4.0. Summary*: Operating system*: Windows Mac OS Linux iOS Android Other Browser*: Chrome Safari Internet Explorer Opera Other Description*: Attach a screenshot (optional):"
            },
            {
                "role": "user",
                "content": "Can you summarize the key topics covered in this lecture?"
            },
            {
                "role": "assistant",
                "content": "This lecture covers foundational algorithmic concepts including problem formulation, correctness proof using induction, efficiency analysis using asymptotic notation, data structures (arrays, linked lists, BSTs, heaps), and sorting methods (selection sort, merge sort, counting sort). Additionally, it introduces the Word-RAM computational model, providing a theoretical framework for measuring algorithm efficiency."
            }
        ]
    },
    {
        "system": "Hello! I am Kiran, an AI assistant from Tampere University, dedicated to providing you with accurate and insightful assistance. My goal is to help you understand concepts, summarize lectures, and answer questions effectively.",
        "conversations": [
            {
                "role": "host",
                "content": "1.6. Lower Bounds and \\(\\Theta\\) Notation — BM40A1500 Data Structures and Algorithms Username Password Forgot your password? Register Username: Password Confirm Password Email: BM40A1500 Data Structures and Algorithms Chapter 1 Algorithm Analysis Show Source | | About « 1.5. Asymptotic Analysis and Upper Bounds :: Contents :: 1.7. Calculating Program Running Time » 1.6. Lower Bounds and \\(\\Theta\\) Notation¶ 1.6.1. Lower Bounds¶ Big-Oh notation describes an upper bound. In other words, big-Oh notation states a claim about the greatest amount of some resource (usually time) that is required by an algorithm for some class of inputs of size \\(n\\) (typically the worst such input, the average of all possible inputs, or the best such input). Similar notation is used to describe the least amount of a resource that an algorithm needs for some class of input. Like big-Oh notation, this is a measure of the algorithm’s growth rate. Like big-Oh notation, it works for any resource, but we most often measure the least amount of time required. And again, like big-Oh notation, we are measuring the resource required for some particular class of inputs: the worst-, average-, or best-case input of size \\(n\\). The lower bound for an algorithm (or a problem, as explained later) is denoted by the symbol \\(\\Omega\\), pronounced “big-Omega” or just “Omega”. The following definition for \\(\\Omega\\) is symmetric with the definition of big-Oh. For \\(\\mathbf{T}(n)\\) a non-negatively valued function, \\(\\mathbf{T}(n)\\) is in set \\(\\Omega(g(n))\\) if there exist two positive constants \\(c\\) and \\(n_0\\) such that \\(\\mathbf{T}(n) \\geq c g(n)\\) for all \\(n > n_0\\). 1 Example 1.6.1 Assume \\(\\mathbf{T}(n) = c_1 n^2 + c_2 n\\) for \\(c_1\\) and \\(c_2 > 0\\). Then, \\[c_1 n^2 + c_2 n \\geq c_1 n^2\\] for all \\(n > 1\\). So, \\(\\mathbf{T}(n) \\geq c n^2\\) for \\(c = c_1\\) and \\(n_0 = 1\\). Therefore, \\(\\mathbf{T}(n)\\) is in \\(\\Omega(n^2)\\) by the definition. It is also true that the equation of the example above is in \\(\\Omega(n)\\). However, as with big-Oh notation, we wish to get the “tightest” (for \\(\\Omega\\) notation, the largest) bound possible. Thus, we prefer to say that this running time is in \\(\\Omega(n^2)\\). Recall the sequential search algorithm to find a value \\(K\\) within an array of integers. In the average and worst cases this algorithm is in \\(\\Omega(n)\\), because in both the average and worst cases we must examine at least \\(cn\\) values (where \\(c\\) is 1/2 in the average case and 1 in the worst case). 1 An alternate (non-equivalent) definition for \\(\\Omega\\) is \\(\\mathbf{T}(n)\\) is in the set \\(\\Omega(g(n))\\) if there exists a positive constant \\(c\\) such that \\(\\mathbf{T}(n) \\geq c g(n)\\) for an infinite number of values for \\(n\\). This definition says that for an “interesting” number of cases, the algorithm takes at least \\(c g(n)\\) time. Note that this definition is not symmetric with the definition of big-Oh. For \\(g(n)\\) to be a lower bound, this definition does not require that \\(\\mathbf{T}(n) \\geq c g(n)\\) for all values of \\(n\\) greater than some constant. It only requires that this happen often enough, in particular that it happen for an infinite number of values for \\(n\\). Motivation for this alternate definition can be found in the following example. Assume a particular algorithm has the following behavior: \\[\\begin{split}\\mathbf{T}(n) = \\left\\{ \\begin{array}{ll} n & \\mbox{for all odd}\\ n \\geq 1\\\\ n^2/100 & \\mbox{for all even}\\ n \\geq 0 \\end{array} \\right.\\end{split}\\] From this definition, \\(n^2/100 \\geq \\frac{1}{100} n^2\\) for all even \\(n \\geq 0\\). So, \\(\\mathbf{T}(n) \\geq c n^2\\) for an infinite number of values of \\(n\\) (i.e., for all even \\(n\\)) for \\(c = 1/100\\). Therefore, \\(\\mathbf{T}(n)\\) is in \\(\\Omega(n^2)\\) by the definition. For this equation for \\(\\mathbf{T}(n)\\), it is true that all inputs of size \\(n\\) take at least \\(cn\\) time. But an infinite number of inputs of size \\(n\\) take \\(cn^2\\) time, so we would like to say that the algorithm is in \\(\\Omega(n^2)\\). Unfortunately, using our first definition will yield a lower bound of \\(\\Omega(n)\\) because it is not possible to pick constants \\(c\\) and \\(n_0\\) such that \\(\\mathbf{T}(n) \\geq c n^2\\) for all \\(n>n_0\\). The alternative definition does result in a lower bound of \\(\\Omega(n^2)\\) for this algorithm, which seems to fit common sense more closely. Fortunately, few real algorithms or computer programs display the pathological behavior of this example. Our first definition for \\(\\Omega\\) generally yields the expected result. As you can see from this discussion, asymptotic bounds notation is not a law of nature. It is merely a powerful modeling tool used to describe the behavior of algorithms. 1.6.2. Theta Notation¶ The definitions for big-Oh and \\(\\Omega\\) give us ways to describe the upper bound for an algorithm (if we can find an equation for the maximum cost of a particular class of inputs of size \\(n\\)) and the lower bound for an algorithm (if we can find an equation for the minimum cost for a particular class of inputs of size \\(n\\)). When the upper and lower bounds are the same within a constant factor, we indicate this by using \\(\\Theta\\) (big-Theta) notation. An algorithm is said to be \\(\\Theta(h(n))\\) if it is in \\(O(h(n))\\) and it is in \\(\\Omega(h(n))\\). Note that we drop the word “in” for \\(\\Theta\\) notation, because there is a strict equality for two equations with the same \\(\\Theta\\). In other words, if \\(f(n)\\) is \\(\\Theta(g(n))\\), then \\(g(n)\\) is \\(\\Theta(f(n))\\). Because the sequential search algorithm is both in \\(O(n)\\) and in \\(\\Omega(n)\\) in the average case, we say it is \\(\\Theta(n)\\) in the average case. Given an algebraic equation describing the time requirement for an algorithm, the upper and lower bounds always meet. That is because in some sense we have a perfect analysis for the algorithm, embodied by the running-time equation. For many algorithms (or their instantiations as programs), it is easy to come up with the equation that defines their runtime behavior. The analysis for most commonly used algorithms is well understood and we can almost always give a \\(\\Theta\\) analysis for them. However, the class of NP-Complete problems all have no definitive \\(\\Theta\\) analysis, just some unsatisfying big-Oh and \\(\\Omega\\) analyses. Even some “simple” programs are hard to analyze. Nobody currently knows the true upper or lower bounds for the following code fragment. while n > 1: if ODD(n): n = 3 * n + 1 else: n = n / 2 While some textbooks and programmers will casually say that an algorithm is “order of” or “big-Oh” of some cost function, it is generally better to use \\(\\Theta\\) notation rather than big-Oh notation whenever we have sufficient knowledge about an algorithm to be sure that the upper and lower bounds indeed match. OpenDSA modules use \\(\\Theta\\) notation in preference to big-Oh notation whenever our state of knowledge makes that possible. Limitations on our ability to analyze certain algorithms may require use of big-Oh or \\(\\Omega\\) notations. In rare occasions when the discussion is explicitly about the upper or lower bound of a problem or algorithm, the corresponding notation will be used in preference to \\(\\Theta\\) notation. 1.6.3. Classifying Functions¶ Given functions \\(f(n)\\) and \\(g(n)\\) whose growth rates are expressed as algebraic equations, we might like to determine if one grows faster than the other. The best way to do this is to take the limit of the two functions as \\(n\\) grows towards infinity, \\[\\lim_{n \\rightarrow \\infty} \\frac{f(n)}{g(n)}.\\] If the limit goes to \\(\\infty\\), then \\(f(n)\\) is in \\(\\Omega(g(n))\\) because \\(f(n)\\) grows faster. If the limit goes to zero, then \\(f(n)\\) is in \\(O(g(n))\\) because \\(g(n)\\) grows faster. If the limit goes to some constant other than zero, then \\(f(n) = \\Theta(g(n))\\) because both grow at the same rate. Example 1.6.2 If \\(f(n) = n^2\\) and \\(g(n) = 2n\\log n\\), is \\(f(n)\\) in \\(O(g(n))\\), \\(\\Omega(g(n))\\), or \\(\\Theta(g(n))\\)? Since \\[\\frac{n^2}{2n\\log n} = \\frac{n}{2\\log n},\\] we easily see that \\[\\lim_{n \\rightarrow \\infty} \\frac{n^2}{2n\\log n} = \\infty\\] because \\(n\\) grows faster than \\(2\\log n\\). Thus, \\(n^2\\) is in \\(\\Omega(2n\\log n)\\). Settings Saving... Server Error Resubmit Contact Us || Privacy | | License « 1.5. Asymptotic Analysis and Upper Bounds :: Contents :: 1.7. Calculating Program Running Time » Contact Us | | Report a bug © Copyright 2011-2023 by OpenDSA Project Contributors and distributed under an MIT license. Last updated on Aug 13, 2024. Created using Sphinx 4.4.0. Summary*: Operating system*: Windows Mac OS Linux iOS Android Other Browser*: Chrome Safari Internet Explorer Opera Other Description*: Attach a screenshot (optional):"
            },
            {
                "role": "user",
                "content": "Can you summarize the key topics covered in this lecture?"
            },
            {
                "role": "assistant",
                "content": "This lecture covers foundational algorithmic concepts including problem formulation, correctness proof using induction, efficiency analysis using asymptotic notation, data structures (arrays, linked lists, BSTs, heaps), and sorting methods (selection sort, merge sort, counting sort). Additionally, it introduces the Word-RAM computational model, providing a theoretical framework for measuring algorithm efficiency."
            }
        ]
    },
    {
        "system": "Hello! I am Kiran, an AI assistant from Tampere University, dedicated to providing you with accurate and insightful assistance. My goal is to help you understand concepts, summarize lectures, and answer questions effectively.",
        "conversations": [
            {
                "role": "host",
                "content": "1.8. Common Misunderstandings — BM40A1500 Data Structures and Algorithms Username Password Forgot your password? Register Username: Password Confirm Password Email: BM40A1500 Data Structures and Algorithms Chapter 1 Algorithm Analysis Show Source | | About « 1.7. Calculating Program Running Time :: Contents :: 1.9. Multiple Parameters » 1.8. Common Misunderstandings¶ Asymptotic analysis is one of the most intellectually difficult topics that undergraduate computer science majors are confronted with. Most people find growth rates and asymptotic analysis confusing and so develop misconceptions about either the concepts or the terminology. It helps to know what the standard points of confusion are, in hopes of avoiding them. One problem with differentiating the concepts of upper and lower bounds is that, for most algorithms that you will encounter, it is easy to recognize the true growth rate for that algorithm. Given complete knowledge about a cost function, the upper and lower bound for that cost function are always the same. Thus, the distinction between an upper and a lower bound is only worthwhile when you have incomplete knowledge about the thing being measured. If this distinction is still not clear, then you should read about analyzing problems . We use \\(\\Theta\\)-notation to indicate that there is no meaningful difference between what we know about the growth rates of the upper and lower bound (which is usually the case for simple algorithms). It is a common mistake to confuse the concepts of upper bound or lower bound on the one hand, and worst case or best case on the other. The best, worst, or average cases each define a cost for a specific input instance (or specific set of instances for the average case). In contrast, upper and lower bounds describe our understanding of the growth rate for that cost measure. So to define the growth rate for an algorithm or problem, we need to determine what we are measuring (the best, worst, or average case) and also our description for what we know about the growth rate of that cost measure (big-Oh, \\(\\Omega\\), or \\(\\Theta\\)). The upper bound for an algorithm is not the same as the worst case for that algorithm for a given input of size \\(n\\). What is being bounded is not the actual cost (which you can determine for a given value of \\(n\\)), but rather the growth rate for the cost. There cannot be a growth rate for a single point, such as a particular value of \\(n\\). The growth rate applies to the change in cost as a change in input size occurs. Likewise, the lower bound is not the same as the best case for a given size \\(n\\). Another common misconception is thinking that the best case for an algorithm occurs when the input size is as small as possible, or that the worst case occurs when the input size is as large as possible. What is correct is that best- and worse-case instances exist for each possible size of input. That is, for all inputs of a given size, say \\(i\\), one (or more) of the inputs of size \\(i\\) is the best and one (or more) of the inputs of size \\(i\\) is the worst. Often (but not always!), we can characterize the best input case for an arbitrary size, and we can characterize the worst input case for an arbitrary size. Ideally, we can determine the growth rate for the characterized best, worst, and average cases as the input size grows. Example 1.8.1 What is the growth rate of the best case for sequential search? For any array of size \\(n\\), the best case occurs when the value we are looking for appears in the first position of the array. This is true regardless of the size of the array. Thus, the best case (for arbitrary size \\(n\\)) occurs when the desired value is in the first of \\(n\\) positions, and its cost is 1. It is not correct to say that the best case occurs when \\(n=1\\). Settings Saving... Server Error Resubmit Contact Us || Privacy | | License « 1.7. Calculating Program Running Time :: Contents :: 1.9. Multiple Parameters » Contact Us | | Report a bug © Copyright 2011-2023 by OpenDSA Project Contributors and distributed under an MIT license. Last updated on Aug 13, 2024. Created using Sphinx 4.4.0. Summary*: Operating system*: Windows Mac OS Linux iOS Android Other Browser*: Chrome Safari Internet Explorer Opera Other Description*: Attach a screenshot (optional):"
            },
            {
                "role": "user",
                "content": "Can you summarize the key topics covered in this lecture?"
            },
            {
                "role": "assistant",
                "content": "This lecture covers foundational algorithmic concepts including problem formulation, correctness proof using induction, efficiency analysis using asymptotic notation, data structures (arrays, linked lists, BSTs, heaps), and sorting methods (selection sort, merge sort, counting sort). Additionally, it introduces the Word-RAM computational model, providing a theoretical framework for measuring algorithm efficiency."
            }
        ]
    },
    {
        "system": "Hello! I am Kiran, an AI assistant from Tampere University, dedicated to providing you with accurate and insightful assistance. My goal is to help you understand concepts, summarize lectures, and answer questions effectively.",
        "conversations": [
            {
                "role": "host",
                "content": "1.9. Multiple Parameters — BM40A1500 Data Structures and Algorithms Username Password Forgot your password? Register Username: Password Confirm Password Email: BM40A1500 Data Structures and Algorithms Chapter 1 Algorithm Analysis Show Source | | About « 1.8. Common Misunderstandings :: Contents :: 1.10. Space Bounds » 1.9. Multiple Parameters¶ Sometimes the proper analysis for an algorithm requires multiple parameters to describe the cost. To illustrate the concept, consider an algorithm to compute the rank ordering for counts of all pixel values in a picture. Pictures are often represented by a two-dimensional array, and a pixel is one cell in the array. The value of a pixel is either the code value for the color, or a value for the intensity of the picture at that pixel. Assume that each pixel can take any integer value in the range 0 to \\(C - 1\\). The problem is to find the number of pixels of each color value and then sort the color values with respect to the number of times each value appears in the picture. Assume that the picture is a rectangle with \\(P\\) pixels. A pseudocode algorithm to solve the problem follows. for i in range(C): # Initialize count count[i] = 0 for i in range(P): # Look at all of the pixels count[value(i)] += 1 # Increment a pixel value count sort(count) # Sort pixel value counts In this example, count is an array of size C that stores the number of pixels for each color value. Function value(i) returns the color value for pixel \\(i\\). The time for the first for loop (which initializes count) is based on the number of colors, \\(C\\). The time for the second loop (which determines the number of pixels with each color) is \\(\\Theta(P)\\). The time for the final line, the call to sort, depends on the cost of the sorting algorithm used. We will assume that the sorting algorithm has cost \\(\\Theta(P \\log P)\\) if \\(P\\) items are sorted, thus yielding \\(\\Theta(P \\log P)\\) as the total algorithm cost. Is this a good representation for the cost of this algorithm? What is actually being sorted? It is not the pixels, but rather the colors. What if \\(C\\) is much smaller than \\(P\\)? Then the estimate of \\(\\Theta(P \\log P)\\) is pessimistic, because much fewer than \\(P\\) items are being sorted. Instead, we should use \\(P\\) as our analysis variable for steps that look at each pixel, and \\(C\\) as our analysis variable for steps that look at colors. Then we get \\(\\Theta(C)\\) for the initialization loop, \\(\\Theta(P)\\) for the pixel count loop, and \\(\\Theta(C \\log C)\\) for the sorting operation. This yields a total cost of \\(\\Theta(P + C \\log C)\\). Why can we not simply use the value of \\(C\\) for input size and say that the cost of the algorithm is \\(\\Theta(C \\log C)\\)? Because, \\(C\\) is typically much less than \\(P\\). For example, a picture might have 1000 \\(\\times\\) 1000 pixels and a range of 256 possible colors. So, \\(P\\) is one million, which is much larger than \\(C \\log C\\). But, if \\(P\\) is smaller, or \\(C\\) larger (even if it is still less than \\(P\\)), then \\(C \\log C\\) can become the larger quantity. Thus, neither variable should be ignored. Contact Us || Privacy | | License « 1.8. Common Misunderstandings :: Contents :: 1.10. Space Bounds » Contact Us | | Report a bug © Copyright 2011-2023 by OpenDSA Project Contributors and distributed under an MIT license. Last updated on Aug 13, 2024. Created using Sphinx 4.4.0. Summary*: Operating system*: Windows Mac OS Linux iOS Android Other Browser*: Chrome Safari Internet Explorer Opera Other Description*: Attach a screenshot (optional):"
            },
            {
                "role": "user",
                "content": "Can you summarize the key topics covered in this lecture?"
            },
            {
                "role": "assistant",
                "content": "This lecture covers foundational algorithmic concepts including problem formulation, correctness proof using induction, efficiency analysis using asymptotic notation, data structures (arrays, linked lists, BSTs, heaps), and sorting methods (selection sort, merge sort, counting sort). Additionally, it introduces the Word-RAM computational model, providing a theoretical framework for measuring algorithm efficiency."
            }
        ]
    },
    {
        "system": "Hello! I am Kiran, an AI assistant from Tampere University, dedicated to providing you with accurate and insightful assistance. My goal is to help you understand concepts, summarize lectures, and answer questions effectively.",
        "conversations": [
            {
                "role": "host",
                "content": "0.2. Problems, Algorithms, and Programs — BM40A1500 Data Structures and Algorithms Username Password Forgot your password? Register Username: Password Confirm Password Email: BM40A1500 Data Structures and Algorithms Chapter 0 Introduction for Data Structures and Algorithms Show Source | | About « 0.1. Data Structures and Algorithms :: Contents :: 0.3. Abstract Data Types » 0.2. Problems, Algorithms, and Programs¶ 0.2.1. Problems¶ Programmers commonly deal with problems, algorithms, and computer programs. These are three distinct concepts. As your intuition would suggest, a problem is a task to be performed. It is best thought of in terms of inputs and matching outputs. A problem definition should not include any constraints on how the problem is to be solved. The solution method should be developed only after the problem is precisely defined and thoroughly understood. However, a problem definition should include constraints on the resources that may be consumed by any acceptable solution. For any problem to be solved by a computer, there are always such constraints, whether stated or implied. For example, any computer program may use only the main memory and disk space available, and it must run in a “reasonable” amount of time. Problems can be viewed as functions in the mathematical sense. A function is a matching between inputs (the domain) and outputs (the range). An input to a function might be a single value or a collection of information. The values making up an input are called the parameters of the function. A specific selection of values for the parameters is called an instance of the problem. For example, the input parameter to a sorting function might be an array of integers. A particular array of integers, with a given size and specific values for each position in the array, would be an instance of the sorting problem. Different instances might generate the same output. However, any problem instance must always result in the same output every time the function is computed using that particular input. This concept of all problems behaving like mathematical functions might not match your intuition for the behavior of computer programs. You might know of programs to which you can give the same input value on two separate occasions, and two different outputs will result. For example, if you type date to a typical Linux command line prompt, you will get the current date. Naturally the date will be different on different days, even though the same command is given. However, there is obviously more to the input for the date program than the command that you type to run the program. The date program computes a function. In other words, on any particular day there can only be a single answer returned by a properly running date program on a completely specified input. For all computer programs, the output is completely determined by the program’s full set of inputs. Even a “random number generator” is completely determined by its inputs (although some random number generating systems appear to get around this by accepting a random input from a physical process beyond the user’s control). The limits to what functions can be implemented by programs is part of the domain of Computability. 0.2.2. Algorithms¶ An algorithm is a method or a process followed to solve a problem. If the problem is viewed as a function, then an algorithm is an implementation for the function that transforms an input to the corresponding output. A problem can be solved by many different algorithms. A given algorithm solves only one problem (i.e., computes a particular function). OpenDSA modules cover many problems, and for several of these problems we will see more than one algorithm. For the important problem of sorting there are over a dozen commonly known algorithms! The advantage of knowing several solutions to a problem is that solution \\(\\mathbf{A}\\) might be more efficient than solution \\(\\mathbf{B}\\) for a specific variation of the problem, or for a specific class of inputs to the problem, while solution \\(\\mathbf{B}\\) might be more efficient than \\(\\mathbf{A}\\) for another variation or class of inputs. For example, one sorting algorithm might be the best for sorting a small collection of integers (which is important if you need to do this many times). Another might be the best for sorting a large collection of integers. A third might be the best for sorting a collection of variable-length strings. By definition, something can only be called an algorithm if it has all of the following properties. It must be correct. In other words, it must compute the desired function, converting each input to the correct output. Note that every algorithm implements some function, because every algorithm maps every input to some output (even if that output is a program crash). At issue here is whether a given algorithm implements the intended function. It is composed of a series of concrete steps. Concrete means that the action described by that step is completely understood — and doable — by the person or machine that must perform the algorithm. Each step must also be doable in a finite amount of time. Thus, the algorithm gives us a “recipe” for solving the problem by performing a series of steps, where each such step is within our capacity to perform. The ability to perform a step can depend on who or what is intended to execute the recipe. For example, the steps of a cookie recipe in a cookbook might be considered sufficiently concrete for instructing a human cook, but not for programming an automated cookie-making factory. There can be no ambiguity as to which step will be performed next. Often it is the next step of the algorithm description. Selection (e.g., the if statement) is normally a part of any language for describing algorithms. Selection allows a choice for which step will be performed next, but the selection process is unambiguous at the time when the choice is made. It must be composed of a finite number of steps. If the description for the algorithm were made up of an infinite number of steps, we could never hope to write it down, nor implement it as a computer program. Most languages for describing algorithms (including English and “pseudocode”) provide some way to perform repeated actions, known as iteration. Examples of iteration in programming languages include the while and for loop constructs. Iteration allows for short descriptions, with the number of steps actually performed controlled by the input. It must terminate. In other words, it may not go into an infinite loop. 0.2.3. Programs¶ We often think of a computer program as an instance, or concrete representation, of an algorithm in some programming language. Algorithms are usually presented in terms of programs, or parts of programs. Naturally, there are many programs that are instances of the same algorithm, because any modern computer programming language can be used to implement the same collection of algorithms (although some programming languages can make life easier for the programmer). To simplify presentation, people often use the terms “algorithm” and “program” interchangeably, despite the fact that they are really separate concepts. By definition, an algorithm must provide sufficient detail that it can be converted into a program when needed. The requirement that an algorithm must terminate means that not all computer programs meet the technical definition of an algorithm. Your operating system is one such program. However, you can think of the various tasks for an operating system (each with associated inputs and outputs) as individual problems, each solved by specific algorithms implemented by a part of the operating system program, and each one of which terminates once its output is produced. 0.2.4. Summary¶ To summarize: A problem is a function or a mapping of inputs to outputs. An algorithm is a recipe for solving a problem whose steps are concrete and unambiguous. Algorithms must be correct, of finite length, and must terminate for all inputs. A program is an instantiation of an algorithm in a programming language. The following slideshow should help you to visualize the differences. Settings Saving... Server Error Resubmit Contact Us || Privacy | | License « 0.1. Data Structures and Algorithms :: Contents :: 0.3. Abstract Data Types » Contact Us | | Report a bug © Copyright 2011-2023 by OpenDSA Project Contributors and distributed under an MIT license. Last updated on Aug 13, 2024. Created using Sphinx 4.4.0. Summary*: Operating system*: Windows Mac OS Linux iOS Android Other Browser*: Chrome Safari Internet Explorer Opera Other Description*: Attach a screenshot (optional):"
            },
            {
                "role": "user",
                "content": "Can you summarize the key topics covered in this lecture?"
            },
            {
                "role": "assistant",
                "content": "This lecture covers foundational algorithmic concepts including problem formulation, correctness proof using induction, efficiency analysis using asymptotic notation, data structures (arrays, linked lists, BSTs, heaps), and sorting methods (selection sort, merge sort, counting sort). Additionally, it introduces the Word-RAM computational model, providing a theoretical framework for measuring algorithm efficiency."
            }
        ]
    },
    {
        "system": "Hello! I am Kiran, an AI assistant from Tampere University, dedicated to providing you with accurate and insightful assistance. My goal is to help you understand concepts, summarize lectures, and answer questions effectively.",
        "conversations": [
            {
                "role": "host",
                "content": "1.7. Calculating Program Running Time — BM40A1500 Data Structures and Algorithms Username Password Forgot your password? Register Username: Password Confirm Password Email: BM40A1500 Data Structures and Algorithms Chapter 1 Algorithm Analysis Show Source | | About « 1.6. Lower Bounds and \\(\\Theta\\) Notation :: Contents :: 1.8. Common Misunderstandings » 1.7. Calculating Program Running Time¶ This modules discusses the analysis for several simple code fragments. We will make use of the algorithm analysis simplifying rules: If \\(f(n)\\) is in \\(O(g(n))\\) and \\(g(n)\\) is in \\(O(h(n))\\), then \\(f(n)\\) is in \\(O(h(n))\\). If \\(f(n)\\) is in \\(O(k g(n))\\) for any constant \\(k > 0\\), then \\(f(n)\\) is in \\(O(g(n))\\). If \\(f_1(n)\\) is in \\(O(g_1(n))\\) and \\(f_2(n)\\) is in \\(O(g_2(n))\\), then \\(f_1(n) + f_2(n)\\) is in \\(O(\\max(g_1(n), g_2(n)))\\). If \\(f_1(n)\\) is in \\(O(g_1(n))\\) and \\(f_2(n)\\) is in \\(O(g_2(n))\\), then \\(f_1(n) f_2(n)\\) is in \\(O(g_1(n) g_2(n))\\). Example 1.7.1 We begin with an analysis of a simple assignment to an integer variable. a = b Because the assignment statement takes constant time, it is \\(\\Theta(1)\\). Example 1.7.2 Consider a simple for loop. sum = 0 for i in range(n): sum += n The first line is \\(\\Theta(1)\\). The for loop is repeated \\(n\\) times. The third line takes constant time so, by simplifying rule (4), the total cost for executing the two lines making up the for loop is \\(\\Theta(n)\\). By rule (3), the cost of the entire code fragment is also \\(\\Theta(n)\\). Example 1.7.3 We now analyze a code fragment with several for loops, some of which are nested. sum = 0 for j in range(n): # First for loop for i in range(j): # is a double loop sum += 1 for k in range(n): # Second for loop A[k] = k This code fragment has three separate statements: the first assignment statement and the two for loops. Again the assignment statement takes constant time; call it \\(c_1\\). The second for loop is just like the one in Example 1.7.2 and takes \\(c_2 n = \\Theta(n)\\) time. The first for loop is a double loop and requires a special technique. We work from the inside of the loop outward. The expression sum++ requires constant time; call it \\(c_3\\). Because the inner for loop is executed \\(j\\) times, by simplifying rule (4) it has cost \\(c_3j\\). The outer for loop is executed \\(n\\) times, but each time the cost of the inner loop is different because it costs \\(c_3j\\) with \\(j\\) changing each time. You should see that for the first execution of the outer loop, \\(j\\) is 1. For the second execution of the outer loop, \\(j\\) is 2. Each time through the outer loop, \\(j\\) becomes one greater, until the last time through the loop when \\(j = n\\). Thus, the total cost of the loop is \\(c_3\\) times the sum of the integers 1 through \\(n\\). We know that \\[\\sum_{i = 1}^{n} i = \\frac{n (n+1)}{2},\\] which is \\(\\Theta(n^2)\\). By simplifying rule (3), \\(\\Theta(c_1 + c_2 n + c_3 n^2)\\) is simply \\(\\Theta(n^2)\\). Example 1.7.4 Compare the asymptotic analysis for the following two code fragments. sum1 = 0 for i in range(n): # First double loop for j in range(n): # do n times sum1 += 1 sum2 = 0 for i in range(n) # Second double loop for j in range(i): # do i times sum2 += 1 In the first double loop, the inner for loop always executes \\(n\\) times. Because the outer loop executes \\(n\\) times, it should be obvious that the statement sum1++ is executed precisely \\(n^2\\) times. The second loop is similar to the one analyzed in the previous example, with cost \\(\\sum_{j = 1}^{n} j\\). This is approximately \\({1 \\over 2} n^2\\). Thus, both double loops cost \\(\\Theta(n^2)\\), though the second requires about half the time of the first. Example 1.7.5 Not all doubly nested for loops are \\(\\Theta(n^2)\\). The following pair of nested loops illustrates this fact. sum1 = 0 k = 1 while k <= n: # Do log n times for j in range(n): # do n times sum1 += 1 k *= 2 sum2 = 0 k = 1 while k <= n: # Do log n times for j in range(k): # do k times sum2 += 1 k *= 2 When analyzing these two code fragments, we will assume that \\(n\\) is a power of two. The first code fragment has its outer for loop executed \\(\\log n+1\\) times because on each iteration \\(k\\) is multiplied by two until it reaches \\(n\\). Because the inner loop always executes \\(n\\) times, the total cost for the first code fragment can be expressed as \\[\\sum_{i=0}^{\\log n} n = n \\log n.\\] So the cost of this first double loop is \\(\\Theta(n \\log n)\\). Note that a variable substitution takes place here to create the summation, with \\(k = 2^i\\). In the second code fragment, the outer loop is also executed \\(\\log n+1\\) times. The inner loop has cost \\(k\\), which doubles each time. The summation can be expressed as \\[\\sum_{i=0}^{\\log n} 2^i = \\Theta(n)\\] where \\(n\\) is assumed to be a power of two and again \\(k = 2^i\\). What about other control statements? While loops are analyzed in a manner similar to for loops. The cost of an if statement in the worst case is the greater of the costs for the then and else clauses. This is also true for the average case, assuming that the size of \\(n\\) does not affect the probability of executing one of the clauses (which is usually, but not necessarily, true). For switch statements, the worst-case cost is that of the most expensive branch. For subroutine calls, simply add the cost of executing the subroutine. There are rare situations in which the probability for executing the various branches of an if or switch statement are functions of the input size. For example, for input of size \\(n\\), the then clause of an if statement might be executed with probability \\(1/n\\). An example would be an if statement that executes the then clause only for the smallest of \\(n\\) values. To perform an average-case analysis for such programs, we cannot simply count the cost of the if statement as being the cost of the more expensive branch. In such situations, the technique of amortized analysis can come to the rescue. Determining the execution time of a recursive subroutine can be difficult. The running time for a recursive subroutine is typically best expressed by a recurrence relation. For example, the recursive factorial function calls itself with a value one less than its input value. The result of this recursive call is then multiplied by the input value, which takes constant time. Thus, the cost of the factorial function, if we wish to measure cost in terms of the number of multiplication operations, is one more than the number of multiplications made by the recursive call on the smaller input. Because the base case does no multiplications, its cost is zero. Thus, the running time for this function can be expressed as \\[T(n) = T(n-1) + 1 \\ \\mbox{for}\\ n>1;\\ \\ T(1) = 0.\\] The closed-form solution for this recurrence relation is \\(\\Theta(n)\\). Contact Us || Privacy | | License « 1.6. Lower Bounds and \\(\\Theta\\) Notation :: Contents :: 1.8. Common Misunderstandings » Contact Us | | Report a bug © Copyright 2011-2023 by OpenDSA Project Contributors and distributed under an MIT license. Last updated on Aug 13, 2024. Created using Sphinx 4.4.0. Summary*: Operating system*: Windows Mac OS Linux iOS Android Other Browser*: Chrome Safari Internet Explorer Opera Other Description*: Attach a screenshot (optional):"
            },
            {
                "role": "user",
                "content": "Can you summarize the key topics covered in this lecture?"
            },
            {
                "role": "assistant",
                "content": "This lecture covers foundational algorithmic concepts including problem formulation, correctness proof using induction, efficiency analysis using asymptotic notation, data structures (arrays, linked lists, BSTs, heaps), and sorting methods (selection sort, merge sort, counting sort). Additionally, it introduces the Word-RAM computational model, providing a theoretical framework for measuring algorithm efficiency."
            }
        ]
    },
    {
        "system": "Hello! I am Kiran, an AI assistant from Tampere University, dedicated to providing you with accurate and insightful assistance. My goal is to help you understand concepts, summarize lectures, and answer questions effectively.",
        "conversations": [
            {
                "role": "host",
                "content": "1.10. Space Bounds — BM40A1500 Data Structures and Algorithms Username Password Forgot your password? Register Username: Password Confirm Password Email: BM40A1500 Data Structures and Algorithms Chapter 1 Algorithm Analysis Show Source | | About « 1.9. Multiple Parameters :: Contents :: 2.1. The List ADT » 1.10. Space Bounds¶ Besides time, space is the other computing resource that is commonly of concern to programmers. Just as computers have become much faster over the years, they have also received greater allotments of memory. Even so, the amount of available disk space or main memory can be significant constraints for algorithm designers. The analysis techniques used to measure space requirements are similar to those used to measure time requirements. However, while time requirements are normally measured for an algorithm that manipulates a particular data structure, space requirements are normally determined for the data structure itself. The concepts of asymptotic analysis for growth rates on input size apply completely to measuring space requirements. Example 1.10.1 What are the space requirements for an array of \\(n\\) integers? If each integer requires \\(c\\) bytes, then the array requires \\(cn\\) bytes, which is \\(\\Theta(n)\\). Example 1.10.2 Imagine that we want to keep track of friendships between \\(n\\) people. We can do this with an array of size \\(n \\times n\\). Each row of the array represents the friends of an individual, with the columns indicating who has that individual as a friend. For example, if person \\(j\\) is a friend of person \\(i\\), then we place a mark in column \\(j\\) of row \\(i\\) in the array. Likewise, we should also place a mark in column \\(i\\) of row \\(j\\) if we assume that friendship works both ways. For \\(n\\) people, the total size of the array is \\(\\Theta(n^2)\\). A data structure’s primary purpose is to store data in a way that allows efficient access to those data. To provide efficient access, it may be necessary to store additional information about where the data are within the data structure. For example, each node of a linked list must store a pointer to the next value on the list. All such information stored in addition to the actual data values is referred to as overhead. Ideally, overhead should be kept to a minimum while allowing maximum access. The need to maintain a balance between these opposing goals is what makes the study of data structures so interesting. One important aspect of algorithm design is referred to as the space/time tradeoff principle. The space/time tradeoff principle says that one can often achieve a reduction in time if one is willing to sacrifice space or vice versa. Many programs can be modified to reduce storage requirements by “packing” or encoding information. “Unpacking” or decoding the information requires additional time. Thus, the resulting program uses less space but runs slower. Conversely, many programs can be modified to pre-store results or reorganize information to allow faster running time at the expense of greater storage requirements. Typically, such changes in time and space are both by a constant factor. A classic example of a space/time tradeoff is the lookup table. A lookup table pre-stores the value of a function that would otherwise be computed each time it is needed. For example, 12! is the greatest value for the factorial function that can be stored in a 32-bit int variable. If you are writing a program that often computes factorials, it is likely to be much more time efficient to simply pre-compute and store the 12 values in a table. Whenever the program needs the value of \\(n!\\) it can simply check the lookup table. (If \\(n > 12\\), the value is too large to store as an int variable anyway.) Compared to the time required to compute factorials, it may be well worth the small amount of additional space needed to store the lookup table. Lookup tables can also store approximations for an expensive function such as sine or cosine. If you compute this function only for exact degrees or are willing to approximate the answer with the value for the nearest degree, then a lookup table storing the computation for exact degrees can be used instead of repeatedly computing the sine function. Note that initially building the lookup table requires a certain amount of time. Your application must use the lookup table often enough to make this initialization worthwhile. Another example of the space/time tradeoff is typical of what a programmer might encounter when trying to optimize space. Here is a simple code fragment for sorting an array of integers. We assume that this is a special case where there are \\(n\\) integers whose values are a permutation of the integers from 0 to \\(n-1\\). This is an example of a binsort. Binsort assigns each value to an array position corresponding to its value. for i in range(len(A)): B[A[i]] = A[i] This is efficient and requires \\(\\Theta(n)\\) time. However, it also requires two arrays of size \\(n\\). Next is a code fragment that places the permutation in order but does so within the same array (thus it is an example of an “in place” sort). for i in range(len(A)): while (A[i] != i): # Swap element A[i] with A[A[i]] swap(A, i, A[i]) Function swap(A, i, j) exchanges elements i and j in array A. It may not be obvious that the second code fragment actually sorts the array. To see that this does work, notice that each pass through the for loop will at least move the integer with value \\(i\\) to its correct position in the array, and that during this iteration, the value of A[i] must be greater than or equal to \\(i\\). A total of at most \\(n\\) swap operations take place, because an integer cannot be moved out of its correct position once it has been placed there, and each swap operation places at least one integer in its correct position. Thus, this code fragment has cost \\(\\Theta(n)\\). However, it requires more time to run than the first code fragment. On my computer the second version takes nearly twice as long to run as the first, but it only requires half the space. A second principle for the relationship between a program’s space and time requirements applies to programs that process information stored on disk. Strangely enough, the disk-based space/time tradeoff principle is almost the reverse of the space/time tradeoff principle for programs using main memory. The disk-based space/time tradeoff principle states that the smaller you can make your disk storage requirements, the faster your program will run. This is because the time to read information from disk is enormous compared to computation time, so almost any amount of additional computation needed to unpack the data is going to be less than the disk-reading time saved by reducing the storage requirements. Naturally this principle does not hold true in all cases, but it is good to keep in mind when designing programs that process information stored on disk. Contact Us || Privacy | | License « 1.9. Multiple Parameters :: Contents :: 2.1. The List ADT » Contact Us | | Report a bug © Copyright 2011-2023 by OpenDSA Project Contributors and distributed under an MIT license. Last updated on Aug 13, 2024. Created using Sphinx 4.4.0. Summary*: Operating system*: Windows Mac OS Linux iOS Android Other Browser*: Chrome Safari Internet Explorer Opera Other Description*: Attach a screenshot (optional):"
            },
            {
                "role": "user",
                "content": "Can you summarize the key topics covered in this lecture?"
            },
            {
                "role": "assistant",
                "content": "This lecture covers foundational algorithmic concepts including problem formulation, correctness proof using induction, efficiency analysis using asymptotic notation, data structures (arrays, linked lists, BSTs, heaps), and sorting methods (selection sort, merge sort, counting sort). Additionally, it introduces the Word-RAM computational model, providing a theoretical framework for measuring algorithm efficiency."
            }
        ]
    },
    {
        "system": "Hello! I am Kiran, an AI assistant from Tampere University, dedicated to providing you with accurate and insightful assistance. My goal is to help you understand concepts, summarize lectures, and answer questions effectively.",
        "conversations": [
            {
                "role": "host",
                "content": "4.10. The AVL Tree — BM40A1500 Data Structures and Algorithms Username Password Forgot your password? Register Username: Password Confirm Password Email: BM40A1500 Data Structures and Algorithms Chapter 4 Recursion and Binary Trees Show Source | | About « 4.9. 2-3 Trees :: Contents :: 4.11. Heaps and Priority Queues » 4.10. The AVL Tree¶ The AVL tree (named for its inventors Adelson-Velskii and Landis) should be viewed as a BST with the following additional property: For every node, the heights of its left and right subtrees differ by at most 1. As long as the tree maintains this property, if the tree contains \\(n\\) nodes, then it has a depth of at most \\(O(\\log n)\\). As a result, search for any node will cost \\(O(\\log n)\\), and if the updates can be done in time proportional to the depth of the node inserted or deleted, then updates will also cost \\(O(\\log n)\\), even in the worst case. The key to making the AVL tree work is to alter the insert and delete routines so as to maintain the balance property. Of course, to be practical, we must be able to implement the revised update routines in \\(\\Theta(\\log n)\\) time. Figure 4.10.1: Example of an insert operation that violates the AVL tree balance property. Prior to the insert operation, all nodes of the tree are balanced (i.e., the depths of the left and right subtrees for every node differ by at most one). After inserting the node with value 5, the nodes with values 7 and 24 are no longer balanced.¶ Consider what happens when we insert a node with key value 5, as shown in Figure 4.10.1. The tree on the left meets the AVL tree balance requirements. After the insertion, two nodes no longer meet the requirements. Because the original tree met the balance requirement, nodes in the new tree can only be unbalanced by a difference of at most 2 in the subtrees. For the bottommost unbalanced node, call it \\(S\\), there are 4 cases: The extra node is in the left child of the left child of \\(S\\). The extra node is in the right child of the left child of \\(S\\). The extra node is in the left child of the right child of \\(S\\). The extra node is in the right child of the right child of \\(S\\). Cases 1 and 4 are symmetrical, as are cases 2 and 3. Note also that the unbalanced nodes must be on the path from the root to the newly inserted node. Our problem now is how to balance the tree in \\(O(\\log n)\\) time. It turns out that we can do this using a series of local operations known as rotations. Cases 1 and 4 can be fixed using a single rotation, as shown in Figure 4.10.2. Cases 2 and 3 can be fixed using a double rotation, as shown in Figure 4.10.3. Figure 4.10.2: A single rotation in an AVL tree. This operation occurs when the excess node (in subtree \\(A\\)) is in the left child of the left child of the unbalanced node labeled \\(S\\). By rearranging the nodes as shown, we preserve the BST property, as well as re-balance the tree to preserve the AVL tree balance property. The case where the excess node is in the right child of the right child of the unbalanced node is handled in the same way.¶ Figure 4.10.3: A double rotation in an AVL tree. This operation occurs when the excess node (in subtree \\(B\\)) is in the right child of the left child of the unbalanced node labeled \\(S\\). By rearranging the nodes as shown, we preserve the BST property, as well as re-balance the tree to preserve the AVL tree balance property. The case where the excess node is in the left child of the right child of \\(S\\) is handled in the same way.¶ The AVL tree insert algorithm begins with a normal BST insert. Then as the recursion unwinds up the tree, we perform the appropriate rotation on any node that is found to be unbalanced. Deletion is similar; however, consideration for unbalanced nodes must begin at the level of the deletemin operation. Example 4.10.1 In Figure 4.10.1 (b), the bottom-most unbalanced node has value 7. The excess node (with value 5) is in the right subtree of the left child of 7, so we have an example of Case 2. This requires a double rotation to fix. After the rotation, 5 becomes the left child of 24, 2 becomes the left child of 5, and 7 becomes the right child of 5. Contact Us || Privacy | | License « 4.9. 2-3 Trees :: Contents :: 4.11. Heaps and Priority Queues » Contact Us | | Report a bug © Copyright 2011-2023 by OpenDSA Project Contributors and distributed under an MIT license. Last updated on Aug 13, 2024. Created using Sphinx 4.4.0. Summary*: Operating system*: Windows Mac OS Linux iOS Android Other Browser*: Chrome Safari Internet Explorer Opera Other Description*: Attach a screenshot (optional):"
            },
            {
                "role": "user",
                "content": "Can you summarize the key topics covered in this lecture?"
            },
            {
                "role": "assistant",
                "content": "This lecture covers foundational algorithmic concepts including problem formulation, correctness proof using induction, efficiency analysis using asymptotic notation, data structures (arrays, linked lists, BSTs, heaps), and sorting methods (selection sort, merge sort, counting sort). Additionally, it introduces the Word-RAM computational model, providing a theoretical framework for measuring algorithm efficiency."
            }
        ]
    },
    {
        "system": "Hello! I am Kiran, an AI assistant from Tampere University, dedicated to providing you with accurate and insightful assistance. My goal is to help you understand concepts, summarize lectures, and answer questions effectively.",
        "conversations": [
            {
                "role": "host",
                "content": "5.5. Backtracking, and Branch and Bound — BM40A1500 Data Structures and Algorithms Username Password Forgot your password? Register Username: Password Confirm Password Email: BM40A1500 Data Structures and Algorithms Chapter 5 Algorithm Design Principles Show Source | | About « 5.4. Divide and Conquer: Quicksort :: Contents :: 5.6. Dynamic Programming » 5.5. Backtracking, and Branch and Bound¶ 5.5.1. Backtracking¶ Backtracking is a method, which allows us to systematically visit all possible combinations (solutions). It is a “brute-force” algorithm, whose implementation is typically straightforward. The method is useful when the number of combinations is so small that we have time to go through them all. In this chapter, we will first familiarize ourselves with the backtracking algorithms, which go through combinations of numbers. After this, we will see how you can use the backtracking in two problems and how to make the search for optimal solution more efficient. 5.5.2. From loops to recursion¶ Suppose we want to go through all combinations of 𝑛 numbers, where each number is an integer between \\(1...m\\). There are a total of \\(m^n\\) such combinations (there are \\(n\\) points and in each point the number can be chosen in \\(m\\) ways). For example, if \\(n=3\\) and \\(m=4\\), the combinations are \\([1,1,1], [1,1,2], [1,1,3], [1,1,4], [1,2,1], [1,2,2], [1,2,3], [1,2,4]\\), etc. If the value of \\(n\\) is known in advance, we can create \\(n\\) nested loops, each of which goes through \\(m\\) numbers. For example, the following code loops through all combinations in case \\(n=3\\)\\(n=3\\) for a in range(m): for b in range(m): for c in range(m): print(a, b, c) This is a good solution in itself, but there is one problem: the value of \\(n\\) affects on the number of loops. If we wanted to change the value of \\(n\\), we would have to change the amount of loops in the code, which is not practicable. However, we can implement a solution recursively using backtracking so that the same code works for all values of \\(n\\). 5.5.3. Implementing the search¶ The following recursive procedure search forms combinations using backtracking. This can be used as brute-force search to find the optimal solution. The parameter \\(k\\) is the position in the array where the next number is placed. If \\(k=n\\) some combination has been completed, in which case it is printed. Otherwise, the search goes through all the ways to place numbers \\(1...m\\) in position \\(k\\) and continues recursively to \\(k+1\\). The search starts with the call search(0), and numbers is an \\(n\\)-sized array in which the combination is formed. def search(k): if k == n: print(numbers) else: for i in range(1, m + 1): numbers[k] = i search(k + 1) Figure 5.5.1 shows how the search starts in case 𝑛=3 and \\(m=4\\). The sign “\\(-\\)” means a number that has not been selected yet. The first level of the search selects the combination to position \\(0\\) of the first number. There are four options for choosing this, since the possible numbers are \\(1...4\\), so the search branches into four parts. After this, the search continues recursively and selects the numbers for the other positions. Figure 5.5.1: Forming the combination \\(n=3\\) and \\(m=4\\).¶ We can evaluate the efficiency of the algorithm by calculating how many times the search procedure is called in total during the search. The procedure is called once with parameter \\(0\\), \\(m\\) times with parameter \\(1\\), \\(m^2\\) times with parameter \\(2\\), etc., so the number of invitations is total \\[1 + m + m^2 + \\dots + m^n = \\frac{m^{n+1} - 1}{m - 1} = \\Theta(m^n)\\] At the last level of recursion, more invitations are made than at all other levels combined. 5.5.4. Subsets¶ Let us then examine the situation, where we want to go through all subsets of a set with \\(n\\) elements. There are a total of \\(2n\\) subsets, because each element either belongs or does not belong to a subset. For example, \\(\\{2,5\\}\\) and :math`{3,5,9}` are subsets of the set \\(\\{2,3,5,9\\}\\). It turns out that we can form subsets by going through all combinations of \\(n\\) numbers, where each number is \\(0\\) or \\(1\\). The idea is that each number in the combination tells whether a certain element belongs to a subset. For example, when the set is \\(\\{2,3,5,9\\}\\), the combination \\([1,0,1,0]\\) matches subsets \\(\\{2,5\\}\\) and the combination \\([0,1,1,1\\)] corresponds to the subset \\(\\{3,5,9\\}\\). The following pseude code shows how we can visit all subsets using backtracking. The procedure search chooses, whether the element in \\(k\\) is included in the subset or not, and marks this information in the array choice. As before, the search starts with the call search(0). def search(k): if k == n: # process the subset else: for i in (0, 1): choice[k] = i search(k + 1) 5.5.5. Permutations¶ With backtracking, we can also loop through all permutations, i.e. different orders of elements. When there are \\(n\\) elements in the set, in total \\(n!\\) permutations can be formed. For example, \\(\\{2,4,1,3\\}\\) and \\(\\{4,3,1,2\\}\\) are permutations of the set \\(\\{1,2,3,4\\}\\). In this situation, we want to go through combinations of \\(n\\) numbers, where each number is between \\(1...n\\) and no number is repeated. We achieve this by adding a new array named included which tells if a certain number is already included. At each step, the search selects only such numbers for the combination which have not been selected before. def search(k): if k == n: # process permutation else: for i in range(1, n + 1): if not included[i]: included[i] = True numbers[k] = i search(k + 1) included[i] = False 5.5.6. The N Queens Problem¶ Next, we will go through two more demanding examples on how to utilize the backtracking to solve problems. Our task is to calculate, in how many ways \\(n\\) queens can be placed on an \\(n \\times n\\) chessboard so that no two queens threaten each other. In chess, queens can threaten each other horizontally, vertically or diagonally. For example, in the case of \\(n=4\\), there are two possible ways to place the queens as shown in Figure 5.5.2. Figure 5.5.2: Solution to the queens problem with \\(n=4\\).¶ We can solve the task by implementing an algorithm, which goes through the board from top to bottom and places one queen for each row. Figure 5.5.3 shows the operation of the search in the case \\(n=4\\). The queen of the first row can be placed in any column, but in the following lines, previous selections limit the search. The figure shows the placement of the second queen, when the first queen is in the second column. In this case, the only option is that the second queen is in the last column, because in all other cases the queens would threaten each other. Figure 5.5.3: Applying backtracking to the \\(n\\) queens problem.¶ The following function search presents the backtracking algorithm which calculates the solutions to the \\(n\\) queens problem: def search(y): if y == n: counter += 1 else: for x in range(n): if can_be_placed(y, x): location[y] = x search(y + 1) We assume that the rows and columns of the board are numbered from \\(0...n-1\\). The parameter \\(y\\) tells on which row the next queen should be placed, and the search starts with the call search(0). If the row is \\(n\\), all queens have already been placed, so one solution has been found. Otherwise, a loop that goes through the possible columns (\\(x\\)) is executed. If the queen can be placed in the column \\(x\\) i.e. it does not threaten any previously placed queen, the table location is updated accordingly (the queen \\(y\\) is in the column \\(x\\)) and the search continues recursively. The function can_be_placed examines whether a new queen can be placed on row \\(y\\) and column \\(x\\). Now we have an algorithm that allows us to review solutions to the queen problem. The algorithm solves cases with small values of \\(n\\) very fast, but with larger values of \\(n\\), the algorithm starts to take a lot of time. The reason for this is that the number of locations where the queens can be placed increases exponentially. For example, in the case of \\(n=20\\), there are already more than 39 billion different solutions. However, we can try to speed up the algorithm by improving it its implementation. One easy boost is to take advantage of symmetry. Each solution to the queen problem is matched by another solution, which is obtained by mirroring the solution horizontally. For example, in Figure 5.5.2, the solutions can be changed to each other by mirroring. Thanks to this observation, we can halve the execution time of the algorithm adding the requirement that the first queen must be placed to the left half of the board, and finally multiplying the answer by two. However, the queen problem is fundamentally a hard problem, and no essentially better solution than brute force is known. Currently, the largest case with a known solution is \\(n=27\\). Processing this case took about a year with a large computing cluster. 5.5.7. Allocating tasks¶ Let’s consider a situation where there are \\(n\\) tasks and \\(n\\) employees. Tasks should be distributed to the employees so that, each employee performs exactly one task. For each task-employee combination the cost for completing the task is known. The goal is to find a solution where the total cost is as low as possible. A table below shows an example case where \\(n=3\\). The optimal way to divide the tasks is that Employee B performs the first task, C performs second task, and A performs the third task. The total cost of this solution is \\(1+2+5=8\\). \\[\\begin{split}\\large \\begin{array}{c|c|c|c} \\text{Employee A} & 4 & 8 & \\textbf{5} \\\\ \\hline \\text{Employee B} & \\textbf{1} & 1 & 3 \\\\ \\hline \\text{Employee C} & 4 & \\textbf{2} & 6 \\\\ \\end{array}\\end{split}\\] We assume that tasks and employees are numbered from \\(0...n-1\\) and we can read from the table array cost[a][b] how much does the task \\(a\\) costs when performed by the employee \\(b\\) We can implement a backtracking algorithm which goes through the tasks in order and chooses an employee for each. The following procedure search takes two parameters: \\(k\\) is the task to be processed next and \\(h\\) is the cost so far. The search starts with the call search(0,0). The table included keeps track of which employees have already been given a task, and the variable \\(p\\) is the total cost in the best solution found so far. Before the search, the value of the variable \\(p\\) is set to \\(\\infty\\) because no solution exists yet. def search(k, h): if k == n: p = min(p, h) else: for i in range(n): if not included[i]: included[i] = True search(k + 1, h + cost[k][i]) included[i] = False This is a working backtracking algorithm, and at the end of the search, the variable \\(p\\) has the total cost of the best solution. However, the algorithm is very slow because it always goes through all the \\(n!\\) possible solutions. Since we only want to find the best solution and not go through all the solutions, we can improve the algorithm by adding a condition which stops forming a solution if it cannot get better than an earlier solution. 5.5.8. Branch And Bound¶ We next test the previous algorithm with the case, where \\(n=20\\) and the table cost contains random integers between \\(1\\) and \\(100\\). In this case, the algorithm described above would go through \\[20! = 2432902008176640000\\] different solutions which would take hundreds of years. In order to solve the case, we need to improve the algorithm so, that it does not go through all solutions but still finds the best solution. This is where a technique called branch and bound can be applied. The idea is to make the backtracking more efficient by reducing the number of solutions to be investigated with suitable upper and lower bounds. The key observation is that we can restrict the search with the help of the variable \\(p\\). This variable has at every moment the total cost of the best solution found so far. Therefore it tells the upper bound on how large the total cost of best solution can be. On the other hand, the variable \\(h\\) has the current cost of solution being formed. This is the lower bound for the total cost. If \\(h>=p\\) the solution being formed cannot get better than the earlier best. We can add to the following inspection to the beginning of the algorithm: def search(k, h): if h >= p: return ... Thanks to this, the formation of the solution ends immediately if its cost is equal to or higher than the cost of the best known solution. With this simple modification the problem (\\(n=20\\)) can be solved within minutes instead of hundreds of years. We can improve the algorithm even more by calculating a more accurate estimate for the lower bound. The cost of the solution under construction is certainly at least \\(h\\) but we can also estimate how much employees allocated for the remaining tasks add to the cost: Here the function estimate should give some estimate on how much does it cost to complete the remaining tasks \\(k...n-1\\). One simple way to get an estimate is to go through the remaining tasks and choose the cheapest employee for each task without caring whether the employee has been selected before. This gives the lower bound for the remaining costs. With this modification the original problem can be solved in seconds. This page was translated and modified from Antti Laaksonen, Tietorakenteet ja algoritmit (Chapter 8) published under Creative Commons BY-NC-SA 4.0 license. Contact Us || Privacy | | License « 5.4. Divide and Conquer: Quicksort :: Contents :: 5.6. Dynamic Programming » Contact Us | | Report a bug © Copyright 2011-2023 by OpenDSA Project Contributors and distributed under an MIT license. Last updated on Aug 13, 2024. Created using Sphinx 4.4.0. Summary*: Operating system*: Windows Mac OS Linux iOS Android Other Browser*: Chrome Safari Internet Explorer Opera Other Description*: Attach a screenshot (optional):"
            },
            {
                "role": "user",
                "content": "Can you summarize the key topics covered in this lecture?"
            },
            {
                "role": "assistant",
                "content": "This lecture covers foundational algorithmic concepts including problem formulation, correctness proof using induction, efficiency analysis using asymptotic notation, data structures (arrays, linked lists, BSTs, heaps), and sorting methods (selection sort, merge sort, counting sort). Additionally, it introduces the Word-RAM computational model, providing a theoretical framework for measuring algorithm efficiency."
            }
        ]
    },
    {
        "system": "Hello! I am Kiran, an AI assistant from Tampere University, dedicated to providing you with accurate and insightful assistance. My goal is to help you understand concepts, summarize lectures, and answer questions effectively.",
        "conversations": [
            {
                "role": "host",
                "content": "4.8. Balanced Trees — BM40A1500 Data Structures and Algorithms Username Password Forgot your password? Register Username: Password Confirm Password Email: BM40A1500 Data Structures and Algorithms Chapter 4 Recursion and Binary Trees Show Source | | About « 4.7. Dictionary Implementation Using a BST :: Contents :: 4.9. 2-3 Trees » 4.8. Balanced Trees¶ The Binary Search Tree has a serious deficiency for practical use as a search structure. That is the fact that it can easily become unbalanced, so that some nodes are deep in the tree. In fact, it is possible for a BST with \\(n\\) nodes to have a depth of \\(n\\), making it no faster to search in the worst case than a linked list. If we could keep the tree balanced in some way, then search cost would only be \\(\\Theta(\\log n)\\), a huge improvement. One solution to this problem is to adopt another search tree structure instead of using a BST at all. An example of such an alternative tree structure is the 2-3 Tree or the B-Tree. But another alternative would be to modify the BST access functions in some way to guarantee that the tree performs well. This is an appealing concept, and the concept works well for heaps, whose access functions maintain the heap in the shape of a complete binary tree. Unfortunately, the heap keeps its balanced shape at the cost of weaker restrictions on the relative values of a node and its children, making it a bad search structure. And requiring that the BST always be in the shape of a complete binary tree requires excessive modification to the tree during update, as we see in this example. Figure 4.8.1: An attempt to re-balance a BST after insertion can be expensive. (a) A BST with six nodes in the shape of a complete binary tree. (b) A node with value 1 is inserted into the BST of (a). To maintain both the complete binary tree shape and the BST property, a major reorganization of the tree is required.¶ If we are willing to weaken the balance requirements, we can come up with alternative update routines that perform well both in terms of cost for the update and in balance for the resulting tree structure. The AVL tree works in this way, using insertion and deletion routines altered from those of the BST to ensure that, for every node, the depths of the left and right subtrees differ by at most one. Contact Us || Privacy | | License « 4.7. Dictionary Implementation Using a BST :: Contents :: 4.9. 2-3 Trees » Contact Us | | Report a bug © Copyright 2011-2023 by OpenDSA Project Contributors and distributed under an MIT license. Last updated on Aug 13, 2024. Created using Sphinx 4.4.0. Summary*: Operating system*: Windows Mac OS Linux iOS Android Other Browser*: Chrome Safari Internet Explorer Opera Other Description*: Attach a screenshot (optional):"
            },
            {
                "role": "user",
                "content": "Can you summarize the key topics covered in this lecture?"
            },
            {
                "role": "assistant",
                "content": "This lecture covers foundational algorithmic concepts including problem formulation, correctness proof using induction, efficiency analysis using asymptotic notation, data structures (arrays, linked lists, BSTs, heaps), and sorting methods (selection sort, merge sort, counting sort). Additionally, it introduces the Word-RAM computational model, providing a theoretical framework for measuring algorithm efficiency."
            }
        ]
    },
    {
        "system": "Hello! I am Kiran, an AI assistant from Tampere University, dedicated to providing you with accurate and insightful assistance. My goal is to help you understand concepts, summarize lectures, and answer questions effectively.",
        "conversations": [
            {
                "role": "host",
                "content": "2.9. Searching in an Array — BM40A1500 Data Structures and Algorithms Username Password Forgot your password? Register Username: Password Confirm Password Email: BM40A1500 Data Structures and Algorithms Chapter 2 Linear Data Structures Show Source | | About « 2.8. Linked Queues :: Contents :: 3.1. Introduction » 2.9. Searching in an Array¶ 2.9.1. Sequential Search¶ If you want to find the position in an unsorted array of \\(n\\) integers that stores a particular value, you cannot really do better than simply looking through the array from the beginning and move toward the end until you find what you are looking for. This algorithm is called sequential search. If you do find it, we call this a successful search. If the value is not in the array, eventually you will reach the end. We will call this an unsuccessful search. Here is a simple implementation for sequential search. # Return the position of an element in a list. # If the element is not found, return -1. def sequentialSearch(elements, e): for i in range(len(elements)): # For each element if elements[i] == e: # if we found it return i # return this position return -1 # Otherwise, return -1 It is natural to ask how long a program or algorithm will take to run. But we do not really care exactly how long a particular program will run on a particular computer. We just want some sort of estimate that will let us compare one approach to solving a problem with another. This is the basic idea of algorithm analysis. In the case of sequential search, it is easy to see that if the value is in position \\(i\\) of the array, then sequential search will look at \\(i\\) values to find it. If the value is not in the array at all, then we must look at \\(n\\) values if the array holds \\(n\\) values. This would be called the worst case for sequential search. Since the amount of work is proportional to \\(n\\), we say that the worst case for sequential search has linear cost. For this reason, the sequential search algorithm is sometimes called linear search. 2.9.2. Binary Search¶ Sequential search is the best that we can do when trying to find a value in an unsorted array. [#]_ But if the array is sorted in increasing order by value, then we can do much better. We use a process called binary search. Binary search begins by examining the value in the middle position of the array; call this position \\(mid\\) and the corresponding value \\(k_{mid}\\). If \\(k_{mid} = K\\), then processing can stop immediately. This is unlikely to be the case, however. Fortunately, knowing the middle value provides useful information that can help guide the search process. In particular, if \\(k_{mid} > K\\), then you know that the value \\(K\\) cannot appear in the array at any position greater than \\(mid\\). Thus, you can eliminate future search in the upper half of the array. Conversely, if \\(k_{mid} < K\\), then you know that you can ignore all positions in the array less than \\(mid\\). Either way, half of the positions are eliminated from further consideration. Binary search next looks at the middle position in that part of the array where value \\(K\\) may exist. The value at this position again allows us to eliminate half of the remaining positions from consideration. This process repeats until either the desired value is found, or there are no positions remaining in the array that might contain the value \\(K\\). Here is an illustration of the binary search method. Settings Saving... Server Error Resubmit With the right math techniques, it is not too hard to show that the cost of binary search on an array of \\(n\\) values is at most \\(\\log n\\). This is because we are repeatedly splitting the size of the subarray that we must look at in half. We stop (in the worst case) when we reach a subarray of size 1. And we can only cut the value of \\(n\\) in half \\(\\log n\\) times before we reach 1. [#]_ 2.9.3. Analyzing Binary Search¶ Settings Saving... Server Error Resubmit Function binarySearch is designed to find the (single) occurrence of \\(K\\) and return its position. A special value is returned if \\(K\\) does not appear in the array. This algorithm can be modified to implement variations such as returning the position of the first occurrence of \\(K\\) in the array if multiple occurrences are allowed, and returning the position of the greatest value less than \\(K\\) when \\(K\\) is not in the array. Comparing sequential search to binary search, we see that as \\(n\\) grows, the \\(\\Theta(n)\\) running time for sequential search in the average and worst cases quickly becomes much greater than the \\(\\Theta(\\log n)\\) running time for binary search. Taken in isolation, binary search appears to be much more efficient than sequential search. This is despite the fact that the constant factor for binary search is greater than that for sequential search, because the calculation for the next search position in binary search is more expensive than just incrementing the current position, as sequential search does. Note however that the running time for sequential search will be roughly the same regardless of whether or not the array values are stored in order. In contrast, binary search requires that the array values be ordered from lowest to highest. Depending on the context in which binary search is to be used, this requirement for a sorted array could be detrimental to the running time of a complete program, because maintaining the values in sorted order requires a greater cost when inserting new elements into the array. This is an example of a tradeoff between the advantage of binary search during search and the disadvantage related to maintaining a sorted array. Only in the context of the complete problem to be solved can we know whether the advantage outweighs the disadvantage. Contact Us || Privacy | | License « 2.8. Linked Queues :: Contents :: 3.1. Introduction » Contact Us | | Report a bug © Copyright 2011-2023 by OpenDSA Project Contributors and distributed under an MIT license. Last updated on Aug 13, 2024. Created using Sphinx 4.4.0. Summary*: Operating system*: Windows Mac OS Linux iOS Android Other Browser*: Chrome Safari Internet Explorer Opera Other Description*: Attach a screenshot (optional):"
            },
            {
                "role": "user",
                "content": "Can you summarize the key topics covered in this lecture?"
            },
            {
                "role": "assistant",
                "content": "This lecture covers foundational algorithmic concepts including problem formulation, correctness proof using induction, efficiency analysis using asymptotic notation, data structures (arrays, linked lists, BSTs, heaps), and sorting methods (selection sort, merge sort, counting sort). Additionally, it introduces the Word-RAM computational model, providing a theoretical framework for measuring algorithm efficiency."
            }
        ]
    },
    {
        "system": "Hello! I am Kiran, an AI assistant from Tampere University, dedicated to providing you with accurate and insightful assistance. My goal is to help you understand concepts, summarize lectures, and answer questions effectively.",
        "conversations": [
            {
                "role": "host",
                "content": "4.4. Binary Trees — BM40A1500 Data Structures and Algorithms Username Password Forgot your password? Register Username: Password Confirm Password Email: BM40A1500 Data Structures and Algorithms Chapter 4 Recursion and Binary Trees Show Source | | About « 4.3. Tracing Recursive Code :: Contents :: 4.5. Binary Search Trees » 4.4. Binary Trees¶ 4.4.1. Introduction¶ Tree structures enable efficient access and efficient update to large collections of data. Binary trees in particular are widely used and relatively easy to implement. But binary trees are useful for many things besides searching. Just a few examples of applications that trees can speed up include prioritizing jobs, describing mathematical expressions and the syntactic elements of computer programs, or organizing the information needed to drive data compression algorithms. This chapter covers terminology used for discussing binary trees, tree traversals, approaches to implementing tree nodes, and various examples of binary trees. 4.4.2. Definitions and Properties¶ A binary tree is made up of a finite set of elements called nodes. This set either is empty or consists of a node called the root together with two binary trees, called the left and right subtrees, which are disjoint from each other and from the root. (Disjoint means that they have no nodes in common.) The roots of these subtrees are children of the root. There is an edge from a node to each of its children, and a node is said to be the parent of its children. If \\(n_1, n_2, ..., n_k\\) is a sequence of nodes in the tree such that \\(n_i\\) is the parent of \\(n_i+1\\) for \\(1 \\leq i < k\\), then this sequence is called a path from \\(n_1\\) to \\(n_k\\). The length of the path is \\(k-1\\). If there is a path from node \\(R\\) to node \\(M\\), then \\(R\\) is an ancestor of \\(M\\), and \\(M\\) is a descendant of \\(R\\). Thus, all nodes in the tree are descendants of the root of the tree, while the root is the ancestor of all nodes. The depth of a node \\(M\\) in the tree is the length of the path from the root of the tree to \\(M\\). The height of a tree is the depth of the deepest node in the tree. All nodes of depth \\(d\\) are at level \\(d\\) in the tree. The root is the only node at level 0, and its depth is 0. A leaf node is any node that has two empty children. An internal node is any node that has at least one non-empty child. Figure 4.4.1: A binary tree. Node \\(A\\) is the root. Nodes \\(B\\) and \\(C\\) are \\(A\\)’s children. Nodes \\(B\\) and \\(D\\) together form a subtree. Node \\(B\\) has two children: Its left child is the empty tree and its right child is \\(D\\). Nodes \\(A\\), \\(C\\), and \\(E\\) are ancestors of \\(G\\). Nodes \\(D\\), \\(E\\), and \\(F\\) make up level 2 of the tree; node \\(A\\) is at level 0. The edges from \\(A\\) to \\(C\\) to \\(E\\) to \\(G\\) form a path of length 3. Nodes \\(D\\), \\(G\\), \\(H\\), and \\(I\\) are leaves. Nodes \\(A\\), \\(B\\), \\(C\\), \\(E\\), and \\(F\\) are internal nodes. The depth of \\(I\\) is 3. The height of this tree is 3. Figure 4.4.2: Two different binary trees. (a) A binary tree whose root has a non-empty left child. (b) A binary tree whose root has a non-empty right child. (c) The binary tree of (a) with the missing right child made explicit. (d) The binary tree of (b) with the missing left child made explicit. Figure 4.4.1 illustrates the various terms used to identify parts of a binary tree. Figure 4.4.2 illustrates an important point regarding the structure of binary trees. Because all binary tree nodes have two children (one or both of which might be empty), the two binary trees of Figure 4.4.2 are not the same. Two restricted forms of binary tree are sufficiently important to warrant special names. Each node in a full binary tree is either (1) an internal node with exactly two non-empty children or (2) a leaf. A complete binary tree has a restricted shape obtained by starting at the root and filling the tree by levels from left to right. In a complete binary tree of height \\(d\\), all levels except possibly level \\(d\\) are completely full. The bottom level has its nodes filled in from the left side. Figure 4.4.3: Examples of full and complete binary trees. Figure 4.4.3 illustrates the differences between full and complete binary trees. 1 There is no particular relationship between these two tree shapes; that is, the tree of Figure 4.4.3 (a) is full but not complete while the tree of Figure 4.4.3 (b) is complete but not full. The heap data structure is an example of a complete binary tree. The Huffman coding tree is an example of a full binary tree. 1 While these definitions for full and complete binary tree are the ones most commonly used, they are not universal. Because the common meaning of the words “full” and “complete” are quite similar, there is little that you can do to distinguish between them other than to memorize the definitions. Here is a memory aid that you might find useful: “Complete” is a wider word than “full”, and complete binary trees tend to be wider than full binary trees because each level of a complete binary tree is as wide as possible. 4.4.3. Binary Tree as a Recursive Data Structure¶ A recursive data structure is a data structure that is partially composed of smaller or simpler instances of the same data structure. For example, linked lists and binary trees can be viewed as recursive data structures. A list is a recursive data structure because a list can be defined as either (1) an empty list or (2) a node followed by a list. A binary tree is typically defined as (1) an empty tree or (2) a node pointing to two binary trees, one its left child and the other one its right child. The recursive relationships used to define a structure provide a natural model for any recursive algorithm on the structure. Settings Saving... Server Error Resubmit Contact Us || Privacy | | License « 4.3. Tracing Recursive Code :: Contents :: 4.5. Binary Search Trees » Contact Us | | Report a bug © Copyright 2011-2023 by OpenDSA Project Contributors and distributed under an MIT license. Last updated on Aug 13, 2024. Created using Sphinx 4.4.0. Summary*: Operating system*: Windows Mac OS Linux iOS Android Other Browser*: Chrome Safari Internet Explorer Opera Other Description*: Attach a screenshot (optional):"
            },
            {
                "role": "user",
                "content": "Can you summarize the key topics covered in this lecture?"
            },
            {
                "role": "assistant",
                "content": "This lecture covers foundational algorithmic concepts including problem formulation, correctness proof using induction, efficiency analysis using asymptotic notation, data structures (arrays, linked lists, BSTs, heaps), and sorting methods (selection sort, merge sort, counting sort). Additionally, it introduces the Word-RAM computational model, providing a theoretical framework for measuring algorithm efficiency."
            }
        ]
    },
    {
        "system": "Hello! I am Kiran, an AI assistant from Tampere University, dedicated to providing you with accurate and insightful assistance. My goal is to help you understand concepts, summarize lectures, and answer questions effectively.",
        "conversations": [
            {
                "role": "host",
                "content": "4.6. Binary Tree Traversals — BM40A1500 Data Structures and Algorithms Username Password Forgot your password? Register Username: Password Confirm Password Email: BM40A1500 Data Structures and Algorithms Chapter 4 Recursion and Binary Trees Show Source | | About « 4.5. Binary Search Trees :: Contents :: 4.7. Dictionary Implementation Using a BST » 4.6. Binary Tree Traversals¶ Often we wish to process a binary tree by “visiting” each of its nodes, each time performing a specific action such as printing the contents of the node. Any process for visiting all of the nodes in some order is called a traversal. Any traversal that lists every node in the tree exactly once is called an enumeration of the tree’s nodes. Some applications do not require that the nodes be visited in any particular order as long as each node is visited precisely once. For other applications, nodes must be visited in an order that preserves some relationship. 4.6.1. Preorder Traversal¶ For example, we might wish to make sure that we visit any given node before we visit its children. This is called a preorder traversal. Figure 4.6.1: A binary tree for traversal examples. Example 4.6.1 The preorder enumeration for the tree of Figure 4.6.1 is A B D C E G F H I. The first node printed is the root. Then all nodes of the left subtree are printed (in preorder) before any node of the right subtree. Settings Saving... Server Error Resubmit 4.6.2. Postorder Traversal¶ Alternatively, we might wish to visit each node only after we visit its children (and their subtrees). For example, this would be necessary if we wish to return all nodes in the tree to free store. We would like to delete the children of a node before deleting the node itself. But to do that requires that the children’s children be deleted first, and so on. This is called a postorder traversal. Example 4.6.2 The postorder enumeration for the tree of Figure 4.6.1 is D B G E H I F C A. Settings Saving... Server Error Resubmit 4.6.3. Inorder Traversal¶ An inorder traversal first visits the left child (including its entire subtree), then visits the node, and finally visits the right child (including its entire subtree). The binary search tree makes use of this traversal to print all nodes in ascending order of value. Example 4.6.3 The inorder enumeration for the tree of Figure 4.6.1 is B D A G E C H F I. Settings Saving... Server Error Resubmit Contact Us || Privacy | | License « 4.5. Binary Search Trees :: Contents :: 4.7. Dictionary Implementation Using a BST » Contact Us | | Report a bug © Copyright 2011-2023 by OpenDSA Project Contributors and distributed under an MIT license. Last updated on Aug 13, 2024. Created using Sphinx 4.4.0. Summary*: Operating system*: Windows Mac OS Linux iOS Android Other Browser*: Chrome Safari Internet Explorer Opera Other Description*: Attach a screenshot (optional):"
            },
            {
                "role": "user",
                "content": "Can you summarize the key topics covered in this lecture?"
            },
            {
                "role": "assistant",
                "content": "This lecture covers foundational algorithmic concepts including problem formulation, correctness proof using induction, efficiency analysis using asymptotic notation, data structures (arrays, linked lists, BSTs, heaps), and sorting methods (selection sort, merge sort, counting sort). Additionally, it introduces the Word-RAM computational model, providing a theoretical framework for measuring algorithm efficiency."
            }
        ]
    },
    {
        "system": "Hello! I am Kiran, an AI assistant from Tampere University, dedicated to providing you with accurate and insightful assistance. My goal is to help you understand concepts, summarize lectures, and answer questions effectively.",
        "conversations": [
            {
                "role": "host",
                "content": "4.5. Binary Search Trees — BM40A1500 Data Structures and Algorithms Username Password Forgot your password? Register Username: Password Confirm Password Email: BM40A1500 Data Structures and Algorithms Chapter 4 Recursion and Binary Trees Show Source | | About « 4.4. Binary Trees :: Contents :: 4.6. Binary Tree Traversals » 4.5. Binary Search Trees¶ 4.5.1. Binary Search Tree Definition¶ A binary search tree (BST) is a binary tree that conforms to the following condition, known as the binary search tree property. All nodes stored in the left subtree of a node whose key value is \\(K\\) have key values less than or equal to \\(K\\). All nodes stored in the right subtree of a node whose key value is \\(K\\) have key values greater than \\(K\\). Figure 4.5.1 shows two BSTs for a collection of values. One consequence of the binary search tree property is that if the BST nodes are printed using an inorder traversal, then the resulting enumeration will be in sorted order from lowest to highest. Figure 4.5.1: Two Binary Search Trees for a collection of values. Tree (a) results if values are inserted in the order 37, 24, 42, 7, 2, 40, 42, 32, 120. Tree (b) results if the same values are inserted in the order 120, 42, 42, 7, 2, 32, 37, 24, 40. Here is a class declaration template for the BST: class Node: def __init__(self, key): self.key = key self.left = self.right = None class BST: def __init__(self): self.root = None # Inserts a new key to the BST def insert(self, key): def __inserthelp(self, node, key): # Check if a key is in the BST def find(self, key): def __findhelp(self, node, key): # Removes a key from the BST def remove(self, key): def __removehelp(self, node, key): # Finds/removes the greatest key value from the BST def __getmax(self, node): def __removemax(self, node): 4.5.2. BST Search¶ The first operation that we will look at in detail will find the record that matches a given key. Notice that in the BST class, public member function find calls private member function __findhelp. Method find takes the search key as an explicit parameter and its BST as an implicit parameter, and returns the record that matches the key. However, the find operation is most easily implemented as a recursive function whose parameters are the root of a subtree and the search key. Member __findhelp has the desired form for this recursive subroutine and is implemented as follows. Settings Saving... Server Error Resubmit 4.5.3. BST Insert¶ Now we look at how to insert a new node into the BST. Settings Saving... Server Error Resubmit Note that, except for the last node in the path, __inserthelp will not actually change the child pointer for any of the nodes that are visited. In that sense, many of the assignments seem redundant. However, the cost of these additional assignments is worth paying to keep the insertion process simple. The alternative is to check if a given assignment is necessary, which is probably more expensive than the assignment! We have to decide what to do when the node that we want to insert has a key value equal to the key of some node already in the tree. If during insert we find a node that duplicates the key value to be inserted, then we have two options. If the application does not allow nodes with equal keys, then this insertion should be treated as an error (or ignored). If duplicate keys are allowed, our convention will be to insert the duplicate in the left subtree. In this implementation, the BST does not allow duplicate keys. In this implementation, the BST does not allow duplicate keys. The shape of a BST depends on the order in which elements are inserted. A new element is added to the BST as a new leaf node, potentially increasing the depth of the tree. Figure 4.5.1 illustrates two BSTs for a collection of values. It is possible for the BST containing \\(n\\) nodes to be a chain of nodes with height \\(n\\). This would happen if, for example, all elements were inserted in sorted order. In general, it is preferable for a BST to be as shallow as possible. This keeps the average cost of a BST operation low. 4.5.4. BST Remove¶ Removing a node from a BST is a bit trickier than inserting a node, but it is not complicated if all of the possible cases are considered individually. Before tackling the general node removal process, we will first see how to remove from a given subtree the node with the largest key value. This routine will be used later by the general node removal function. Settings Saving... Server Error Resubmit The return value of the __deletemax method is the subtree of the current node with the maximum-valued node in the subtree removed. Similar to the __inserthelp method, each node on the path back to the root has its right child node reassigned to the subtree resulting from its call to the __deletemax method. A useful companion method is __getmax which returns the maximum value in the subtree. def __getmax(self, node): if node.right == None: return node.key else: return self.__getmax(node.right) Now we are ready for the __removehelp method. Removing a node with given key value \\(R\\) from the BST requires that we first find \\(R\\) and then remove it from the tree. So, the first part of the remove operation is a search to find \\(R\\). Once \\(R\\) is found, there are several possibilities. If \\(R\\) has no children, then \\(R\\)’s parent has its pointer set to NULL. If \\(R\\) has one child, then \\(R\\)’s parent has its pointer set to \\(R\\)’s child (similar to __deletemax). The problem comes if \\(R\\) has two children. One simple approach, though expensive, is to set \\(R\\)’s parent to point to one of \\(R\\)’s subtrees, and then reinsert the remaining subtree’s nodes one at a time. A better alternative is to find a value in one of the subtrees that can replace the value in \\(R\\). Thus, the question becomes: Which value can substitute for the one being removed? It cannot be any arbitrary value, because we must preserve the BST property without making major changes to the structure of the tree. Which value is most like the one being removed? The answer is the least key value greater than the one being removed, or else the greatest key value less than (or equal to) the one being removed. If either of these values replace the one being removed, then the BST property is maintained. Settings Saving... Server Error Resubmit When duplicate node values do not appear in the tree, it makes no difference whether the replacement is the greatest value from the left subtree or the least value from the right subtree. If duplicates are stored in the left subtree, then we must select the replacement from the left subtree. 1 To see why, call the least value in the right subtree \\(L\\). If multiple nodes in the right subtree have value \\(L\\), selecting \\(L\\) as the replacement value for the root of the subtree will result in a tree with equal values to the right of the node now containing \\(L\\). Selecting the greatest value from the left subtree does not have a similar problem, because it does not violate the Binary Search Tree Property if equal values appear in the left subtree. 1 Alternatively, if we prefer to store duplicate values in the right subtree, then we must replace a deleted node with the least value from its right subtree. 4.5.5. BST Analysis¶ The cost for __findhelp and __inserthelp is the depth of the node found or inserted. The cost for __removehelp is the depth of the node being removed, or in the case when this node has two children, the depth of the node with smallest value in its right subtree. Thus, in the worst case, the cost for any one of these operations is the depth of the deepest node in the tree. This is why it is desirable to keep BSTs balanced, that is, with least possible height. If a binary tree is balanced, then the height for a tree of \\(n\\) nodes is approximately \\(\\log n\\). However, if the tree is completely unbalanced, for example in the shape of a linked list, then the height for a tree with \\(n\\) nodes can be as great as \\(n\\). Thus, a balanced BST will in the average case have operations costing \\(\\Theta(\\log n)\\), while a badly unbalanced BST can have operations in the worst case costing \\(\\Theta(n)\\). Consider the situation where we construct a BST of \\(n\\) nodes by inserting records one at a time. If we are fortunate to have them arrive in an order that results in a balanced tree (a “random” order is likely to be good enough for this purpose), then each insertion will cost on average \\(\\Theta(\\log n)\\), for a total cost of \\(\\Theta(n \\log n)\\). However, if the records are inserted in order of increasing value, then the resulting tree will be a chain of height \\(n\\). The cost of insertion in this case will be \\(\\sum_{i=1}^{n} i = \\Theta(n^2)\\). Traversing a BST costs \\(\\Theta(n)\\) regardless of the shape of the tree. Each node is visited exactly once, and each child pointer is followed exactly once. While the BST is simple to implement and efficient when the tree is balanced, the possibility of its being unbalanced is a serious liability. There are techniques for organizing a BST to guarantee good performance. Two examples are the AVL tree and the splay tree. There also exist other types of search trees that are guaranteed to remain balanced, such as the 2-3 Tree. Contact Us || Privacy | | License « 4.4. Binary Trees :: Contents :: 4.6. Binary Tree Traversals » Contact Us | | Report a bug © Copyright 2011-2023 by OpenDSA Project Contributors and distributed under an MIT license. Last updated on Aug 13, 2024. Created using Sphinx 4.4.0. Summary*: Operating system*: Windows Mac OS Linux iOS Android Other Browser*: Chrome Safari Internet Explorer Opera Other Description*: Attach a screenshot (optional):"
            },
            {
                "role": "user",
                "content": "Can you summarize the key topics covered in this lecture?"
            },
            {
                "role": "assistant",
                "content": "This lecture covers foundational algorithmic concepts including problem formulation, correctness proof using induction, efficiency analysis using asymptotic notation, data structures (arrays, linked lists, BSTs, heaps), and sorting methods (selection sort, merge sort, counting sort). Additionally, it introduces the Word-RAM computational model, providing a theoretical framework for measuring algorithm efficiency."
            }
        ]
    },
    {
        "system": "Hello! I am Kiran, an AI assistant from Tampere University, dedicated to providing you with accurate and insightful assistance. My goal is to help you understand concepts, summarize lectures, and answer questions effectively.",
        "conversations": [
            {
                "role": "host",
                "content": "4.7. Dictionary Implementation Using a BST — BM40A1500 Data Structures and Algorithms Username Password Forgot your password? Register Username: Password Confirm Password Email: BM40A1500 Data Structures and Algorithms Chapter 4 Recursion and Binary Trees Show Source | | About « 4.6. Binary Tree Traversals :: Contents :: 4.8. Balanced Trees » 4.7. Dictionary Implementation Using a BST¶ A simple implementation for the Dictionary ADT can be based on sorted or unsorted lists. When implementing the dictionary with an unsorted list, inserting a new record into the dictionary can be performed quickly by putting it at the end of the list. However, searching an unsorted list for a particular record requires \\(\\Theta(n)\\) time in the average case. For a large database, this is probably much too slow. Alternatively, the records can be stored in a sorted list. If the list is implemented using a linked list, then no speedup to the search operation will result from storing the records in sorted order. On the other hand, if we use a sorted array-based list to implement the dictionary, then binary search can be used to find a record in only \\(\\Theta(\\log n)\\) time. However, insertion will now require \\(\\Theta(n)\\) time on average because, once the proper location for the new record in the sorted list has been found, many records might be shifted to make room for the new record. Is there some way to organize a collection of records so that inserting records and searching for records can both be done quickly? We can do this with a binary search tree (BST). The advantage of using the BST is that all major operations (insert, search, and remove) are \\(\\Theta(\\log n)\\) in the average case. Of course, if the tree is badly balanced, then the cost can be as bad as \\(\\Theta(n)\\). Contact Us || Privacy | | License « 4.6. Binary Tree Traversals :: Contents :: 4.8. Balanced Trees » Contact Us | | Report a bug © Copyright 2011-2023 by OpenDSA Project Contributors and distributed under an MIT license. Last updated on Aug 13, 2024. Created using Sphinx 4.4.0. Summary*: Operating system*: Windows Mac OS Linux iOS Android Other Browser*: Chrome Safari Internet Explorer Opera Other Description*: Attach a screenshot (optional):"
            },
            {
                "role": "user",
                "content": "Can you summarize the key topics covered in this lecture?"
            },
            {
                "role": "assistant",
                "content": "This lecture covers foundational algorithmic concepts including problem formulation, correctness proof using induction, efficiency analysis using asymptotic notation, data structures (arrays, linked lists, BSTs, heaps), and sorting methods (selection sort, merge sort, counting sort). Additionally, it introduces the Word-RAM computational model, providing a theoretical framework for measuring algorithm efficiency."
            }
        ]
    },
    {
        "system": "Hello! I am Kiran, an AI assistant from Tampere University, dedicated to providing you with accurate and insightful assistance. My goal is to help you understand concepts, summarize lectures, and answer questions effectively.",
        "conversations": [
            {
                "role": "host",
                "content": "3.5. Bucket Hashing — BM40A1500 Data Structures and Algorithms Username Password Forgot your password? Register Username: Password Confirm Password Email: BM40A1500 Data Structures and Algorithms Chapter 3 Hashing Show Source | | About « 3.4. Open Hashing :: Contents :: 3.6. Collision Resolution » 3.5. Bucket Hashing¶ 3.5.1. Bucket Hashing¶ Closed hashing stores all records directly in the hash table. Each record \\(R\\) with key value \\(k_R\\) has a home position that is \\(\\textbf{h}(k_R)\\), the slot computed by the hash function. If \\(R\\) is to be inserted and another record already occupies \\(R\\)’s home position, then \\(R\\) will be stored at some other slot in the table. It is the business of the collision resolution policy to determine which slot that will be. Naturally, the same policy must be followed during search as during insertion, so that any record not found in its home position can be recovered by repeating the collision resolution process. One implementation for closed hashing groups hash table slots into buckets. The \\(M\\) slots of the hash table are divided into \\(B\\) buckets, with each bucket consisting of \\(M/B\\) slots. The hash function assigns each record to the first slot within one of the buckets. If this slot is already occupied, then the bucket slots are searched sequentially until an open slot is found. If a bucket is entirely full, then the record is stored in an overflow bucket of infinite capacity at the end of the table. All buckets share the same overflow bucket. A good implementation will use a hash function that distributes the records evenly among the buckets so that as few records as possible go into the overflow bucket. When searching for a record, the first step is to hash the key to determine which bucket should contain the record. The records in this bucket are then searched. If the desired key value is not found and the bucket still has free slots, then the search is complete. If the bucket is full, then it is possible that the desired record is stored in the overflow bucket. In this case, the overflow bucket must be searched until the record is found or all records in the overflow bucket have been checked. If many records are in the overflow bucket, this will be an expensive process. Settings Saving... Server Error Resubmit 3.5.2. An Alternate Approach¶ A simple variation on bucket hashing is to hash a key value to some slot in the hash table as though bucketing were not being used. If the home position is full, then we search through the rest of the bucket to find an empty slot. If all slots in this bucket are full, then the record is assigned to the overflow bucket. The advantage of this approach is that initial collisions are reduced, because any slot can be a home position rather than just the first slot in the bucket. Settings Saving... Server Error Resubmit Bucket methods are good for implementing hash tables stored on disk, because the bucket size can be set to the size of a disk block. Whenever search or insertion occurs, the entire bucket is read into memory. Because the entire bucket is then in memory, processing an insert or search operation requires only one disk access, unless the bucket is full. If the bucket is full, then the overflow bucket must be retrieved from disk as well. Naturally, overflow should be kept small to minimize unnecessary disk accesses. Contact Us || Privacy | | License « 3.4. Open Hashing :: Contents :: 3.6. Collision Resolution » Contact Us | | Report a bug © Copyright 2011-2023 by OpenDSA Project Contributors and distributed under an MIT license. Last updated on Aug 13, 2024. Created using Sphinx 4.4.0. Summary*: Operating system*: Windows Mac OS Linux iOS Android Other Browser*: Chrome Safari Internet Explorer Opera Other Description*: Attach a screenshot (optional):"
            },
            {
                "role": "user",
                "content": "Can you summarize the key topics covered in this lecture?"
            },
            {
                "role": "assistant",
                "content": "This lecture covers foundational algorithmic concepts including problem formulation, correctness proof using induction, efficiency analysis using asymptotic notation, data structures (arrays, linked lists, BSTs, heaps), and sorting methods (selection sort, merge sort, counting sort). Additionally, it introduces the Word-RAM computational model, providing a theoretical framework for measuring algorithm efficiency."
            }
        ]
    },
    {
        "system": "Hello! I am Kiran, an AI assistant from Tampere University, dedicated to providing you with accurate and insightful assistance. My goal is to help you understand concepts, summarize lectures, and answer questions effectively.",
        "conversations": [
            {
                "role": "host",
                "content": "5.6. Dynamic Programming — BM40A1500 Data Structures and Algorithms Username Password Forgot your password? Register Username: Password Confirm Password Email: BM40A1500 Data Structures and Algorithms Chapter 5 Algorithm Design Principles Show Source | | About « 5.5. Backtracking, and Branch and Bound :: Contents :: 5.7. Introduction to Probabilistic Algorithms » 5.6. Dynamic Programming¶ 5.6.1. Dynamic Programming¶ Dynamic programming is an algorithm design technique that can improve the efficiency of any inherently recursive algorithm that repeatedly re-solves the same subproblems. Using dynamic programming requires two steps: You find a recursive solution to a problem where subproblems are redundantly solved many times. Optimize the recursive algorithm to eliminate re-solving subproblems. The resulting algorithm may be recursive or iterative. The iterative form is commonly referred to by the term dynamic programming. We will see first how to remove redundancy with a simple problem, computing Fibonacci numbers. Then we introduce the knapsack problem, and show how it can be solved efficiently with dynamic programming. 5.6.2. Computing Fibonacci Numbers¶ Consider the recursive function for computing the \\(n\\)’th Fibonacci number. # Recursively generates and returns the n'th Fibonacci number def fibr(n): if n <= 0: return -1 if n == 1 or n == 2: # Base case return 1 return fibr(n-1) + fibr(n-2) # Recursive call The cost of this recursive algorithm (in terms of function calls) is the size of the \\(n\\)’th Fibonacci number itself, which is exponential on \\(n\\) (approximately \\(1.62^n\\) ). Why is this so expensive? Primarily because two recursive calls are made by the function, and the work that they do is largely redundant. That is, each of the two calls is recomputing most of the series, as is each sub-call, and so on. Thus, the smaller values of the function are being recomputed a huge number of times. If we could eliminate this redundancy, the cost would be greatly reduced. The approach that we will use can also improve any algorithm that spends most of its time recomputing common subproblems. The following slideshow explains the redundancy problem. Settings Saving... Server Error Resubmit Looking at the final tree, we see that there are only seven unique subproblems to solve (for Fibonacci values 0 through 6). The graphical representation below is called a dependency graph, and shows the dependencies for the subproblems. Note that the dependency graph was laid out on in a one dimensional table of size seven, corresponding to the unique subproblems invoked by the algorithm. This table can simply store the value of each subproblem. In this way, redundant calls can be avoided because the value of a subproblem which was previously computed can be read from its corresponding cell in the table without the need to recompute it again. The table can be used to derive two alternative, but efficient, algorithms. One way to accomplish this goal is to keep a table of values, and first check the table to see if the computation can be avoided. This technique is called memoization. Here is a straightforward example of doing so. Note that it mirrors the original version of the Fibonacci recursive algorithm. def fibrt(n): # Assume Values has at least n slots, # and all slots are initialized to 0 if n <= 0: return -1 if n <= 2: # Base case return 1 if Values[n] == 0: Values[n] = fibrt(n-1) + fibrt(n-2) # Recursive call return Values[n] This version of the algorithm will not compute a value more than once, so its cost is linear. The corresponding recursion tree is shown below. Note that the first occurrence of each recursive call invokes two recursive calls. However, subsequent occurrences of such a call do not produce additional calls because they just read the contents of its corresponding cell. A second technique is called tabulation. The dependency graph must be analyzed to infer an alternative computation order for the subproblems. The only restriction is that a subproblem can only be computed when the subproblems it depends on have been computed. In addition, the value of each subproblem must be stored in the table. In the case of computing a value in the Fibonacci series, we reverse the order to calculate the series from the starting point, and implement this by a simple loop. Unfortunately, since it does not have any similarity to the original recursive algorithm, there is no mechanical way to get from the original recursive form to the dynamic programming form. An additional optimization can be made. Of course, we didn’t actually need to use a table storing all of the values, since future computations do not need access to all prior subproblems. Instead, we could build the value by working from 0 and 1 up to \\(n\\) rather than backwards from \\(n\\) down to 0 and 1. Going up from the bottom we only need to store the previous two values of the function, as is done by our iterative version. def fibi(n): if n <= 0: return -1 curr, prev, past = 1, 1, 0 # curr holds current Fib value for i in range(3, n+1): # Compute next value past = prev # past holds fibi(i-2) prev = curr # prev holfd fibi(i-1) curr = past + prev # curr now holds fibi(i) return curr Recomputing of subproblems comes up in many algorithms. It is not so common that we can store only a few prior results as we did for fibi. Thus, there are many times where storing a complete table of subresults will be useful. The approach shown above to designing an algorithm that works by storing a table of results for subproblems is called dynamic programming when it is applied to optimization algorithms. The name is somewhat arcane, because it doesn’t bear much obvious similarity to the process that is taking place when storing subproblems in a table. However, it comes originally from the field of dynamic control systems, which got its start before what we think of as computer programming. The act of storing precomputed values in a table for later reuse is referred to as “programming” in that field. Dynamic programming algorithms are usually implemented with the tabulation technique described above. Thus, fibi better represents the most common form of dynamic programming than does fibrt, even though it doesn’t use the complete table. 5.6.3. The Knapsack Problem¶ We will next consider a problem that appears with many variations in a variety of commercial settings. Many businesses need to package items with the greatest efficiency. One way to describe this basic idea is in terms of packing items into a knapsack, and so we will refer to this as the Knapsack Problem. We will first define a particular formulation of the knapsack problem, and then we will discuss an algorithm to solve it based on dynamic programming. There are many other versions for the problem. Some versions ask for the greatest amount that will fit, others introduce values to the items along with size. We will look at a fairly easy to understand variation. Assume that we have a knapsack with a certain amount of space that we will define using integer value \\(K\\). We also have \\(n\\) items each with a certain size such that that item \\(i\\) has integer size \\(k_i\\). The problem is to find a subset of the \\(n\\) items whose sizes exactly sum to \\(K\\), if one exists. For example, if our knapsack has capacity \\(K = 5\\) and the two items are of size \\(k_1 = 2\\) and \\(k_2 = 4\\), then no such subset exists. But if we add a third item of size \\(k_3 = 1\\), then we can fill the knapsack exactly with the second and third items. We can define the problem more formally as: Find \\(S \\subset \\{1, 2, ..., n\\}\\) such that \\[\\sum_{i \\in S} k_i = K.\\] Example 5.6.1 Assume that we are given a knapsack of size \\(K = 163\\) and 10 items of sizes 4, 9, 15, 19, 27, 44, 54, 68, 73, 101. Can we find a subset of the items that exactly fills the knapsack? You should take a few minutes and try to do this before reading on and looking at the answer. One solution to the problem is: 19, 27, 44, 73. Example 5.6.2 Having solved the previous example for knapsack of size 163, how hard is it now to solve for a knapsack of size 164? Try it. Unfortunately, knowing the answer for 163 is of almost no use at all when solving for 164. One solution is: 9, 54, 101. If you tried solving these examples, you probably found yourself doing a lot of trial-and-error and a lot of backtracking. To come up with an algorithm, we want an organized way to go through the possible subsets. Is there a way to make the problem smaller, so that we can apply recursion? We essentially have two parts to the input: The knapsack size \\(K\\) and the \\(n\\) items. It probably will not do us much good to try and break the knapsack into pieces and solve the sub-pieces (since we already saw that knowing the answer for a knapsack of size 163 did nothing to help us solve the problem for a knapsack of size 164). So, what can we say about solving the problem with or without the \\(n\\)’th item? This seems to lead to a way to break down the problem. If the \\(n\\)’th item is not needed for a solution (that is, if we can solve the problem with the first \\(n-1\\) items) then we can also solve the problem when the \\(n\\)’th item is available (we just ignore it). On the other hand, if we do include the \\(n\\)’th item as a member of the solution subset, then we now would need to solve the problem with the first \\(n-1\\) items and a knapsack of size \\(K - k_n\\) (since the \\(n\\)’th item is taking up \\(k_n\\) space in the knapsack). To organize this process, we can define the problem in terms of two parameters: the knapsack size \\(K\\) and the number of items \\(n\\). Denote a given instance of the problem as \\(P(n, K)\\). Now we can say that \\(P(n, K)\\) has a solution if and only if there exists a solution for either \\(P(n-1, K)\\) or \\(P(n-1, K-k_n)\\). That is, we can solve \\(P(n, K)\\) only if we can solve one of the sub problems where we use or do not use the \\(n\\) th item. Of course, the ordering of the items is arbitrary. We just need to give them some order to keep things straight. Continuing this idea, to solve any subproblem of size \\(n-1\\), we need only to solve two subproblems of size \\(n-2\\). And so on, until we are down to only one item that either fills the knapsack or not. Continuing this idea, to solve any subproblem of size \\(n-1\\), we need only to solve two subproblems of size \\(n-2\\). And so on, until we are down to only one item that either fits the knapsack or not. Assuming that \\(P(i, S)\\) represents the problem for object i and after, and with size s still free in the knapsack, the following algorithm expresses the ideas. if \\(P(n-1, K)\\) has a solution, then \\(P(n, K)\\) has a solution else if \\(P(n-1, K-k_n)\\) has a solution then \\(P(n, K)\\) has a solution else \\(P(n, K)\\) has no solution. Although this algorithm is correct, it naturally leads to a cost expressed by the recurrence relation \\(\\mathbf{T}(n) = 2\\mathbf{T}(n-1) + c = \\Theta(2^n)\\). That can be pretty expensive! But… we should quickly realize that there are only \\(n(K+1)\\) subproblems to solve! Clearly, there is the possibility that many subproblems are being solved repeatedly. This is a natural opportunity to apply dynamic programming. If we draw the recursion tree of this naive recursive algorithm and derive its corresponding dependency graph, we notice that all the recursive calls can be laid out on an array of size \\(n \\times K+1\\) to contain the solutions for all subproblems \\(P(i, k), 0 \\leq i \\leq n-1, 0 \\leq k \\leq K\\). Example 5.6.3 Consider the instance of the Knapsack Problem for \\(K=10\\) and five items with sizes 9, 2, 7, 4, 1. The recursion tree generated by the recursive algorithm follows, where each node contains the index of the object under consideration (from 0 to 4) and the size available of the knapsack. Settings Saving... Server Error Resubmit The dependency graph for this problem instance, laid out in a table of size \\(n × K + 1\\), follows: As mentioned above, there are two approaches to actually solving the problem. One is memoization, that is, to start with our problem of size \\(P(n, K)\\) and make recursive calls to solve the subproblems, each time checking the array to see if a subproblem has been solved, and filling in the corresponding cell in the array whenever we get a new subproblem solution. The other is tabulation. Conceivably we could adopt one of several computation orders, although the most “natural” is to start filling the array for row 0 (which indicates a successful solution only for a knapsack of size \\(k_0\\)). We then fill in the succeeding rows from \\(i=1\\) to \\(n\\). def knapsack(items, K): table = [[\"\"] * (K + 1) for _ in range(len(items))] table[0][0] = \"O\" # OMIT table[0][items[0]] = \"I\" # INCLUDE for i in range(1, len(items)): table[i][0] = \"O\" for k in range(1, K + 1): if k >= items[i] and table[i - 1][k - items[i]] != \"\": table[i][k] += \"I\" if table[i - 1][k] != \"\": table[i][k] += \"O\" return table In other words, a new slot in the array gets its solution by looking at most at two slots in the preceding row. Since filling each slot in the array takes constant time, the total cost of the algorithm is \\(\\Theta(nK)\\). Example 5.6.4 Consider again the instance of the Knapsack Problem for K=10 and five items with sizes 9, 2, 7, 4, 1. A tabulation algorithm will fill a table of size n×K+1 starting from object i=0 up to object i=4, filling all the cells in the table in a top-down fashion. \\[\\begin{split}\\begin{array}{l|ccccccccccc} &0&1&2&3&4&5&6&7&8&9&10\\\\ \\hline k_0\\!=\\!9&O&-&-&-&-&-&-&-&-&I&\\\\ k_1\\!=\\!2&O&-&I&-&-&-&-&-&-&O&-\\\\ k_2\\!=\\!7&O&-&O&-&-&-&-&I&-&I/O&-\\\\ k_3\\!=\\!4&O&-&O&-&I&-&I&O&-&O&-\\\\ k_4\\!=\\!1&O&I&O&I&O&I&O&I/O&I&O&I \\end{array}\\end{split}\\] Key: -: No solution for \\(P(i, k)\\). O: Solution(s) for \\(P(i, k)\\) with \\(i\\) omitted. I: Solution(s) for \\(P(i, k)\\) with \\(i\\) included. I/O: Solutions for \\(P(i, k)\\) with \\(i\\) included AND omitted. For example, \\(P(2, 9)\\) stores value I/O. It contains O because \\(P(1, 9)\\) has a solution (so, this item is not needed along that path). It contains I because \\(P(1,2) = P(1, 9-7)\\) has a solution (so, this item is needed along that path). Since \\(P(4, 10)\\) is marked with I, it has a solution. We can determine what that solution actually is by recognizing that it includes \\(k_4\\) (of size 1), which then leads us to look at the solution for \\(P(3, 9)\\). This in turn has a solution that omits \\(k_3\\) (of size 4), leading us to \\(P(2, 9)\\). At this point, we can either use item \\(k_2\\) or not. We can find a solution by taking one valid path through the table. We can find all solutions by following all branches when there is a choice. Note that the table is first filled with the values of the different subproblems, and later we inferred the sequence of decisions that allows computing an optimal solution from the values stored in the table. This last phase of the algorithm precludes the possibility of actually reducing the size of the table. Otherwise, the table for the knapsack problem could have been reduced to a one dimensional array. 5.6.4. Chained Matrix Multiplication¶ Many engineering problems require multiplying a lot of matrices. Sometimes really large matrices. It turns out to make a big difference in which order we do the computation. First, let’s recall the basics. If we have two matrices (on of \\(r\\) rows and \\(s\\) columns, and the other of \\(s\\) rows and \\(t\\) columns), then the result will be a matrix of \\(r\\) rows and \\(t\\) columns. What we really care about is that the cost of the matrix multiplication is dominated by the number of terms that have to be multipled together. Here, it would be a total cost of \\(r \\times s \\times t\\) multiplications (plus some additions that we will ignore since the time is dominated by the multiplications). The other thing to realize is this: Of course it matters whether we multiply \\(A \\times B\\) or \\(B \\times A\\). But let’s assume that we already have determined the order that they go in (that it should be \\(A \\times B\\) But we still have choices to make if there are many matrices to multiply together The thing that we need to consider is this: If we want to multiply three matrices, and we know the order, we still have a choice of how to group them. In other words, we can multiply three matrices as either \\(A(BC)\\) or \\((AB)C\\), and the answer will be the same in the end. However, as we see below, it can matter a lot which way we do this in terms of the cost of getting that answer. Settings Saving... Server Error Resubmit To solve this problem efficiently (of how to group the order of the multiplications), we should notice that there are a lot of duplicate nodes in the recursion tree. But there are only a relatively limited number of actual subproblems to solve. For instance, we repeatedly need to decide the best order to multiply ABC. And to solve that, we repeatedly compute AB’s cost, and BC’s cost. One way to speed this up is simply to remember the answers whenever we compute them. This is called memoization. Whenever we ask the question again, we simply use the stored result. This implies that we have a good way to remember where to store them, that is, how to organize the subproblems to easily check if the problem has already been solved. So, how do we organize the subproblems when there are \\(n\\) matrices to multiply, labeled 1 to \\(n\\)? One way is to use a table of size \\(n \\times n\\). In this table, the entry at \\([i, j]\\) is the cost for the best solution of multiplying matrices \\(i\\) to \\(j\\). So, the upper left corner (entry \\([1, n]\\)) is the full solution. Entries on the main diagonal are simply a single matrix (no multiplication). Only the upper left triangle has entries (since there is no meaning to the cost for multiplying matrix 5 through matrix 3, only for multiplying matrix 3 through matrix 5). Now, when we need to compute a series of matrices from \\(i\\) to \\(j\\), we just look in position \\([i, j]\\) in the table. If there is an answer there, we use it. Otherwise, we do the computation, and note it in the table. Contact Us || Privacy | | License « 5.5. Backtracking, and Branch and Bound :: Contents :: 5.7. Introduction to Probabilistic Algorithms » Contact Us | | Report a bug © Copyright 2011-2023 by OpenDSA Project Contributors and distributed under an MIT license. Last updated on Aug 13, 2024. Created using Sphinx 4.4.0. Summary*: Operating system*: Windows Mac OS Linux iOS Android Other Browser*: Chrome Safari Internet Explorer Opera Other Description*: Attach a screenshot (optional):"
            },
            {
                "role": "user",
                "content": "Can you summarize the key topics covered in this lecture?"
            },
            {
                "role": "assistant",
                "content": "This lecture covers foundational algorithmic concepts including problem formulation, correctness proof using induction, efficiency analysis using asymptotic notation, data structures (arrays, linked lists, BSTs, heaps), and sorting methods (selection sort, merge sort, counting sort). Additionally, it introduces the Word-RAM computational model, providing a theoretical framework for measuring algorithm efficiency."
            }
        ]
    },
    {
        "system": "Hello! I am Kiran, an AI assistant from Tampere University, dedicated to providing you with accurate and insightful assistance. My goal is to help you understand concepts, summarize lectures, and answer questions effectively.",
        "conversations": [
            {
                "role": "host",
                "content": "6.4. All-Pairs Shortest Paths — BM40A1500 Data Structures and Algorithms Username Password Forgot your password? Register Username: Password Confirm Password Email: BM40A1500 Data Structures and Algorithms Chapter 6 Graphs Show Source | | About « 6.3. Shortest-Paths Problems :: Contents :: 6.5. Minimal Cost Spanning Trees » 6.4. All-Pairs Shortest Paths¶ We next consider the problem of finding the shortest distance between all pairs of vertices in the graph, called the all-pairs shortest paths problem. To be precise, for every \\(u, v \\in \\mathbf{V}\\), calculate \\(d(u, v)\\). One solution is to run Dijkstra’s algorithm for finding the shortest path \\(|\\mathbf{V}|\\) times, each time computing the shortest path from a different start vertex. If \\(\\mathbf{G}\\) is sparse (that is, \\(|\\mathbf{E}| = \\Theta(|\\mathbf{V}|)\\)) then this is a good solution, because the total cost will be \\(\\Theta(|\\mathbf{V}|^2 + |\\mathbf{V}||\\mathbf{E}| \\log |\\mathbf{V}|) = \\Theta(|\\mathbf{V}|^2 \\log |\\mathbf{V}|)\\) for the version of Dijkstra’s algorithm based on priority queues. For a dense graph, the priority queue version of Dijkstra’s algorithm yields a cost of \\(\\Theta(|\\mathbf{V}|^3 \\log |\\mathbf{V}|)\\), but the version using MinVertex yields a cost of \\(\\Theta(|\\mathbf{V}|^3)\\). Another solution that limits processing time to \\(\\Theta(|\\mathbf{V}|^3)\\) regardless of the number of edges is known as Floyd’s algorithm. It is an example of dynamic programming. The chief problem with solving this problem is organizing the search process so that we do not repeatedly solve the same subproblems. We will do this organization through the use of the \\(k\\)-path. Define a k-path from vertex \\(v\\) to vertex \\(u\\) to be any path whose intermediate vertices (aside from \\(v\\) and \\(u\\)) all have indices less than \\(k\\). A 0-path is defined to be a direct edge from \\(v\\) to \\(u\\). Figure illustrates the concept of \\(k\\)-paths. Figure 6.4.1: An example of \\(k\\)-paths in Floyd’s algorithm. Path 1, 3 is a 0-path by definition. Path 3, 0, 2 is not a 0-path, but it is a 1-path (as well as a 2-path, a 3-path, and a 4-path) because the largest intermediate vertex is 0. Path 1, 3, 2 is a 4-path, but not a 3-path because the intermediate vertex is 3. All paths in this graph are 4-paths. Define \\({\\rm D}_k(v, u)\\) to be the length of the shortest \\(k\\)-path from vertex \\(v\\) to vertex \\(u\\). Assume that we already know the shortest \\(k\\)-path from \\(v\\) to \\(u\\). The shortest \\((k+1)\\)-path either goes through vertex \\(k\\) or it does not. If it does go through \\(k\\), then the best path is the best \\(k\\)-path from \\(v\\) to \\(k\\) followed by the best \\(k\\)-path from \\(k\\) to \\(u\\). Otherwise, we should keep the best \\(k\\)-path seen before. Floyd’s algorithm simply checks all of the possibilities in a triple loop. Here is the implementation for Floyd’s algorithm. At the end of the algorithm, array D stores the all-pairs shortest distances. # Compute all-pairs shortest paths def Floyd(G, D): for i in range(G.n()): # Initialize D with weights for j in range(G.n()): if G.weight(i, j) != 0: D[i][j] = G.weight(i, j) for k in range(G.n()): # Compute all k paths for i in range(G.n()): for j in range(G.n()): if (D[i][k] != math.inf and D[k][j] != math.inf and D[i][j] > D[i][k] + D[k][j] ): D[i][j] = D[i][k] + D[k][j] Clearly this algorithm requires \\(\\Theta(|\\mathbf{V}|^3)\\) running time, and it is the best choice for dense graphs because it is (relatively) fast and easy to implement. Contact Us || Privacy | | License « 6.3. Shortest-Paths Problems :: Contents :: 6.5. Minimal Cost Spanning Trees » Contact Us | | Report a bug © Copyright 2011-2023 by OpenDSA Project Contributors and distributed under an MIT license. Last updated on Aug 13, 2024. Created using Sphinx 4.4.0. Summary*: Operating system*: Windows Mac OS Linux iOS Android Other Browser*: Chrome Safari Internet Explorer Opera Other Description*: Attach a screenshot (optional):"
            },
            {
                "role": "user",
                "content": "Can you summarize the key topics covered in this lecture?"
            },
            {
                "role": "assistant",
                "content": "This lecture covers foundational algorithmic concepts including problem formulation, correctness proof using induction, efficiency analysis using asymptotic notation, data structures (arrays, linked lists, BSTs, heaps), and sorting methods (selection sort, merge sort, counting sort). Additionally, it introduces the Word-RAM computational model, providing a theoretical framework for measuring algorithm efficiency."
            }
        ]
    },
    {
        "system": "Hello! I am Kiran, an AI assistant from Tampere University, dedicated to providing you with accurate and insightful assistance. My goal is to help you understand concepts, summarize lectures, and answer questions effectively.",
        "conversations": [
            {
                "role": "host",
                "content": "Index — BM40A1500 Data Structures and Algorithms Username Password Forgot your password? Register Username: Password Confirm Password Email: BM40A1500 Data Structures and Algorithms | About Contents Index Symbols | A | B | C | D | E | F | G | H | I | J | K | L | M | N | O | P | Q | R | S | T | U | V | W | Z Symbols 2-3 tree 80/20 rule A abstract data type accept accepting state acceptor activation record acyclic graph address adjacency list adjacency matrix adjacent ADT adversary adversary argument aggregate type algorithm algorithm analysis alias all-pairs shortest paths problem allocated allocation alphabet alphabet trie amortized analysis amortized cost ancestor antisymmetric approximation algorithm arm array array-based list array-based queue array-based stack ASCII character coding assembly code asymptotic algorithm analysis asymptotic analysis attribute automata automatic variable average case average seek time AVL Tree B B$^*$-tree B$^+$-tree B-tree backing storage backtracking bad reference bag balanced tree base base case base class base type basic operation best case best fit BFS big-Oh notation binary insert sort binary relation binary search binary search tree binary search tree property binary tree binary trie binning Binsort bintree bit vector bitmap block Boolean expression Boolean variable boom bounding box branch-and-bounds algorithm breadth-first search break-even point BST bubble sort bucket bucket hashing bucket sort buddy method buffer buffer passing buffer pool buffering C caching call stack Cartesian product ceiling CFG CFL child circular first fit circular list class class hierarchy clause client clique closed closed hash system closed-form solution cluster CNF code generation code optimization cohesion Collatz sequence collision collision resolution, [1] collision resolution policy comparable comparator comparison compile-time polymorphism compiler complete binary tree complete graph complex number Composite design pattern composite type composition computability computation computational complexity theory configuration Conjunctive Normal Form connected component connected graph constant running time constructive induction container container class context-free grammar context-free language context-sensitive grammar cost cost model countable countably infinite CPU current position cycle cylinder cylinder index cylinder overflow D DAG data field data item data member data structure selecting data type deallocated deallocation debugging decideability decision problem decision tree deep copy degree delegation mental model for recursion dense graph depth depth-first search depth-first search tree dequeue dereference derivation descendant deserialization design pattern deterministic deterministic algorithm Deterministic Finite Acceptor Deterministic Finite Automata DFA DFS DFT diagonalization argument dictionary dictionary search digraph Dijkstra's algorithm diminishing increment sort direct access direct proof directed acyclic graph directed edge directed graph dirty bit Discrete Fourier Transform discriminator disjoint disjoint sets disk access disk controller disk drive disk I/O disk-based space/time tradeoff distance divide and conquer divide-and-conquer recurrences divide-and-guess domain double buffering double hashing double rotation doubly linked list DSA dynamic dynamic allocation dynamic array dynamic memory allocation dynamic programming E edge edit distance efficient element empirical comparison empty encapsulation enqueue entry-sequenced file enumeration equidistribution property equivalence class equivalence relation equivalent estimation evaluation exact-match query exceptions exchange exchange sort expanding the recurrence exponential growth rate expression tree extent external fragmentation external sort F factorial failure policy family of languages FIFO file allocation table file manager file processing file structure final state FIND Finite Automata Finite State Acceptor Finite State Automata Finite State Machine first fit fixed-length coding floor Floyd's algorithm flush flyweight folding method Ford and Johnson sort forest free block free block list free store free tree freelist frequency count FSA FSM full binary tree theorem full tree function G garbage garbage collection general tree grammar graph greedy algorithm growth rate guess-and-test guided traversal H halt state halted configuration halting problem handle hanging configuration happy path testing hard algorithm hard problem harmonic series hash function hash system hash table hashing, [1] hashing function head header node heap Heapsort heapsort height height balanced heuristic heuristic algorithm home position home slot homogeneity Huffman codes Huffman coding tree Huffman tree I I/O head image-space decomposition in degree incident index file indexing induction hypothesis induction step induction variable information theoretic lower bound inherit initial state inode inorder traversal Insertion Sort instance variable integer function inter-sector gap interface intermediate code intermediate code generation internal fragmentation internal node internal sort interpolation interpolation search interpreter inversion inverted file inverted list irreflexive ISAM iterator J job jump search K K-ary tree k-path kd tree key key sort key space key-space decomposition key-value pair knapsack problem Kruskal's algorithm L labeled graph language Las Vegas algorithms leaf node least frequently used least recently used left recursive length level lexical analysis lexical scoping LFU lifetime LIFO linear congruential method linear growth rate linear index linear order linear probing linear probing by steps linear search link node linked list linked stack list literal load factor local storage local variable locality of reference logarithm logical file logical form lookup table lower bound lower bounds proof LRU M main memory map mapping mark array mark/sweep algorithm master theorem matching matching problem max heap maximal match maximum lower bound maximum match MCST measure of cost member member function memory allocation memory deallocation memory hierarchy memory leak memory manager memory pool memory request merge insert sort Mergesort, [1] message message passing metaphor method mid-square method min heap minimal-cost spanning tree minimum external path weight mod model modulus Monte Carlo algorithms move-to-front MST multi-dimensional search key multi-dimensional search structure multilist N natural numbers necessary fallacy neighbor NFA node non-deterministic non-deterministic algorithm non-deterministic choice non-deterministic polynomial time algorithm non-strict partial order non-terminal Nondeterministic Finite Acceptor Nondeterministic Finite Automata NP NP-Complete NP-Completeness proof NP-hard nth roots of unity O object object-oriented programming paradigm object-space decomposition octree Omega notation one-way list open addressing open hash system operating system optimal static ordering optimization problem out degree overflow overflow bucket overhead P page parameter parent parent pointer representation parity parity bit parse tree parser partial order partially ordered set partition pass by reference pass by value path path compression PDA peripheral storage permutation persistent physical file physical form Pigeonhole Principle pivot, [1] platter point quadtree point-region quadtree pointee pointer pointer-based implementation for binary tree nodes polymorphism pop poset position postorder traversal potential powerset PR quadtree prefix property preorder traversal Prim's algorithm primary clustering primary index primary key primary key index primary storage primitive data type primitive element primitive nth root of unity priority priority queue probabilistic algorithm probabilistic data structure probe function probe sequence problem problem instance problem lower bound problem upper bound procedural procedural programming paradigm production production rule program promotion proof proof by contradiction proof by induction proving the contrapositive pseudo polynomial pseudo random pseudo-random probing push pushdown automata Q quadratic growth rate quadratic probing quadtree queue Quicksort, [1] R radix radix sort RAM random access random access memory random permutation randomized algorithm range range query read/write head rebalancing operation recognize record recurrence relation recurrence with full history recursion recursive call recursive data structure recursive function recursively enumerable Red-Black Tree reduction reference reference count algorithm reference parameter reflexive Region Quadtree regular expression regular grammar regular language relation replacement selection reserved block resource constraints root rotation rotational delay rotational latency run run file run-time polymorphism runtime environment runtime stack S scanner scope search key search lower bound search problem search tree search trie searching secondary clustering secondary index secondary key secondary key index secondary storage sector sector header seed seek selection sort self-organizing list self-organizing list heuristic separate chaining sequence sequential access sequential fit sequential search sequential tree representation serialization set set former set product shallow copy Shellsort shifting method shortest path sibling signature signature file simple cycle simple path simple type simulating recursion single rotation single-source shortest paths problem singly linked list skip list slot snowplow argument software engineering software reuse solution space solution tree sorted list sorting lower bound sorting problem space/time tradeoff sparse graph sparse matrix spatial spatial application spatial attribute spatial data spatial data structure spindle Splay Tree splaying stable stack stack frame stack variable stale pointer start state start symbol state State Machine static static scoping Strassen's algorithm strategy stream strict partial order strong induction subclass subgraph subset subtract-and-guess subtree successful search summation superset symbol table symmetric symmetric matrix syntax analysis T tail terminal testing Theta notation token tombstone topological sort total order total path length Towers of Hanoi problem track track-to-track seek time trailer node transducer transitive transpose trap state traversal tree tree traversal trie truth table tuple Turing machine Turing-acceptable Turing-computable function Turing-decidable two-coloring type U unary notation uncountable uncountably infinite underflow undirected edge undirected graph uninitialized UNION UNION/FIND unit production unsolveable problem unsorted list unsuccessful search unvisited upper bound V value parameter variable-length coding vector vertex virtual memory visit visited visitor volatile W weight weighted graph weighted path length weighted union rule working memory worst case worst fit Z zigzig Zipf distribution zone Contact Us || Privacy | | License Contents Contact Us | | Report a bug © Copyright 2011-2023 by OpenDSA Project Contributors and distributed under an MIT license. Last updated on Aug 13, 2024. Created using Sphinx 4.4.0. Summary*: Operating system*: Windows Mac OS Linux iOS Android Other Browser*: Chrome Safari Internet Explorer Opera Other Description*: Attach a screenshot (optional):"
            },
            {
                "role": "user",
                "content": "Can you summarize the key topics covered in this lecture?"
            },
            {
                "role": "assistant",
                "content": "This lecture covers foundational algorithmic concepts including problem formulation, correctness proof using induction, efficiency analysis using asymptotic notation, data structures (arrays, linked lists, BSTs, heaps), and sorting methods (selection sort, merge sort, counting sort). Additionally, it introduces the Word-RAM computational model, providing a theoretical framework for measuring algorithm efficiency."
            }
        ]
    },
    {
        "system": "Hello! I am Kiran, an AI assistant from Tampere University, dedicated to providing you with accurate and insightful assistance. My goal is to help you understand concepts, summarize lectures, and answer questions effectively.",
        "conversations": [
            {
                "role": "host",
                "content": "8.1. Glossary — BM40A1500 Data Structures and Algorithms Username Password Forgot your password? Register Username: Password Confirm Password Email: BM40A1500 Data Structures and Algorithms Chapter 8 Appendix Show Source | | About « 7.6. Coping with NP-Complete Problems :: Contents 8.1. Glossary¶ 2-3 tree¶A specialized form of the B-tree where each internal node has either 2 children or 3 children. Key values are ordered to maintain the binary search tree property. The 2-3 tree is always height balanced, and its insert, search, and remove operations all have \\(\\Theta(\\log n)\\) cost. 80/20 rule¶Given a typical application where there is a collection of records and a series of search operations for records, the 80/20 rule is an empirical observation that 80% of the record accessess typically go to 20% of the records. The exact values varies between data collections, and is related to the concept of locality of reference. abstract data type¶Abbreviated ADT. The specification of a data type within some language, independent of an implementation. The interface for the ADT is defined in terms of a type and a set of operations on that type. The behavior of each operation is determined by its inputs and outputs. An ADT does not specify how the data type is implemented. These implementation details are hidden from the user of the ADT and protected from outside access, a concept referred to as encapsulation. accept¶When a finite automata executes on a string and terminates in an accepting state, it is said to accept the string. The finite automata is said to accept the language that consists of all strings for which the finite automata completes execution in an accepting state. accepting state¶Part of the definition of a finite automata is to designate some states as accepting states. If the finite automata executes on an input string and completes the computation in an accepting state, then the machine is said to accept the string. acceptor¶In formal languages, any machine whose primary purpose is to determine whether a string is accepted (is recognized to be in a language) or rejected. This is in contrast to a machine that computes some value. activation record¶The entity that is stored on the runtime stack during program execution. It stores any active local variable and the return address from which a new subroutine is being called, so that this information can be recovered when the subroutine terminates. acyclic graph¶In graph terminology, a graph that contains no cycles. address¶A location in memory. adjacency list¶An implementation for a graph that uses an (array-based) list to represent the vertices of the graph, and each vertex is in turn represented by a (linked) list of the vertices that are neighbors. adjacency matrix¶An implementation for a graph that uses a 2-dimensional array where each row and each column corresponds to a vertex in the graph. A given row and column in the matrix corresponds to an edge from the vertex corresponding to the row to the vertex corresponding to the column. adjacent¶Two nodes of a tree or two vertices of a graph are said to be adjacent if they have an edge connecting them. If the edge is directed from \\(a\\) to \\(b\\), then we say that \\(a\\) is adjacent to \\(b\\), and \\(b\\) is adjacent from \\(a\\). ADT¶Abbreviation for abstract data type. adversary¶A fictional construct introduced for use in an adversary argument. adversary argument¶A type of lower bounds proof for a problem where a (fictional) “adversary” is assumed to control access to an algorithm’s input, and which yields information about that input in such a way that will drive the cost for any proposed algorithm to solve the problem as high as possible. So long as the adversary never gives an answer that conflicts with any previous answer, it is permitted to do whatever necessary to make the algorithm require as much cost as possible. aggregate type¶A data type whose members have subparts. For example, a typical database record. Another term for this is composite type. algorithm¶A method or a process followed to solve a problem. algorithm analysis¶A less formal version of the term asymptotic algorithm analysis, generally used as a synonym for asymptotic analysis. alias¶Another name for something. In programming, this usually refers to two references that refer to the same object. all-pairs shortest paths problem¶Given a graph with weights or distances on the edges, find the shortest paths between every pair of vertices in the graph. One approach to solving this problem is Floyd’s algorithm, which uses the dynamic programming algorithmic technique. allocated¶allocation¶Reserving memory for an object in the Heap memory. alphabet¶The characters or symbols that strings in a given language may be composed of. alphabet trie¶A trie data structure for storing variable-length strings. Level \\(i\\) of the tree corresponds to the letter in position \\(i\\) of the string. The root will have potential branches on each intial letter of string. Thus, all strings starting with “a” will be stored in the “a” branch of the tree. At the second level, such strings will be separated by branching on the second letter. amortized analysis¶An algorithm analysis techique that looks at the total cost for a series of operations and amortizes this total cost over the full series. This is as opposed to considering every individual operation to independently have the worst case cost, which might lead to an overestimate for the total cost of the series. amortized cost¶The total cost for a series of operations to be used in an amortized analysis. ancestor¶In a tree, for a given node \\(A\\), any node on a path from \\(A\\) up to the root is an ancestor of \\(A\\). antisymmetric¶In set notation, relation \\(R\\) is antisymmetric if whenever \\(aRb\\) and \\(bRa\\), then \\(a = b\\), for all \\(a, b \\in \\mathbf{S}\\). approximation algorithm¶An algorthm for an optimization problem that finds a good, but not necessarily cheapest, solution. arm¶In the context of an I/O head, this attaches the sensor on the I/O head to the boom. array¶A data type that is used to store elements in consecutive memory locations and refers to them by an index. array-based list¶An implementation for the list ADT that uses an array to store the list elements. Typical implementations fix the array size at creation of the list, and the overhead is the number of array positions that are presently unused. array-based queue¶Analogous to an array-based list, this uses an array to store the elements when implementing the queue ADT. array-based stack¶Analogous to an array-based list, this uses an array to store the elements when implementing the stack ADT. ASCII character coding¶American Standard Code for Information Interchange. A commonly used method for encoding characters using a binary code. Standard ASCII uses an 8-bit code to represent upper and lower case letters, digits, some punctuation, and some number of non-printing characters (such as carrage return). Now largely replaced by UTF-8 encoding. assembly code¶A form of intermediate code created by a compiler that is easy to convert into the final form that the computer can execute. An assembly language is typically a direct mapping of one or a few instructions that the CPU can execute into a mnemonic form that is relatively easy for a human to read. asymptotic algorithm analysis¶A more formal term for asymptotic analysis. asymptotic analysis¶A method for estimating the efficiency of an algorithm or computer program by identifying its growth rate. Asymptotic analysis also gives a way to define the inherent difficulty of a problem. We frequently use the term algorithm analysis to mean the same thing. attribute¶In object-oriented programming, a synonym for data members. automata¶Synonym for finite state machine. automatic variable¶A synonym for local variable. When program flow enters and leaves the variable’s scope, automatic variables will be allocated and de-allocated automatically. average case¶In algorithm analysis, the average of the costs for all problem instances of a given input size \\(n\\). If not all problem instances have equal probability of occurring, then average case must be calculated using a weighted average. average seek time¶Expected (average) time to perform a seek operation on a disk drive, assuming that the seek is between two randomly selected tracks. This is one of two metrics commonly provided by disk drive vendors for disk drive performance, with the other being track-to-track seek time. AVL Tree¶A variant implementation for the BST, which differs from the standard BST in that it uses modified insert and remove methods in order to keep the tree balanced. Similar to a Splay Tree in that it uses the concept of rotations in the insert and remove operations. B$^*$-tree¶A variant on the B$^+$-tree. The \\(\\mathrm{B}^*\\) tree is identical to the \\(\\mathrm{B}^+\\) tree, except for the rules used to split and merge nodes. Instead of splitting a node in half when it overflows, the \\(\\mathrm{B}^*\\) tree gives some records to its neighboring sibling, if possible. If the sibling is also full, then these two nodes split into three. Similarly, when a node underflows, it is combined with its two siblings, and the total reduced to two nodes. Thus, the nodes are always at least two thirds full. B$^+$-tree¶The most commonly implemented form of B-tree. A B$^+$-tree does not store data at the internal nodes, but instead only stores search key values as direction finders for the purpose of searching through the tree. Only the leaf nodes store a reference to the actual data records. B-tree¶A method for indexing a large collection of records. A B-tree is a balanced tree that typically has high branching factor (commonly as much as 100 children per internal node), causing the tree to be very shallow. When stored on disk, the node size is selected to be same as the desired unit of I/O (so some multiple of the disk sector size). This makes it easy to gain access to the record associated with a given search key stored in the tree with few disk accesses. The most commonly implemented variant of the B-tree is the B$^+$-tree. backing storage¶In the context of a caching system or buffer pool, backing storage is the relatively large but slower source of data that needs to be cached. For example, in a virtual memory, the disk drive would be the backing storage. In the context of a web browser, the Internet might be considered the backing storage. backtracking¶A heuristic for brute-force search of a solution space. It is essentially a depth-first search of the solution space. This can be improved using a branch-and-bounds algorithm. bad reference¶A reference is referred to as a bad reference if it is allocated but not initialized. bag¶In set notation, a bag is a collection of elements with no order (like a set), but which allows for duplicate-valued elements (unlike a set). balanced tree¶A tree where the subtrees meet some criteria for being balanced. Two possibilities are that the tree is height balanced, or that the tree has a roughly equal number of nodes in each subtree. base¶Synonym for radix. base case¶In recursion or proof by induction, the base case is the termination condition. This is a simple input or value that can be solved (or proved in the case of induction) without resorting to a recursive call (or the induction hypothesis). base class¶In object-oriented programming, a class from which another class inherits. The class that inherits is called a subclass. base type¶The data type for the elements in a set. For example, the set might consist of the integer values 3, 5, and 7. In this example, the base type is integers. basic operation¶Examples of basic operations include inserting a data item into the data structure, deleting a data item from the data structure, and finding a specified data item. best case¶In algorithm analysis, the problem instance from among all problem instances for a given input size \\(n\\) that has least cost. Note that the best case is not when \\(n\\) is small, since we are referring to the best from a class of inputs (i.e, we want the best of those inputs of size \\(n\\)). best fit¶In a memory manager, best fit is a heuristic for deciding which free block to use when allocating memory from a memory pool. Best fit will always allocate from the smallest free block that is large enough to service the memory request. The rationale is that this will be the method that best preserves large blocks needed for unusually large requests. The disadvantage is that it tends to cause external fragmentation in the form of small, unuseable memory blocks. BFS¶Abbreviation for breadth-first search. big-Oh notation¶In algorithm analysis, a shorthand notation for describing the upper bound for an algorithm or problem. binary insert sort¶A variation on insertion sort where the position of the value being inserted is located by binary search, and then put into place. In normal usage this is not an improvement on standard insertion sort because of the expense of moving many items in the array. But it is directly useful if the cost of comparison is high compared to that of moving an element, or is theoretically useful if we only care to count the cost of comparisons. binary relation¶In set theory, a relation defined by a collection of binary tuples. binary search¶A standard recursive algorithm for finding the record with a given search key value within a sorted list. It runs in \\(O(\\log n)\\) time. At each step, look at the middle of the current sublist, and throw away the half of the records whose keys are either too small or too large. binary search tree¶A binary tree that imposes the following constraint on its node values: The search key value for any node \\(A\\) must be greater than the (key) values for all nodes in the left subtree of \\(A\\), and less than the key values for all nodes in the right subtree of \\(A\\). Some convention must be adopted if multiple nodes with the same key value are permitted, typically these are required to be in the right subtree. binary search tree property¶The defining relationship between the key values for nodes in a BST. All nodes stored in the left subtree of a node whose key value is \\(K\\) have key values less than or equal to \\(K\\). All nodes stored in the right subtree of a node whose key value is \\(K\\) have key values greater than \\(K\\). binary tree¶A finite set of nodes which is either empty, or else has a root node together two binary trees, called the left and right subtrees, which are disjoint from each other and from the root. binary trie¶A binary tree whose structure is that of a trie. Generally this is an implementation for a search tree. This means that the search key values are thought of a binary digits, with the digit in the position corresponding to this a node’s level in the tree indicating a left branch if it is “0”, or a right branch if it is “1”. Examples include the Huffman coding tree and the Bintree. binning¶In hashing, binning is a type of hash function. Say we are given keys in the range 0 to 999, and have a hash table of size 10. In this case, a possible hash function might simply divide the key value by 100. Thus, all keys in the range 0 to 99 would hash to slot 0, keys 100 to 199 would hash to slot 1, and so on. In other words, this hash function “bins” the first 100 keys to the first slot, the next 100 keys to the second slot, and so on. This approach tends to make the hash function dependent on the distribution of the high-order bits of the keys. Binsort¶A sort that works by taking each record and placing it into a bin based on its value. The bins are then gathered up in order to sort the list. It is generally not practical in this form, but it is the conceptual underpinning of the radix sort. bintree¶A spatial data structure in the form of binary trie, typically used to store point data in two or more dimensions. Similar to a PR quadtree except that at each level, it splits one dimension in half. Since many leaf nodes of the PR quadtree will contain no data points, implementation often makes use of the flyweight design pattern. bitmap¶bit vector¶An array that stores a single bit at each position. Typically these bits represent Boolean variables associated with a collection of objects, such that the \\(i\\) th bit is the Boolean value for the \\(i\\) th object. block¶A unit of storage, usually referring to storage on a disk drive or other peripheral storage device. A block is the basic unit of I/O for that device. Boolean expression¶A Boolean expression is comprised of Boolean variables combined using the operators AND (\\(\\cdot\\)), OR (\\(+\\)), and NOT (to negate Boolean variable \\(x\\) we write \\(\\overline{x}\\)). Boolean variable¶A variable that takes on one of the two values True and False. boom¶In the context of an I/O head, is the central structure to which all of the I/O heads are attached. Thus, the all move together during a seek operation. bounding box¶A box (usually aligned to the coordinate axes of the reference system) that contains a (potentially complex) object. In graphics and computational geometry, complex objects might be associated with a bounding box for use by algorithms that search for objects in a particular location. The idea is that if the bounding box is not within the area of interest, then neither is the object. Checking the bounding box is cheaper than checking the object, but it does require some time. So if enough objects are not outside the area of interest, this approach will not save time. But if most objects are outside of the area of interest, then checking bounding boxes first can save a lot of time. branch-and-bounds algorithm¶A variation on backtracking that applies to optimization problems. We traverse the solution tree as with backtracking. Proceeding deeper in the solution tree generally requires additional cost. We remember the best-cost solution found so far. If the cost of the current branch in the tree exceeds the best tour cost found so far, then we know to stop pursuing this branch of the tree. At this point we can immediately back up and take another branch. breadth-first search¶A graph traversal algorithm. As the name implies, all immediate neighbors for a node are visited before any more-distant nodes are visited. BFS is driven by a queue. A start vertex is placed on the queue. Then, until the queue is empty, a node is taken off the queue, visited, and and then any unvisited neighbors are placed onto the queue. break-even point¶The point at which two costs become even when measured as the function of some variable. In particular, used to compare the space requirements of two implementations. For example, when comparing the space requirements of an array-based list implementation versus a linked list implementation, the key issue is how full the list is compared to its capacity limit (for the array-based list). The point where the two representations would have the same space cost is the break-even point. As the list becomes more full beyond this point, the array-based list implementation becomes more space efficent, while as the list becomes less full below this point, the linked list implementation becomes more space efficient. BST¶Abbreviation for binary search tree. bubble sort¶A simple sort that requires \\(Theta(n^2)\\) time in best, average, and worst cases. Even an optimized version will normally run slower than insertion sort, so it has little to recommend it. bucket¶In bucket hashing, a bucket is a sequence of slots in the hash table that are grouped together. bucket hashing¶A method of hashing where multiple slots of the hash table are grouped together to form a bucket. The hash function then either hashes to some bucket, or else it hashes to a home slot in the normal way, but this home slot is part of some bucket. Collision resolution is handled first by attempting to find a free position within the same bucket as the home slot. If the bucket if full, then the record is placed in an overflow bucket. bucket sort¶A variation on the Binsort, where each bin is associated with a range of key values. This will require some method of sorting the records placed into each bin. buddy method¶In a memory manager, an alternative to using a free block list and a sequential fit method to seach for a suitable free block to service a memory request. Instead, the memory pool is broken down as needed into smaller chunks by splitting it in half repeatedly until the smallest power of 2 that is as big or bigger than the size of the memory request is reached. The name comes from the fact that the binary representation for the start of the block positions only differ by one bit for adjacent blocks of the same size. These are referred to as “buddies” and will be merged together if both are free. buffer¶A block of memory, most often in primary storage. The size of a buffer is typically one or a multiple of the basic unit of I/O that is read or written on each access to secondary storage such as a disk drive. buffer passing¶An approach to implementing the ADT for a buffer pool, where a pointer to a buffer is passed between the client and the buffer pool. This is in contrast to a message passing approach, it is most likely to be used for long messages or when the message size is always the same as the buffer size, such as when implementing a B-tree. buffer pool¶A collection of one or more buffers. The buffer pool is an example of a cache. It is stored in primary storage, and holds data that is expected to be used in the near future. When a data value is requested, the buffer pool is searched first. If the value is found in the buffer pool, then secondary storage need not be accessed. If the value is not found in the buffer pool, then it must be fetched from secondary storage. A number of traditional heuristics have been developed for deciding which data to flush from the buffer pool when new data must be stored, such as least recently used. buffering¶A synonym for caching. More specifically, it refers to an arrangement where all accesses to data (such as on a peripheral storage device) must be done in multiples of some minimum unit of storage. On a disk drive, this basic or smallest unit of I/O is a sector. It is called “buffering” because the block of data returned by such an access is stored in a buffer. caching¶The concept of keeping selected data in main memory. The goal is to have in main memory the data values that are most likely to be used in the near future. An example of a caching technique is the use of a buffer pool. call stack¶Known also as execution stack. A stack that stores the function call sequence and the return address for each function. Cartesian product¶For sets, this is another name for the set product. ceiling¶Written \\(\\lceil x \\rceil\\), for real value \\(x\\) the ceiling is the least integer \\(\\geq x\\). child¶In a tree, the set of nodes directly pointed to by a node \\(R\\) are the children of \\(R\\). circular first fit¶In a memory manager, circular first fit is a heuristic for deciding which free block to use when allocating memory from a memory pool. Circular first fit is a minor modification on first fit memory allocation, where the last free block allocated from is remembered, and search for the next suitable free block picks up from there. Like first fit, it has the advantage that it is typically not necessary to look at all free blocks on the free block list to find a suitable free block. And it has the advantage over first fit that it spreads out memory allocations evenly across the free block list. This might help to minimize external fragmentation. circular list¶A list ADT implementation variant where the last element of the list provides access to the first element of the list. class¶In the object-oriented programming paradigm an ADT and its implementation together make up a class. An instantiation of a class within a program is termed an object. class hierarchy¶In object-oriented programming, a set of classes and their interrelationships. One of the classes is the base class, and the others are subclasses that inherit either directly or indirectly from the base class. clause¶In a Boolean expression, a clause is one or more literals OR’ed together. client¶The user of a service. For example, the object or part of the program that calls a memory manager class is the client of that memory manager. Likewise the class or code that calls a buffer pool. clique¶In graph terminology, a clique is a subgraph, defined as any subset \\(U\\) of the graph’s vertices such that every vertex in \\(U\\) has an edge to every other vertex in \\(U\\). The size of the clique is the number of vertices in the clique. closed¶A set is closed over a (binary) operation if, whenever the operation is applied to two members of the set, the result is a member of the set. closed hash system¶A hash system where all records are stored in slots of the hash table. This is in contrast to an open hash system. closed-form solution¶An algebraic equation with the same value as a summation or recurrence relation. The process of replacing the summation or recurrence with its closed-form solution is known as solving the summation or recurrence. cluster¶In file processing, a collection of physically adjacent sectors that define the smallest allowed allocation unit of space to a disk file. The idea of requiring space to be allocated in multiples of sectors is that this will reduce the number of extents required to store the file, which reduces the expected number of seek operations reuquired to process a series of disk accesses to the file. The disadvantage of large cluster size is that it increases internal fragmentation since any space not actually used by the file in the last cluster is wasted. code generation¶A phase in a compiler that transforms intermediate code into the final executable form of the code. More generally, this can refer to the process of turning a parse tree (that determines the correctness of the structure of the program) into actual instructions that the computer can execute. code optimization¶A phase in a compiler that makes changes in the code (typically assembly code) with the goal of replacing it with a version of the code that will run faster while performing the same computation. cohesion¶In object-oriented programming, a term that refers to the degree to which a class has a single well-defined role or responsibility. Collatz sequence¶For a given integer value \\(n\\), the sequence of numbers that derives from performing the following computatin on \\(n\\) while (n > 1) if (ODD(n)) n = 3 * n + 1; else n = n / 2; This is famous because, while it terminates for any value of \\(n\\) that you try, it has never been proven to be a fact that this always terminates. collision¶In a hash system, this refers to the case where two search keys are mapped by the hash function to the same slot in the hash table. This can happen on insertion or search when another record has already been hashed to that slot. In this case, a closed hash system will require a process known as collision resolution to find the location of the desired record. collision resolution¶The outcome of a collision resolution policy. collision resolution policy¶In hashing, the process of resolving a collision. Specifically in a closed hash system, this is the process of finding the proper position in a hash table that contains the desired record if the hash function did not return the correct position for that record due to a collision with another record. comparable¶The concept that two objects can be compared to determine if they are equal or not, or to determine which one is greater than the other. In set notation, elements \\(x\\) and \\(y\\) of a set are comparable under a given relation \\(R\\) if either \\(xRy\\) or \\(yRx\\). To be reliably compared for a greater/lesser relationship, the values being compared must belong to a total order. In programming, the property of a data type such that two elements of the type can be compared to determine if they the same (a weaker version), or which of the two is larger (a stronger version). Comparable is also the name of an interface in Java that asserts a comparable relationship between objects with a class, and .compareTo() is the Comparable interface method that implements the actual comparison between two objects of the class. comparator¶A function given as a parameter to a method of a library (or alternatively, a parameter for a C++ template or a Java generic). The comparator function concept provides a generic way encapulates the process of performing a comparison between two objects of a specific type. For example, if we want to write a generic sorting routine, that can handle any record type, we can require that the user of the sorting routine pass in a comparator function to define how records in the collection are to be compared. comparison¶The act of comparing two keys or records. For many data types, a comparison has constant time cost. The number of comparisons required is often used as a measure of cost for sorting and searching algorithms. compile-time polymorphism¶A form of polymorphism known as Overloading. Overloaded methods have the same names, but different signatures as a method available elsewhere in the class. Compare to run-time polymorphism. compiler¶A computer program that reads computer programs and converts them into a form that can be directly excecuted by some form of computer. The major phases in a compiler include lexical analysis, syntax analysis, intermediate code generation, code optimization, and code generation. More broadly, a compiler can be viewed as parsing the program to verify that it is syntactically correct, and then doing code generation to convert the hig-level program into something that the computer can execute. complete binary tree¶A binary tree where the nodes are filled in row by row, with the bottom row filled in left to right. Due to this requirement, there is only one tree of \\(n\\) nodes for any value of \\(n\\). Since storing the records in an array in row order leads to a simple mapping from a node’s position in the array to its parent, siblings, and children, the array representation is most commonly used to implement the complete binary tree. The heap data structure is a complete binary tree with partial ordering constraints on the node values. complete graph¶A graph where every vertex connects to every other vertex. complex number¶In mathematics, an imaginary number, that is, a number with a real component and an imaginary component. Composite design pattern¶Given a class hierarchy representing a set of objects, and a container for a collection of objects, the composite design pattern addresses the relationship between the object hierarchy and a bunch of behaviors on the objects. In the composite design, each object is required to implement the collection of behaviors. This is in contrast to the procedural approach where a behavior (such as a tree traversal) is implemented as a method on the object collection (such as a tree). Procedural tree traversal requires that the tree have a method that understands what to do when it encounters any of the object types (internal or leaf nodes) that the tree might contain. The composite approach would have the tree call the “traversal” method on its root node, which then knows how to perform the “traversal” behavior. This might in turn require invoking the traversal method of other objects (in this case, the children of the root). composite type¶A type whose members have subparts. For example, a typical database record. Another term for this is aggregate type. composition¶Relationships between classes based on usage rather than inheritance, i.e. a HAS-A relationship. For example, some code in class ‘A’ has a reference to some other class ‘B’. computability¶A branch of computer science that deals with the theory of solving problems through computation. More specificially, it deals with the limits to what problems (functions) are computable. An example of a famous problem that cannot in principle be solved by a computer is the halting problem. computation¶In a finite automata, a computation is a sequence of configurations for some length \\(n \\geq 0\\). In general, it is a series of operations that the machine performs. computational complexity theory¶A branch of the theory of computation in theoretical computer science and mathematics that focuses on classifying computational problems according to their inherent difficulty, and relating those classes to each other. An example is the study of NP-Complete problems. configuration¶For a finite automata, a complete specification for the current condition of the machine on some input string. This includes the current state that the machine is in, and the current condition of the string, including which character is about to be processed. Conjunctive Normal Form¶CNF¶A Boolean expression written as a series of clauses that are AND’ed together. connected component¶In an undirected graph, a subset of the nodes such that each node in the subset can be reached from any other node in that subset. connected graph¶An undirected graph is a connected graph if there is at least one path from any vertex to any other. constant running time¶The cost of a function whose running time is not related to its input size. In Theta notation, this is traditionally written as \\(\\Theta(1)\\). constructive induction¶A process for finding the closed form for a recurrence relation, that involves substituting in a guess for the closed form to replace the recursive part(s) of the recurrence. Depending on the goal (typically either to show that the hypothesized growth rate is right, or to find the precise constants), one then manipulates the resulting non-recursive equation. container¶container class¶A data structure that stores a collection of records. Typical examples are arrays, search trees, and hash tables. context-free grammar¶A grammar comprised only of productions of the form \\(A \\rightarrow x\\) where \\(A\\) is a non-terminal and \\(x\\) is a series of one or more terminals and non-terminals. That is, the given non-terminal \\(A\\) can be replaced at any time. context-free language¶CFL¶The set of languages that can be defined by context-sensitive grammars. context-sensitive grammar¶CFG¶A grammar comprised only of productions of the form \\(xAy \\rightarrow xvy\\) where \\(A\\) is a non-terminal and \\(x\\) and \\(y\\) are each a series of one or more terminals and non-terminals. That is, the given non-terminal \\(A\\) can be replaced only when it is within the proper context. cost¶The amount of resources that the solution consumes. cost model¶In algorithm analysis, a definition for the cost of each basic operation performed by the algorithm, along with a definition for the size of the input. Having these definitions allows us to calculate the cost to run the algorithm on a given input, and from there determine the growth rate of the algorithm. A cost model would be considered “good” if it yields predictions that conform to our understanding of reality. countably infinite¶countable¶A set is countably infinite if it contains a finite number of elements, or (for a set with an infinite number of elements) if there exists a one-to-one mapping from the set to the set of integers. CPU¶Acronym for Central Processing Unit, the primary processing device for a computer. current position¶A property of some list ADTs, where there is maintained a “current position” state that can be referred to later. cycle¶In graph terminology, a cycle is a path of length three or more that connects some vertex \\(v_1\\) to itself. cylinder¶A disk drive normally consists of a stack of platters. While this might not be so true today, traditionally all of the I/O heads moved together during a seek operation. Thus, when a given I/O head is positioned over a particular track on a platter, the other I/O heads are also positioned over the corresponding track on their platters. That collection of tracks is called a cylinder. A given cylinder represents all of the data that can be read from all of the platters without doing another seek operation. cylinder index¶In the ISAM system, a simple linear index that stores the lowest key value stored in each cylinder. cylinder overflow¶In the ISAM system, this is space reserved for storing any records that can not fit in their respective cylinder. DAG¶Abbreviation for directed acyclic graph. data field¶In object-oriented programming, a synonym for data member. data item¶A piece of information or a record whose value is drawn from a type. data member¶The variables that together define the space required by a data item are referred to as data members. Some of the commonly used synonyms include data field, attribute, and instance variable. data structure¶The implementation for an ADT. data type¶A type together with a collection of operations to manipulate the type. deallocated¶deallocation¶Free the memory allocated to an unused object. debugging¶Correcting a program once we determine that it is not operating as we intend. This is in contrast to testing. decideability¶In computability theory, the question of whether a question can be answered. A typical example is whether to instances within a system are equivalent. “Do two computer programs do the same thing?” is a variation of the halting problem (and is not decideable). “Do two DFAs do the same thing?” is decideable. decision problem¶A problem whose output is either “YES” or “NO”. decision tree¶A theoretical construct for modeling the behavior of algorithms. Each point at which the algorithm makes a decision (such as an if statement) is modeled by a branch in the tree that represents the algorithms behavior. Decision trees can be used in lower bounds proofs, such as the proof that sorting requires \\(\\Omega(n \\log n)\\) comparisons in the worst case. deep copy¶Copying the actual content of a pointee. degree¶In graph terminology, the degree for a vertex is its number of neighbors. In a directed graph, the in degree is the number of edges directed into the vertex, and the out degree is the number of edges directed out of the vertex. In tree terminology, the degree for a node is its number of children. delegation mental model for recursion¶A way of thinking about the process of recursion. The recursive function “delegates” most of the work when it makes the recursive call. The advantage of the delegation mental model for recursion is that you don’t need to think about how the delegated task is performed. It just gets done. dense graph¶A graph where the actual number of edges is a large fraction of the possible number of edges. Generally, this is interpreted to mean that the degree for any vertex in the graph is relatively high. depth¶The depth of a node \\(M\\) in a tree is the length of the path from the root of the tree to \\(M\\). depth-first search¶A graph traversal algorithm. Whenever a \\(v\\) is visited during the traversal, DFS will recursively visit all of \\(v\\) ‘s unvisited neighbors. depth-first search tree¶A tree that can be defined by the operation of a depth-first search (DFS) on a graph. This tree would consist of the nodes of the graph and a subset of the edges of the graph that was followed during the DFS. dequeue¶A specialized term used to indicate removing an element from a queue. dereference¶Accessing the value of the pointee for some reference variable. Commonly, this happens in a language like Java when using the “dot” operator to access some field of an object. derivation¶In formal languages, the process of executing a series of production rules from a grammar. A typical example of a derivation would be the series of productions executed to go from the start symbol to a given string. descendant¶In a tree, the set of all nodes that have a node \\(A\\) as an ancestor are the descendants of \\(A\\). In other words, all of the nodes that can be reached from \\(A\\) by progressing downwards in tree. Another way to say it is: The children of \\(A\\), their children, and so on. deserialization¶The process of returning a serialized representation for a data structure back to its original in-memory form. design pattern¶An abstraction for describing the design of programs, that is, the interactions of objects and classes. Experienced software designers learn and reuse patterns for combining software components, and design patterns allow this design knowledge to be passed on to new programmers more quickly. deterministic¶Any finite automata in which, for every pair of state and symbol, there is only a single transition. This means that whenever the machine is in a given state and sees a given symbol, only a single thing can happen. This is in contrast to a non-deterministic finite automata, which has at least one state with multiple transitions on at least one symbol. deterministic algorithm¶An algorithm that does not involve any element of randomness, and so its behavior on a given input will always be the same. This is in contrast to a randomized algorithm. Deterministic Finite Automata¶Deterministic Finite Acceptor¶DFA¶An automata or abstract machine that can process an input string (shown on a tape) from left to right. There is a control unit (with states), behavior defined for what to do when in a given state and with a given symbol on the current square of the tape. All that we can “do” is change state before going to the next letter to the right. DFS¶Abbreviation for depth-first search. diagonalization argument¶A proof technique for proving that a set is uncountably infinite. The approach is to show that, no matter what order the elements of the set are put in, a new element of the set can be constructed that is not in that ordering. This is done by changing the \\(i\\) th value or position of the element to be different from that of the \\(i\\) th element in the proposed ordering. dictionary¶An abstract data type or interface for a data structure or software subsystem that supports insertion, search, and deletion of records. dictionary search¶A close relative of an interpolation search. In a classical (paper) dictionary of words in a natural language, there are markings for where in the dictionary the words with a given letter start. So in typical usage of such a dictionary, words are found by opening the dictionary to some appropriate place within the pages that contain words starting with that letter. digraph¶Abbreviation for directed graph. Dijkstra’s algorithm¶An algorithm to solve the single-source shortest paths problem in a graph. This is a greedy algorithm. It is nearly identical to Prim’s algorithm for finding a minimal-cost spanning tree, with the only difference being the calculation done to update the best-known distance. diminishing increment sort¶Another name for Shellsort. direct access¶A storage device, such as a disk drive, that has some ability to move to a desired data location more-or-less directly. This is in contrast to a sequential access storage device such as a tape drive. direct proof¶In general, a direct proof is just a “logical explanation”. A direct proof is sometimes referred to as an argument by deduction. This is simply an argument in terms of logic. Often written in English with words such as “if … then”, it could also be written with logic notation such as \\(P \\Rightarrow Q\\). directed acyclic graph¶A graph with no cycles. Abbreviated as DAG. Note that a DAG is not necessarily a tree since a given node might have multiple parents. directed edge¶An edge that goes from vertex to another. In contrast, an undirected edge simply links to vertices without a direction. directed graph¶A graph whose edges each are directed from one of its defining vertices to the other. dirty bit¶Within a buffer pool, a piece of information associated with each buffer that indicates whether the contents of the buffer have changed since being read in from backing storage. When the buffer is flushed from the buffer pool, the buffer’s contents must be written to the backing storage if the dirty bit is set (that is, if the contents have changed). This means that a relatively expensive write operation is required. In contrast, if the dirty bit is not set, then it is unnecessary to write the contents to backing storage, thus saving time over not keeping track of whether the contents have changed or not. Discrete Fourier Transform¶DFT¶Let \\(a = [a_0, a_1, ..., a_{n-1}]^T\\) be a vector that stores the coefficients for a polynomial being evaluated. We can then do the calculations to evaluate the polynomial at the \\(n\\) th \\(roots of unity <nth roots of unit>\\) by multiplying the \\(A_{z}\\) matrix by the coefficient vector. The resulting vector \\(F_{z}\\) is called the Discrete Fourier Transform (or DFT) for the polynomial. discriminator¶A part of a multi-dimensional search key. Certain tree data structures such as the bintree and the kd tree operate by making branching decisions at nodes of the tree based on a single attribute of the multi-dimensional key, with the attribute determined by the level of the node in the tree. For example, in 2 dimensions, nodes at the odd levels in the tree might branch based on the \\(x\\) value of a coordinate, while at the even levels the tree would branch based on the \\(y\\) value of the coordinate. Thus, the \\(x\\) coordinate is the discriminator for the odd levels, while the \\(y\\) coordinate is the discriminator for the even levels. disjoint¶Two parts of a data structure or two collections with no objects in common are disjoint. This term is often used in conjunction with a data structure that has nodes (such as a tree). Also used in the context of sets, where two subsets are disjoint if they share no elements. disjoint sets¶A collection of sets, any pair of which share no elements in common. A collection of disjoint sets partitions some objects such that every object is in exactly one of the disjoint sets. disk access¶The act of reading data from a disk drive (or other form of peripheral storage). The number of times data must be read from (or written to) a disk is often a good measure of cost for an algorithm that involves disk I/O, since this is usually the dominant cost. disk controller¶The control mechanism for a disk drive. Responsible for the action of reading or writing a sector of data. disk drive¶An example of peripheral storage or secondary storage. Data access times are typically measured in thousandths of a second (milliseconds), which is roughly a million times slower than access times for RAM, which is an example of a primary storage device. Reads from and writes to a disk drive are always done in terms of some minimum size, which is typically called a block. The block size is 512 bytes on most disk drives. Disk drives and RAM are typical parts of a computer’s memory hierarchy. disk I/O¶Refers to the act of reading data from or writing data to a disk drive. All disk reads and writes are done in units of a sector or block. disk-based space/time tradeoff¶In contrast to the standard space/time tradeoff, this principle states that the smaller you can make your disk storage requirements, the faster your program will run. This is because the time to read information from disk is enormous compared to computation time, so almost any amount of additional computation needed to unpack the data is going to be less than the disk-reading time saved by reducing the storage requirements. distance¶In graph representations, a synonym for weight. divide and conquer¶A technique for designing algorithms where a solution is found by breaking the problem into smaller (similar) subproblems, solving the subproblems, then combining the subproblem solutions to form the solution to the original problem. This process is often implemented using recursion. divide-and-conquer recurrences¶A common form of recurrence relation that have the form \\[{\\bf T}(n) = a{\\bf T}(n/b) + cn^k; \\quad {\\bf T}(1) = c\\] where \\(a\\), \\(b\\), \\(c\\), and \\(k\\) are constants. In general, this recurrence describes a problem of size \\(n\\) divided into \\(a\\) subproblems of size \\(n/b\\), while \\(cn^k\\) is the amount of work necessary to combine the partial solutions. divide-and-guess¶A technique for finding a closed-form solution to a summation or recurrence relation. domain¶The set of possible inputs to a function. double buffering¶The idea of using multiple buffers to allow the CPU to operate in parallel with a peripheral storage device. Once the first buffer’s worth of data has been read in, the CPU can process this while the next block of data is being read from the peripheral storage. For this idea to work, the next block of data to be processed must be known or predicted with reasonable accuracy. double hashing¶A collision resolution method. A second hash function is used to generate a value \\(c\\) on the key. That value is then used by this key as the step size in linear probing by steps. Since different keys use different step sizes (as generated by the second hash function), this process avoids the clustering caused by standard linear probing by steps. double rotation¶A type of rebalancing operation used by the Splay Tree and AVL Tree. doubly linked list¶A linked list implementation variant where each list node contains access pointers to both the previous element and the next element on the list. DSA¶Abbreviation for Data Structures and Algorithms. dynamic¶Something that is changes (in contrast to static). In computer programming, dynamic normally refers to something that happens at run time. For example, run-time analysis is analysis of the program’s behavior, as opposed to its (static) text or structure Dynamic binding or dynamic memory allocation occurs at run time. dynamic allocation¶The act of creating an object from free store. In C++, Java, and JavaScript, this is done using the new operator. dynamic array¶Arrays, once allocated, are of fixed size. A dynamic array puts an interface around the array so as to appear to allow the array to grow and shrink in size as necessary. Typically this is done by allocating a new copy, copying the contents of the old array, and then returning the old array to free store. If done correctly, the amortized cost for dynamically resizing the array can be made constant. In some programming languages such as Java, the term vector is used as a synonym for dynamic array. dynamic memory allocation¶A programming technique where linked objects in a data structure are created from free store as needed. When no longer needed, the object is either returned to free store or left as garbage, depending on the programming language. dynamic programming¶An approach to designing algorithms that works by storing a table of results for subproblems. A typical cause for excessive cost in recursive algorithms is that different branches of the recursion might solve the same subproblem. Dynamic programming uses a table to store information about which subproblems have already been solved, and uses the stored information to immediately give the answer for any repeated attempts to solve that subproblem. edge¶The connection that links two nodes in a tree, linked list, or graph. edit distance¶Given strings \\(S\\) and \\(T\\), the edit distance is a measure for the number of editing steps required to convert \\(S\\) into \\(T\\). efficient¶A solution is said to be efficient if it solves the problem within the required resource constraints. A solution is sometimes said to be efficient if it requires fewer resources than known alternatives, regardless of whether it meets any particular requirements. element¶One value or member in a set. empirical comparison¶An approach to comparing to things by actually seeing how they perform. Most typically, we are referring to the comparison of two programs by running each on a suite of test data and measuring the actual running times. Empirical comparison is subject to many possible complications, including unfair selection of test data, and inaccuracies in the time measurements due to variations in the computing environment between various executions of the programs. empty¶For a container class, the state of containing no elements. encapsulation¶In programming, the concept of hiding implementation details from the user of an ADT, and protecting data members of an object from outside access. enqueue¶A specialized term used to indicate inserting an element onto a queue. entry-sequenced file¶A file that stores records in the order that they were added to the file. enumeration¶The process by which a traversal lists every object in the container exactly once. Thus, a traversal that prints the nodes is said to enumerate the nodes. An enumeration can also refer to the actual listing that is produced by the traversal (as well as the process that created that listing). equidistribution property¶In random number theory, this means that a given series of random numbers cannot be described more briefly than simply listing it out. equivalence class¶An equivalence relation can be used to partition a set into equivalence classes. equivalence relation¶Relation \\(R\\) is an equivalence relation on set \\(\\mathbf{S}\\) if it is reflexive, symmetric, and transitive. equivalent¶In the study of formal langauges, two entities are equivalent if they accept the same language. That is, entities \\(M_1\\) and \\(M_2\\) are equivalent if \\(L(M_1) = L(M_2)\\). Two mechanisms for representing or recognizing languages are equivalent if the collection of languages that each accept are the same. For example, DFAs and NFAs are equivalent because every DFA is technically an NFA, and every NFA can be converted into a DFA. estimation¶As a technical skill, this is the process of generating a rough estimate in order to evaluate the feasibility of a proposed solution. This is sometimes known as “back of the napkin” or “back of the envelope” calculation. The estimation process can be formalized as (1) determine the major parameters that affect the problem, (2) derive an equation that relates the parameters to the problem, then (3) select values for the parameters and apply the equation to yield an estimated solution. evaluation¶The act of finding the value for a polynomial at a given point. exact-match query¶Records are accessed by unique identifier. exceptions¶Exceptions are techniques used to predict possible runtime errors and handle them properly. exchange¶A swap of adjacent records in an array. exchange sort¶A sort that relies solely on exchanges (swaps of adjacent records) to reorder the list. Insertion Sort and Bubble Sort are examples of exchange sorts. All exchange sorts require \\(\\Theta(n^2)\\) time in the worst case. expanding the recurrence¶A technique for solving a recurrence relation. The idea is to replace the recursive part of the recurrence with a copy of recurrence. exponential growth rate¶A growth rate function where \\(n\\) (the input size) appears in the exponent. For example, \\(2^n\\). expression tree¶A tree structure meant to represent a mathematical expression. Internal nodes of the expression tree are operators in the expression, with the subtrees being the sub-expressions that are its operand. All leaf nodes are operands. extent¶A physically contiguous block of sectors on a disk drive that are all part of a given disk file. The fewer extents needed to store the data for a disk file, generally the fewer seek operations that will be required to process a series of disk access operations on that file. external fragmentation¶A condition that arises when a series of memory requests result in lots of small free blocks, no one of which is useful for servicing typical requests. external sort¶A sorting algorithm that is applied to data stored in peripheral storage such as on a disk drive. This is in contrast to an internal sort that works on data stored in main memory. factorial¶The factorial function is defined as \\(f(n) = n f(n-1)\\) for \\(n > 0\\). failure policy¶In a memory manager, a failure policy is the response that takes place when there is no way to satisfy a memory request from the current free blocks in the memory pool. Possibilities include rejecting the request, expanding the memory pool, collecting garbage, and reorganizing the memory pool (to collect together free space). family of languages¶Given some class or type of finite automata (for example, the deterministic finite automata), the set of languages accepted by that class of finite automata is called a family. For example, the regular languages is a family defined by the DFAs. FIFO¶Abbreviation for “first-in, first-out”. This is the access paradigm for a queue, and an old terminology for the queue is “FIFO list”. file allocation table¶A legacy file system architecture orginially developed for DOS and then used in Windows. It is still in use in many small-scale peripheral devices such as USB memory sticks and digital camera memory. file manager¶A part of the operating system responsible for taking requests for data from a logical file and mapping those requests to the physical location of the data on disk. file processing¶The domain with Computer Science that deals with processing data stored on a disk drive (in a file), or more broadly, dealing with data stored on any peripheral storage device. Two fundamental properties make dealing with data on a peripheral device different from dealing with data in main memory: (1) Reading/writing data on a peripheral storage device is far slower than reading/writing data to main memory (for example, a typical disk drive is about a million times slower than RAM). (2) All I/O to a peripheral device is typically in terms of a block of data (for example, nearly all disk drives do all I/O in terms of blocks of 512 bytes). file structure¶The organization of data on peripheral storage, such as a disk drive or DVD drive. final state¶A required element of any acceptor. When computation on a string ends in a final state, then the machine accepts the string. Otherwise the machine rejects the string. FIND¶One half of the UNION/FIND algorithm for managing disjoint sets. It is the process of moving upwards in a tree to find the tree’s root. Finite State Acceptor¶A simple type of finite state automata, an acceptor’s only ability is to accept or reject a string. So, a finite state acceptor does not have the ability to modify the input tape. If computation on the string ends in a final state, then the the string is accepted, otherwise it is rejected. Finite State Machine¶FSM¶Finite State Automata¶FSA¶Finite Automata¶Any abstract state machine, generally represented as a graph where the nodes are the states, and the edges represent transitions between nodes that take place when the machine is in that node (state) and sees an appropriate input. See, as an example, Deterministic Finite Automata. first fit¶In a memory manager, first fit is a heuristic for deciding which free block to use when allocating memory from a memory pool. First fit will always allocate the first free block on the free block list that is large enough to service the memory request. The advantage of this approach is that it is typically not necessary to look at all free blocks on the free block list to find a suitable free block. The disadvantage is that it is not “intelligently” selecting what might be a better choice of free block. fixed-length coding¶Given a collection of objects, a fixed-length coding scheme assigns a code to each object in the collection using codes that are all of the same length. Standard ASCII and Unicode representations for characters are both examples of fixed-length coding schemes. This is in contrast to variable-length coding. floor¶Written \\(\\lfloor x \\rfloor\\), for real value \\(x\\) the floor is the greatest integer \\(\\leq x\\). Floyd’s algorithm¶An algorithm to solve the all-pairs shortest paths problem. It uses the dynamic programming algorithmic technique, and runs in \\(\\Theta(n^3)\\) time. As with any dynamic programming algorithm, the key issue is to avoid duplicating work by using proper bookkeeping on the algorithm’s progress through the solution space. The basic idea is to first find all the direct edge costs, then improving those costs by allowing paths through vertex 0, then the cheapest paths involving paths going through vertices 0 and 1, and so on. flush¶The act of removing data from a cache, most typically because other data considered of higher future value must replace it in the cache. If the data being flushed has been modified since it was first read in from secondary storage (and the changes are meant to be saved), then it must be written back to that secondary storage. The the context of a buffer pool, the process of removing the contents stored in a buffer when that buffer is required in order to store new data. If the buffer’s contents have been changed since having been read in from backing storage (this fact would normally be tracked by using a dirty bit), then they must be copied back to the backing storage before the buffer can be reused. flyweight¶A design pattern that is meant to solve the following problem: You have an application with many objects. Some of these objects are identical in the information that they contain, and the role that they play. But they must be reached from various places, and conceptually they really are distinct objects. Because there is so much duplication of the same information, we want to reduce memory cost by sharing that space. For example, in document layout, the letter “C” might be represented by an object that describes that character’s strokes and bounding box. However, we do not want to create a separate “C” object everywhere in the document that a “C” appears. The solution is to allocate a single copy of the shared representation for “C” objects. Then, every place in the document that needs a “C” in a given font, size, and typeface will reference this single copy. The various instances of references to a specific form of “C” are called flyweights. Flyweights can also be used to implement the empty leaf nodes of the bintree and PR quadtree. folding method¶In hashing, an approach to implementing a hash function. Most typically used when the key is a string, the folding method breaks the string into pieces (perhaps each letter is a piece, or a small series of letters is a piece), converts the letter(s) to an integer value (typically by using its underlying encoding value), and summing up the pieces. Ford and Johnson sort¶A sorting algorithm that is close to the theoretical minimum number of key comparisons necessary to sort. Generally not considered practical in practice due to the fact that it is not efficient in terms of the number of records that need to be moved. It consists of first sorting pairs of nodes into winners and losers (of the pairs comparisons), then (recursively) sorting the winners of the pairs, and then finally carefully selecting the order in which the losers are added to the chain of sorted items. forest¶A collection of one or more trees. free block¶A block of unused space in a memory pool. free block list¶In a memory manager, the list that stores the necessary information about the current free blocks. Generally, this is done with some sort of linked list, where each node of the linked list indicates the start position and length of the free block in the memory pool. free store¶Space available to a program during runtime to be used for dynamic allocation of objects. The free store is distinct from the runtime stack. The free store is sometimes referred to as the heap, which can be confusing because heap more often refers to a specific data structure. Most programming languages provide functions to allocate (and maybe to deallocate) objects from the free store, such as new in C++ and Java. free tree¶A connected, undirected graph with no simple cycles. An equivalent definition is that a free tree is connected and has \\(|\\mathbf{V}| - 1\\) edges. freelist¶A simple and faster alternative to using free store when the objects being dynamically allocated are all of the same size (and thus are interchangeable). Typically implemented as a linked stack, released objects are put on the front of the freelist. When a request is made to allocate an object, the freelist is checked first and it provides the object if possible. If the freelist is empty, then a new object is allocated from free store. frequency count¶A heuristic used to maintain a self-organizing list. Under this heuristic, a count is maintained for every record. When a record access is made, its count is increased. If this makes its count greater than that of another record in the list, it moves up toward the front of the list accordingly so as to keep the list sorted by frequency. Analogous to the least frequently used heuristic for maintaining a buffer pool. full binary tree theorem¶This theorem states that the number of leaves in a non-empty full binary tree is one more than the number of internal nodes. Equivalently, then number of null pointers in a standard pointer-based implementation for binary tree nodes is one more than the number of nodes in the binary tree. full tree¶A binary tree is full if every node is either a leaf node or else it is an internal node with two non-empty children. function¶In mathematics, a matching between inputs (the domain) and outputs (the range). In programming, a subroutine that takes input parameters and uses them to compute and return a value. In this case, it is usually considered bad practice for a function to change any global variables (doing so is called a side effect). garbage¶In memory management, any memory that was previously (dynamically) allocated by the program during runtime, but which is no longer accessible since all pointers to the memory have been deleted or overwritten. In some languages, garbage can be recovered by garbage collection. In languages such as C and C++ that do not support garbage collection, so creating garbage is considered a memory leak. garbage collection¶Languages with garbage collection such Java, JavaScript, Lisp, and Scheme will periodically reclaim garbage and return it to free store. general tree¶A tree in which any given node can have any number of children. This is in contrast to, for example, a binary tree where each node has a fixed number of children (some of which might be null). General tree nodes tend to be harder to implement for this reason. grammar¶A formal definition for what strings make up a language, in terms of a set of production rules. graph¶A graph \\(\\mathbf{G} = (\\mathbf{V}, \\mathbf{E})\\) consists of a set of vertices \\(\\mathbf{V}\\) and a set of edges \\(\\mathbf{E}\\), such that each edge in \\(\\mathbf{E}\\) is a connection between a pair of vertices in \\(\\mathbf{V}\\). greedy algorithm¶An algorithm that makes locally optimal choices at each step. growth rate¶In algorithm analysis, the rate at which the cost of the algorithm grows as the size of its input grows. guess-and-test¶A technique used when trying to determine the closed-form solution for a summation or recurrence relation. Given a hypothesis for the closed-form solution, if it is correct, then it is often relatively easy to prove that using induction. guided traversal¶A tree traversal that does not need to visit every node in the tree. An example would be a range query in a BST. halt state¶In a finite automata, a designated state which causes the machine to immediately halt when it is entered. halted configuration¶A halted configuration occurs in a Turing machine when the machine transitions into the halt state. halting problem¶The halting problem is to answer this question: Given a computer program \\(P\\) and an input \\(I\\), will program \\(P\\) halt when executed on input \\(I\\)? This problem has been proved impossible to solve in the general case. Thus, it is an example of an unsolveable problem. handle¶When using a memory manager to store data, the client will pass data to be stored (the message) to the memory manager, and the memory manager will return to the client a handle. The handle encodes the necessary information that the memory manager can later use to recover and return the message to the client. This is typically the location and length of the message within the memory pool. hanging configuration¶A hanging configuration occurs in a Turing machine when the I/O head moves to the left from the left-most square of the tape, or when the machine goes into an infinite loop. happy path testing¶Testing “proper” inputs or uses of a program. hard algorithm¶“Hard” is traditionally defined in relation to running time, and a “hard” algorithm is defined to be an algorithm with exponential running time. hard problem¶“Hard” is traditionally defined in relation to running time, and a “hard” problem is defined to be one whose best known algorithm requires exponential running time. harmonic series¶The sum of reciprocals from 1 to \\(n\\) is called the Harmonic Series, and is written \\({\\cal H}_n\\). This sum has a value between \\(\\log_e n\\) and \\(\\log_e n + 1\\). hash function¶In a hash system, the function that converts a key value to a position in the hash table. The hope is that this position in the hash table contains the record that matches the key value. hash system¶The implementation for search based on hash lookup in a hash table. The search key is processed by a hash function, which returns a position in a hash table, which hopefully is the correct position in which to find the record corresponding to the search key. hash table¶The data structure (usually an array) that stores data records for lookup using hashing. hashing¶A search method that uses a hash function to convert a search key value into a position within a hash table. In a properly implemented hash system, that position in the table will have high probability of containing the record that matches the key value. Sometimes, the hash function will return an position that does not store the desired key, due to a process called collision. In that case, the desired record is found through a process known as collision resolution. head¶The beginning of a list. header node¶Commonly used in implementations for a linked list or related structure, this node preceeds the first element of the list. Its purpose is to simplify the code implementation by reducing the number of special cases that must be programmed for. heap¶This term has two different meanings. Uncommonly, it is a synonym for free store. Most often it is used to refer to a particular data structure. This data structure is a complete binary tree with the requirement that every node has a value greater than its children (called a max heap), or else the requirement that every node has a value less than its children (called a min heap). Since it is a complete binary tree, a heap is nearly always implemented using an array rather than an explicit tree structure. To add a new value to a heap, or to remove the extreme value (the max value in a max-heap or min value in a min-heap) and update the heap, takes \\(\\Theta(\\log n)\\) time in the worst case. However, if given all of the values in an unordered array, the values can be re-arranged to form a heap in only \\(\\Theta(n)\\) time. Due to its space and time efficiency, the heap is a popular choice for implementing a priority queue. heapsort¶A sorting algorithm that costs \\(\\Theta(n \\log n)\\) time in the best, average, and worst cases. It tends to be slower than Mergesort and Quicksort. It works by building a max heap, and then repeatedly removing the item with maximum key value (moving it to the end of the heap) until all elements have been removed (and replaced at their proper location in the array). height¶The height of a tree is one more than the depth of the deepest node in the tree. height balanced¶The condition the depths of each subtree in a tree are roughly the same. heuristic¶A way to solve a problem that is not guarenteed to be optimal. While it might not be guarenteed to be optimal, it is generally expected (by the agent employing the heuristic) to provide a reasonably efficient solution. heuristic algorithm¶A type of approximation algorithm, that uses a heuristic to find a good, but not necessarily cheapest, solution to an optimization problem. home position¶In hashing, a synonym for home slot. home slot¶In hashing, this is the slot in the hash table determined for a given key by the hash function. homogeneity¶In a container class, this is the property that all objects stored in the ncontainer are of the same class. For example, if you have a list intended to store Payroll records, is it possible for the programmer to insert an integer onto the list instead? Huffman codes¶The codes given to a collection of letters (or other symbols) through the process of Huffman coding. Huffman coding uses a Huffman coding tree to generate the codes. The codes can be of variable length, such that the letters which are expected to appear most frequently are shorter. Huffman coding is optimal whenever the true frequencies are known, and the frequency of a letter is independent of the context of that letter in the message. Huffman coding tree¶A Huffman coding tree is a full binary tree that is used to represent letters (or other symbols) efficiently. Each letter is associated with a node in the tree, and is then given a Huffman code based on the position of the associated node. A Huffman coding tree is an example of a binary trie. Huffman tree¶Shorter form of the term Huffman coding tree. I/O head¶On a disk drive (or similar device), the part of the machinery that actually reads data from the disk. image-space decomposition¶A from of key-space decomposition where the key space splitting points is predetermined (typically by splitting in half). For example, a Huffman coding tree splits the letters being coded into those with codes that start with 0 on the left side, and those with codes that start with 1 on the right side. This regular decomposition of the key space is the basis for a trie data structure. An image-space decomposition is in opposition to an object-space decomposition. in degree¶In graph terminology, the in degree for a vertex is the number of edges directed into the vertex. incident¶In graph terminology, an edge connecting two vertices is said to be incident with those vertices. The two vertices are said to be adjacent. index file¶A file whose records consist of key-value pairs where the pointers are referencing the complete records stored in another file. indexing¶The process of associating a search key with the location of a corresponding data record. The two defining points to the concept of an index is the association of a key with a record, and the fact that the index does not actually store the record itself but rather it stores a reference to the record. In this way, a collection of records can be supported by multiple indices, typically a separate index for each key field in the record. induction hypothesis¶The key assumption used in a proof by induction, that the theorem to be proved holds for smaller instances of the theorem. The induction hypothesis is equivalent to the recursive call in a recursive function. induction step¶Part of a proof by induction. In its simplest form, this is a proof of the implication that if the theorem holds for $n-1$, then it holds for $n$. As an alternative, see strong induction. induction variable¶The variable used to parameterize the theorem being proved by induction. For example, if we seek to prove that the sum of the integers from 1 to $n$ is $n(n+1)/2$, then $n$ is the induction variable. An induction variable must be an integer. information theoretic lower bound¶A lower bound on the amount of resources needed to solve a problem based on the number of bits of information needed to uniquely specify the answer. Sometimes referred to as a “Shannon theoretic lower bound” due to Shannon’s work on information theory and entropy. An example is that sorting has a lower bound of \\(\\Omega(\\log_2 n!)\\) because there are \\(n!\\) possible orderings for \\(n\\) values. This observation alone does not make the lower bound tight, because it is possible that no algorithm could actually reach the information theory lower limit. inherit¶In object-oriented programming, the process by which a subclass gains data members and methods from a base class. initial state¶A synonym for start state. inode¶Short for “index node”. In UNIX-style file systems, specific disk sectors that hold indexing information to define the layout of the file system. inorder traversal¶In a binary tree, a traversal that first recursively visits the left child, then visits the root, and then recursively visits the right child. In a binary search tree, this traversal will enumerate the nodes in sorted order. Insertion Sort¶A sorting algorithm with \\(\\Theta(n^2)\\) average and worst case cost, and \\(Theta(n)\\) best case cost. This best case cost makes it useful when we have reason to expect the input to be nearly sorted. instance variable¶In object-oriented programming, a synonym for data member. integer function¶Any function whose input is an integer and whose output is an integer. It can be proved by diagonalization that the set of integer functions is uncountably infinite. inter-sector gap¶On a disk drive, a physical gap in the data that occurs between the sectors. This allows the I/O head detect the end of the sector. interface¶An interface is a class-like structure that only contains method signatures and fields. An interface does not contain an implementation of the methods or any data members. intermediate code¶A step in a typical compiler is to transform the original high-level language into a form on which it is easier to do other stages of the process. For example, some compilers will transform the original high-level source code into assembly code on which it can do code optimization, before translating it into its final executable form. intermediate code generation¶A phase in a compiler, that walks through a parse tree to produce simple assembly code. internal fragmentation¶A condition that occurs when more than \\(m\\) bytes are allocated to service a memory request for \\(m\\) bytes, wasting free storage. This is often done to simplify memory management. internal node¶In a tree, any node that has at least one non-empty child is an internal node. internal sort¶A sorting algorithm that is applied to data stored in main memory. This is in contrast to an external sort that is meant to work on data stored in peripheral storage such as on a disk drive. interpolation¶The act of finding the coefficients of a polynomial, given the values at some points. A polynomal of degree \\(n-1\\) requires \\(n\\) points to interpolate the coefficients. interpolation search¶Given a sorted array, and knowing the first and last key values stored in some subarray known to contain search key \\(K\\), interpolation search will compute the expected location of \\(K\\) in the subarray as a fraction of the distance between the known key values. So it will next check that computed location, thus narrowing the search for the next iteration. Given reasonable key value distribution, the average case for interpolation search will be \\(\\Theta(\\log \\log n)\\), or better than the expected cost of binary search. Nonetheless, binary search is expected to be faster in nearly all practical situations due to the small difference between the two costs, combined with the higher constant factors required to implement interpolation search as compared to binary search. interpreter¶In contrast to a compiler that translates a high-level program into something that can be repeatedly executed to perform a computation, an interpreter directly performs computation on the high-level langauge. This tends to make the computation much slower than if it were performed on the directly executable version produced by a compiler. inversion¶A measure of how disordered a series of values is. For each element \\(X\\) in the series, count one inversion for each element to left of \\(X\\) that is greater than the value of \\(X\\) (and so must ultimately be moved to the right of \\(X\\) during a sorting process). inverted file¶Synonym for inverted list when the inverted list is stored in a disk file. inverted list¶An index which links secondary keys to either the associated primary key or the actual record in the database. irreflexive¶In set notation, binary relation \\(R\\) on set \\(S\\) is irreflexive if \\(aRa\\) is never in the relation for any \\(a \\in \\mathbf{S}\\). ISAM¶Indexed Sequential Access Method: an obsolete method for indexing data for (at the time) fast retrieval. More generally, the term is used also to generically refer to an index that supports both sequential and keyed access to data records. Today, that would nearly always be implemented using a B-Tree. iterator¶In a container such as a List, a separate class that indicates position within the container, with support for traversing through all elements in the container. job¶Common name for processes or tasks to be run by an operating system. They typically need to be processed in order of importance, and so are kept organized by a priority queue. Another common use for this term is for a collection of tasks to be ordered by a topological sort. jump search¶An algorithm for searching a sorted list, that falls between sequential search and binary search in both computational cost and conceptual complexity. The idea is to keep jumping by some fixed number of positions until a value is found that is bigger than search key \\(K\\), then do a sequential search over the subarray that is now known to contain the search key. The optimal number of steps to jump will be \\(\\sqrt{n}\\) for an array of size \\(n\\), and the worst case cost will be \\(\\Theta(\\sqrt{n})\\). K-ary tree¶A type of full tree where every internal node has exactly \\(K\\) children. k-path¶In Floyd’s algorithm, a k-path is a path between two vertices \\(i\\) and \\(j\\) that can only go through vertices with an index value less than or equal to \\(k\\). kd tree¶A spatial data structure that uses a binary tree to store a collection of data records based on their (point) location in space. It uses the concept of a discriminator at each level to decide which single component of the multi-dimensional search key to branch on at that level. It uses a key-space decomposition, meaning that all data records in the left subtree of a node have a value on the corresponding discriminator that is less than that of the node, while all data records in the right subtree have a greater value. The bintree is the image-space decomposition analog of the kd tree. key¶A field or part of a larger record used to represent that record for the purpose of searching or comparing. Another term for search key. key sort¶Any sorting operation applied to a collection of key-value pairs where the value in this case is a reference to a complete record (that is, a pointer to the record in memory or a position for a record on disk). This is in contrast to a sorting operation that works directly on a collection of records. The intention is that the collection of key-value pairs is far smaller than the collection of records themselves. As such, this might allow for an internal sort when sorting the records directly would require an external sort. The collection of key-value pairs can also act as an index. key space¶The range of values that a key value may take on. key-space decomposition¶The idea that the range for a search key will be split into pieces. There are two general approaches to this: object-space decomposition and image-space decomposition. key-value pair¶A standard solution for solving the problem of how to relate a key value to a record (or how to find the key for a given record) within the context of a particular index. The idea is to simply store as records in the index pairs of keys and records. Specifically, the index will typically store a copy of the key along with a reference to the record. The other standard solution to this problem is to pass a comparator function to the index. knapsack problem¶While there are many variations of this problem, here is a typical version: Given knapsack of a fixed size, and a collection of objects of various sizes, is there a subset of the objects that exactly fits into the knapsack? This problem is known to be NP-complete, but can be solved for problem instances in practical time relatively quickly using dynamic programming. Thus, it is considered to have pseudo-polynomial cost. An optimization problem version is to find the subset that can fit with the greatest amount of items, either in terms of their total size, or in terms of the sum of values associated with each item. Kruskal’s algorithm¶An algorithm for computing the MCST of a graph. During processing, it makes use of the UNION/FIND process to efficiently determine of two vertices are within the same subgraph. labeled graph¶A graph with labels associated with the nodes. language¶A subset of the strings that can be generated from a given alphabet. Las Vegas algorithms¶A form of randomized algorithm. We always find the maximum value, and “usually” we find it fast. Such algorithms have a guaranteed result, but do not guarantee fast running time. leaf node¶In a binary tree, leaf node is any node that has two empty children. (Note that a binary tree is defined so that every node has two children, and that is why the leaf node has to have two empty children, rather than no children.) In a general tree, any node is a leaf node if it has no children. least frequently used¶Abbreviated LFU, it is a heuristic that can be used to decide which buffer in a buffer pool to flush when data in the buffer pool must be replaced by new data being read into a cache. However, least recently used is more popular than LFU. Analogous to the frequency count heuristic for maintaining a self-organizing list. least recently used¶Abbreviated LRU, it is a popular heuristic to use for deciding which buffer in a buffer pool to flush when data in the buffer pool must be replaced by new data being read into a cache. Analogous to the move-to-front heuristic for maintaining a self-organizing list. left recursive¶In automata theory, a production is left recursive if it is of the form \\(A \\rightarrow Ax\\), \\(A \\in V, x \\in (V \\cup T)^*\\) where \\(V\\) is the set of non-terminals and \\(T\\) is the set of terminals in the grammar. length¶In a list, the number of elements. In a string, the number of characters. level¶In a tree, all nodes of depth \\(d\\) are at level \\(d\\) in the tree. The root is the only node at level 0, and its depth is 0. lexical analysis¶A phase of a compiler or interpreter responsible for reading in characters of the program or language and grouping them into tokens. lexical scoping¶Within programming languages, the convention of allowing access to a variable only within the block of code in which the variable is defined. A synonym for static scoping. LFU¶Abbreviation for least frequently used. lifetime¶For a variable, lifetime is the amount of time it will exist before it is destroyed. LIFO¶Abbreviation for “Last-In, First-Out”. This is the access paradigm for a stack, and an old terminolgy for the stack is “LIFO list”. linear congruential method¶In random number theory, a process for computing the next number in a pseudo-random sequence. Starting from a seed, the next term \\(r(i)\\) in the series is calculated from term \\(r(i-1)\\) by the equation \\[r(i) = (r(i-1)\\times b) \\bmod t\\] where \\(b\\) and \\(t\\) are constants. These constants must be well chosen for the resulting series of numbers to have desireable properties as a random number sequence. linear growth rate¶For input size \\(n\\), a growth rate of \\(cn\\) (for \\(c\\) any positive constant). In other words, the cost of the associated function is linear on the input size. linear index¶A form of indexing that stores key-value pairs in a sorted array. Typically this is used for an index to a large collection of records stored on disk, where the linear index itself might be on disk or in main memory. It allows for efficient search (including for range queries), but it is not good for inserting and deleting entries in the array. Therefore, it is an ideal indexing structure when the system needs to do range queries but the collection of records never changes once the linear index has been created. linear order¶Another term for total order. linear probing¶In hashing, this is the simplest collision resolution method. Term \\(i\\) of the probe sequence is simply \\(i\\), meaning that collision resolution works by moving sequentially through the hash table from the home slot. While simple, it is also inefficient, since it quickly leads to certain free slots in the hash table having higher probability of being selected during insertion or search. linear probing by steps¶In hashing, this collision resolution method is a variation on simple linear probing. Some constant \\(c\\) is defined such that term \\(i\\) of the probe sequence is \\(ci\\). This means that collision resolution works by moving sequentially through the hash table from the home slot in steps of size \\(c\\). While not much improvement on linear probing, it forms the basis of another collision resolution method called double hashing, where each key uses a value for \\(c\\) defined by a second hash function. linear search¶Another name for sequential search. link node¶A widely used supporting object that forms the basic building block for a linked list and similar data structures. A link node contains one or more fields that store data, and a pointer or reference to another link node. linked list¶An implementation for the list ADT that uses dynamic allocation of link nodes to store the list elements. Common variants are the singly linked list, doubly linked list and circular list. The overhead required is the pointers in each link node. linked stack¶Analogous to a linked list, this uses dynamic allocation of nodes to store the elements when implementing the stack ADT. list¶A finite, ordered sequence of data items known as elements. This is close to the mathematical concept of a sequence. Note that “ordered” in this definition means that the list elements have position. It does not refer to the relationship between key values for the list elements (that is, “ordered” does not mean “sorted”). literal¶In a Boolean expression, a literal is a Boolean variable or its negation. In the context of compilers, it is any constant value. Similar to a terminal. load factor¶In hashing this is the fraction of the hash table slots that contain a record. Hash systems usually try to keep the load factor below 50%. local storage¶local storage. local variable¶A variable declared within a function or method. It exists only from the time when the function is called to when the function exits. When a function is suspended (due to calling another function), the function’s local variables are stored in an activation record on the runtime stack. locality of reference¶The concept that accesses within a collection of records is not evenly distributed. This can express itself as some small fraction of the records receiving the bulk of the accesses (80/20 rule). Alternatively, it can express itself as an increased probability that the next or future accesses will come close to the most recent access. This is the fundamental property for success of caching. logarithm¶The logarithm of base \\(b\\) for value \\(y\\) is the power to which \\(b\\) is raised to get \\(y\\). logical file¶In file processing, the programmer’s view of a random access file stored on disk as a contiguous series of bytes, with those bytes possibly combining to form data records. This is in contrast to the physical file. logical form¶The definition for a data type in terms of an ADT. Contrast to the physical form for the data type. lookup table¶A table of pre-calculated values, used to speed up processing time when the values are going to be viewed many times. The costs to this approach are the space required for the table and the time required to compute the table. This is an example of a space/time tradeoff. lower bound¶In algorithm analysis, a growth rate that is always less than or equal to the growth rate of the algorithm in question. In practice, this is the fastest-growing function that we know grows no faster than all but a constant number of inputs. It could be a gross under-estimate of the truth. Since the lower bound for the algorithm can be very different for different situations (such as the best case or worst case), we typically have to specify which situation we are referring to. lower bounds proof¶A proof regarding the lower bound, with this term most typically referring to the lower bound for any possible algorithm to solve a given problem. Many problems have a simple lower bound based on the concept that the minimum amount of processing is related to looking at all of the problem’s input. However, some problems have a higher lower bound than that. For example, the lower bound for the problem of sorting (\\(\\Omega(n \\log n)\\)) is greater than the input size to sorting (\\(n\\)). Proving such “non-trivial” lower bounds for problems is notoriously difficult. LRU¶Abbreviation for least recently used. main memory¶A synonym for primary storage. In a computer, typically this will be RAM. map¶A data structure that relates a key to a record. mapping¶A function that maps every element of a given set to a unique element of another set; a correspondence. mark array¶It is typical in graph algorithms that there is a need to track which nodes have been visited at some point in the algorithm. An array of bits or values called the mark array is often maintained for this purpose. mark/sweep algorithm¶An algorithm for garbage collection. All accessible variables, and any space that is reachable by a chain of pointers from any accessible variable, is “marked”. Then a sequential sweep of all memory in the pool is made. Any unmarked memory locations are assumed to not be needed by the program and can be considered as free to be reused. master theorem¶A theorem that makes it easy to solve divide-and-conquer recurrences. matching¶In graph theory, a pairing (or match) of various nodes in a graph. matching problem¶Any problem that involves finding a matching in a graph with some desired property. For example, a well-known NP-complete problem is to find a maximum match for an undirected graph. max heap¶A heap where every node has a key value greater than its children. As a consequence, the node with maximum key value is at the root. maximal match¶In a graph, any matching that leaves no pair of unmatched vertices that are connected. A maximal matching is not necessarily a maximum match. In other words, there might be a larger matching than the maximal matching that was found. maximum lower bound¶The lower bound for the problem of finding the maximum value in an unsorted list is \\(\\Omega(n)\\). maximum match¶In a graph, the largest possible matching. MCST¶MST¶Abbreviation for minimal-cost spanning tree. measure of cost¶When comparing two things, such as two algorithms, some event or unit must be used as the basic unit of comparison. It might be number of milliseconds needed or machine instructions expended by a program, but it is usually desirable to have a way to do comparison between two algorithms without writing a program. Thus, some other measure of cost might be used as a basis for comparison between the algorithms. For example, when comparing two sorting algorthms it is traditional to use as a measure of cost the number of comparisons made between the key values of record pairs. member¶In set notation, this is a synonym for element. In abstract design, a data item is a member of a type. In an object-oriented language, data members are data fields in an object. member function¶Each operation associated with the ADT is implemented by a member function or method. memory allocation¶In a memory manager, the act of honoring a request for memory. memory deallocation¶In a memory manager, the act of freeing a block of memory, which should create or add to a free block. memory hierarchy¶The concept that a computer system stores data in a range of storage types that range from fast but expensive (primary storage) to slow but cheap (secondary storage). When there is too much data to store in primary storage, the goal is to have the data that is needed soon or most often in the primary storage as much as possible, by using caching techniques. memory leak¶In programming, the act of creating garbage. In languages such as C and C++ that do not support garbage collection, repeated memory leaks will evenually cause the program to terminate. memory manager¶Functionality for managing a memory pool. Typically, the memory pool is viewed as an array of bytes by the memory manager. The client of the memory manager will request a collection of (adjacent) bytes of some size, and release the bytes for reuse when the space is no longer needed. The memory manager should not know anything about the interpretation of the data that is being stored by the client into the memory pool. Depending on the precise implementation, the client might pass in the data to be stored, in which case the memory manager will deal with the actual copy of the data into the memory pool. The memory manager will return to the client a handle that can later be used by the client to retrieve the data. memory pool¶Memory (usually in RAM but possibly on disk or peripheral storage device) that is logically viewed as an array of memory positions. A memory pool is usually managed by a memory manager. memory request¶In a memory manager, a request from some client to the memory manager to reserve a block of memory and store some bytes there. merge insert sort¶A synonym for the Ford and Johnson sort. Mergesort¶A sorting algorithm that requires \\(\\Theta(n \\log n)\\) in the best, average, and worst cases. Conceptually it is simple: Split the list in half, sort the halves, then merge them together. It is a bit complicated to implement efficiently on an array. message¶In a memory manager implementation (particularly a memory manager implemented with a message passing style of interface), the message is the data that the client of the memory manager wishes to have stored in the memory pool. The memory manager will reply to the client by returning a handle that defines the location and size of the message as stored in the memory pool. The client can later recover the message by passing the handle back to the memory manager. message passing¶A common approach to implementing the ADT for a memory manager or buffer pool, where the contents of a message to be stored is explicitly passed between the client and the memory manager. This is in contrast to a buffer passing approach. metaphor¶Humans deal with complexity by assigning a label to an assembly of objects or concepts and then manipulating the label in place of the assembly. Cognitive psychologists call such a label a metaphor. method¶In the object-oriented programming paradigm, a method is an operation on a class. A synonym for member function. mid-square method¶In hashing, an approach to implementing a hash function. The key value is squared, and some number of bits from the middle of the resulting value are extracted as the hash code. Some care must be taken to extract bits that tend to actually be in the middle of the resulting value, which requires some understanding of the typical key values. When done correctly, this has the advantage of having the hash code be affected by all bits of the key min heap¶A heap where every node has a key value less than its children. As a consequence, the node with minimum key value is at the root. minimal-cost spanning tree¶Abbreviated as MCST, or sometimes as MST. Derived from a weighted graph, the MCST is the subset of the graph’s edges that maintains the connectivitiy of the graph while having lowest total cost (as defined by the sum of the weights of the edges in the MCST). The result is referred to as a tree because it would never have a cycle (since an edge could be removed from the cycle and still preserve connectivity). Two algorithms to solve this problem are Prim’s algorithm and Kruskal’s algorithm. minimum external path weight¶Given a collection of objects, each associated with a leaf node in a tree, the binary tree with minimum external path weight is the one with the minimum sum of weighted path lengths for the given set of leaves. This concept is used to create a Huffman coding tree, where a letter with high weight should have low depth, so that it will count the least against the total path length. As a result, another letter might be pushed deeper in the tree if it has less weight. mod¶Abbreviation for the modulus function. model¶A simplification of reality that preserves only the essential elements. With a model, we can more easily focus on and reason about these essentials. In algorithm analysis, we are especially concerned with the cost model for measuring the cost of an algorithm. modulus¶The modulus function returns the remainder of an integer division. Sometimes written \\(n \\bmod m\\) in mathematical expressions, the syntax in many programming languages is n % m. Monte Carlo algorithms¶A form of randomized algorithm. We find the maximum value fast, or we don’t get an answer at all (but fast). While such algorithms have good running time, their result is not guaranteed. move-to-front¶A heuristic used to maintain a self-organizing list. Under this heuristic, whenever a record is accessed it is moved to the front of the list. Analogous to the least recently used heuristic for maintaining a buffer pool. multi-dimensional search key¶A search key containing multiple parts, that works in conjunction with a multi-dimensional search structure. Most typically, a spatial search key representing a position in multi-dimensional (2 or 3 dimensions) space. But a multi-dimensional key could be used to organize data within non-spatial dimensions, such as temperature and time. multi-dimensional search structure¶A data structure used to support efficient search on a multi-dimensional search key. The main concept here is that a multi-dimensional search structure works more efficiently by considering the multiple parts of the search key as a whole, rather than making independent searches on each one-dimensional component of the key. A primary example is a spatial data structure that can efficiently represent and search for records in multi-dimensional space. multilist¶A list that may contain sublists. This term is sometimes used as a synonym to the term bag. natural numbers¶Zero and the positive integers. necessary fallacy¶A common mistake in a lower bounds proof for a problem, where the proof makes an inappropriate assumption that any algorithm must operate in some manner (typically in the way that some known algorithm behaves). neighbor¶In a graph, a node \\(w\\) is said to be a neighbor of node \\(v\\) if there is an edge from \\(v\\) to \\(w\\). node¶The objects that make up a linked structure such as a linked list or binary tree. Typically, nodes are allocated using dynamic memory allocation. In graph terminology, the nodes are more commonly called vertices. non-deterministic¶In a finite automata, at least one state has multiple transitions on at least one symbol. This means that it is not deterministic about what transition to take in that situation. A non-deterministic machine is said to accept a string if it completes execution on the string in an accepting state under at least one choice of non-deterministic transitions. Generally, non-determinism can be simulated with a deterministic machine by alternating between the execution that would take place under each of the branching choices. non-deterministic algorithm¶An algorithm that may operate using a non-deterministic choice operation. non-deterministic choice¶An operation that captures the concept of nondeterminism. A nondeterministic choice can be viewed as either “correctly guessing” between a set of choices, or implementing each of the choices in parallel. In the parallel view, the nondeterminism was successful if at least one of the choices leads to a correct answer. non-deterministic polynomial time algorithm¶An algorithm that runs in polynomial time, and which may (or might not) use non-deterministic choice. non-strict partial order¶In set notation, a relation that is reflexive, antisymmetric, and transitive. non-terminal¶In contrast to a terminal, a non-terminal is an abstract state in a production rule. Begining with the start symbol, all non-terminals must be converted into terminals in order to complete a derivation. Nondeterministic Finite Automata¶Nondeterministic Finite Acceptor¶NFA¶An automata or abstract machine that can process an input string (shown on a tape) from left to right. There is a control unit (with states), behavior defined for what to do when in a given state and with a given symbol on the current square of the tape. All that we can “do” is change state before going to the next letter to the right. In distinction from a DFA, a NFA may have multiple transitions from a given state on the same input symbol, or there can be a transition from a given state on the empty string. NP¶An abbreviation for non-deterministic polynomial. NP-Complete¶A class of problems that are related to each other in this way: If ever one such problem is proved to be solvable in polynomial time, or proved to require exponential time, then all other NP-Complete problems will cost likewise. Since so many real-world problems have been proved to be NP-Complete, it would be extremely useful to determine if they have polynomial or exponential cost. But so far, nobody has been able to determine the truth of the situation. A more technical definition is that a problem is NP-Complete if it is in NP and is NP-hard. NP-Completeness proof¶A type of reduction used to demonstrate that a particular problem is NP-complete. Specifically, an NP-Completeness proof must first show that the problem is in class NP, and then show (by using a reduction to another NP-Complete problem) that the problem is NP-hard. NP-hard¶A problem that is “as hard as” any other problem in NP. That is, Problem X is NP-hard if any algorithm in NP can be reduced to X in polynomial time. nth roots of unity¶All of the points along the unit circle in the complex plane that represent multiples of the primitive nth root of unity. object¶An instance of a class, that is, something that is created and takes up storage during the execution of a computer program. In the object-oriented programming paradigm, objects are the basic units of operation. Objects have state in the form of data members, and they know how to perform certain actions (methods). object-oriented programming paradigm¶An approach to problem-solving where all computations are carried out using objects. object-space decomposition¶A from of key-space decomposition where the key space is determined by the actual values of keys that are found. For example, a BST stores a key value in its root, and all other values in the tree with lesser value are in the left subtree. Thus, the root value has split (or decomposed) the key space for that key based on its value into left and right parts. An object-space decomposition is in opposition to an image-space decomposition. octree¶The three-dimensional equivalent of the quadtree would be a tree with \\(2^3\\) or eight branches. Omega notation¶In algorithm analysis, \\(\\Omega\\) notation is used to describe a lower bound. Roughly (but not completely) analogous to big-Oh notation used to define an upper bound. one-way list¶A synonym for a singly linked list. open addressing¶A synonym for closed hashing. open hash system¶A hash system where multiple records might be associated with the same slot of a hash table. Typically this is done using a linked list to store the records. This is in contrast to a closed hash system. operating system¶The control program for a computer. Its purpose is to control hardware, manage resources, and present a standard interface to these to other software components. optimal static ordering¶A theoretical construct defining the best static (non-changing) order in which to place a collection of records so as to minimize the number of records visited by a series of sequential searches. It is a useful concept for the purpose of defining a theoretical optimum against which to compare the performance for a self-organizing list heuristic. optimization problem¶Any problem where there are a (typically large) collection of potential solutions, and the goal is to find the best solution. An example is the Traveling Salesman Problem, where visiting \\(n\\) cities in some order has a cost, and the goal is to visit in the cheapest order. out degree¶In graph terminology, the out degree for a vertex is the number of edges directed out of the vertex. overflow¶The condition where the amount of data stored in an entity has exceeded its capacity. For example, a node in a B-tree can store a certain number of records. If a record is attempted to be inserted into a node that is full, then something has to be done to handle this case. overflow bucket¶In bucket hashing, this is the bucket into which a record is placed if the bucket containing the record’s home slot is full. The overflow bucket is logically considered to have infinite capacity, though in practice search and insert will become relatively expensive if many records are stored in the overflow bucket. overhead¶All information stored by a data structure aside from the actual data. For example, the pointer fields in a linked list or BST, or the unused positions in an array-based list. page¶A term often used to refer to the contents of a single buffer within a buffer pool or other virtual memory. This corresponds to a single block or sector of data from backing storage, which is the fundamental unit of I/O. parameter¶The values making up an input to a function. parent¶In a tree, the node \\(P\\) that directly links to a node \\(A\\) is the parent of \\(A\\). \\(A\\) is the child of \\(P\\). parent pointer representation¶For trees, a node implementation where each node stores only a pointer to its parent, rather than to its children. This makes it easy to go up the tree toward the root, but not down the tree toward the leaves. This is most appropriate for solving the UNION/FIND problem. parity¶The concept of matching even-ness or odd-ness, the basic idea behind using a parity bit for error detection. parity bit¶A common method for checking if transmission of a sequence of bits has been performed correctly. The idea is to count the number of 1 bits in the sequence, and set the parity bit to 1 if this number is odd, and 0 if it is even. Then, the transmitted sequence of bits can be checked to see if its parity matches the value of the parity bit. This will catch certain types of errors, in particular if the value for a single bit has been reversed. This was used, for example, in early versions of ASCII character coding. parse tree¶A tree that represents the syntactic structure of an input string, making it easy to compare against a grammar to see if it is syntactically correct. parser¶A part of a compiler that takes as input the program text (or more typically, the tokens from the scanner), and verifies that the program is syntactically correct. Typically it will build a parse tree as part of the process. partial order¶In set notation, a binary relation is called a partial order if it is antisymmetric and transitive. If the relation is also reflexive, then it is a non-strict partial order. Alternatively, if the relation is also irreflexive, then it is a strict partial order. partially ordered set¶The set on which a partial order is defined is called a partially ordered set. partition¶In Quicksort, the process of splitting a list into two sublists, such that one sublist has values less than the pivot value, and the other with values greater than the pivot. This process takes \\(\\Theta(i)\\) time on a sublist of length \\(i\\). pass by reference¶A reference to the variable is passed to the called function. So, any modifications will affect the original variable. pass by value¶A copy of a variable is passed to the called function. So, any modifications will not affect the original variable. path¶In tree or graph terminology, a sequence of vertices \\(v_1, v_2, ..., v_n\\) forms a path of length \\(n-1\\) if there exist edges from \\(v_i\\) to \\(v_{i+1}\\) for \\(1 \\leq i < n\\). path compression¶When implementing the UNION/FIND algorithm, path compression is a local optimization step that can be performed during the FIND step. Once the root of the tree for the current object has been found, the path to the root can be traced a second time, with all objects in the tree made to point directly to the root. This reduces the depth of the tree from typically \\(\\Theta(\\log n)\\) to nearly constant. peripheral storage¶Any storage device that is not part of the core processing of the computer (that is, RAM). A typical example is a disk drive. permutation¶A permutation of a sequence \\(\\mathbf{S}\\) is the elements of \\(\\mathbf{S}\\) arranged in some order. persistent¶In the context of computer memory, this refers to a memory that does not lose its stored information when the power is turned off. physical file¶The collection of sectors that comprise a file on a disk drive. This is in contrast to the logical file. physical form¶The implementation of a data type as a data structure. Contrast to the physical form for the data type. Pigeonhole Principle¶A commonly used lemma in Mathematics. A typical variant states: When \\(n+1\\) objects are stored in \\(n\\) locations, at least one of the locations must store two or more of the objects. pivot¶In Quicksort, the value that is used to split the list into sublists, one with lesser values than the pivot, the other with greater values than the pivot. platter¶In a disk drive, one of a series of flat disks that comprise the storage space for the drive. Typically, each surface (top and bottom) of each platter stores data, and each surface has its own I/O head. point quadtree¶A spatial data structure for storing point data. It is similar to a PR quadtree in that it (in two dimensions) splits the world into four parts. However, it splits using an object-space decomposition. That is, quadrant containing the point is split into four parts at the point. It is similar to the kd tree which splits alternately in each dimension, except that it splits in all dimensions at once. point-region quadtree¶Formal name for what is commonly referred to as a PR quadtree. pointee¶The term pointee refers to anything that is pointed to by a pointer or reference. pointer¶A variable whose value is the address of another variable; a link. pointer-based implementation for binary tree nodes¶A common way to implement binary tree nodes. Each node stores a data value (or a reference to a data value), and pointers to the left and right children. If either or both of the children does not exist, then a null pointer is stored. polymorphism¶An object-oriented programming term meaning one name, many forms. It describes the ability of software to change its behavior dynamically. Two basic forms exist: run-time polymorphism and compile-time polymorphism. pop¶A specialized term used to indicate removing an element from a stack. poset¶Another name for a partially ordered set. position¶The defining property of the list ADT, this is the concept that list elements are in a position. Many list ADTs support access by position. postorder traversal¶In a binary tree, a traversal that first recursively visits the left child, then recursively visits the right child, and then visits the root. potential¶A concept related to amortized analysis. Potential is the total or currently available amount of work that can be done. powerset¶For a set \\(\\mathbf{S}\\), the power set is the set of all possible subsets for \\(\\mathbf{S}\\). PR quadtree¶A type of quadtree that stores point data in two dimensions. The root of the PR quadtree represents some square region of 2d space. If that space stores more than one data point, then the region is decomposed into four equal subquadrants, each represented recursively by a subtree of the PR quadtree. Since many leaf nodes of the PR quadtree will contain no data points, implementation often makes use of the Flyweight design pattern. Related to the bintree. prefix property¶Given a collection of strings, the collection has the prefix property if no string in the collection is a prefix for another string in the collection. The significance is that, given a long string composed of members of the collection, it can be uniquely decomposed into the constituent members. An example of such a collection of strings with the prefix property is a set of Huffman codes. preorder traversal¶In a binary tree, a traversal that first visits the root, then recursively visits the left child, then recursively visits the right child. Prim’s algorithm¶A greedy algorithm for computing the MCST of a graph. It is nearly identical to Dijkstra’s algorithm for solving the single-source shortest paths problem, with the only difference being the calculation done to update the best-known distance. primary clustering¶In hashing, the tendency in certain collision resolution methods to create clustering in sections of the hash table. The classic example is linear probing. This tends to happen when a group of keys follow the same probe sequence during collision resolution. primary index¶Synonym for primary key index. primary key¶A unique identifier for a record. primary key index¶Relates each primary key value with a pointer to the actual record on disk. primary storage¶The faster but more expensive memory in a computer, most often RAM in modern computers. This is in contrast to secondary storage, which together with primary storage devices make up the computer’s memory hierarchy. primitive data type¶In Java, one of a particular group of simple types that are not implemented as objects. An example is an int. primitive element¶In set notation, this is a single element that is a member of the base type for the set. This is as opposed to an element of the set being another set. primitive nth root of unity¶The \\(n\\) th root of 1. Normally a complex number. An intuitive way to view this is one \\(n\\) th of the unit circle in the complex plain. priority¶A quantity assigned to each of a collection of jobs or tasks that indicate importance for order of processing. For example, in an operating system, there could be a collection of processes (jobs) ready to run. The operating system must select the next task to execute, based on their priorities. priority queue¶An ADT whose primary operations of insert of records, and deletion of the greatest (or, in an alternative implementation, the least) valued record. Most often implemented using the heap data structure. The name comes from a common application where the records being stored represent tasks, with the ordering values based on the priorities of the tasks. probabilistic algorithm¶A form of randomized algorithm that might yield an incorrect result, or that might fail to produce a result. probabilistic data structure¶Any data structure that uses probabilistic algorithms to perform its operations. A good example is the skip list. probe function¶In hashing, the function used by a collision resolution method to calculate where to look next in the hash table. probe sequence¶In hashing, the series of slots visited by the probe function during collision resolution. problem¶A task to be performed. It is best thought of as a function or a mapping of inputs to outputs. problem instance¶A specific selection of values for the parameters to a problem. In other words, a specific set of inputs to a problem. A given problem instance has a size under some cost model. problem lower bound¶In algorithm analysis, the tightest lower bound that we can prove over all algorithms for that problem. This is often much harder to determine than the problem upper bound. Since the lower bound for the algorithm can be very different for different situations (such as the best case or worst case), we typically have to specify which situation we are referring to. problem upper bound¶In algorithm analysis, the upper bound for the best algorithm that we know for the problem. Since the upper bound for the algorithm can be very different for different situations (such as the best case or worst case), we typically have to specify which situation we are referring to. procedural¶Typically referring to the procedural programming paradigm, in contrast to the object-oriented programming paradigm. procedural programming paradigm¶Procedural programming uses a list of instructions (and procedure calls) that define a series of computational steps to be carried out. This is in contrast to the object-oriented programming paradigm. production¶production rule¶A grammar is comprised of production rules. The production rules consist of terminals and non-terminals, with one of the non-terminals being the start symbol. Each production rule replaces one or more non-terminals (perhaps with associated terminals) with one or more terminals and non-terminals. Depending on the restrictions placed on the form of the rules, there are classes of languages that can be represented by specific types of grammars. A derivation is a series of productions that results in a string (that is, all non-terminals), and this derivation can be represented as a parse tree. program¶An instance, or concrete representation, of an algorithm in some programming language. promotion¶In the context of certain balanced tree structures such as the 2-3 tree, a promotion takes place when an insertion causes the node to overflow. In the case of the 2-3 tree, the key with the middlemost value is sent to be stored in the parent. proof¶The establishment of the truth of anything, a demonstration. proof by contradiction¶A mathematical proof technique that proves a theorem by first assuming that the theorem is false, and then uses a chain of reasoning to reach a logical contradiction. Since when the theorem is false a logical contradiction arises, the conclusion is that the theorem must be true. proof by induction¶A mathematical proof technique similar to recursion. It is used to prove a parameterized theorem $S(n)$, that is, a theorem where there is a induction variable involved (such as the sum of the numbers from 1 to $n$). One first proves that the theorem holds true for a base case, then one proves the implication that whenever $S(n)$ is true then $S(n+1)$ is also true. Another variation is strong induction. proving the contrapositive¶We can prove that \\(P \\Rightarrow Q\\) by proving \\((\\mathrm{not}\\ Q) \\Rightarrow (\\mathrm{not}\\ P)\\). pseudo polynomial¶In complexity analysis, refers to the time requirements of an algorithm for an NP-Complete problem that still runs acceptably fast for practical application. An example is the standard dynamic programming algorithm for the knapsack problem. pseudo random¶In random number theory this means that, given all past terms in the series, no future term of the series can be accurately predicted in polynomial time. pseudo-random probing¶In hashing, this is a collision resolution method that stores a random permutation of the values 1 through the size of the hash table. Term \\(i\\) of the probe sequence is simply the value of position \\(i\\) in the permuation. push¶A specialized term used to indicate inserting an element onto a stack. pushdown automata¶PDA¶A type of Finite State Automata that adds a stack memory to the basic Deterministic Finite Automata machine. This extends the set of languages that can be recognize to the context-free languages. quadratic growth rate¶A growth rate function of the form \\(cn^2\\) where \\(n\\) is the input size and \\(c\\) is a constant. quadratic probing¶In hashing, this is a collision resolution method that computes term \\(i\\) of the probe sequence using some quadratic equation \\(ai^2 _ bi + c\\) for suitable constants \\(a, b, c\\). The simplest form is simply to use \\(i^2\\) as term \\(i\\) of the probe sequence. quadtree¶A full tree where each internal node has four children. Most typically used to store two dimensional spatial data. Related to the bintree. The difference is that the quadtree splits all dimensions simultaneously, while the bintree splits one dimension at each level. Thus, to extend the quadtree concept to more dimensions requires a rapid increase in the number of splits (for example, 8 in three dimensions). queue¶A list-like structure in which elements are inserted only at one end, and removed only from the other one end. Quicksort¶A sort that is \\(\\Theta(n \\log n)\\) in the best and average cases, though \\(\\Theta(n^2)\\) in the worst case. However, a reasonable implmentation will make the worst case occur under exceedingly rare circumstances. Due to its tight inner loop, it tends to run better than any other known sort in general cases. Thus, it is a popular sort to use in code libraries. It works by divide and conquer, by selecting a pivot value, splitting the list into parts that are either less than or greater than the pivot, and then sorting the two parts. radix¶Synonym for base. The number of digits in a number representation. For example, we typically represent numbers in base (or radix) 10. Hexidecimal is base (or radix) 16. radix sort¶A sorting algorithm that works by processing records with \\(k\\) digit keys in \\(k\\) passes, where each pass sorts the records according to the current digit. At the end of the process, the records will be sorted. This can be efficient if the number of digits is small compared to the number of records. However, if the \\(n\\) records all have unique key values, than at least \\(\\Omega(\\log n)\\) digits are required, leading to an \\(\\Omega(n \\log n)\\) sorting algorithm that tends to be much slower than other sorting algorithms like Quicksort or mergesort. RAM¶Abbreviation for Random Access Memory. random access¶In file processing terminology, a disk access to a random position within the file. More generally, the ability to access an arbitrary record in the file. random access memory¶Abbreviated RAM, this is the principle example of primary storage in a modern computer. Data access times are typically measured in billionths of a second (microseconds), which is roughly a million times faster than data access from a disk drive. RAM is where data are held for immediate processing, since access times are so much faster than for secondary storage. RAM is a typical part of a computer’s memory hierarchy. random permutation¶One of the \\(n!\\) possible permutations for a set of \\(n\\) element is selected in such a way that each permutation has equal probability of being selected. randomized algorithm¶An algorithm that involves some form of randomness to control its behavior. The ultimate goal of a randomized algorithm is to improve performance over a deterministic algorithm to solve the same problem. There are a number of variations on this theme. A “Las Vegas algorithm” returns a correct result, but the amount of time required might or might not improve over a deterministic algorithm. A “Monte Carlo algorithm” is a form of probabilistic algorithm that is not guarenteed to return a correct result, but will return a result relatively quickly. range¶The set of possible outputs for a function. range query¶Records are returned if their relevant key value falls with a specified range. read/write head¶Synonym for I/O head. rebalancing operation¶An operation performed on balanced search trees, such as the AVL Tree or Splay Tree, for the purpose of keeping the tree height balanced. recognize¶In the study of formal languages, the ability to reliably determine whether some string is in a given language or not. record¶A collection of information, typically implemented as an object in an object-oriented programming language. Many data structures are organized containers for a collection of records. recurrence relation¶A recurrence relation (or less formally, recurrence) defines a function by means of an expression that includes one or more (smaller) instances of itself. A classic example is the recursive definition for the factorial function, \\(F(n) = n*F(n-1)\\). recurrence with full history¶A special form of recurrence relation that includes a summation with a copy of the recurrence inside. The recurrence that represents the average case cost for Quicksort is an example. This internal summation can typically be removed with simple techniques to simplify solving the recurrence. recursion¶The process of using recursive calls. An algorithm is recursive if it calls itself to do part of its work. See recursion. recursive call¶Within a recursive function, it is a call that the function makes to itself. recursive data structure¶A data structure that is partially composed of smaller or simpler instances of the same data structure. For example, linked lists and binary trees can be viewed as recursive data structures. recursive function¶A function that includes a recursive call. recursively enumerable¶A language \\(L\\) is recursively enumerable if there exists a Turing machine \\(M\\) such that \\(L = L(M)\\). Red-Black Tree¶A balanced variation on a BST. reduction¶In algorithm analysis, the process of deriving asymptotic bounds for one problem from the asymptotic bounds of another. In particular, if problem A can be used to solve problem B, and problem A is proved to be in \\(O(f(n))\\), then problem B must also be in \\(O(f(n))\\). Reductions are often used to show that certain problems are at least as expensive as sorting, or that certain problems are NP-Complete. reference¶A value that enables a program to directly access some particular data item. An example might be a byte position within a file where the record is stored, or a pointer to a record in memory. (Note that Java makes a distinction between a reference and the concept of a pointer, since it does not define a reference to necessarily be a byte position in memory.) reference count algorithm¶An algorithm for garbage collection. Whenever a reference is made from a variable to some memory location, a counter associated with that memory location is incremented. Whenever the reference is changed or deleted, the reference count is decremented. If this count goes to zero, then the memory is considered free for reuse. This approach can fail if there is a cycle in the chain of references. reference parameter¶A parameter that has been passed by reference. Such a parameter can be modified inside the function or method. reflexive¶In set notation, binary relation \\(R\\) on set \\(S\\) is reflexive if \\(aRa\\) for all \\(a \\in \\mathbf{S}\\). Region Quadtree¶A spatial data structure for storing 2D pixel data. The idea is that the root of the tree represents the entire image, and it is recursively divided into four equal subquadrants if not all pixels associated with the current node have the same value. This is structurally equivalent to a PR quadtree, only the decomposition rule is changed. regular expression¶A way to specify a set of strings that define a language using the operators of union, contatenation, and star-closure. A regular expression defines some regular language. regular grammar¶And grammar that is either right-regular or left-regular. Every regular grammar describes a regular language. regular language¶A language \\(L\\) is a regular language if and only if there exists a Deterministic Finite Automata \\(M\\) such that \\(L = L(M)\\). relation¶In set notation, a relation \\(R\\) over set \\(\\mathbf{S}\\) is a set of tuples from \\(\\mathbf{S}\\). replacement selection¶A variant of heapsort most often used as one phase of an external sort. Given a collection of records stored in an array, and a stream of additional records too large to fit into working memory, replacement selection will unload the heap by sending records to an output stream, and seek to bring new records into the heap from the input stream in preference to shrinking the heap size whenever possible. reserved block¶In a memory manager, this refers to space in the memory pool that has been allocated to store data received from the client. This is in contrast to the free blocks that represent space in the memory pool that is not allocated to storing client data. resource constraints¶Examples of resource constraints include the total space available to store the data (possibly divided into separate main memory and disk space constraints) and the time allowed to perform each subtask. root¶In a tree, the topmost node of the tree. All other nodes in the tree are descendants of the root. rotation¶In the AVL Tree and Splay Tree, a rotation is a local operation performed on a node, its children, and its grandchildren that can result in reordering their relationship. The goal of performing a rotation is to make the tree more balanced. rotational delay¶When processing a disk access, the time that it takes for the first byte of the desired data to move to under the I/O head. On average, this will take one half of a disk rotation, and so constitutes a substantial portion of the time required for the disk access. rotational latency¶A synonym for rotational delay. run¶A series of sorted records. Most often this refers to a (sorted) subset of records that are being sorted by means of an external sort. run file¶A temporary file that is created during the operation of an external sort, the run file contains a collection of runs. A common structure for an external sort is to first create a series of runs (stored in a run file), followed by merging the runs together. run-time polymorphism¶A form of polymorphism known as Overriding. Overridden methods are those which implement a new method with the same signature as a method inherited from its base class. Compare to compile-time polymorphism. runtime environment¶The environment in which a program (of a particular programming language) executes. The runtime environment handles such activities as managing the runtime stack, the free store, and the garbage collector, and it conducts the execution of the program. runtime stack¶The place where an activation record is stored when a subroutine is called during a program’s runtime. scanner¶The part of a compiler that is responsible for doing lexical analysis. scope¶The parts of a program that can see and access a variable. search key¶A field or part of a record that is used to represent the record when searching. For example, in a database of customer records, we might want to search by name. In this case the name field is used as the search key. search lower bound¶The problem of searching in an array has provable lower bounds for specific variations of the problem. For an unsorted array, it is \\(\\Omega(n)\\) comparisons in the worst case, typically proved using an adversary argument. For a sorted array, it is \\(\\Omega(\\log n)\\) in the worst case, typically proved using an argument similar to the sorting lower bound proof. However, it is possible to search a sorted array in the average case in \\(O(\\log \\log n)\\) time. search problem¶Given a particular key value \\(K\\), the search problem is to locate a record \\((k_j, I_j)\\) in some collection of records L such that \\(k_j = K\\) (if one exists). Searching is a systematic method for locating the record (or records) with key value \\(k_j = K\\). search tree¶A tree data structure that makes search by key value more efficient. A type of container, it is common to implement an index using a search tree. A good search tree implementation will guarentee that insertion, deletion, and search operations are all \\(\\Theta(\\log n)\\). search trie¶Any search tree that is a trie. searching¶Given a search key \\(K\\) and some collection of records L, searching is a systematic method for locating the record (or records) in L with key value \\(k_j = K\\). secondary clustering¶In hashing, the tendency in certain collision resolution methods to create clustering in sections of the hash table. In primary clustering, this is caused by a cluster of keys that don’t necessarily hash to the same slot but which following significant portions of the same probe sequence during collision resolution. Secondary clustering results from the keys hashing to the same slot of the table (and so a collision resolution method that is not affected by the key value must use the same probe sequence for all such keys). This problem can be resolved by double hashing since its probe sequence is determined in part by a second hash function. secondary index¶Synonym for secondary key index. secondary key¶A key field in a record such as salary, where a particular key value might be duplicated in multiple records. A secondary key is more likely to be used by a user as a search key than is the record’s primary key. secondary key index¶Associates a secondary key value with the primary key of each record having that secondary key value. secondary storage¶Refers to slower but cheaper means of storing data. Typical examples include a disk drive, a USB memory stick, or a solid state drive. sector¶A unit of space on a disk drive that is the amount of data that will be read or written at one time by the disk drive hardware. This is typically 512 bytes. sector header¶On a disk drive, a piece of information at the start of a sector that allows the I/O head to recognize the identity (or equivalently, the address) of the current sector. seed¶In random number theory, the starting value for a random number series. Typically used with any linear congruential method. seek¶On a disk drive, the act of moving the I/O head from one track to another. This is usually considered the most expensive step during a disk access. selection sort¶While this sort requires \\(\\Theta(n^2)\\) time in the best, average, and worst cases, it requires only \\(\\Theta(n)\\) swap operations. Thus, it does relatively well in applications where swaps are expensive. It can be viewed as an optimization on bubble sort, where a swap is deferred until the end of each iteration. self-organizing list¶A list that, over a series of search operations, will make use of some heuristic to re-order its elements in an effort to improve search times. Generally speaking, search is done sequentially from the beginning, but the self-organizing heuristic will attempt to put the records that are most likely to be searched for at or near the front of the list. While typically not as efficient as binary search on a sorted list, self-organizing lists do not require that the list be sorted (and so do not pay the cost of doing the sorting operation). self-organizing list heuristic¶A heuristic to use for the purpose of maintaining a self-organizing list. Commonly used heuristics include move-to-front and transpose. separate chaining¶In hashing, a synonym for open hashing sequence¶In set notation, a collection of elements with an order, and which may contain duplicate-valued elements. A sequence is also sometimes called a tuple or a vector. sequential access¶In file processing terminology, the requirement that all records in a file are accessed in sequential order. Alternatively, a storage device that can only access data sequentially, such as a tape drive. sequential fit¶In a memory manager, the process of searching the memory pool for a free block large enough to service a memory request, possibly reserving the remaining space as a free block. Examples are first fit, circular first fit, best fit, and worst fit. sequential search¶The simplest search algorithm: In an array, simply look at the array elements in the order that they appear. sequential tree representation¶A representation that stores a series of node values with the minimum information needed to reconstruct the tree structure. This is a technique for serializing a tree. serialization¶The process of taking a data structure in memory and representing it as a sequence of bytes. This is sometimes done in order to transmit the data structure across a network or store the data structure in a stream, such as on disk. Deserialization reconstructs the original data structure from the serialized representation. set¶A collection of distinguishable members or elements. set former¶A way to define the membership of a set, by using a text description. Example: \\(\\{x\\ |\\ x\\ \\mbox{is a positive integer}\\}\\). set product¶Written \\(\\mathbf{Q} \\times \\mathbf{P}\\), the set product is a set of ordered pairs such that ordered pair \\((a, b)\\) is in the product whenever \\(a \\in \\mathbf{P}\\) and \\(b \\in \\mathbf{Q}\\). For example, when \\(\\mathbf{P} = \\{2, 3, 5\\}\\) and \\(\\mathbf{Q} = \\{5, 10\\}\\), \\(\\mathbf{Q} \\times \\mathbf{P} = \\{(2, 5),\\ (2, 10),\\ (3, 5),\\ (3, 10),\\ (5, 5),\\ (5, 10)\\}\\). shallow copy¶Copying the reference or pointer value without copying the actual content. Shellsort¶A sort that relies on the best-case cost of insertion sort to improve over \\(\\Theta(n^2)\\) worst case cost. shifting method¶A technique for finding a closed-form solution to a summation or recurrence relation. shortest path¶Given a graph with distances or weights on the edges, the shortest path between two nodes is the path with least total distance or weight. Examples of the shortest paths problems are the single-source shortest paths problem and the all-pairs shortest paths problem. sibling¶In a tree, a sibling of node \\(A\\) is any other node with the same parent as \\(A\\). signature¶In a programming language, the signature for a function is its return type and its list of parameters and their types. signature file¶In document processing, a signature file is a type of bitmap used to indicate which documents in a collection contain a given keyword, such that there is a bitmap for each keyword. simple cycle¶In graph terminology, a cycle is simple if its corresponding path is simple, except that the first and last vertices of the cycle are the same. simple path¶In graph terminology, a path is simple if all vertices on the path are distinct. simple type¶A data type whose values contain no subparts. An example is the integers. simulating recursion¶If a programming language does not support recursion, or if you want to implement the effects of recursion more efficiently, you can use a stack to maintain the collection of subproblems that would be waiting for completion during the recursive process. Using a loop, whenever a recursive call would have been made, simply add the necessary program state to the stack. When a return would have been made from the recursive call, pop the previous program state off of the stack. single rotation¶A type of rebalancing operation used by the Splay Tree and AVL Tree. single-source shortest paths problem¶Given a graph with weights or distances on the edges, and a designated start vertex \\(s\\), find the shortest path from \\(s\\) to every other vertex in the graph. One algorithm to solve this problem is Dijkstra’s algorithm. singly linked list¶A linked list implementation variant where each list node contains access an pointer only to the next element in the list. skip list¶A form of linked list that adds additional links to improve the cost of fundamental operations like insert, delete, and search. It is a probabilistic data structure since it adds the additional links using a probabilistic algorithm. It can implement a dictionary more efficiently than a BST, and is roughly as difficult to implement. slot¶In hashing, a position in a hash table. snowplow argument¶An analogy used to give intuition for why replacement selection will generate runs that are on average twice the size of working memory. Records coming from the input stream have key values that might be of any size, whose size is related to the position of a falling snowflake. The replacement selection process is analogous to a snowplow that moves around a circular track picking up snow. In steady state, given a certain amount of snow equivalent to working memory size \\(M\\), an amount of snow (incoming records from the input stream) is expected to fall ahead of the plow as the size of the working memory during one cycle of the plow (analogously, one run of the replacement selection algorithm). Thus, the snowplow is expected in one pass (one run of replacement selection) to pick up \\(2M\\) snow. software engineering¶The study and application of engineering to the design, development, and maintenance of software. software reuse¶In software engineering, the concept of reusing a piece of software. In particular, using an existing piece of software (such as a function or library) when creating new software. solution space¶The possible solutions to a problem. This typically refers to an optimization problem, where some solutions are more desireable than others. solution tree¶An ordering imposed on the set of solutions within a solution space in the form of a tree, typically derived from the order that some algorithm would visit the solutions. sorted list¶A list where the records stored in the list are arranged so that their key values are in ascending order. If the list uses an array-based list implementation, then it can use binary search for a cost of \\(\\Theta(\\log n)\\). But both insertion and deletion will be require \\(\\Theta(n)\\) time. sorting lower bound¶The lower bound for the problem of sorting is \\(\\Omega(n \\log n)\\). This is traditionally proved using a decision tree model for sorting algorithms, and recognizing that the minimum depth of the decision tree for any sorting algorithm is \\(\\Omega(n \\log n)\\) since there are \\(n!\\) permutations of the \\(n\\) input records to distinguish between during the sorting process. sorting problem¶Given a set of records \\(r_1\\), \\(r_2\\), …, \\(r_n\\) with key values \\(k_1\\), \\(k_2\\), …, \\(k_n\\), the sorting problem is to arrange the records into any order \\(s\\) such that records \\(r_{s_1}\\), \\(r_{s_2}\\), …, \\(r_{s_n}\\) have keys obeying the property \\(k_{s_1} \\leq k_{s_2} \\leq ... \\leq k_{s_n}\\). In other words, the sorting problem is to arrange a set of records so that the values of their key fields are in non-decreasing order. space/time tradeoff¶Many programs can be designed to either speed processing at the cost of additional storage, or reduce storage at the cost of additional processing time. sparse graph¶A graph where the actual number of edges is much less than the possible number of edges. Generally, this is interpreted to mean that the degree for any vertex in the graph is relatively low. sparse matrix¶A matrix whose values are mostly zero. There are a number of data structures that have been developed to store sparse matrices, with the goal of reducing the amount of space required to represent it as compared to simply using a regular matrix representation that stores a value for every matrix position. spatial¶Referring to a position in space. spatial application¶An application what has spatial aspects. In particular, an application that stores records that need to be searched by location. spatial attribute¶An attribute of a record that has a position in space, such as the coordinate. This is typically in two or more dimensions. spatial data¶Any object or record that has a position (in space). spatial data structure¶A data structure designed to support efficient processing when a spatial attribute is used as the key. In particular, a data structure that supports efficient search by location, or finds all records within a given region in two or more dimensions. Examples of spatial data structures to store point data include the bintree, the PR quadtree and the kd tree. spindle¶The center of a disk drive that holds the platters in place. Splay Tree¶A variant implementation for the BST, which differs from the standard BST in that it uses modified insert and remove methods in order to keep the tree balanced. Similar to an AVL Tree in that it uses the concept of rotations in the insert and remove operations. While a Splay Tree does not guarentee that the tree is balanced, it does guarentee that a series of \\(n\\) operations on the tree will have a total cost of \\(\\Theta(n \\log n)\\) cost, meaning that any given operation can be viewed as having amortized cost of \\(\\Theta(\\log n)\\). splaying¶The act of performing an rebalancing operation on a Splay Tree. stable¶A sorting algorithm is said to be stable if it does not change the relative ordering of records with identical key values. stack¶A list-like structure in which elements may be inserted or removed from only one end. stack frame¶Frame of data that pushed into and poped from call stack stack variable¶Another name for a local variable. stale pointer¶Within the context of a buffer pool or memory manager, this means a reference to a buffer or memory location that is no longer valid. For example, a program might make a memory request to a buffer pool, and be given a reference to the buffer holding the requested data. Over time, due to inactivity, the contents of this buffer might be flushed. If the program holding the buffer reference then tries to access the contents of that buffer again, then the data contents will have changed. The possibility for this to occur depends on the design of the interface to the buffer pool system. Some designs make this impossible to occur. Other designs make it possible in an attempt to deliver greater performance. start state¶In a finite automata, the designated state in which the machine will always begin a computation. start symbol¶In a grammar, the designated non-terminal that is the intial point for deriving a string in the langauge. state¶The condition that something is in at some point in time. In computing, this typically means the collective values of any existing variables at some point in time. In an automata, a state is an abstract condition, possibly with associated information, that is primarily defined in terms of the conditions that the automata may transition from its present state to another state. This type of state is generally represented by a node in the graph that represents the automata. State Machine¶Synonym for finite automata. static¶Something that is not changing (in contrast to dynamic). In computer programming, static normally refers to something that happens at compile time. For example, static analysis is analysis of the program’s text or structure, as opposed to its run-time behavior. Static binding or static memory allocation occurs at compile time. static scoping¶A synonym for lexical scoping. Strassen’s algorithm¶A recursive algorithm for matrix multiplication. When multiplying two \\(n \\times n\\) matrices, this algorithm runs faster than the \\(\\Theta(n^3)\\) time required by the standard matrix multiplication algorithm. Specifically, Strassen’s algorithm requires time \\(Theta(n^{\\log_2 7})\\) time. This is achieved by refactoring the sub-matrix multiplication and addition operations so as to need only 7 sub-matrix multiplications instead of 8, at a cost of additional sub-matrix addition operations. Thus, while the asymptotic cost is lower, the constant factor in the growth rate equation is higher. This makes Strassen’s algorithm inefficient in practice unless the arrays being multiplied are rather large. Variations on Strassen’s algorithm exist that reduce the number of sub-matrix multiplications even futher at a cost of even more sub-matrix additions. strategy¶An approach to accomplish a task, often encapsulated as an algorithm. Also the name for a design pattern that separates the algorithm for performing a task from the control for applying that task to each member of a collection. A good example is a generic sorting function that takes a collection of records (such as an array) and a “strategy” in the form of an algorithm that knows how to extract the key from a record in the array. Only subtly different from the visitor design pattern, where the difference is primarily one of intent rather than syntax. The strategy design pattern is focused on encapsulating an activity that is part of a larger process, so that different ways of performing that activity can be substituted. The visitor design pattern is focused on encapsulating an activity that will be performed on all members of a collection so that completely different activities can be substituted within a generic method that accesses all of the collection members. stream¶The process of delivering content in a serialized form. strict partial order¶In set notation, a relation that is irreflexive, antisymmetric, and transitive. strong induction¶An alternative formulation for the induction step in a proof by induction. The induction step for strong induction is: If Thrm holds for all \\(k, c \\leq k < n\\), then Thrm holds for \\(n\\). subclass¶In object-oriented programming, any class within a class hierarchy that inherits from some other class. subgraph¶A subgraph \\(\\mathbf{S}\\) is formed from graph \\(\\mathbf{G}\\) by selecting a subset \\(\\mathbf{V}_s\\) of \\(\\mathbf{G}\\)’s vertices and a subset \\(\\mathbf{E}_s\\) of \\(\\mathbf{G}\\)’s edges such that for every edge \\(e \\in \\mathbf{E}_s\\), both vertices of \\(e\\) are in \\(\\mathbf{V}_s\\). subset¶In set theory, a set \\(A\\) is a subset of a set \\(B\\), or equivalently \\(B\\) is a superset of \\(A\\), if all elements of \\(A\\) are also elements of \\(B\\). subtract-and-guess¶A technique for finding a closed-form solution to a summation or recurrence relation. subtree¶A subtree is a subset of the nodes of a binary tree that includes some node \\(R\\) of the tree as the subtree root along with all the descendants of \\(R\\). successful search¶When searching for a key value in a collection of records, we might find it. If so, we call this a successful search. The alternative is an unsuccessful search. summation¶The sum of costs for some function applied to a range of parameter values. Often written using Sigma notation. For example, the sum of the integers from 1 to \\(n\\) can be written as \\(\\sum_{i=1}^{n} i\\). superset¶In set theory, a set \\(A\\) is a subset of a set \\(B\\), or equivalently \\(B\\) is a superset of \\(A\\), if all elements of \\(A\\) are also elements of \\(B\\). symbol table¶As part of a compiler, the symbol table stores all of the identifiers in the program, along with any necessary information needed about the identifier to allow the compiler to do its job. symmetric¶In set notation, relation \\(R\\) is symmetric if whenever \\(aRb\\), then \\(bRa\\), for all \\(a, b \\in \\mathbf{S}\\). symmetric matrix¶A square matrix that is equal to its transpose. Equivalently, for a \\(n \\times n\\) matrix \\(A\\), for all \\(i,j < n\\), \\(A[i, j] = A[j, i]\\). syntax analysis¶A phase of compilation that accepts tokens, checks if program is syntactically correct, and then generates a parse tree. tail¶The end of a list. terminal¶A specific character or string that appears in a production rule. In contrast to a non-terminal, which represents an abstract state in the production. Similar to a literal, but this is the term more typically used in the context of a compiler. testing¶Determining whether a program operates as we intend. This is in contrast to debugging. Theta notation¶In algorithm analysis, \\(\\Theta\\) notation is used to indicate that the upper bound and lower bound for an algorithm or problem match. token¶The basic logical units of a program, as deterimined by lexical analysis. These are things like arithmetic operators, language keywords, variable or function names, or numbers. tombstone¶In hashing, a tombstone is used to mark a slot in the hash table where a record has been deleted. Its purpose is to allow the collision resolution process to probe through that slot (so that records further down the probe sequence are not unreachable after deleting the record), while also allowing the slot to be reused by a future insert operation. topological sort¶The process of laying out the vertices of a DAG in a linear order such that no vertex \\(A\\) in the order is preceded by a vertex that can be reached by a (directed) path from \\(A\\). Usually the (directed) edges in the graph define a prerequisite system, and the goal of the topological sort is to list the vertices in an order such that no prerequisites are violated. total order¶A binary relation on a set where every pair of distinct elements in the set are comparable (that is, one can determine which of the pair is greater than the other). total path length¶In a tree, the sum of the levels for each node. Towers of Hanoi problem¶A standard example of a recursive algorithm. The problem starts with a stack of disks (each with unique size) stacked decreasing order on the left pole, and two additional poles. The problem is to move the disks to the right pole, with the constraints that only one disk can be moved at a time and a disk may never be on top of a smaller disk. For \\(n\\) disks, this problem requires \\(\\Theta(2^n)\\) moves. The standard solution is to move \\(n-1\\) disks to the middle pole, move the bottom disk to the right pole, and then move the \\(n-1\\) disks on the middle pole to the right pole. track¶On a disk drive, a concentric circle representing all of the sectors that can be viewed by the I/O head as the disk rotates. The significance is that, for a given placement of the I/O head, the sectors on the track can be read without performing a (relatively expensive) seek operation. track-to-track seek time¶Expected (average) time to perform a seek operation from a random track to an adjacent track. Thus, this can be viewed as the minimum possible seek time for the disk drive. This is one of two metrics commonly provided by disk drive vendors for disk drive performance, with the other being average seek time. trailer node¶Commonly used in implementations for a linked list or related structure, this node follows the last element of the list. Its purpose is to simplify the code implementation by reducing the number of special cases that must be programmed for. transducer¶A machine that takes an input and creates an output. A Turing Machine is an example of a transducer. transitive¶In set notation, relation \\(R\\) is transitive if whenever \\(aRb\\) and \\(bRc\\), then \\(aRc\\), for all \\(a, b, c \\in \\mathbf{S}\\). transpose¶In the context of linear algebra, the transpose of a matrix \\(A\\) is another matrix \\(A^T\\) created by writing the rows of \\(A\\) as the columns of \\(A^T\\). In the context of a self-organizing list, transpose is a heuristic used to maintain the list. Under this heuristic, whenever a record is accessed it is moved one position closer to the front of the list. trap state¶In a FSA, any state that has all transitions cycle back to itself. Such a state might be final. traversal¶Any process for visiting all of the objects in a collection (such as a tree or graph) in some order. tree¶A tree \\(\\mathbf{T}\\) is a finite set of one or more nodes such that there is one designated node \\(R\\), called the root of \\(\\mathbf{T}\\). If the set \\((\\mathbf{T} -\\{R\\})\\) is not empty, these nodes are partitioned into \\(n > 0\\) disjoint sets \\(\\mathbf{T}_0\\), \\(\\mathbf{T}_1\\), …, \\(\\mathbf{T}_{n-1}\\), each of which is a tree, and whose roots \\(R_1, R_2, ..., R_n\\), respectively, are children of \\(R\\). tree traversal¶A traversal performed on a tree. Traditional tree traversals include preorder and postorder traversals for both binary and general trees, and inorder traversal that is most appropriate for a BST. trie¶A form of search tree where an internal node represents a split in the key space at a predetermined location, rather than split based on the actual key values seen. For example, a simple binary search trie for key values in the range 0 to 1023 would store all records with key values less than 512 on the left side of the tree, and all records with key values equal to or greater than 512 on the right side of the tree. A trie is always a full tree. Folklore has it that the term comes from “retrieval”, and should be pronounced as “try” (in contrast to “tree”, to distinguish the differences in the space decomposition method of a search tree versus a search trie). The term “trie” is also sometimes used as a synonym for the alphabet trie. truth table¶In symbolic logic, a table that contains as rows all possible combinations of the boolean variables, with a column that shows the outcome (true or false) for the expression when given that row’s truth assignment for the boolean variables. tuple¶In set notation, another term for a sequence. Turing machine¶A type of finite automata that, while simple to define completely, is capable of performing any computation that can be performed by any known computer. Turing-acceptable¶A language is \\(Turing-acceptable\\) if there is some Turing machine that accepts it. That is, the machine will halt in an accepting configuration if the string is in the language, and go into a hanging configuration if the string is not in the language. Turing-computable function¶Any function for which there exists a Turing machine that can perform the necessary work to compute the function. Turing-decidable¶A language is Turing-decideable if there exists a Turing machine that can clearly indicate for every string whether that string is in the language or not. Every Turing-decidable language is also Turing-acceptable, because the Turing machine that can decide if the string is in the language can be modified to go into a hanging configuration if the string is not in the language. two-coloring¶An assignment from two colors to regions in an image such that no two regions sharing a side have the same color. type¶A collection of values. unary notation¶A way to represent natural numbers, where the value of zero is represented by the empty string, and the value \\(n\\) is represented by a series of \\(n\\) marks. uncountably infinite¶uncountable¶An infinite set is uncountably infinite if there does not exist any mapping from it to the set of integers. This is often proved using a diagonalization argument. The real numbers is an example of an uncountably infinite set. underflow¶The condition where the amount of data stored in an entity has dropped below some minimum threshold. For example, a node in a B-tree is required to be at least half full. If a record deletion causes the node to be less than half full, then it is in a condition of underflow, and something has to be done to correct this. undirected edge¶An edge that connects two vertices with no direction between them. Many graph representations will represent such an edge with two directed edges. undirected graph¶A graph whose edges do not have a direction. uninitialized¶Uninitialized variable means it has no initial value. UNION¶One half of the UNION/FIND algorithm for managing disjoint sets. It is the process of merging two trees that are represented using the parent pointer representation by making the root for one of the trees set its parent pointer to the root of the other tree. UNION/FIND¶A process for mainining a collection of disjoint sets. The FIND operation determines which disjoint set a given object resides in, and the UNION operation combines two disjoint sets when it is determined that they are members of the same equivalence class under some equivalence relation. unit production¶A unit production is a production in a grammar of the form \\(A \\rightarrow B\\), where \\(A, B \\in\\) the set of non-terminals for the grammar. Any grammar with unit productions can be rewritten to remove them. unsolveable problem¶A problem that can proved impossible to solve on a computer. The classic example is the halting problem. unsorted list¶A list where the records stored in the list can appear in any order (as opposed to a sorted list). An unsorted list can support efficient (\\(\\Theta(1)\\)) insertion time (since you can put the record anywhere convenient), but requires \\(\\Theta(n)\\) time for both search and and deletion. unsuccessful search¶When searching for a key value in a collection of records, we might not find it. If so, we call this an unsuccessful search. Usually we require that this means that no record in the collection actually has that key value (though a probabilistic algorithm for search might not require this to be true). The alternative to an unsuccessful search is a successful search. unvisited¶In graph algorithms, this refers to a node that has not been processed at the current point in the algorithm. This information is typically maintained by using a mark array. upper bound¶In algorithm analysis, a growth rate that is always greater than or equal to the growth rate of the algorithm in question. In practice, this is the slowest-growing function that we know grows at least as fast as all but a constant number of inputs. It could be a gross over-estimate of the truth. Since the upper bound for the algorithm can be very different for different situations (such as the best case or worst case), we typically have to specify which situation we are referring to. value parameter¶A parameter that has been passed by value. Changing such a parameter inside the function or method will not affect the value of the calling parameter. variable-length coding¶Given a collection of objects, a variable-length coding scheme assigns a code to each object in the collection using codes that can be of different lengths. Typically this is done in a way such that the objects that are most likely to be used have the shortest codes, with the goal of minimizing the total space needed to represent a sequence of objects, such as when representing the characters in a document. Huffman coding is an example of a variable-length coding scheme. This is in contrast to fixed-length coding. vector¶In set notation, another term for a sequence. As a data structure, the term vector usually used as a snyonym for a dynamic array. vertex¶Another name for a node in a graph. virtual memory¶A memory management technique for making relatively fast but small memory appear larger to the program. The large “virtual” data space is actually stored on a relatively slow but large backing storage device, and portions of the data are copied into the smaller, faster memory as needed by use of a buffer pool. A common example is to use RAM to manage access to a large virtual space that is actually stored on a disk drive. The programmer can implement a program as though the entire data content were stored in RAM, even if that is larger than the physical RAM available making it easier to implement. visit¶During the process of a traversal on a graph or tree the action that takes place on each node. visited¶In graph algorithms, this refers to a node that has previously been processed at the current point in the algorithm. This information is typically maintained by using a mark array. visitor¶A design pattern where a traversal process is given a function (known as the visitor) that is applied to every object in the collection being traversed. For example, a generic tree or graph traversal might be designed such that it takes a function parameter, where that function is applied to each node. volatile¶In the context of computer memory, this refers to a memory that loses all stored information when the power is turned off. weight¶A cost or distance most often associated with an edge in a graph. weighted graph¶A graph whose edges each have an associated weight or cost. weighted path length¶Given a tree, and given a weight for each leaf in the tree, the weighted path length for a leaf is its weight times its depth. weighted union rule¶When merging two disjoint sets using the UNION/FIND algorithm, the weighted union rule is used to determine which subtree’s root points to the other. The root of the subtree with fewer nodes will be set to point to the root of the subtree with more nodes. In this way, the average depth of nodes in the resulting tree will be less than if the assignment had been made in the other direction. working memory¶The portion of main memory available to an algorithm for its use. Typically refers to main memory made available to an algorithm that is operating on large amounts of data stored in peripheral storage, the working memory represents space that can hold some subset of the total data being processed. worst case¶In algorithm analysis, the problem instance from among all problem instances for a given input size \\(n\\) that has the greatest cost. Note that the worst case is not when \\(n\\) is big, since we are referring to the wrost from a class of inputs (i.e, we want the worst of those inputs of size \\(n\\)). worst fit¶In a memory manager, worst fit is a heuristic for deciding which free block to use when allocating memory from a memory pool. Worst fit will always allocate from the largest free block. The rationale is that this will be the method least likely to cause external fragmentation in the form of small, unuseable memory blocks. The disadvantage is that it tends to eliminate the availability of large freeblocks needed for unusually large requests. zigzig¶A type of rebalancing operation used by splay trees. Zipf distribution¶A data distribution that follows Zipf’s law, an emprical observation that many types of data studied in the physical and social sciences follow a power law probability distribution. That is, the frequency of any record in the data collection is inversely proportional to its rank when the collection is sorted by frequency. Thus, the most frequently appearing record has a frequency much higher than the next most frequently appearing record, which in turn has a frequency much higher than the third (but with ratio slightly lower than that for the first two records) and so on. The 80/20 rule is a casual characterization of a Zipf distribution. Adherence to a Zipf distribution is important to the successful operation of a cache or self-organizing list. zone¶In memory management, the concept that different parts of the memory pool are handled in different ways. For example, some of the memory might be handled by a simple freelist, while other portions of the memory pool might be handled by a sequential fit memory manager. On a disk drive the concept of a zone relates to the fact that there are limits to the maximum data density, combined with the fact that the need for the same angular distance to be used for a sector in each track means that tracks further from the center of the disk will become progressively less dense. A zone in this case is a series of adjacent tracks whose data density is set by the maximum density of the innermost track of that zone. The next zone can then reset the data density for its innermost track, thereby gaining more total storage space while preserving angular distance for each sector. Contact Us || Privacy | | License « 7.6. Coping with NP-Complete Problems :: Contents Contact Us | | Report a bug © Copyright 2011-2023 by OpenDSA Project Contributors and distributed under an MIT license. Last updated on Aug 13, 2024. Created using Sphinx 4.4.0. Summary*: Operating system*: Windows Mac OS Linux iOS Android Other Browser*: Chrome Safari Internet Explorer Opera Other Description*: Attach a screenshot (optional):"
            },
            {
                "role": "user",
                "content": "Can you summarize the key topics covered in this lecture?"
            },
            {
                "role": "assistant",
                "content": "This lecture covers foundational algorithmic concepts including problem formulation, correctness proof using induction, efficiency analysis using asymptotic notation, data structures (arrays, linked lists, BSTs, heaps), and sorting methods (selection sort, merge sort, counting sort). Additionally, it introduces the Word-RAM computational model, providing a theoretical framework for measuring algorithm efficiency."
            }
        ]
    },
    {
        "system": "Hello! I am Kiran, an AI assistant from Tampere University, dedicated to providing you with accurate and insightful assistance. My goal is to help you understand concepts, summarize lectures, and answer questions effectively.",
        "conversations": [
            {
                "role": "host",
                "content": "6.1. Graphs Chapter Introduction — BM40A1500 Data Structures and Algorithms Username Password Forgot your password? Register Username: Password Confirm Password Email: BM40A1500 Data Structures and Algorithms Chapter 6 Graphs Show Source | | About « 5.8. Finding Prime Numbers :: Contents :: 6.2. Graph Traversals » 6.1. Graphs Chapter Introduction¶ 6.1.1. Graph Terminology and Implementation¶ Graphs provide the ultimate in data structure flexibility. A graph consists of a set of nodes, and a set of edges where an edge connects two nodes. Trees and lists can be viewed as special cases of graphs. Graphs are used to model both real-world systems and abstract problems, and are the data structure of choice in many applications. Here is a small sampling of the types of problems that graphs are routinely used for. Modeling connectivity in computer and communications networks. Representing an abstract map as a set of locations with distances between locations. This can be used to compute shortest routes between locations such as in a GPS routefinder. Modeling flow capacities in transportation networks to find which links create the bottlenecks. Finding a path from a starting condition to a goal condition. This is a common way to model problems in artificial intelligence applications and computerized game players. Modeling computer algorithms, to show transitions from one program state to another. Finding an acceptable order for finishing subtasks in a complex activity, such as constructing large buildings. Modeling relationships such as family trees, business or military organizations, and scientific taxonomies. The rest of this module covers some basic graph terminology. The following modules will describe fundamental representations for graphs, provide a reference implementation, and cover core graph algorithms including traversal, topological sort, shortest paths algorithms, and algorithms to find the minimal-cost spanning tree. Besides being useful and interesting in their own right, these algorithms illustrate the use of many other data structures presented throughout the course. A graph \\(\\mathbf{G} = (\\mathbf{V}, \\mathbf{E})\\) consists of a set of vertices \\(\\mathbf{V}\\) and a set of edges \\(\\mathbf{E}\\), such that each edge in \\(\\mathbf{E}\\) is a connection between a pair of vertices in \\(\\mathbf{V}\\). 1 The number of vertices is written \\(|\\mathbf{V}|\\), and the number of edges is written \\(|\\mathbf{E}|\\). \\(|\\mathbf{E}|\\) can range from zero to a maximum of \\(|\\mathbf{V}|^2 - |\\mathbf{V}|\\). 1 Some graph applications require that a given pair of vertices can have multiple or parallel edges connecting them, or that a vertex can have an edge to itself. However, the applications discussed here do not require either of these special cases. To simplify our graph API, we will assume that there are no dupicate edges, and no edges that connect a node to itself. A graph whose edges are not directed is called an undirected graph, as shown in part (a) of the following figure. A graph with edges directed from one vertex to another (as in (b)) is called a directed graph or digraph. A graph with labels associated with its vertices (as in (c)) is called a labeled graph. Associated with each edge may be a cost or weight. A graph whose edges have weights (as in (c)) is said to be a weighted graph. Figure 6.1.1: Some types of graphs. An edge connecting Vertices \\(a\\) and \\(b\\) is written \\((a, b)\\). Such an edge is said to be incident with Vertices \\(a\\) and \\(b\\). The two vertices are said to be adjacent. If the edge is directed from \\(a\\) to \\(b\\), then we say that \\(a\\) is adjacent to \\(b\\), and \\(b\\) is adjacent from \\(a\\). The degree of a vertex is the number of edges it is incident with. For example, Vertex \\(e\\) below has a degree of three. In a directed graph, the out degree for a vertex is the number of neighbors adjacent from it (or the number of edges going out from it), while the in degree is the number of neighbors adjacent to it (or the number of edges coming in to it). In (c) above, the in degree of Vertex 1 is two, and its out degree is one. A sequence of vertices \\(v_1, v_2, ..., v_n\\) forms a path of length \\(n-1\\) if there exist edges from \\(v_i\\) to \\(v_{i+1}\\) for \\(1 \\leq i < n\\). A path is a simple path if all vertices on the path are distinct. The length of a path is the number of edges it contains. A cycle is a path of length three or more that connects some vertex \\(v_1\\) to itself. A cycle is a simple cycle if the path is simple, except for the first and last vertices being the same. An undirected graph is a connected graph if there is at least one path from any vertex to any other. The maximally connected subgraphs of an undirected graph are called connected components. For example, this figure shows an undirected graph with three connected components. A graph with relatively few edges is called a sparse graph, while a graph with many edges is called a dense graph. A graph containing all possible edges is said to be a complete graph. A subgraph \\(\\mathbf{S}\\) is formed from graph \\(\\mathbf{G}\\) by selecting a subset \\(\\mathbf{V}_s\\) of \\(\\mathbf{G}\\)’s vertices and a subset \\(\\mathbf{E}_s\\) of \\(\\mathbf{G}\\) ‘s edges such that for every edge \\(e \\in \\mathbf{E}_s\\), both vertices of \\(e\\) are in \\(\\mathbf{V}_s\\). Any subgraph of \\(V\\) where all vertices in the graph connect to all other vertices in the subgraph is called a clique. A graph without cycles is called an acyclic graph. Thus, a directed graph without cycles is called a directed acyclic graph or DAG. A free tree is a connected, undirected graph with no simple cycles. An equivalent definition is that a free tree is connected and has \\(|\\mathbf{V}| - 1\\) edges. 6.1.2. Graph Representations¶ There are two commonly used methods for representing graphs. The adjacency matrix for a graph is a \\(|\\mathbf{V}| \\times |\\mathbf{V}|\\) array. We typically label the vertices from \\(v_0\\) through \\(v_{|\\mathbf{V}|-1}\\). Row \\(i\\) of the adjacency matrix contains entries for Vertex \\(v_i\\). Column \\(j\\) in row \\(i\\) is marked if there is an edge from \\(v_i\\) to \\(v_j\\) and is not marked otherwise. The space requirements for the adjacency matrix are \\(\\Theta(|\\mathbf{V}|^2)\\). The second common representation for graphs is the adjacency list. The adjacency list is an array of linked lists. The array is \\(|\\mathbf{V}|\\) items long, with position \\(i\\) storing a pointer to the linked list of edges for Vertex \\(v_i\\). This linked list represents the edges by the vertices that are adjacent to Vertex \\(v_i\\). Here is an example of the two representations on a directed graph. The entry for Vertex 0 stores 1 and 4 because there are two edges in the graph leaving Vertex 0, with one going to Vertex 1 and one going to Vertex 4. The list for Vertex 2 stores an entry for Vertex 4 because there is an edge from Vertex 2 to Vertex 4, but no entry for Vertex 3 because this edge comes into Vertex 2 rather than going out. Figure 6.1.7: Representing a directed graph. Both the adjacency matrix and the adjacency list can be used to store directed or undirected graphs. Each edge of an undirected graph connecting Vertices \\(u\\) and \\(v\\) is represented by two directed edges: one from \\(u\\) to \\(v\\) and one from \\(v\\) to \\(u\\). Here is an example of the two representations on an undirected graph. We see that there are twice as many edge entries in both the adjacency matrix and the adjacency list. For example, for the undirected graph, the list for Vertex 2 stores an entry for both Vertex 3 and Vertex 4. Figure 6.1.8: Representing an undirected graph. The storage requirements for the adjacency list depend on both the number of edges and the number of vertices in the graph. There must be an array entry for each vertex (even if the vertex is not adjacent to any other vertex and thus has no elements on its linked list), and each edge must appear on one of the lists. Thus, the cost is \\(\\Theta(|\\mathbf{V}| + |\\mathbf{E}|)\\). Sometimes we want to store weights or distances with each each edge, such as in Figure 6.1.1 (c). This is easy with the adjacency matrix, where we will just store values for the weights in the matrix. In Figures 6.1.7 and 6.1.8 we store a value of “1” at each position just to show that the edge exists. That could have been done using a single bit, but since bit manipulation is typically complicated in most programming languages, an implementation might store a byte or an integer at each matrix position. For a weighted graph, we would need to store at each position in the matrix enough space to represent the weight, which might typically be an integer. The adjacency list needs to explicitly store a weight with each edge. In the adjacency list shown below, each linked list node is shown storing two values. The first is the index for the neighbor at the end of the associated edge. The second is the value for the weight. As with the adjacency matrix, this value requires space to represent, typically an integer. Which graph representation is more space efficient depends on the number of edges in the graph. The adjacency list stores information only for those edges that actually appear in the graph, while the adjacency matrix requires space for each potential edge, whether it exists or not. However, the adjacency matrix requires no overhead for pointers, which can be a substantial cost, especially if the only information stored for an edge is one bit to indicate its existence. As the graph becomes denser, the adjacency matrix becomes relatively more space efficient. Sparse graphs are likely to have their adjacency list representation be more space efficient. Example 6.1.1 Assume that a vertex index requires two bytes, a pointer requires four bytes, and an edge weight requires two bytes. Then, each link node in the adjacency list needs \\(2 + 2 + 4 = 8\\) bytes. The adjacency matrix for the directed graph above requires \\(2 |\\mathbf{V}^2| = 50\\) bytes while the adjacency list requires \\(4 |\\mathbf{V}| + 8 |\\mathbf{E}| = 68\\) bytes. For the undirected version of the graph above, the adjacency matrix requires the same space as before, while the adjacency list requires \\(4 |\\mathbf{V}| + 8 |\\mathbf{E}| = 116\\) bytes (because there are now 12 edges represented instead of 6). The adjacency matrix often requires a higher asymptotic cost for an algorithm than would result if the adjacency list were used. The reason is that it is common for a graph algorithm to visit each neighbor of each vertex. Using the adjacency list, only the actual edges connecting a vertex to its neighbors are examined. However, the adjacency matrix must look at each of its \\(|\\mathbf{V}|\\) potential edges, yielding a total cost of \\(\\Theta(|\\mathbf{V}^2|)\\) time when the algorithm might otherwise require only \\(\\Theta(|\\mathbf{V}| + |\\mathbf{E}|)\\) time. This is a considerable disadvantage when the graph is sparse, but not when the graph is closer to full. Contact Us || Privacy | | License « 5.8. Finding Prime Numbers :: Contents :: 6.2. Graph Traversals » Contact Us | | Report a bug © Copyright 2011-2023 by OpenDSA Project Contributors and distributed under an MIT license. Last updated on Aug 13, 2024. Created using Sphinx 4.4.0. Summary*: Operating system*: Windows Mac OS Linux iOS Android Other Browser*: Chrome Safari Internet Explorer Opera Other Description*: Attach a screenshot (optional):"
            },
            {
                "role": "user",
                "content": "Can you summarize the key topics covered in this lecture?"
            },
            {
                "role": "assistant",
                "content": "This lecture covers foundational algorithmic concepts including problem formulation, correctness proof using induction, efficiency analysis using asymptotic notation, data structures (arrays, linked lists, BSTs, heaps), and sorting methods (selection sort, merge sort, counting sort). Additionally, it introduces the Word-RAM computational model, providing a theoretical framework for measuring algorithm efficiency."
            }
        ]
    },
    {
        "system": "Hello! I am Kiran, an AI assistant from Tampere University, dedicated to providing you with accurate and insightful assistance. My goal is to help you understand concepts, summarize lectures, and answer questions effectively.",
        "conversations": [
            {
                "role": "host",
                "content": "6.3. Shortest-Paths Problems — BM40A1500 Data Structures and Algorithms Username Password Forgot your password? Register Username: Password Confirm Password Email: BM40A1500 Data Structures and Algorithms Chapter 6 Graphs Show Source | | About « 6.2. Graph Traversals :: Contents :: 6.4. All-Pairs Shortest Paths » 6.3. Shortest-Paths Problems¶ On a road map, a road connecting two towns is typically labeled with its distance. We can model a road network as a directed graph whose edges are labeled with real numbers. These numbers represent the distance (or other cost metric, such as travel time) between two vertices. These labels may be called weights, costs, or distances, depending on the application. Given such a graph, a typical problem is to find the total length of the shortest path between two specified vertices. This is not a trivial problem, because the shortest path may not be along the edge (if any) connecting two vertices, but rather may be along a path involving one or more intermediate vertices. For example, in Figure 6.3.1, the cost of the path from \\(A\\) to \\(B\\) to \\(D\\) is 15. The cost of the edge directly from \\(A\\) to \\(D\\) is 20. The cost of the path from \\(A\\) to \\(C\\) to \\(B\\) to \\(D\\) is 10. Thus, the shortest path from \\(A\\) to \\(D\\) is 10 (rather than along the edge connecting \\(A\\) to \\(D\\)). We use the notation \\(\\mathbf{d}(A, D) = 10\\) to indicate that the shortest distance from \\(A\\) to \\(D\\) is 10. In Figure 6.3.1, there is no path from \\(E\\) to \\(B\\), so we set \\(\\mathbf{d}(E, B) = \\infty\\). We define \\(\\mathbf{w}(A, D) = 20\\) to be the weight of edge \\((A, D)\\), that is, the weight of the direct connection from \\(A\\) to \\(D\\). Because there is no edge from \\(E\\) to \\(B\\), \\(\\mathbf{w}(E, B) = \\infty\\). Note that \\(\\mathbf{w}(D, A) = \\infty\\) because the graph of Figure 6.3.1 is directed. We assume that all weights are positive. Figure 6.3.1: Example graph for shortest-path definitions. 6.3.1. Single-Source Shortest Paths¶ We will now present an algorithm to solve the single-source shortest paths problem. Given Vertex \\(S\\) in Graph \\(\\mathbf{G}\\), find a shortest path from \\(S\\) to every other vertex in \\(\\mathbf{G}\\). We might want only the shortest path between two vertices, \\(S\\) and \\(T\\). However in the worst case, finding the shortest path from \\(S\\) to \\(T\\) requires us to find the shortest paths from \\(S\\) to every other vertex as well. So there is no better algorithm (in the worst case) for finding the shortest path to a single vertex than to find shortest paths to all vertices. The algorithm described here will only compute the distance to every such vertex, rather than recording the actual path. Recording the path requires only simple modifications to the algorithm. Computer networks provide an application for the single-source shortest-paths problem. The goal is to find the cheapest way for one computer to broadcast a message to all other computers on the network. The network can be modeled by a graph with edge weights indicating time or cost to send a message to a neighboring computer. For unweighted graphs (or whenever all edges have the same cost), the single-source shortest paths can be found using a simple breadth-first search. When weights are added, BFS will not give the correct answer. One approach to solving this problem when the edges have differing weights might be to process the vertices in a fixed order. Label the vertices \\(v_0\\) to \\(v_{n-1}\\), with \\(S = v_0\\). When processing Vertex \\(v_1\\), we take the edge connecting \\(v_0\\) and \\(v_1\\). When processing \\(v_2\\), we consider the shortest distance from \\(v_0\\) to \\(v_2\\) and compare that to the shortest distance from \\(v_0\\) to \\(v_1\\) to \\(v_2\\). When processing Vertex \\(v_i\\), we consider the shortest path for Vertices \\(v_0\\) through \\(v_{i-1}\\) that have already been processed. Unfortunately, the true shortest path to \\(v_i\\) might go through Vertex \\(v_j\\) for \\(j > i\\). Such a path will not be considered by this algorithm. However, the problem would not occur if we process the vertices in order of distance from \\(S\\). Assume that we have processed in order of distance from \\(S\\) to the first \\(i-1\\) vertices that are closest to \\(S\\); call this set of vertices \\(\\mathbf{S}\\). We are now about to process the \\(i\\) th closest vertex; call it \\(X\\). A shortest path from \\(S\\) to \\(X\\) must have its next-to-last vertex in \\(S\\). Thus, \\[\\mathbf{d}(S, X) = \\min_{U \\in \\mathbf{S}}(\\mathbf{d}(S, U) + \\mathbf{w}(U, X)).\\] In other words, the shortest path from \\(S\\) to \\(X\\) is the minimum over all paths that go from \\(S\\) to \\(U\\), then have an edge from \\(U\\) to \\(X\\), where \\(U\\) is some vertex in \\(\\mathbf{S}\\). This solution is usually referred to as Dijkstra’s algorithm. It works by maintaining a distance estimate \\(\\mathbf{D}(X)\\) for all vertices \\(X\\) in \\(\\mathbf{V}\\). The elements of \\(\\mathbf{D}\\) are initialized to the value INFINITE. Vertices are processed in order of distance from \\(S\\). Whenever a vertex \\(v\\) is processed, \\(\\mathbf{D}(X)\\) is updated for every neighbor \\(X\\) of \\(V\\). Here is an implementation for Dijkstra’s algorithm. At the end, array D will contain the shortest distance values. # Compute shortest path distances from s, store them in D def Dijkstra(G, s, D): for i in range(G.nodeCount()): # Initialize D[i] = INFINITY D[s] = 0 for i in range(G.nodeCount()): # Process the vertices v = minVertex(G, D) # Find next-closest vertex G.setValue(v, VISITED) if D[v] == INFINITY: return # Unreachable for w in G.neighbors(v): if D[w] > D[v] + G.weight(v, w): D[w] = D[v] + G.weight(v, w) Settings Saving... Server Error Resubmit There are two reasonable solutions to the key issue of finding the unvisited vertex with minimum distance value during each pass through the main for loop. The first method is simply to scan through the list of \\(|\\mathbf{V}|\\) vertices searching for the minimum value, as follows: # Find the unvisited vertex with the smalled distance def minVertex(G, D): v = 0 # Initialize v to any unvisited vertex for i in range(G.nodeCount()): if G.getValue(i) != VISITED: v = i break for i in range(G.nodeCount()): # Now find smallest value if G.getValue(i) != VISITED and D[i] < D[v]: v = i return v Because this scan is done \\(|\\mathbf{V}|\\) times, and because each edge requires a constant-time update to D, the total cost for this approach is \\(\\Theta(|\\mathbf{V}|^2 + |\\mathbf{E}|) = \\Theta(|\\mathbf{V}|^2)\\), because \\(|\\mathbf{E}|\\) is in \\(O(|\\mathbf{V}|^2)\\). An alternative approach is to store unprocessed vertices in a min-heap ordered by their distance from the processed vertices. The next-closest vertex can be found in the heap in \\(\\Theta(\\log |\\mathbf{V}|)\\) time. Every time we modify \\(\\mathbf{D}(X)\\), we could reorder \\(X\\) in the heap by deleting and reinserting it. This is an example of a priority queue with priority update. To implement true priority updating, we would need to store with each vertex its position within the heap so that we can remove its old distances whenever it is updated by processing new edges. A simpler approach is to add the new (always smaller) distance value for a given vertex as a new record in the heap. The smallest value for a given vertex currently in the heap will be found first, and greater distance values found later will be ignored because the vertex will already be marked as VISITED. The only disadvantage to repeatedly inserting distance values in this way is that it will raise the number of elements in the heap from \\(\\Theta(|\\mathbf{V}|)\\) to \\(\\Theta(|\\mathbf{E}|)\\) in the worst case. But in practice this only adds a slight increase to the depth of the heap. The time complexity is \\(\\Theta((|\\mathbf{V}| + |\\mathbf{E}|) \\log |\\mathbf{E}|)\\), because for each edge that we process we must reorder the heap. We use the KVPair class to store key-value pairs in the heap, with the edge weight as the key and the target vertex as the value. here is the implementation for Dijkstra’s algorithm using a heap. # Dijkstra's shortest-paths: priority queue version def DijkstraPQ(G, s, D): int v # The current vertex E = [None] * G.edgeCount() # Heap for edges E[0] = (0, s) # Initial vertex H = MinHeap(E, 1, G.edgeCount()) for i in range(G.nodeCount()): # Initialize distance D[i] = INFINITY D[s] = 0 for i in range(G.nodeCount()): # For each vertex temp = H.removemin() if temp is None: return # Unreachable nodes exist v = temp.value() while G.getValue(v) == VISITED: temp = H.removemin() if temp is None: return # Unreachable nodes exist v = temp.value() G.setValue(v, VISITED) if D[v] == INFINITY: return # Unreachable for w in G.neighbors(v): if D[w] > D[v] + G.weight(v, w): # Update D D[w] = D[v] + G.weight(v, w) H.insert((D[w], w)) Using MinVertex to scan the vertex list for the minimum value is more efficient when the graph is dense, that is, when \\(|\\mathbf{E}|\\) approaches \\(|\\mathbf{V}|^2\\). Using a heap is more efficient when the graph is sparse because its cost is \\(\\Theta((|\\mathbf{V}| + |\\mathbf{E}|) \\log |\\mathbf{E}|)\\). However, when the graph is dense, this cost can become as great as \\(\\Theta(|\\mathbf{V}|^2 \\log |\\mathbf{E}|) = \\Theta(|V|^2 \\log |V|)\\). Contact Us || Privacy | | License « 6.2. Graph Traversals :: Contents :: 6.4. All-Pairs Shortest Paths » Contact Us | | Report a bug © Copyright 2011-2023 by OpenDSA Project Contributors and distributed under an MIT license. Last updated on Aug 13, 2024. Created using Sphinx 4.4.0. Summary*: Operating system*: Windows Mac OS Linux iOS Android Other Browser*: Chrome Safari Internet Explorer Opera Other Description*: Attach a screenshot (optional):"
            },
            {
                "role": "user",
                "content": "Can you summarize the key topics covered in this lecture?"
            },
            {
                "role": "assistant",
                "content": "This lecture covers foundational algorithmic concepts including problem formulation, correctness proof using induction, efficiency analysis using asymptotic notation, data structures (arrays, linked lists, BSTs, heaps), and sorting methods (selection sort, merge sort, counting sort). Additionally, it introduces the Word-RAM computational model, providing a theoretical framework for measuring algorithm efficiency."
            }
        ]
    },
    {
        "system": "Hello! I am Kiran, an AI assistant from Tampere University, dedicated to providing you with accurate and insightful assistance. My goal is to help you understand concepts, summarize lectures, and answer questions effectively.",
        "conversations": [
            {
                "role": "host",
                "content": "6.2. Graph Traversals — BM40A1500 Data Structures and Algorithms Username Password Forgot your password? Register Username: Password Confirm Password Email: BM40A1500 Data Structures and Algorithms Chapter 6 Graphs Show Source | | About « 6.1. Graphs Chapter Introduction :: Contents :: 6.3. Shortest-Paths Problems » 6.2. Graph Traversals¶ Many graph applications need to visit the vertices of a graph in some specific order based on the graph’s topology. This is known as a graph traversal and is similar in concept to a tree traversal. Recall that tree traversals visit every node exactly once, in some specified order such as preorder, inorder, or postorder. Multiple tree traversals exist because various applications require the nodes to be visited in a particular order. For example, to print a BST’s nodes in ascending order requires an inorder traversal as opposed to some other traversal. Standard graph traversal orders also exist. Each is appropriate for solving certain problems. For example, many problems in artificial intelligence programming are modeled using graphs. The problem domain might consist of a large collection of states, with connections between various pairs of states. Solving this sort of problem requires getting from a specified start state to a specified goal state by moving between states only through the connections. Typically, the start and goal states are not directly connected. To solve this problem, the vertices of the graph must be searched in some organized manner. Graph traversal algorithms typically begin with a start vertex and attempt to visit the remaining vertices from there. Graph traversals must deal with a number of troublesome cases. First, it might not be possible to reach all vertices from the start vertex. This occurs when the graph is not connected. Second, the graph might contain cycles, and we must make sure that cycles do not cause the algorithm to go into an infinite loop. Graph traversal algorithms can solve both of these problems by flagging vertices as VISITED when appropriate. At the beginning of the algorithm, no vertex is flagged as VISITED. The flag for a vertex is set when the vertex is first visited during the traversal. If a flagged vertex is encountered during traversal, it is not visited a second time. This keeps the program from going into an infinite loop when it encounters a cycle. Once the traversal algorithm completes, we can check to see if all vertices have been processed by checking whether they have the VISITED flag set. If not all vertices are flagged, we can continue the traversal from another unvisited vertex. Note that this process works regardless of whether the graph is directed or undirected. To ensure visiting all vertices, graphTraverse could be called as follows on a graph \\(\\mathbf{G}\\): def graphTraverse(G): for v in range(G.nodeCount()): # Initialize G.setValue(v, None) for v in range(G.nodeCount()): # Traverse if G.getValue(v) != VISITED: doTraversal(G, v) Function doTraversal might be implemented by using one of the graph traversals described next. 6.2.1. Depth-First Search¶ Our first method for organized graph traversal is called depth-first search (DFS). Whenever a vertex \\(v\\) is visited during the search, DFS will recursively visit all of \\(v\\) ‘s unvisited neighbors. Equivalently, DFS will add all edges leading out of \\(v\\) to a stack. The next vertex to be visited is determined by popping the stack and following that edge. The effect is to follow one branch through the graph to its conclusion, then it will back up and follow another branch, and so on. The DFS process can be used to define a depth-first search tree. This tree is composed of the edges that were followed to any new (unvisited) vertex during the traversal, and leaves out the edges that lead to already visited vertices. DFS can be applied to directed or undirected graphs. This visualization shows a graph and the result of performing a DFS on it, resulting in a depth-first search tree. Settings Saving... Server Error Resubmit Here is an implementation for the DFS algorithm. def DFS(G, v): PreVisit(G, v) G.setValue(v, VISITED) for n in G.neighbors(v): if G.getValue(n) != VISITED: DFS(G, n) PostVisit(G, v) This implementation contains calls to functions PreVisit and PostVisit. These functions specify what activity should take place during the search. Just as a preorder tree traversal requires action before the subtrees are visited, some graph traversals require that a vertex be processed before ones further along in the DFS. Alternatively, some applications require activity after the remaining vertices are processed; hence the call to function PostVisit. This would be a natural opportunity to make use of the visitor design pattern. The following visualization shows a random graph each time that you start it, so that you can see the behavior on different examples. It can show you DFS run on a directed graph or an undirected graph. Be sure to look at an example for each type of graph. DFS processes each edge once in a directed graph. In an undirected graph, DFS processes each edge from both directions. Each vertex must be visited, but only once, so the total cost is \\(\\Theta(|\\mathbf{V}| + |\\mathbf{E}|)\\). 6.2.2. Breadth-First Search¶ Our second graph traversal algorithm is known as a breadth-first search (BFS). BFS examines all vertices connected to the start vertex before visiting vertices further away. BFS is implemented similarly to DFS, except that a queue replaces the recursion stack. Note that if the graph is a tree and the start vertex is at the root, BFS is equivalent to visiting vertices level by level from top to bottom. This visualization shows a graph and the result of performing a BFS on it, resulting in a breadth-first search tree. Settings Saving... Server Error Resubmit Here is an implementation for BFS. def BFS(G, v): Q = new LQueue(G.nodeCount()) Q.enqueue(v) G.setValue(v, VISITED) while Q.length() > 0: # Process each vertex on Q v = Q.dequeue() PreVisit(G, v) for n in G.neighbors(v): if G.getValue(n) != VISITED: # Put neighbors on Q G.setValue(n, VISITED) Q.enqueue(n) PostVisit(G, v) The following visualization shows a random graph each time that you start it, so that you can see the behavior on different examples. It can show you BFS run on a directed graph or an undirected graph. Be sure to look at an example for each type of graph. Contact Us || Privacy | | License « 6.1. Graphs Chapter Introduction :: Contents :: 6.3. Shortest-Paths Problems » Contact Us | | Report a bug © Copyright 2011-2023 by OpenDSA Project Contributors and distributed under an MIT license. Last updated on Aug 13, 2024. Created using Sphinx 4.4.0. Summary*: Operating system*: Windows Mac OS Linux iOS Android Other Browser*: Chrome Safari Internet Explorer Opera Other Description*: Attach a screenshot (optional):"
            },
            {
                "role": "user",
                "content": "Can you summarize the key topics covered in this lecture?"
            },
            {
                "role": "assistant",
                "content": "This lecture covers foundational algorithmic concepts including problem formulation, correctness proof using induction, efficiency analysis using asymptotic notation, data structures (arrays, linked lists, BSTs, heaps), and sorting methods (selection sort, merge sort, counting sort). Additionally, it introduces the Word-RAM computational model, providing a theoretical framework for measuring algorithm efficiency."
            }
        ]
    },
    {
        "system": "Hello! I am Kiran, an AI assistant from Tampere University, dedicated to providing you with accurate and insightful assistance. My goal is to help you understand concepts, summarize lectures, and answer questions effectively.",
        "conversations": [
            {
                "role": "host",
                "content": "5.2. The Greedy Approach — BM40A1500 Data Structures and Algorithms Username Password Forgot your password? Register Username: Password Confirm Password Email: BM40A1500 Data Structures and Algorithms Chapter 5 Algorithm Design Principles Show Source | | About « 5.1. Algorithm Design Principles :: Contents :: 5.3. Divide and Conquer: Mergesort » 5.2. The Greedy Approach¶ Algorithm utilizing the greedy approach proceed step by step. On each step, the best option is selected based on the information available at that time (locally optimal choice). It is good to note that there are often multiple ways to define what is the optimal choice. For example, we need to visit a set of cities and we want to develop an algorithm that selects the route between the cities so that the total distance is as small as possible. The locally optimal choice might be a) the nearest city from the current city, or b) the nearest two cities that are not part of the route yet or are end points of the existing route. Depending on the choice we might end up in different solution. Often the choice that seems the best on the current step does not lead to the optimal solution at then end, but the greedy approach often helps to find a reasonably good solution. Therefore, the greedy approach is suitable when we want to find a good solution fast, but we do not necessarily need the optimal solution. However, there are also problems for which a greedy algorithm that always finds the optimal solution is known. Example 5.2.1 We need to schedule jobs out of a set of \\(N\\) jobs so that the profit is maximized. Each job takes unit time for execution. Each job has a deadline and profit. Profit is earned only if the job is completed by the deadline. \\[\\begin{split}\\begin{array}{c|c|c} \\textbf{Job} & \\textbf{Deadline} & \\textbf{Profit} \\\\ \\hline 1 & 3 & 100 \\\\ 2 & 2 & 50 \\\\ 3 & 1 & 20 \\\\ 4 & 2 & 100 \\end{array}\\end{split}\\] The simple solutions is to test all the possible schedules (orders of jobs) and to select then the one that gives the highest profit. However, there are \\(N!\\) different permutations of \\(N\\) jobs. Therefore such brute-force solution has a running time of \\(\\Theta(N!)\\). With a greedy approach we can be much more efficient. Let us consider the following simple greedy algorithm: # J = array of jobs; D = array of deadlines; P = array of profits def schedule(J, D, P): J.sort() # Sort the jobs in decreasing order of profit S = [None] * len(J) # Schedule array counter = 0 # counter to the amount of jobs in the schedule for i in range(len(J)): if D[i] >= counter + 1: S[counter] = J[i] # Add the job to the schedule if the job will be done before the deadline counter += 1 return S The algorithm “greedily” starts with the most profitable job, proceeds to the second most profitable, and so on. We start with sorting the job list which can be done in \\(\\Theta(N \\log(N))\\) time. The actual scheduling part goes through the job list once and is \\(\\Theta(N)\\). Therefore, the whole algorithm is \\(\\Theta(N \\log(N))\\). If we apply the algorithm to the above table of jobs we get the following solution: \\[\\begin{split}\\begin{array}{c|c|c|c} \\textbf{Job} & \\textbf{Deadline} & \\textbf{Profit} & \\textbf{Schedule} \\\\ \\hline 1 & 3 & 100 & 2 \\\\ 2 & 2 & 50 & - \\\\ 3 & 1 & 20 & - \\\\ 4 & 2 & 100 & 1 \\end{array}\\end{split}\\] And the total profit is \\(220\\). We can do better than this. Let us consider the following modification of the greedy algorithm: # J = array of jobs; D = array of deadlines; P = array of profits def schedule(J, D, P): J.sort() # Sort the jobs in decreasing order of profit S = [None] * len(J) # Schedule array for i in range(len(J)): for j in range(D[i] - 1, -1, -1): # find the latest possible free slot meeting the job's deadline if not S[j]: S[j] = J[i] # Add the job to the schedule if the job will be done before the deadline break return S The new version of the algorithm does not only choose the most profitable jobs first but also schedule them in locally optimal manner: \\[\\begin{split}\\begin{array}{c|c|c|c} \\textbf{Job} & \\textbf{Deadline} & \\textbf{Profit} & \\textbf{Schedule} \\\\ \\hline 1 & 3 & 100 & 3 \\\\ 2 & 2 & 50 & 1 \\\\ 3 & 1 & 20 & - \\\\ 4 & 2 & 100 & 2 \\end{array}\\end{split}\\] Now, the total profit is \\(270\\). It can be proven that the latter greedy algorithm provides always the optimal solution. However, due to the two nested loops, the average running time of this algorithm is \\(\\Theta(N^2)\\). Contact Us || Privacy | | License « 5.1. Algorithm Design Principles :: Contents :: 5.3. Divide and Conquer: Mergesort » Contact Us | | Report a bug © Copyright 2011-2023 by OpenDSA Project Contributors and distributed under an MIT license. Last updated on Aug 13, 2024. Created using Sphinx 4.4.0. Summary*: Operating system*: Windows Mac OS Linux iOS Android Other Browser*: Chrome Safari Internet Explorer Opera Other Description*: Attach a screenshot (optional):"
            },
            {
                "role": "user",
                "content": "Can you summarize the key topics covered in this lecture?"
            },
            {
                "role": "assistant",
                "content": "This lecture covers foundational algorithmic concepts including problem formulation, correctness proof using induction, efficiency analysis using asymptotic notation, data structures (arrays, linked lists, BSTs, heaps), and sorting methods (selection sort, merge sort, counting sort). Additionally, it introduces the Word-RAM computational model, providing a theoretical framework for measuring algorithm efficiency."
            }
        ]
    },
    {
        "system": "Hello! I am Kiran, an AI assistant from Tampere University, dedicated to providing you with accurate and insightful assistance. My goal is to help you understand concepts, summarize lectures, and answer questions effectively.",
        "conversations": [
            {
                "role": "host",
                "content": "3.6. Collision Resolution — BM40A1500 Data Structures and Algorithms Username Password Forgot your password? Register Username: Password Confirm Password Email: BM40A1500 Data Structures and Algorithms Chapter 3 Hashing Show Source | | About « 3.5. Bucket Hashing :: Contents :: 3.7. Deletion » 3.6. Collision Resolution¶ 3.6.1. Collision Resolution¶ We now turn to the most commonly used form of hashing: closed hashing with no bucketing, and a collision resolution policy that can potentially use any slot in the hash table. During insertion, the goal of collision resolution is to find a free slot in the hash table when the home position for the record is already occupied. We can view any collision resolution method as generating a sequence of hash table slots that can potentially hold the record. The first slot in the sequence will be the home position for the key. If the home position is occupied, then the collision resolution policy goes to the next slot in the sequence. If this is occupied as well, then another slot must be found, and so on. This sequence of slots is known as the probe sequence, and it is generated by some probe function that we will call p. Insertion works as follows. # Insert record E with key value K into hash table HT def hashInsert(E, K): pos = home = h(K) # Init probe sequence i = 1 while HT[pos].key != EMPTYKEY: # e.g. EMPTYKEY = None if K == HT[pos].key # Duplicates not allowed return pos = (home + p(K, i)) % M # Probe i += 1 HT[pos].elem = E HT[pos].key = K Method hashInsert first checks to see if the home slot for the key is empty. If the home slot is occupied, then we use the probe function \\(\\textbf{p}(k, i)\\) to locate a free slot in the table. Function p has two parameters, the key \\(k\\) and a count \\(i\\) of where in the probe sequence we wish to be. That is, to get the first position in the probe sequence after the home slot for key \\(K\\), we call \\(\\textbf{p}(K, 1)\\). For the next slot in the probe sequence, call \\(\\textbf{p}(K, 2)\\). Note that the probe function returns an offset from the original home position, rather than a slot in the hash table. Thus, the for loop in hashInsert is computing positions in the table at each iteration by adding the value returned from the probe function to the home position. The \\(i\\) th call to p returns the \\(i\\) th offset to be used. Searching in a hash table follows the same probe sequence that was followed when inserting records. In this way, a record not in its home position can be recovered. An implementation for the search procedure is as follows. # search record with key value K in the hashtable HT def hashSearch(K): home = pos = h(K) # Home and inital positions for key i = 1 while key != HT[pos].key and EMPTYKEY != HT[pos].key: pos = (home + p(K, i)) % M # Next on probe sequence i += 1 return HT[pos].elem # return EMPTYELEM if pos is not occupied in HT Both the insert and the search routines assume that at least one slot on the probe sequence of every key will be empty. Otherwise they will continue in an infinite loop on unsuccessful searches. Thus, the hash system should keep a count of the number of records stored, and refuse to insert into a table that has only one free slot. The simplest approach to collision resolution is simply to move down the table from the home slot until a free slot is found. This is known as linear probing. The probe function for simple linear probing is \\(\\textbf{p}(K, i) = i\\). That is, the \\(i\\) th offset on the probe sequence is just \\(i\\), meaning that the \\(i\\) th step is simply to move down \\(i\\) slots in the table. Once the bottom of the table is reached, the probe sequence wraps around to the beginning of the table (since the last step is to mod the result to the table size). Linear probing has the virtue that all slots in the table will be candidates for inserting a new record before the probe sequence returns to the home position. Settings Saving... Server Error Resubmit Can you see any reason why this might not be the best approach to collision resolution? 3.6.2. The Problem with Linear Probing¶ While linear probing is probably the first idea that comes to mind when considering collision resolution policies, it is not the only one possible. Probe function p allows us many options for how to do collision resolution. In fact, linear probing is one of the worst collision resolution methods. The main problem is illustrated by the next slideshow. Settings Saving... Server Error Resubmit Again, the ideal behavior for a collision resolution mechanism is that each empty slot in the table will have equal probability of receiving the next record inserted (assuming that every slot in the table has equal probability of being hashed to initially). This tendency of linear probing to cluster items together is known as primary clustering. Small clusters tend to merge into big clusters, making the problem worse. The objection to primary clustering is that it leads to long probe sequences. Contact Us || Privacy | | License « 3.5. Bucket Hashing :: Contents :: 3.7. Deletion » Contact Us | | Report a bug © Copyright 2011-2023 by OpenDSA Project Contributors and distributed under an MIT license. Last updated on Aug 13, 2024. Created using Sphinx 4.4.0. Summary*: Operating system*: Windows Mac OS Linux iOS Android Other Browser*: Chrome Safari Internet Explorer Opera Other Description*: Attach a screenshot (optional):"
            },
            {
                "role": "user",
                "content": "Can you summarize the key topics covered in this lecture?"
            },
            {
                "role": "assistant",
                "content": "This lecture covers foundational algorithmic concepts including problem formulation, correctness proof using induction, efficiency analysis using asymptotic notation, data structures (arrays, linked lists, BSTs, heaps), and sorting methods (selection sort, merge sort, counting sort). Additionally, it introduces the Word-RAM computational model, providing a theoretical framework for measuring algorithm efficiency."
            }
        ]
    },
    {
        "system": "Hello! I am Kiran, an AI assistant from Tampere University, dedicated to providing you with accurate and insightful assistance. My goal is to help you understand concepts, summarize lectures, and answer questions effectively.",
        "conversations": [
            {
                "role": "host",
                "content": "3.7. Deletion — BM40A1500 Data Structures and Algorithms Username Password Forgot your password? Register Username: Password Confirm Password Email: BM40A1500 Data Structures and Algorithms Chapter 3 Hashing Show Source | | About « 3.6. Collision Resolution :: Contents :: 4.1. Introduction » 3.7. Deletion¶ When deleting records from a hash table, there are two important considerations. Deleting a record must not hinder later searches. In other words, the search process must still pass through the newly emptied slot to reach records whose probe sequence passed through this slot. Thus, the delete process cannot simply mark the slot as empty, because this will isolate records further down the probe sequence. We do not want to make positions in the hash table unusable because of deletion. The freed slot should be available to a future insertion. Both of these problems can be resolved by placing a special mark in place of the deleted record, called a tombstone. The tombstone indicates that a record once occupied the slot but does so no longer. If a tombstone is encountered when searching along a probe sequence, the search procedure continues with the search. When a tombstone is encountered during insertion, that slot can be used to store the new record. However, to avoid inserting duplicate keys, it will still be necessary for the search procedure to follow the probe sequence until a truly empty position has been found, simply to verify that a duplicate is not in the table. However, the new record would actually be inserted into the slot of the first tombstone encountered. Settings Saving... Server Error Resubmit The use of tombstones allows searches to work correctly and allows reuse of deleted slots. However, after a series of intermixed insertion and deletion operations, some slots will contain tombstones. This will tend to lengthen the average distance from a record’s home position to the record itself, beyond where it could be if the tombstones did not exist. A typical database application will first load a collection of records into the hash table and then progress to a phase of intermixed insertions and deletions. After the table is loaded with the initial collection of records, the first few deletions will lengthen the average probe sequence distance for records (it will add tombstones). Over time, the average distance will reach an equilibrium point because insertions will tend to decrease the average distance by filling in tombstone slots. For example, after initially loading records into the database, the average path distance might be 1.2 (i.e., an average of 0.2 accesses per search beyond the home position will be required). After a series of insertions and deletions, this average distance might increase to 1.6 due to tombstones. This seems like a small increase, but it is three times longer on average beyond the home position than before deletions. Two possible solutions to this problem are Do a local reorganization upon deletion to try to shorten the average path length. For example, after deleting a key, continue to follow the probe sequence of that key and swap records further down the probe sequence into the slot of the recently deleted record (being careful not to remove any key from its probe sequence). This will not work for all collision resolution policies. Periodically rehash the table by reinserting all records into a new hash table. Not only will this remove the tombstones, but it also provides an opportunity to place the most frequently accessed records into their home positions. Contact Us || Privacy | | License « 3.6. Collision Resolution :: Contents :: 4.1. Introduction » Contact Us | | Report a bug © Copyright 2011-2023 by OpenDSA Project Contributors and distributed under an MIT license. Last updated on Aug 13, 2024. Created using Sphinx 4.4.0. Summary*: Operating system*: Windows Mac OS Linux iOS Android Other Browser*: Chrome Safari Internet Explorer Opera Other Description*: Attach a screenshot (optional):"
            },
            {
                "role": "user",
                "content": "Can you summarize the key topics covered in this lecture?"
            },
            {
                "role": "assistant",
                "content": "This lecture covers foundational algorithmic concepts including problem formulation, correctness proof using induction, efficiency analysis using asymptotic notation, data structures (arrays, linked lists, BSTs, heaps), and sorting methods (selection sort, merge sort, counting sort). Additionally, it introduces the Word-RAM computational model, providing a theoretical framework for measuring algorithm efficiency."
            }
        ]
    },
    {
        "system": "Hello! I am Kiran, an AI assistant from Tampere University, dedicated to providing you with accurate and insightful assistance. My goal is to help you understand concepts, summarize lectures, and answer questions effectively.",
        "conversations": [
            {
                "role": "host",
                "content": "3.2. Hash Function Principles — BM40A1500 Data Structures and Algorithms Username Password Forgot your password? Register Username: Password Confirm Password Email: BM40A1500 Data Structures and Algorithms Chapter 3 Hashing Show Source | | About « 3.1. Introduction :: Contents :: 3.3. Sample Hash Functions » 3.2. Hash Function Principles¶ Hashing generally takes records whose key values come from a large range and stores those records in a table with a relatively small number of slots. Collisions occur when two records hash to the same slot in the table. If we are careful—or lucky—when selecting a hash function, then the actual number of collisions will be few. Unfortunately, even under the best of circumstances, collisions are nearly unavoidable. To illustrate, consider a classroom full of students. What is the probability that some pair of students shares the same birthday (i.e., the same day of the year, not necessarily the same year)? If there are 23 students, then the odds are about even that two will share a birthday. This is despite the fact that there are 365 days in which students can have birthdays (ignoring leap years). On most days, no student in the class has a birthday. With more students, the probability of a shared birthday increases. The mapping of students to days based on their birthday is similar to assigning records to slots in a table (of size 365) using the birthday as a hash function. Note that this observation tells us nothing about which students share a birthday, or on which days of the year shared birthdays fall. Try it for yourself. You can use the calculator to see the probability of a collision. The default values are set to show the number of people in a room such that the chance of a duplicate is just over 50%. But you can set any table size and any number of records to determine the probability of a collision under those conditions. Use the calculator to answer the following questions. What is the minimum number of people that need to be in the room in order for there to be at least a 60% chance of two sharing a birthday? What is the minimum number of items that we need to hash to a table with 1000 slots to have at least a 50% chance of a collision? To be practical, a database organized by hashing must store records in a hash table that is not so large that it wastes space. To balance time and space efficiency, this means that the hash table should be around half full . Because collisions are extremely likely to occur under these conditions (by chance, any record inserted into a table that is half full should have a collision half of the time), does this mean that we need not worry about how well a hash function does at avoiding collisions? Absolutely not. The difference between using a good hash function and a bad hash function makes a big difference in practice in the number of records that must be examined when searching or inserting to the table. Technically, any function that maps all possible key values to a slot in the hash table is a hash function. In the extreme case, even a function that maps all records to the same slot in the array is a hash function, but it does nothing to help us find records during a search operation. We would like to pick a hash function that maps keys to slots in a way that makes each slot in the hash table have equal probablility of being filled for the actual set keys being used. Unfortunately, we normally have no control over the distribution of key values for the actual records in a given database or collection. So how well any particular hash function does depends on the actual distribution of the keys used within the allowable key range. In some cases, incoming data are well distributed across their key range. For example, if the input is a set of random numbers selected uniformly from the key range, any hash function that assigns the key range so that each slot in the hash table receives an equal share of the range will likely also distribute the input records uniformly within the table. However, in many applications the incoming records are highly clustered or otherwise poorly distributed. When input records are not well distributed throughout the key range it can be difficult to devise a hash function that does a good job of distributing the records throughout the table, especially if the input distribution is not known in advance. There are many reasons why data values might be poorly distributed. Natural frequency distributions tend to follow a common pattern where a few of the entities occur frequently while most entities occur relatively rarely. For example, consider the populations of the 100 largest cities in the United States. If you plot these populations on a numberline, most of them will be clustered toward the low side, with a few outliers on the high side. This is an example of a Zipf distribution. Viewed the other way, the home town for a given person is far more likely to be a particular large city than a particular small town. Collected data are likely to be skewed in some way. Field samples might be rounded to, say, the nearest 5 (i.e., all numbers end in 5 or 0). If the input is a collection of common English words, the beginning letter will be poorly distributed. Note that for items 2 and 3 on this list, either high- or low-order bits of the key are poorly distributed. When designing hash functions, we are generally faced with one of two situations: We know nothing about the distribution of the incoming keys. In this case, we wish to select a hash function that evenly distributes the key range across the hash table, while avoiding obvious opportunities for clustering such as hash functions that are sensitive to the high- or low-order bits of the key value. We know something about the distribution of the incoming keys. In this case, we should use a distribution-dependent hash function that avoids assigning clusters of related key values to the same hash table slot. For example, if hashing English words, we should not hash on the value of the first character because this is likely to be unevenly distributed. In the next module, you will see several examples of hash functions that illustrate these points. Contact Us || Privacy | | License « 3.1. Introduction :: Contents :: 3.3. Sample Hash Functions » Contact Us | | Report a bug © Copyright 2011-2023 by OpenDSA Project Contributors and distributed under an MIT license. Last updated on Aug 13, 2024. Created using Sphinx 4.4.0. Summary*: Operating system*: Windows Mac OS Linux iOS Android Other Browser*: Chrome Safari Internet Explorer Opera Other Description*: Attach a screenshot (optional):"
            },
            {
                "role": "user",
                "content": "Can you summarize the key topics covered in this lecture?"
            },
            {
                "role": "assistant",
                "content": "This lecture covers foundational algorithmic concepts including problem formulation, correctness proof using induction, efficiency analysis using asymptotic notation, data structures (arrays, linked lists, BSTs, heaps), and sorting methods (selection sort, merge sort, counting sort). Additionally, it introduces the Word-RAM computational model, providing a theoretical framework for measuring algorithm efficiency."
            }
        ]
    },
    {
        "system": "Hello! I am Kiran, an AI assistant from Tampere University, dedicated to providing you with accurate and insightful assistance. My goal is to help you understand concepts, summarize lectures, and answer questions effectively.",
        "conversations": [
            {
                "role": "host",
                "content": "3.3. Sample Hash Functions — BM40A1500 Data Structures and Algorithms Username Password Forgot your password? Register Username: Password Confirm Password Email: BM40A1500 Data Structures and Algorithms Chapter 3 Hashing Show Source | | About « 3.2. Hash Function Principles :: Contents :: 3.4. Open Hashing » 3.3. Sample Hash Functions¶ 3.3.1. Simple Mod Function¶ Consider the following hash function used to hash integers to a table of sixteen slots. def h(x): return x % 16 Here “%” is the symbol for the mod function. Settings Saving... Server Error Resubmit Recall that the values 0 to 15 can be represented with four bits (i.e., 0000 to 1111). The value returned by this hash function depends solely on the least significant four bits of the key. Because these bits are likely to be poorly distributed (as an example, a high percentage of the keys might be even numbers, which means that the low order bit is zero), the result will also be poorly distributed. This example shows that the size of the table \\(M\\) can have a big effect on the performance of a hash system because the table size is typically used as the modulus to ensure that the hash function produces a number in the range 0 to \\(M-1\\). 3.3.2. Binning¶ Say we are given keys in the range 0 to 999, and have a hash table of size 10. In this case, a possible hash function might simply divide the key value by 100. Thus, all keys in the range 0 to 99 would hash to slot 0, keys 100 to 199 would hash to slot 1, and so on. In other words, this hash function “bins” the first 100 keys to the first slot, the next 100 keys to the second slot, and so on. Binning in this way has the problem that it will cluster together keys if the distribution does not divide evenly on the high-order bits. In the above example, if more records have keys in the range 900-999 (first digit 9) than have keys in the range 100-199 (first digit 1), more records will hash to slot 9 than to slot 1. Likewise, if we pick too big a value for the key range and the actual key values are all relatively small, then most records will hash to slot 0. A similar, analogous problem arises if we were instead hashing strings based on the first letter in the string. Settings Saving... Server Error Resubmit In general with binning we store the record with key value \\(i\\) at array position \\(i/X\\) for some value \\(X\\) (using integer division). A problem with Binning is that we have to know the key range so that we can figure out what value to use for \\(X\\). Let’s assume that the keys are all in the range 0 to 999. Then we want to divide key values by 100 so that the result is in the range 0 to 9. There is no particular limit on the key range that binning could handle, so long as we know the maximum possible value in advance so that we can figure out what to divide the key value by. Alternatively, we could also take the result of any binning computation and then mod by the table size to be safe. So if we have keys that are bigger than 999 when dividing by 100, we can still make sure that the result is in the range 0 to 9 with a mod by 10 step at the end. Binning looks at the opposite part of the key value from the mod function. The mod function, for a power of two, looks at the low-order bits, while binning looks at the high-order bits. Or if you want to think in base 10 instead of base 2, modding by 10 or 100 looks at the low-order digits, while binning into an array of size 10 or 100 looks at the high-order digits. As another example, consider hashing a collection of keys whose values follow a normal distribution, as illustrated by Figure 3.3.1. Keys near the mean of the normal distribution are far more likely to occur than keys near the tails of the distribution. For a given slot, think of where the keys come from within the distribution. Binning would be taking thick slices out of the distribution and assign those slices to hash table slots. If we use a hash table of size 8, we would divide the key range into 8 equal-width slices and assign each slice to a slot in the table. Since a normal distribution is more likely to generate keys from the middle slice, the middle slot of the table is most likely to be used. In contrast, if we use the mod function, then we are assigning to any given slot in the table a series of thin slices in steps of 8. In the normal distribution, some of these slices associated with any given slot are near the tails, and some are near the center. Thus, each table slot is equally likely (roughly) to get a key value. Figure 3.3.1: A comparison of binning vs. modulus as a hash function.¶ 3.3.3. The Mid-Square Method¶ A good hash function to use with integer key values is the mid-square method. The mid-square method squares the key value, and then takes out the middle \\(r\\) bits of the result, giving a value in the range 0 to \\(2^{r}-1\\). This works well because most or all bits of the key value contribute to the result. For example, consider records whose keys are 4-digit numbers in base 10, as shown in Figure 3.3.2. The goal is to hash these key values to a table of size 100 (i.e., a range of 0 to 99). This range is equivalent to two digits in base 10. That is, \\(r = 2\\). If the input is the number 4567, squaring yields an 8-digit number, 20857489. The middle two digits of this result are 57. All digits of the original key value (equivalently, all bits when the number is viewed in binary) contribute to the middle two digits of the squared value. Thus, the result is not dominated by the distribution of the bottom digit or the top digit of the original key value. Of course, if the key values all tend to be small numbers, then their squares will only affect the low-order digits of the hash value. Figure 3.3.2: An example of the mid-square method. This image shows the traditional gradeschool long multiplication process. The value being squared is 4567. The result of squaring is 20857489. At the bottom, of the image, the value 4567 is show again, with each digit at the bottom of a “V”. The associated “V” is showing the digits from the result that are being affected by each digit of the input. That is, “4” affects the output digits 2, 0, 8, 5, an 7. But it has no affect on the last 3 digits. The key point is that the middle two digits of the result (5 and 7) are affected by every digit of the input.¶ Here is a little calculator for you to see how this works. Start with ‘4567’ as an example. 3.3.4. A Simple Hash Function for Strings¶ Now we will examine some hash functions suitable for storing strings of characters. We start with a simple summation function. def sascii(x, M): sum = sum(ord(c) for c in x) return sum % M This function sums the ASCII values of the letters in a string. If the hash table size \\(M\\) is small compared to the resulting summations, then this hash function should do a good job of distributing strings evenly among the hash table slots, because it gives equal weight to all characters in the string. This is an example of the folding method to designing a hash function. Note that the order of the characters in the string has no effect on the result. A similar method for integers would add the digits of the key value, assuming that there are enough digits to keep any one or two digits with bad distribution from skewing the results of the process and generate a sum much larger than \\(M\\). As with many other hash functions, the final step is to apply the modulus operator to the result, using table size \\(M\\) to generate a value within the table range. If the sum is not sufficiently large, then the modulus operator will yield a poor distribution. For example, because the ASCII value for ‘A’ is 65 and ‘Z’ is 90, sum will always be in the range 650 to 900 for a string of ten upper case letters. For a hash table of size 100 or less, a reasonable distribution results. For a hash table of size 1000, the distribution is terrible because only slots 650 to 900 can possibly be the home slot for some key value, and the values are not evenly distributed even within those slots. Now you can try it out with this calculator. 3.3.5. String Folding¶ Here is a much better hash function for strings. # Use folding on a string, summed 4 bytes at a time def sfold(s, M): sum = 0 mul = 1 for i in range(s.length()): mul = 1 if i % 4 == 0 else mul * 256 sum += ord(s[i]) * mul return abs(sum) % M This function takes a string as input. It processes the string four bytes at a time, and interprets each of the four-byte chunks as a single long integer value. The integer values for the four-byte chunks are added together. In the end, the resulting sum is converted to the range 0 to \\(M-1\\) using the modulus operator. For example, if the string “aaaabbbb” is passed to sfold, then the first four bytes (“aaaa”) will be interpreted as the integer value 1,633,771,873, and the next four bytes (“bbbb”) will be interpreted as the integer value 1,650,614,882. Their sum is 3,284,386,755 (when treated as an unsigned integer). If the table size is 101 then the modulus function will cause this key to hash to slot 75 in the table. Now you can try it out with this calculator. For any sufficiently long string, the sum for the integer quantities will typically cause a 32-bit integer to overflow (thus losing some of the high-order bits) because the resulting values are so large. But this causes no problems when the goal is to compute a hash function. The reason that hashing by summing the integer representation of four letters at a time is superior to summing one letter at a time is because the resulting values being summed have a bigger range. This still only works well for strings long enough (say at least 7-12 letters), but the original method would not work well for short strings either. There is nothing special about using four characters at a time. Other choices could be made. Another alternative would be to fold two characters at a time. Contact Us || Privacy | | License « 3.2. Hash Function Principles :: Contents :: 3.4. Open Hashing » Contact Us | | Report a bug © Copyright 2011-2023 by OpenDSA Project Contributors and distributed under an MIT license. Last updated on Aug 13, 2024. Created using Sphinx 4.4.0. Summary*: Operating system*: Windows Mac OS Linux iOS Android Other Browser*: Chrome Safari Internet Explorer Opera Other Description*: Attach a screenshot (optional):"
            },
            {
                "role": "user",
                "content": "Can you summarize the key topics covered in this lecture?"
            },
            {
                "role": "assistant",
                "content": "This lecture covers foundational algorithmic concepts including problem formulation, correctness proof using induction, efficiency analysis using asymptotic notation, data structures (arrays, linked lists, BSTs, heaps), and sorting methods (selection sort, merge sort, counting sort). Additionally, it introduces the Word-RAM computational model, providing a theoretical framework for measuring algorithm efficiency."
            }
        ]
    },
    {
        "system": "Hello! I am Kiran, an AI assistant from Tampere University, dedicated to providing you with accurate and insightful assistance. My goal is to help you understand concepts, summarize lectures, and answer questions effectively.",
        "conversations": [
            {
                "role": "host",
                "content": "3.1. Introduction — BM40A1500 Data Structures and Algorithms Username Password Forgot your password? Register Username: Password Confirm Password Email: BM40A1500 Data Structures and Algorithms Chapter 3 Hashing Show Source | | About « 2.9. Searching in an Array :: Contents :: 3.2. Hash Function Principles » 3.1. Introduction¶ Hashing is a method for storing and retrieving records from a database. It lets you insert, delete, and search for records based on a search key value. When properly implemented, these operations can be performed in constant time. In fact, a properly tuned hash system typically looks at only one or two records for each search, insert, or delete operation. This is far better than the \\(O(\\log n)\\) average cost required to do a binary search on a sorted array of \\(n\\) records, or the \\(O(\\log n)\\) average cost required to do an operation on a binary search tree. However, even though hashing is based on a very simple idea, it is surprisingly difficult to implement properly. Designers need to pay careful attention to all of the details involved with implementing a hash system. A hash system stores records in an array called a hash table, which we will call HT. Hashing works by performing a computation on a search key K in a way that is intended to identify the position in HT that contains the record with key K. The function that does this calculation is called the hash function, and will be denoted by the letter h. Since hashing schemes place records in the table in whatever order satisfies the needs of the address calculation, records are not ordered by value. A position in the hash table is also known as a slot. The number of slots in hash table HT will be denoted by the variable \\(M\\) with slots numbered from 0 to \\(M-1\\). The goal for a hashing system is to arrange things such that, for any key value K and some hash function \\(h\\), \\(i = \\mathbf{h}(K)\\) is a slot in the table such that \\(0 <= i < M\\), and we have the key of the record stored at HT[i] equal to K. Hashing is not good for applications where multiple records with the same key value are permitted. Hashing is not a good method for answering range searches. In other words, we cannot easily find all records (if any) whose key values fall within a certain range. Nor can we easily find the record with the minimum or maximum key value, or visit the records in key order. Hashing is most appropriate for answering the question, ‘What record, if any, has key value K?’ For applications where all search is done by exact-match queries, hashing is the search method of choice because it is extremely efficient when implemented correctly. As this tutorial shows, however, there are many approaches to hashing and it is easy to devise an inefficient implementation. Hashing is suitable for both in-memory and disk-based searching and is one of the two most widely used methods for organizing large databases stored on disk (the other is the B-tree). As a simple (though unrealistic) example of hashing, consider storing \\(n\\) records, each with a unique key value in the range 0 to \\(n-1\\). A record with key k can be stored in HT[k], and so the hash function is \\(\\mathbf{h}(k) = k\\). To find the record with key value k, look in HT[k]. Settings Saving... Server Error Resubmit In most applications, there are many more values in the key range than there are slots in the hash table. For a more realistic example, suppose the key can take any value in the range 0 to 65,535 (i.e., the key is a two-byte unsigned integer), and that we expect to store approximately 1000 records at any given time. It is impractical in this situation to use a hash table with 65,536 slots, because then the vast majority of the slots would be left empty. Instead, we must devise a hash function that allows us to store the records in a much smaller table. Because the key range is larger than the size of the table, at least some of the slots must be mapped to from multiple key values. Given a hash function h and two keys \\(k_1\\) and \\(k_2\\), if \\(\\mathbf{h}(k_1) = \\beta = \\mathbf{h}(k_2)\\) where \\(\\beta\\) is a slot in the table, then we say that \\(k_1\\) and \\(k_2\\) have a collision at slot \\(\\beta\\) under hash function h. Finding a record with key value K in a database organized by hashing follows a two-step procedure: Compute the table location \\(\\mathbf{h}(K)\\). Starting with slot \\(\\mathbf{h}(K)\\), locate the record containing key K using (if necessary) a collision resolution policy . Contact Us || Privacy | | License « 2.9. Searching in an Array :: Contents :: 3.2. Hash Function Principles » Contact Us | | Report a bug © Copyright 2011-2023 by OpenDSA Project Contributors and distributed under an MIT license. Last updated on Aug 13, 2024. Created using Sphinx 4.4.0. Summary*: Operating system*: Windows Mac OS Linux iOS Android Other Browser*: Chrome Safari Internet Explorer Opera Other Description*: Attach a screenshot (optional):"
            },
            {
                "role": "user",
                "content": "Can you summarize the key topics covered in this lecture?"
            },
            {
                "role": "assistant",
                "content": "This lecture covers foundational algorithmic concepts including problem formulation, correctness proof using induction, efficiency analysis using asymptotic notation, data structures (arrays, linked lists, BSTs, heaps), and sorting methods (selection sort, merge sort, counting sort). Additionally, it introduces the Word-RAM computational model, providing a theoretical framework for measuring algorithm efficiency."
            }
        ]
    },
    {
        "system": "Hello! I am Kiran, an AI assistant from Tampere University, dedicated to providing you with accurate and insightful assistance. My goal is to help you understand concepts, summarize lectures, and answer questions effectively.",
        "conversations": [
            {
                "role": "host",
                "content": "4.11. Heaps and Priority Queues — BM40A1500 Data Structures and Algorithms Username Password Forgot your password? Register Username: Password Confirm Password Email: BM40A1500 Data Structures and Algorithms Chapter 4 Recursion and Binary Trees Show Source | | About « 4.10. The AVL Tree :: Contents :: 4.12. Heapsort » 4.11. Heaps and Priority Queues¶ 4.11.1. Heaps and Priority Queues¶ There are many situations, both in real life and in computing applications, where we wish to choose the next “most important” from a collection of people, tasks, or objects. For example, doctors in a hospital emergency room often choose to see next the “most critical” patient rather than the one who arrived first. When scheduling programs for execution in a multitasking operating system, at any given moment there might be several programs (usually called jobs) ready to run. The next job selected is the one with the highest priority. Priority is indicated by a particular value associated with the job (and might change while the job remains in the wait list). When a collection of objects is organized by importance or priority, we call this a priority queue. A normal queue data structure will not implement a priority queue efficiently because search for the element with highest priority will take \\(\\Theta(n)\\) time. A list, whether sorted or not, will also require \\(\\Theta(n)\\) time for either insertion or removal. A BST that organizes records by priority could be used, with the total of \\(n\\) inserts and \\(n\\) remove operations requiring \\(\\Theta(n \\log n)\\) time in the average case. However, there is always the possibility that the BST will become unbalanced, leading to bad performance. Instead, we would like to find a data structure that is guaranteed to have good performance for this special application. This section presents the heap 1 data structure. A heap is defined by two properties. First, it is a complete binary tree, so heaps are nearly always implemented using the array representation for complete binary trees. Second, the values stored in a heap are partially ordered. This means that there is a relationship between the value stored at any node and the values of its children. There are two variants of the heap, depending on the definition of this relationship. 1 Note that the term “heap” is also sometimes used to refer to free store. A max heap has the property that every node stores a value that is greater than or equal to the value of either of its children. Because the root has a value greater than or equal to its children, which in turn have values greater than or equal to their children, the root stores the maximum of all values in the tree. A min heap has the property that every node stores a value that is less than or equal to that of its children. Because the root has a value less than or equal to its children, which in turn have values less than or equal to their children, the root stores the minimum of all values in the tree. Note that there is no necessary relationship between the value of a node and that of its sibling in either the min heap or the max heap. For example, it is possible that the values for all nodes in the left subtree of the root are greater than the values for every node of the right subtree. We can contrast BSTs and heaps by the strength of their ordering relationships. A BST defines a total order on its nodes in that, given the positions for any two nodes in the tree, the one to the “left” (equivalently, the one appearing earlier in an inorder traversal) has a smaller key value than the one to the “right”. In contrast, a heap implements a partial order. Given their positions, we can determine the relative order for the key values of two nodes in the heap only if one is a descendant of the other. Min heaps and max heaps both have their uses. For example, the Heapsort uses the max heap, while the Replacement Selection algorithm used for external sorting uses a min heap. The examples in the rest of this section will use a max heap. Be sure not to confuse the logical representation of a heap with its physical implementation by means of the array-based complete binary tree. The two are not synonymous because the logical view of the heap is actually a tree structure, while the typical physical implementation uses an array. One way to build a heap is to insert the elements one at a time. Method insert will insert a new element \\(V\\) into the heap. Settings Saving... Server Error Resubmit You might expect the heap insertion process to be similar to the insert function for a BST, starting at the root and working down through the heap. However, this approach is not likely to work because the heap must maintain the shape of a complete binary tree. Equivalently, if the heap takes up the first \\(n\\) positions of its array prior to the call to insert, it must take up the first \\(n+1\\) positions after. To accomplish this, insert first places \\(V\\) at position \\(n\\) of the array. Of course, \\(V\\) is unlikely to be in the correct position. To move \\(V\\) to the right place, it is compared to its parent’s value. If the value of \\(V\\) is less than or equal to the value of its parent, then it is in the correct place and the insert routine is finished. If the value of \\(V\\) is greater than that of its parent, then the two elements swap positions. From here, the process of comparing \\(V\\) to its (current) parent continues until \\(V\\) reaches its correct position. Since the heap is a complete binary tree, its height is guaranteed to be the minimum possible. In particular, a heap containing \\(n\\) nodes will have a height of \\(\\Theta(\\log n)\\). Intuitively, we can see that this must be true because each level that we add will slightly more than double the number of nodes in the tree (the \\(i\\) th level has \\(2^i\\) nodes, and the sum of the first \\(i\\) levels is \\(2^{i+1}-1\\)). Starting at 1, we can double only \\(\\log n\\) times to reach a value of \\(n\\). To be precise, the height of a heap with \\(n\\) nodes is \\(\\lceil \\log n + 1 \\rceil\\). Each call to insert takes \\(\\Theta(\\log n)\\) time in the worst case, because the value being inserted can move at most the distance from the bottom of the tree to the top of the tree. Thus, to insert \\(n\\) values into the heap, if we insert them one at a time, will take \\(\\Theta(n \\log n)\\) time in the worst case. 4.11.2. Building a Heap¶ If all \\(n\\) values are available at the beginning of the building process, we can build the heap faster than just inserting the values into the heap one by one. Consider this example, with two possible ways to heapify an initial set of values in an array. Settings Saving... Server Error Resubmit Figure 4.11.1: Two series of exchanges to build a max heap. (a) This heap is built by a series of nine exchanges in the order (4-2), (4-1), (2-1), (5-2), (5-4), (6-3), (6-5), (7-5), (7-6). (b) This heap is built by a series of four exchanges in the order (5-2), (7-3), (7-1), (6-1). From this example, it is clear that the heap for any given set of numbers is not unique, and we see that some rearrangements of the input values require fewer exchanges than others to build the heap. So, how do we pick the best rearrangement? One good algorithm stems from induction. Suppose that the left and right subtrees of the root are already heaps, and \\(R\\) is the name of the element at the root. This situation is illustrated by this figure: Figure 4.11.2: Final stage in the heap-building algorithm. Both subtrees of node \\(R\\) are heaps. All that remains is to push \\(R\\) down to its proper level in the heap. In this case there are two possibilities. \\(R\\) has a value greater than or equal to its two children. In this case, construction is complete. \\(R\\) has a value less than one or both of its children. \\(R\\) should be exchanged with the child that has greater value. The result will be a heap, except that \\(R\\) might still be less than one or both of its (new) children. In this case, we simply continue the process of “pushing down” \\(R\\) until it reaches a level where it is greater than its children, or is a leaf node. This process is implemented by the private method siftdown. This approach assumes that the subtrees are already heaps, suggesting that a complete algorithm can be obtained by visiting the nodes in some order such that the children of a node are visited before the node itself. One simple way to do this is simply to work from the high index of the array to the low index. Actually, the build process need not visit the leaf nodes (they can never move down because they are already at the bottom), so the building algorithm can start in the middle of the array, with the first internal node. Here is a visualization of the heap build process. Settings Saving... Server Error Resubmit What is the cost of buildHeap? Clearly it is the sum of the costs for the calls to siftdown. Each siftdown operation can cost at most the number of levels it takes for the node being sifted to reach the bottom of the tree. In any complete tree, approximately half of the nodes are leaves and so cannot be moved downward at all. One quarter of the nodes are one level above the leaves, and so their elements can move down at most one level. At each step up the tree we get half the number of nodes as were at the previous level, and an additional height of one. The maximum sum of total distances that elements can go is therefore \\[\\sum_{i=1}^{\\log n} (i-1)\\frac{n}{2^i} = \\frac{n}{2}\\sum_{i=1}^{\\log n} \\frac{i-1}{2^{i-1}}.\\] The summation on the right is known to have a closed-form solution of approximately 2, so this algorithm takes \\(\\Theta(n)\\) time in the worst case. This is far better than building the heap one element at a time, which would cost \\(\\Theta(n \\log n)\\) in the worst case. It is also faster than the \\(\\Theta(n \\log n)\\) average-case time and \\(\\Theta(n^2)\\) worst-case time required to build the BST. Settings Saving... Server Error Resubmit 4.11.3. Removing from the heap or updating an object’s priority¶ Settings Saving... Server Error Resubmit Because the heap is \\(\\log n\\) levels deep, the cost of deleting the maximum element is \\(\\Theta(\\log n)\\) in the average and worst cases. Settings Saving... Server Error Resubmit For some applications, objects might get their priority modified. One solution in this case is to remove the object and reinsert it. To do this, the application needs to know the position of the object in the heap. Another option is to change the priority value of the object, and then update its position in the heap. Note that a remove operation implicitly has to do this anyway, since when the last element in the heap is swapped with the one being removed, that value might be either too small or too big for its new position. So we use a utility method called update in both the remove and modify methods to handle this process. 4.11.4. Priority Queues¶ The heap is a natural implementation for the priority queue discussed at the beginning of this section. Jobs can be added to the heap (using their priority value as the ordering key) when needed. Method removemax can be called whenever a new job is to be executed. Some applications of priority queues require the ability to change the priority of an object already stored in the queue. This might require that the object’s position in the heap representation be updated. Unfortunately, a max heap is not efficient when searching for an arbitrary value; it is only good for finding the maximum value. However, if we already know the index for an object within the heap, it is a simple matter to update its priority (including changing its position to maintain the heap property) or remove it. The remove method takes as input the position of the node to be removed from the heap. A typical implementation for priority queues requiring updating of priorities will need to use an auxiliary data structure that supports efficient search for objects (such as a BST). Records in the auxiliary data structure will store the object’s heap index, so that the object’s priority can be updated. Priority queues can be helpful for solving graph problems such as single-source shortest paths and minimal-cost spanning tree. For a story about Priority Queues and dragons, see Computational Fairy Tales: Stacks, Queues, Priority Queues, and the Prince's Complaint Line. Contact Us || Privacy | | License « 4.10. The AVL Tree :: Contents :: 4.12. Heapsort » Contact Us | | Report a bug © Copyright 2011-2023 by OpenDSA Project Contributors and distributed under an MIT license. Last updated on Aug 13, 2024. Created using Sphinx 4.4.0. Summary*: Operating system*: Windows Mac OS Linux iOS Android Other Browser*: Chrome Safari Internet Explorer Opera Other Description*: Attach a screenshot (optional):"
            },
            {
                "role": "user",
                "content": "Can you summarize the key topics covered in this lecture?"
            },
            {
                "role": "assistant",
                "content": "This lecture covers foundational algorithmic concepts including problem formulation, correctness proof using induction, efficiency analysis using asymptotic notation, data structures (arrays, linked lists, BSTs, heaps), and sorting methods (selection sort, merge sort, counting sort). Additionally, it introduces the Word-RAM computational model, providing a theoretical framework for measuring algorithm efficiency."
            }
        ]
    },
    {
        "system": "Hello! I am Kiran, an AI assistant from Tampere University, dedicated to providing you with accurate and insightful assistance. My goal is to help you understand concepts, summarize lectures, and answer questions effectively.",
        "conversations": [
            {
                "role": "host",
                "content": "4.12. Heapsort — BM40A1500 Data Structures and Algorithms Username Password Forgot your password? Register Username: Password Confirm Password Email: BM40A1500 Data Structures and Algorithms Chapter 4 Recursion and Binary Trees Show Source | | About « 4.11. Heaps and Priority Queues :: Contents :: 5.1. Algorithm Design Principles » 4.12. Heapsort¶ 4.12.1. Heapsort¶ A good sorting algorithm can be devised based on a tree structure more suited to the purpose. In particular, we would like the tree to be balanced, space efficient, and fast. The algorithm should take advantage of the fact that sorting is a special-purpose application in that all of the values to be stored are available at the start. This means that we do not necessarily need to insert one value at a time into the tree structure. Heapsort is based on the heap data structure. Heapsort has all of the advantages just listed. The complete binary tree is balanced, its array representation is space efficient, and we can load all values into the tree at once, taking advantage of the efficient buildheap function. The asymptotic performance of Heapsort when all of the records have unique key values is \\(\\Theta(n \\log n)\\) in the best, average, and worst cases. .. It is not as fast as Quicksort in the average case (by a constant .. factor), but Heapsort has special properties that will make it .. particularly useful for .. external sorting algorithms, .. used when sorting data sets too large to fit in main memory. Settings Saving... Server Error Resubmit A complete implementation is as follows. def heapsort(A): # The heap constructor invokes the buildheap method H = MaxHeap(A) # Now sort for i in range(len(A)): H.removemax(); # Removemax places max at end of heap Later in the course, you will learn sorting algorithms that are a bit faster than Heapsort in the average case. These include, for example, Quicksort that is faster than Heapsort by a constant factor (technically both are still Theta(n log n)). However, Heapsort has one special advantage over the other commonly used sorting algorithms. Building the heap is relatively cheap, requiring Theta(n) time. Removing the maximum-valued record from the heap requires Theta(log n) time in the worst case. Thus, if we wish to find the k records with the largest key values in an array, we can do so in time Theta(n + k log n). If k is small, this is a substantial improvement over the time required to find the k largest-valued records using one of the other sorting methods described earlier (many of which would require sorting all of the array first). Another special case arises when all of the records being sorted have the same key value. This represents the best case for Heapsort. This is because removing the smallest value requires only constant time, since the value swapped to the top is never pushed down the heap. Contact Us || Privacy | | License « 4.11. Heaps and Priority Queues :: Contents :: 5.1. Algorithm Design Principles » Contact Us | | Report a bug © Copyright 2011-2023 by OpenDSA Project Contributors and distributed under an MIT license. Last updated on Aug 13, 2024. Created using Sphinx 4.4.0. Summary*: Operating system*: Windows Mac OS Linux iOS Android Other Browser*: Chrome Safari Internet Explorer Opera Other Description*: Attach a screenshot (optional):"
            },
            {
                "role": "user",
                "content": "Can you summarize the key topics covered in this lecture?"
            },
            {
                "role": "assistant",
                "content": "This lecture covers foundational algorithmic concepts including problem formulation, correctness proof using induction, efficiency analysis using asymptotic notation, data structures (arrays, linked lists, BSTs, heaps), and sorting methods (selection sort, merge sort, counting sort). Additionally, it introduces the Word-RAM computational model, providing a theoretical framework for measuring algorithm efficiency."
            }
        ]
    },
    {
        "system": "Hello! I am Kiran, an AI assistant from Tampere University, dedicated to providing you with accurate and insightful assistance. My goal is to help you understand concepts, summarize lectures, and answer questions effectively.",
        "conversations": [
            {
                "role": "host",
                "content": "Chapter 0 Introduction for Data Structures and Algorithms — BM40A1500 Data Structures and Algorithms Username Password Forgot your password? Register Username: Password Confirm Password Email: BM40A1500 Data Structures and Algorithms Table Of Contents Show Source | | About Contents :: 0.1. Data Structures and Algorithms » Search the book Enter search terms or a module, class or function name. Chapter 0 Introduction for Data Structures and Algorithms¶ 0.1. Data Structures and Algorithms 0.1.1. Introduction 0.1.2. A Philosophy of Data Structures 0.1.3. Selecting a Data Structure 0.2. Problems, Algorithms, and Programs 0.2.1. Problems 0.2.2. Algorithms 0.2.3. Programs 0.2.4. Summary 0.3. Abstract Data Types Chapter 1 Algorithm Analysis¶ 1.1. Chapter Introduction 1.2. Comparing Algorithms 1.2.1. Introduction 1.2.2. Basic Operations and Input Size 1.2.3. Growth Rates 1.3. Best, Worst, and Average Cases 1.4. Faster Computer, or Faster Algorithm? 1.5. Asymptotic Analysis and Upper Bounds 1.5.1. Upper Bounds 1.5.2. Simplifying Rules 1.5.3. Summary 1.6. Lower Bounds and \\(\\Theta\\) Notation 1.6.1. Lower Bounds 1.6.2. Theta Notation 1.6.3. Classifying Functions 1.7. Calculating Program Running Time 1.8. Common Misunderstandings 1.9. Multiple Parameters 1.10. Space Bounds Chapter 2 Linear Data Structures¶ 2.1. The List ADT 2.1.1. Defining the ADT 2.2. Array-Based List Implementation 2.2.1. Insert 2.2.2. Append and Remove 2.3. Linked Lists 2.3.1. Linked Lists 2.3.2. Why This Has Problems 2.3.3. A Better Solution 2.3.4. Linked List insertion 2.3.5. Linked List Remove 2.4. Comparison of List Implementations 2.4.1. Space Comparison 2.4.2. Time Comparison 2.5. Stacks 2.5.1. Stack Terminology and Implementation 2.5.2. Array-Based Stacks 2.5.3. Pop 2.6. Linked Stacks 2.6.1. Linked Stack Implementation 2.6.2. Linked Stack Push 2.6.3. Linked Stack Pop 2.6.4. Comparison of Array-Based and Linked Stacks 2.7. Queues 2.7.1. Queue Terminology and Implementation 2.7.2. Array-Based Queues 2.7.3. The Circular Queue 2.8. Linked Queues 2.8.1. Linked Enqueue 2.8.2. Linked Dequeue 2.8.3. Comparison of Array-Based and Linked Queues 2.9. Searching in an Array 2.9.1. Sequential Search 2.9.2. Binary Search 2.9.3. Analyzing Binary Search Chapter 3 Hashing¶ 3.1. Introduction 3.2. Hash Function Principles 3.3. Sample Hash Functions 3.3.1. Simple Mod Function 3.3.2. Binning 3.3.3. The Mid-Square Method 3.3.4. A Simple Hash Function for Strings 3.3.5. String Folding 3.4. Open Hashing 3.4.1. Open Hashing 3.5. Bucket Hashing 3.5.1. Bucket Hashing 3.5.2. An Alternate Approach 3.6. Collision Resolution 3.6.1. Collision Resolution 3.6.2. The Problem with Linear Probing 3.7. Deletion Chapter 4 Recursion and Binary Trees¶ 4.1. Introduction 4.2. Writing a recursive function 4.3. Tracing Recursive Code 4.3.1. A Domino Analogy 4.3.2. Towers of Hanoi 4.4. Binary Trees 4.4.1. Introduction 4.4.2. Definitions and Properties 4.4.3. Binary Tree as a Recursive Data Structure 4.5. Binary Search Trees 4.5.1. Binary Search Tree Definition 4.5.2. BST Search 4.5.3. BST Insert 4.5.4. BST Remove 4.5.5. BST Analysis 4.6. Binary Tree Traversals 4.6.1. Preorder Traversal 4.6.2. Postorder Traversal 4.6.3. Inorder Traversal 4.7. Dictionary Implementation Using a BST 4.8. Balanced Trees 4.9. 2-3 Trees 4.10. The AVL Tree 4.11. Heaps and Priority Queues 4.11.1. Heaps and Priority Queues 4.11.2. Building a Heap 4.11.3. Removing from the heap or updating an object’s priority 4.11.4. Priority Queues 4.12. Heapsort 4.12.1. Heapsort Chapter 5 Algorithm Design Principles¶ 5.1. Algorithm Design Principles 5.2. The Greedy Approach 5.3. Divide and Conquer: Mergesort 5.3.1. Mergesort Concepts 5.4. Divide and Conquer: Quicksort 5.4.1. Introduction 5.4.2. Partition 5.4.3. Putting It Together 5.4.4. Quicksort Analysis 5.5. Backtracking, and Branch and Bound 5.5.1. Backtracking 5.5.2. From loops to recursion 5.5.3. Implementing the search 5.5.4. Subsets 5.5.5. Permutations 5.5.6. The N Queens Problem 5.5.7. Allocating tasks 5.5.8. Branch And Bound 5.6. Dynamic Programming 5.6.1. Dynamic Programming 5.6.2. Computing Fibonacci Numbers 5.6.3. The Knapsack Problem 5.6.4. Chained Matrix Multiplication 5.7. Introduction to Probabilistic Algorithms 5.7.1. Probabilistic Algorithms 5.8. Finding Prime Numbers Chapter 6 Graphs¶ 6.1. Graphs Chapter Introduction 6.1.1. Graph Terminology and Implementation 6.1.2. Graph Representations 6.2. Graph Traversals 6.2.1. Depth-First Search 6.2.2. Breadth-First Search 6.3. Shortest-Paths Problems 6.3.1. Single-Source Shortest Paths 6.4. All-Pairs Shortest Paths 6.5. Minimal Cost Spanning Trees 6.5.1. Prim’s Algorithm 6.5.2. Prim’s Algorithm Alternative Implementation 6.6. Kruskal’s Algorithm Chapter 7 NP-completeness¶ 7.1. Limits to Computing 7.1.1. Limits to Computing 7.2. Reductions 7.2.1. Example: The Pairing Problem 7.2.2. Reduction and Finding a Lower Bound 7.2.3. The Reduction Template 7.2.4. Two Multiplication Examples 7.2.5. Bounds Theorems 7.2.6. The Cost of Making a Simple Polygon 7.3. NP-Completeness 7.3.1. Hard Problems 7.3.2. The Theory of NP-Completeness 7.4. Examples of NP-complete problems 7.4.1. Formula Satisfiability 7.4.2. Vertex Cover 7.4.3. Hamiltonian Cycle 7.4.4. Traveling Salesman 7.5. NP-Completeness Proofs 7.5.1. NP-Completeness Proofs 7.5.2. Hamiltonian Cycle to Traveling Salesman 7.6. Coping with NP-Complete Problems Chapter 8 Appendix¶ 8.1. Glossary Index Search Page Contact Us || Privacy | | License Contents :: 0.1. Data Structures and Algorithms » Contact Us | | Report a bug © Copyright 2011-2023 by OpenDSA Project Contributors and distributed under an MIT license. Last updated on Aug 13, 2024. Created using Sphinx 4.4.0. Summary*: Operating system*: Windows Mac OS Linux iOS Android Other Browser*: Chrome Safari Internet Explorer Opera Other Description*: Attach a screenshot (optional):"
            },
            {
                "role": "user",
                "content": "Can you summarize the key topics covered in this lecture?"
            },
            {
                "role": "assistant",
                "content": "This lecture covers foundational algorithmic concepts including problem formulation, correctness proof using induction, efficiency analysis using asymptotic notation, data structures (arrays, linked lists, BSTs, heaps), and sorting methods (selection sort, merge sort, counting sort). Additionally, it introduces the Word-RAM computational model, providing a theoretical framework for measuring algorithm efficiency."
            }
        ]
    },
    {
        "system": "Hello! I am Kiran, an AI assistant from Tampere University, dedicated to providing you with accurate and insightful assistance. My goal is to help you understand concepts, summarize lectures, and answer questions effectively.",
        "conversations": [
            {
                "role": "host",
                "content": "0.1. Data Structures and Algorithms — BM40A1500 Data Structures and Algorithms Username Password Forgot your password? Register Username: Password Confirm Password Email: BM40A1500 Data Structures and Algorithms Chapter 0 Introduction for Data Structures and Algorithms Show Source | | About « Chapter 0 Introduction for Data Structures and Algorithms :: Contents :: 0.2. Problems, Algorithms, and Programs » 0.1. Data Structures and Algorithms¶ 0.1.1. Introduction¶ How many cities with more than 250,000 people lie within 500 miles of Dallas, Texas? How many people in my company make over $100,000 per year? Can we connect all of our telephone customers with less than 1,000 miles of cable? To answer questions like these, it is not enough to have the necessary information. We must organize that information in a way that allows us to find the answers in time to satisfy our needs. Representing information is fundamental to computer science. The primary purpose of most computer programs is not to perform calculations, but to store and retrieve information—usually as fast as possible. For this reason, the study of data structures and the algorithms that manipulate them is at the heart of computer science. And that is what this book is about—helping you to understand how to structure information to support efficient processing. Any course on Data Structures and Algorithms will try to teach you about three things: It will present a collection of commonly used data structures and algorithms. These form a programmer’s basic “toolkit”. For many problems, some data structure or algorithm in the toolkit will provide a good solution. We focus on data structures and algorithms that have proven over time to be most useful. It will introduce the idea of tradeoffs, and reinforce the concept that there are costs and benefits associated with every data structure or algorithm. This is done by describing, for each data structure, the amount of space and time required for typical operations. For each algorithm, we examine the time required for key input types. It will teach you how to measure the effectiveness of a data structure or algorithm. Only through such measurement can you determine which data structure in your toolkit is most appropriate for a new problem. The techniques presented also allow you to judge the merits of new data structures that you or others might invent. There are often many approaches to solving a problem. How do we choose between them? At the heart of computer program design are two (sometimes conflicting) goals: To design an algorithm that is easy to understand, code, and debug. To design an algorithm that makes efficient use of the computer’s resources. Ideally, the resulting program is true to both of these goals. We might say that such a program is “elegant.” While the algorithms and program code examples presented here attempt to be elegant in this sense, it is not the purpose of this book to explicitly treat issues related to goal (1). These are primarily concerns for the discipline of Software Engineering. Rather, we mostly focus on issues relating to goal (2). How do we measure efficiency? Our method for evaluating the efficiency of an algorithm or computer program is called asymptotic analysis. Asymptotic analysis also gives a way to define the inherent difficulty of a problem. Throughout the book we use asymptotic analysis techniques to estimate the time cost for every algorithm presented. This allows you to see how each algorithm compares to other algorithms for solving the same problem in terms of its efficiency. 0.1.2. A Philosophy of Data Structures¶ You might think that with ever more powerful computers, program efficiency is becoming less important. After all, processor speed and memory size still continue to improve. Won’t today’s efficiency problem be solved by tomorrow’s hardware? As we develop more powerful computers, our history so far has always been to use that additional computing power to tackle more complex problems, be it in the form of more sophisticated user interfaces, bigger problem sizes, or new problems previously deemed computationally infeasible. More complex problems demand more computation, making the need for efficient programs even greater. Unfortunately, as tasks become more complex, they become less like our everyday experience. So today’s computer scientists must be trained to have a thorough understanding of the principles behind efficient program design, because their ordinary life experiences often do not apply when designing computer programs. In the most general sense, a data structure is any data representation and its associated operations. Even an integer or floating point number stored on the computer can be viewed as a simple data structure. More commonly, people use the term “data structure” to mean an organization or structuring for a collection of data items. A sorted list of integers stored in an array is an example of such a structuring. These ideas are explored further in a discussion of Abstract Data Types. Given sufficient space to store a collection of data items, it is always possible to search for specified items within the collection, print or otherwise process the data items in any desired order, or modify the value of any particular data item. The most obvious example is an unsorted array containing all of the data items. It is possible to perform all necessary operations on an unsorted array. However, using the proper data structure can make the difference between a program running in a few seconds and one requiring many days. For example, searching for a given record in a hash table is much faster than searching for it in an unsorted array. A solution is said to be efficient if it solves the problem within the required resource constraints. Examples of resource constraints include the total space available to store the data—possibly divided into separate main memory and disk space constraints—and the time allowed to perform each subtask. A solution is sometimes said to be efficient if it requires fewer resources than known alternatives, regardless of whether it meets any particular requirements. The cost of a solution is the amount of resources that the solution consumes. Most often, cost is measured in terms of one key resource such as time, with the implied assumption that the solution meets the other resource constraints. 0.1.3. Selecting a Data Structure¶ It should go without saying that people write programs to solve problems. However, sometimes programmers forget this. So it is crucial to keep this truism in mind when selecting a data structure to solve a particular problem. Only by first analyzing the problem to determine the performance goals that must be achieved can there be any hope of selecting the right data structure for the job. Poor program designers ignore this analysis step and apply a data structure that they are familiar with but which is inappropriate to the problem. The result is typically a slow program. Conversely, there is no sense in adopting a complex representation to “improve” a program that can meet its performance goals when implemented using a simpler design. When selecting a data structure to solve a problem, you should follow these steps. Analyze your problem to determine the basic operations that must be supported. Examples of basic operations include inserting a data item into the data structure, deleting a data item from the data structure, and finding a specified data item. Quantify the resource constraints for each operation. Select the data structure that best meets these requirements. This three-step approach to selecting a data structure operationalizes a data-centered view of the design process. The first concern is for the data and the operations to be performed on them, the next concern is the representation for those data, and the final concern is the implementation of that representation. Resource constraints on certain key operations, such as search, inserting data records, and deleting data records, normally drive the data structure selection process. Many issues relating to the relative importance of these operations are addressed by the following three questions, which you should ask yourself whenever you must choose a data structure. Are all data items inserted into the data structure at the beginning, or are insertions interspersed with other operations? Static applications (where the data are loaded at the beginning and never change) typically get by with simpler data structures to get an efficient implementation, while dynamic applications often require something more complicated. Can data items be deleted? If so, this will probably make the implementation more complicated. Are all data items processed in some well-defined order, or is search for specific data items allowed? “Random access” search generally requires more complex data structures. Each data structure has associated costs and benefits. In practice, it is hardly ever true that one data structure is better than another for use in all situations. If one data structure or algorithm is superior to another in all respects, the inferior one will usually have long been forgotten. For nearly every data structure and algorithm presented in this book, you will see examples of where it is the best choice. Some of the examples might surprise you. A data structure requires a certain amount of space for each data item it stores, a certain amount of time to perform a single basic operation, and a certain amount of programming effort. Each problem has constraints on available space and time. Each solution to a problem makes use of the basic operations in some relative proportion, and the data structure selection process must account for this. Only after a careful analysis of your problem’s characteristics can you determine the best data structure for the task. Example 0.1.1 A bank must support many types of transactions with its customers, but we will examine a simple model where customers wish to open accounts, close accounts, and add money or withdraw money from accounts. We can consider this problem at two distinct levels: (1) the requirements for the physical infrastructure and workflow process that the bank uses in its interactions with its customers, and (2) the requirements for the database system that manages the accounts. The typical customer opens and closes accounts far less often than accessing the account. Customers are willing to spend many minutes during the process of opening or closing the account, but are typically not willing to wait more than a brief time for individual account transactions such as a deposit or withdrawal. These observations can be considered as informal specifications for the time constraints on the problem. It is common practice for banks to provide two tiers of service. Human tellers or automated teller machines (ATMs) support customer access to account balances and updates such as deposits and withdrawals. Special service representatives are typically provided (during restricted hours) to handle opening and closing accounts. Teller and ATM transactions are expected to take little time. Opening or closing an account can take much longer (perhaps up to an hour from the customer’s perspective). From a database perspective, we see that ATM transactions do not modify the database significantly. For simplicity, assume that if money is added or removed, this transaction simply changes the value stored in an account record. Adding a new account to the database is allowed to take several minutes. Deleting an account need have no time constraint, because from the customer’s point of view all that matters is that all the money be returned (equivalent to a withdrawal). From the bank’s point of view, the account record might be removed from the database system after business hours, or at the end of the monthly account cycle. When considering the choice of data structure to use in the database system that manages customer accounts, we see that a data structure that has little concern for the cost of deletion, but is highly efficient for search and moderately efficient for insertion, should meet the resource constraints imposed by this problem. Records are accessible by unique account number (sometimes called an exact-match query). One data structure that meets these requirements is the hash table. Hash tables allow for extremely fast exact-match search. A record can be modified quickly when the modification does not affect its space requirements. Hash tables also support efficient insertion of new records. While deletions can also be supported efficiently, too many deletions lead to some degradation in performance for the remaining operations. However, the hash table can be reorganized periodically to restore the system to peak efficiency. Such reorganization can occur offline so as not to affect ATM transactions. Example 0.1.2 A company is developing a database system containing information about cities and towns in the United States. There are many thousands of cities and towns, and the database program should allow users to find information about a particular place by name (another example of an exact-match query). Users should also be able to find all places that match a particular value or range of values for attributes such as location or population size. This is known as a range query. A reasonable database system must answer queries quickly enough to satisfy the patience of a typical user. For an exact-match query, a few seconds is satisfactory. If the database is meant to support range queries that can return many cities that match the query specification, the user might tolerate the entire operation to take longer, perhaps on the order of a minute. To meet this requirement, it will be necessary to support operations that process range queries efficiently by processing all cities in the range as a batch, rather than as a series of operations on individual cities. The hash table suggested in the previous example is inappropriate for implementing our city database, because it cannot perform efficient range queries. The B$^+$-tree supports large databases, insertion and deletion of data records, and range queries. However, a simple linear index would be more appropriate if the database is created once, and then never changed, such as an atlas distributed on a CD or accessed from a website. Contact Us || Privacy | | License « Chapter 0 Introduction for Data Structures and Algorithms :: Contents :: 0.2. Problems, Algorithms, and Programs » Contact Us | | Report a bug © Copyright 2011-2023 by OpenDSA Project Contributors and distributed under an MIT license. Last updated on Aug 13, 2024. Created using Sphinx 4.4.0. Summary*: Operating system*: Windows Mac OS Linux iOS Android Other Browser*: Chrome Safari Internet Explorer Opera Other Description*: Attach a screenshot (optional):"
            },
            {
                "role": "user",
                "content": "Can you summarize the key topics covered in this lecture?"
            },
            {
                "role": "assistant",
                "content": "This lecture covers foundational algorithmic concepts including problem formulation, correctness proof using induction, efficiency analysis using asymptotic notation, data structures (arrays, linked lists, BSTs, heaps), and sorting methods (selection sort, merge sort, counting sort). Additionally, it introduces the Word-RAM computational model, providing a theoretical framework for measuring algorithm efficiency."
            }
        ]
    },
    {
        "system": "Hello! I am Kiran, an AI assistant from Tampere University, dedicated to providing you with accurate and insightful assistance. My goal is to help you understand concepts, summarize lectures, and answer questions effectively.",
        "conversations": [
            {
                "role": "host",
                "content": "6.6. Kruskal’s Algorithm — BM40A1500 Data Structures and Algorithms Username Password Forgot your password? Register Username: Password Confirm Password Email: BM40A1500 Data Structures and Algorithms Chapter 6 Graphs Show Source | | About « 6.5. Minimal Cost Spanning Trees :: Contents :: 7.1. Limits to Computing » 6.6. Kruskal’s Algorithm¶ Our next MCST algorithm is commonly referred to as Kruskal’s algorithm. Kruskal’s algorithm is also a simple, greedy algorithm. First partition the set of vertices into \\(|\\mathbf{V}|\\) disjoint sets, each consisting of one vertex. Then process the edges in order of weight. An edge is added to the MCST, and two disjoint sets combined, if the edge connects two vertices in different disjoint sets. This process is repeated until only one disjoint set remains. Settings Saving... Server Error Resubmit The edges can be processed in order of weight by using a min-heap. This is generally faster than sorting the edges first, because in practice we need only visit a small fraction of the edges before completing the MCST. This is an example of finding only a few smallest elements in a list. The only tricky part to this algorithm is determining if two vertices belong to the same equivalence class. Fortunately, the ideal algorithm is available for the purpose — the UNION/FIND. Here is an implementation for Kruskal’s algorithm. Class KruskalElem is used to store the edges on the min-heap. # Kruskal's MST algorithm def Kruskal(G): A = ParPtrTree(G.nodeCount()) # Equivalence array E = [None] * G.edgeCount() # Minheap array edgecnt = 0 # Count of edges for i in range(G.nodeCount()): # Put edges in the array for w in G.neighbors(i): E[edgecnt] = (G.weight(i, w), i, w) edgecnt += 1 H = MinHeap(E, edgecnt, edgecnt) numMST = G.nodeCount() # Initially n disjoint classes while numMST>1: # Combine equivalence classes temp = H.removemin() # Next cheapest edge if temp is None: return # Must have disconnected vertices _w, v, u = temp if A.differ(v, u): # If in different classes A.UNION(v, u) # Combine equiv classes AddEdgetoMST(v, u) # Add this edge to MST numMST -= 1 # One less MST Kruskal’s algorithm is dominated by the time required to process the edges. The differ and UNION functions are nearly constant in time if path compression and weighted union is used. Thus, the total cost of the algorithm is \\(\\Theta(|\\mathbf{E}| \\log |\\mathbf{E}|)\\) in the worst case, when nearly all edges must be processed before all the edges of the spanning tree are found and the algorithm can stop. More often the edges of the spanning tree are the shorter ones,and only about \\(|\\mathbf{V}|\\) edges must be processed. If so, the cost is often close to \\(\\Theta(|\\mathbf{V}| \\log |\\mathbf{E}|)\\) in the average case. Contact Us || Privacy | | License « 6.5. Minimal Cost Spanning Trees :: Contents :: 7.1. Limits to Computing » Contact Us | | Report a bug © Copyright 2011-2023 by OpenDSA Project Contributors and distributed under an MIT license. Last updated on Aug 13, 2024. Created using Sphinx 4.4.0. Summary*: Operating system*: Windows Mac OS Linux iOS Android Other Browser*: Chrome Safari Internet Explorer Opera Other Description*: Attach a screenshot (optional):"
            },
            {
                "role": "user",
                "content": "Can you summarize the key topics covered in this lecture?"
            },
            {
                "role": "assistant",
                "content": "This lecture covers foundational algorithmic concepts including problem formulation, correctness proof using induction, efficiency analysis using asymptotic notation, data structures (arrays, linked lists, BSTs, heaps), and sorting methods (selection sort, merge sort, counting sort). Additionally, it introduces the Word-RAM computational model, providing a theoretical framework for measuring algorithm efficiency."
            }
        ]
    },
    {
        "system": "Hello! I am Kiran, an AI assistant from Tampere University, dedicated to providing you with accurate and insightful assistance. My goal is to help you understand concepts, summarize lectures, and answer questions effectively.",
        "conversations": [
            {
                "role": "host",
                "content": "7.1. Limits to Computing — BM40A1500 Data Structures and Algorithms Username Password Forgot your password? Register Username: Password Confirm Password Email: BM40A1500 Data Structures and Algorithms Chapter 7 NP-completeness Show Source | | About « 6.6. Kruskal’s Algorithm :: Contents :: 7.2. Reductions » 7.1. Limits to Computing¶ 7.1.1. Limits to Computing¶ By now you have studied many data structures that can be used in a wide variety of problems, and many examples of efficient algorithms. In general, our search algorithms strive to be at worst in \\(O(\\log n)\\) to find a record, and our sorting algorithms strive to be in \\(O(n \\log n)\\). You might have come across a few algorithms have higher asymptotic complexity. Both Floyd’s all-pairs shortest-paths algorithm and standard matrix multiply have running times of \\(\\Theta(n^3)\\) (though for both, the amount of data being processed is \\(\\Theta(n^2)\\) since they both act on \\(n \\times n\\) matricies). We can solve many problems efficiently because we have available (and choose to use) efficient algorithms. Given any problem for which you know some algorithm, it is always possible to write an inefficient algorithm to “solve” the problem. For example, consider a sorting algorithm that tests every possible permutation of its input until it finds the correct permutation that provides a sorted list. The running time for this algorithm would be unacceptably high, because it is proportional to the number of permutations which is \\(n!\\) for \\(n\\) inputs. When solving the minimum-cost spanning tree problem, if we were to test every possible subset of edges to see which forms the shortest minimum spanning tree, the amount of work would be proportional to \\(2^{|{\\rm E}|}\\) for a graph with \\(|{\\rm E}|\\) edges. Fortunately, for both of these problems we have more clever algorithms that allow us to find answers (relatively) quickly without explicitly testing every possible solution. Unfortunately, there are many computing problems for which the best possible algorithm takes a long time to run. A simple example is the Towers of Hanoi problem which requires \\(2^n\\) moves to “solve” a tower with \\(n\\) disks. It is not possible for any computer program that solves the Towers of Hanoi problem to run in less than \\(\\Omega(2^n)\\) time, because that many moves must be printed out. Besides those problems whose solutions must take a long time to run, there are also many problems for which we simply do not know if there are efficient algorithms or not. The best algorithms that we know for such problems are very slow, but perhaps there are better ones waiting to be discovered. Of course, while having a problem with high running time is bad, it is even worse to have a problem that cannot be solved at all! Such problems (which are called unsolveable problems) do exist. The classic example of such a problem is deciding whether an arbitrary computer program will go into an infinite loop when processing a specified input. This is known as the halting problem. Contact Us || Privacy | | License « 6.6. Kruskal’s Algorithm :: Contents :: 7.2. Reductions » Contact Us | | Report a bug © Copyright 2011-2023 by OpenDSA Project Contributors and distributed under an MIT license. Last updated on Aug 13, 2024. Created using Sphinx 4.4.0. Summary*: Operating system*: Windows Mac OS Linux iOS Android Other Browser*: Chrome Safari Internet Explorer Opera Other Description*: Attach a screenshot (optional):"
            },
            {
                "role": "user",
                "content": "Can you summarize the key topics covered in this lecture?"
            },
            {
                "role": "assistant",
                "content": "This lecture covers foundational algorithmic concepts including problem formulation, correctness proof using induction, efficiency analysis using asymptotic notation, data structures (arrays, linked lists, BSTs, heaps), and sorting methods (selection sort, merge sort, counting sort). Additionally, it introduces the Word-RAM computational model, providing a theoretical framework for measuring algorithm efficiency."
            }
        ]
    },
    {
        "system": "Hello! I am Kiran, an AI assistant from Tampere University, dedicated to providing you with accurate and insightful assistance. My goal is to help you understand concepts, summarize lectures, and answer questions effectively.",
        "conversations": [
            {
                "role": "host",
                "content": "2.1. The List ADT — BM40A1500 Data Structures and Algorithms Username Password Forgot your password? Register Username: Password Confirm Password Email: BM40A1500 Data Structures and Algorithms Chapter 2 Linear Data Structures Show Source | | About « 1.10. Space Bounds :: Contents :: 2.2. Array-Based List Implementation » 2.1. The List ADT¶ We all have an intuitive understanding of what we mean by a “list”. We want to turn this intuitive understanding into a concrete data structure with implementations for its operations. The most important concept related to lists is that of position. In other words, we perceive that there is a first element in the list, a second element, and so on. So, define a list to be a finite, ordered sequence of data items known as elements. This is close to the mathematical concept of a sequence. “Ordered” in this definition means that each element has a position in the list. So the term “ordered” in this context does not mean that the list elements are sorted by value. (Of course, we can always choose to sort the elements on the list if we want; it’s just that keeping the elements sorted is not an inherent property of being a list.) Each list element must have some data type. In the simple list implementations discussed in this chapter, all elements of the list are usually assumed to have the same data type, although there is no conceptual objection to lists whose elements have differing data types if the application requires it. The operations defined as part of the list ADT do not depend on the elemental data type. For example, the list ADT can be used for lists of integers, lists of characters, lists of payroll records, even lists of lists. A list is said to be empty when it contains no elements. The number of elements currently stored is called the length of the list. The beginning of the list is called the head, the end of the list is called the tail. We need some notation to show the contents of a list, so we will use the same angle bracket notation that is normally used to represent sequences. To be consistent with standard array indexing, the first position on the list is denoted as 0. Thus, if there are \\(n\\) elements in the list, they are given positions 0 through \\(n-1\\) as \\(\\langle\\ a_0,\\ a_1,\\ ...,\\ a_{n-1}\\ \\rangle\\). The subscript indicates an element’s position within the list. Using this notation, the empty list would appear as \\(\\langle\\ \\rangle\\). 2.1.1. Defining the ADT¶ What basic operations do we want our lists to support? Our common intuition about lists tells us that a list should be able to grow and shrink in size as we insert and remove elements. We should be able to insert and remove elements from anywhere in the list. We should be able to gain access to any element’s value, either to read it or to change it. We must be able to create and clear (or reinitialize) lists. It is also convenient to access the next or previous element from the “current” one. Now we can define the ADT for a list object in terms of a set of operations on that object. We will use an interface to formally define the list ADT. List defines the member functions that any list implementation inheriting from it must support, along with their parameters and return types. True to the notion of an ADT, an interface does not specify how operations are implemented. Two complete implementations are presented later in later modules, both of which use the same list ADT to define their operations. But they are considerably different in approaches and in their space/time tradeoffs. The code below presents our list ADT in Python. Any implementation for a container class such as a list should be able to support different data types for the elements. Languages that support generics (Java) or templates (C++) give more control over the element types. The comments given with each member function describe what it is intended to do. However, an explanation of the basic design should help make this clearer. Given that we wish to support the concept of a sequence, with access to any position in the list, the need for many of the member functions such as insert and moveToPos is clear. The key design decision embodied in this ADT is support for the concept of a current position. For example, member moveToStart sets the current position to be the first element on the list, while methods next and prev move the current position to the next and previous elements, respectively. The intention is that any implementation for this ADT support the concept of a current position. The current position is where any action such as insertion or deletion will take place. An alternative design is to factor out position as a separate position object, sometimes referred to as an iterator. # List class ADT. class List: # Remove all contents from the list, so it is once again empty. def clear(self): # Insert \"it\" at the current location. def insert(self, it): # Append \"it\" at the end of the list. def append(self, it): # Remove and return the current element. def remove(self): # Set the current position to the start of the list. def moveToStart(self): # Set the current position to the end of the list. def moveToEnd(self): # Move the current position one step left, no change if already at beginning. def prev(self): # Move the current position one step right, no change if already at end. def next(self): # Return the number of elements in the list. def length(self): # Return the position of the current element. def currPos(self): # Set the current position to \"pos\". def moveToPos(self, pos): # Return true if current position is at end of the list. def isAtEnd(self): # Return the current element. def getValue(self): # Tell if the list is empty or not. def isEmpty(): Settings Saving... Server Error Resubmit The List member functions allow you to build a list with elements in any desired order, and to access any desired position in the list. You might notice that the clear method is a “convenience” method, since it could be implemented by means of the other member functions in the same asymptotic time. A list can be iterated through as follows: L.moveToStart() while not L.isAtEnd(): it = L.getValue() doSomething(it) L.next() In this example, each element of the list in turn is stored in it, and passed to the doSomething function. The loop terminates when the current position reaches the end of the list. The list class declaration presented here is just one of many possible interpretations for lists. Our list interface provides most of the operations that one naturally expects to perform on lists and serves to illustrate the issues relevant to implementing the list data structure. As an example of using the list ADT, here is a function to return true if there is an occurrence of a given integer in the list, and false otherwise. The find method needs no knowledge about the specific list implementation, just the list ADT. def find(L, k): \"\"\"Return true if k is in list L, false otherwise.\"\"\" for n in L: if k == n: return true # Found k return false # k not found In languages that support it, this implementation for find could be rewritten as a generic or template with respect to the element type. While making it more flexible, even generic types still are limited in their ability to handle different data types stored on the list. In particular, for the find function generic types would only work when the description for the object being searched for (k in the function) is of the same type as the objects themselves. They also have to be comparable when using the == operator. A more realistic situation is that we are searching for a record that contains a key field whose value matches k. Similar functions to find and return a composite type based on a key value can be created using the list implementation, but to do so requires some agreement between the list ADT and the find function on the concept of a key, and on how keys may be compared. There are two standard approaches to implementing lists, the array-based list, and the linked list. Contact Us || Privacy | | License « 1.10. Space Bounds :: Contents :: 2.2. Array-Based List Implementation » Contact Us | | Report a bug © Copyright 2011-2023 by OpenDSA Project Contributors and distributed under an MIT license. Last updated on Aug 13, 2024. Created using Sphinx 4.4.0. Summary*: Operating system*: Windows Mac OS Linux iOS Android Other Browser*: Chrome Safari Internet Explorer Opera Other Description*: Attach a screenshot (optional):"
            },
            {
                "role": "user",
                "content": "Can you summarize the key topics covered in this lecture?"
            },
            {
                "role": "assistant",
                "content": "This lecture covers foundational algorithmic concepts including problem formulation, correctness proof using induction, efficiency analysis using asymptotic notation, data structures (arrays, linked lists, BSTs, heaps), and sorting methods (selection sort, merge sort, counting sort). Additionally, it introduces the Word-RAM computational model, providing a theoretical framework for measuring algorithm efficiency."
            }
        ]
    },
    {
        "system": "Hello! I am Kiran, an AI assistant from Tampere University, dedicated to providing you with accurate and insightful assistance. My goal is to help you understand concepts, summarize lectures, and answer questions effectively.",
        "conversations": [
            {
                "role": "host",
                "content": "2.4. Comparison of List Implementations — BM40A1500 Data Structures and Algorithms Username Password Forgot your password? Register Username: Password Confirm Password Email: BM40A1500 Data Structures and Algorithms Chapter 2 Linear Data Structures Show Source | | About « 2.3. Linked Lists :: Contents :: 2.5. Stacks » 2.4. Comparison of List Implementations¶ 2.4.1. Space Comparison¶ Now that you have seen two substantially different implementations for lists, it is natural to ask which is better. In particular, if you must implement a list for some task, which implementation should you choose? Given a collection of elements to store, they take up some amount of space whether they are simple integers or large objects with many fields. Any container data structure like a list then requires some additional space to organize the elements being stored. This additional space is called overhead. Array-based lists have the disadvantage that their size must be predetermined before the array can be allocated. Array-based lists cannot grow beyond their predetermined size. Whenever the list contains only a few elements, a substantial amount of space might be tied up in a largely empty array. This empty space is the overhead required by the array-based list. Linked lists have the advantage that they only need space for the objects actually on the list. There is no limit to the number of elements on a linked list, as long as there is free store memory available. The amount of space required by a linked list is \\(\\Theta(n)\\), while the space required by the array-based list implementation is \\(\\Omega(n)\\), but can be greater. Array-based lists have the advantage that there is no wasted space for an individual element. Linked lists require that an extra pointer for the next field be added to every list node. So the linked list has these next pointers as overhead. If the element size is small, then the overhead for links can be a significant fraction of the total storage. When the array for the array-based list is completely filled, there is no wasted space, and so no overhead. The array-based list will then be more space efficient, by a constant factor, than the linked implementation. A simple formula can be used to determine whether the array-based list or the linked list implementation will be more space efficient in a particular situation. Call \\(n\\) the number of elements currently in the list, \\(P\\) the size of a pointer in storage units (typically four bytes), \\(E\\) the size of a data element in storage units (this could be anything, from one bit for a Boolean variable on up to thousands of bytes or more for complex records), and \\(D\\) the maximum number of list elements that can be stored in the array. The amount of space required for the array-based list is \\(DE\\), regardless of the number of elements actually stored in the list at any given time. The amount of space required for the linked list is \\(n(P + E)\\). The smaller of these expressions for a given value \\(n\\) determines the more space-efficient implementation for \\(n\\) elements. In general, the linked implementation requires less space than the array-based implementation when relatively few elements are in the list. Conversely, the array-based implementation becomes more space efficient when the array is close to full. Using the equation, we can solve for \\(n\\) to determine the break-even point beyond which the array-based implementation is more space efficient in any particular situation. This occurs when \\[n > DE/(P + E).\\] If \\(P = E\\), then the break-even point is at \\(D/2\\). This would happen if the element field is either a four-byte int value or a pointer, and the next field is a typical four-byte pointer. That is, the array-based implementation would be more efficient (if the link field and the element field are the same size) whenever the array is more than half full. As a rule of thumb, linked lists are more space efficient when implementing lists whose number of elements varies widely or is unknown. Array-based lists are generally more space efficient when the user knows in advance approximately how large the list will become, and can be confident that the list will never grow beyond a certain limit. 2.4.2. Time Comparison¶ Array-based lists are faster for access by position. Positions can easily be adjusted forwards or backwards by the next and prev methods. These operations always take \\(\\Theta(1)\\) time. In contrast, singly linked lists have no explicit access to the previous element, and access by position requires that we march down the list from the front (or the current position) to the specified position. Both of these operations require \\(\\Theta(n)\\) time in the average and worst cases, if we assume that each position on the list is equally likely to be accessed on any call to prev or moveToPos. Given a pointer to a suitable location in the list, the insert and remove methods for linked lists require only \\(\\Theta(1)\\) time. Array-based lists must shift the remainder of the list up or down within the array. This requires \\(\\Theta(n)\\) time in the average and worst cases. For many applications, the time to insert and delete elements dominates all other operations. For this reason, linked lists are often preferred to array-based lists. When implementing the array-based list, an implementor could allow the size of the array to grow and shrink depending on the number of elements that are actually stored. This data structure is known as a dynamic array. For example, both the Java and C++/STL Vector classes implement a dynamic array, and JavaScript arrays are always dynamic. Dynamic arrays allow the programmer to get around the limitation on the traditional array that its size cannot be changed once the array has been created. This also means that space need not be allocated to the dynamic array until it is to be used. The disadvantage of this approach is that it takes time to deal with space adjustments on the array. Each time the array grows in size, its contents must be copied. A good implementation of the dynamic array will grow and shrink the array in such a way as to keep the overall cost for a series of insert/delete operations relatively inexpensive, even though an occasional insert/delete operation might be expensive. A simple rule of thumb is to double the size of the array when it becomes full, and to cut the array size in half when it becomes one quarter full. To analyze the overall cost of dynamic array operations over time, we need to use a technique known as amortized analysis. Contact Us || Privacy | | License « 2.3. Linked Lists :: Contents :: 2.5. Stacks » Contact Us | | Report a bug © Copyright 2011-2023 by OpenDSA Project Contributors and distributed under an MIT license. Last updated on Aug 13, 2024. Created using Sphinx 4.4.0. Summary*: Operating system*: Windows Mac OS Linux iOS Android Other Browser*: Chrome Safari Internet Explorer Opera Other Description*: Attach a screenshot (optional):"
            },
            {
                "role": "user",
                "content": "Can you summarize the key topics covered in this lecture?"
            },
            {
                "role": "assistant",
                "content": "This lecture covers foundational algorithmic concepts including problem formulation, correctness proof using induction, efficiency analysis using asymptotic notation, data structures (arrays, linked lists, BSTs, heaps), and sorting methods (selection sort, merge sort, counting sort). Additionally, it introduces the Word-RAM computational model, providing a theoretical framework for measuring algorithm efficiency."
            }
        ]
    },
    {
        "system": "Hello! I am Kiran, an AI assistant from Tampere University, dedicated to providing you with accurate and insightful assistance. My goal is to help you understand concepts, summarize lectures, and answer questions effectively.",
        "conversations": [
            {
                "role": "host",
                "content": "2.2. Array-Based List Implementation — BM40A1500 Data Structures and Algorithms Username Password Forgot your password? Register Username: Password Confirm Password Email: BM40A1500 Data Structures and Algorithms Chapter 2 Linear Data Structures Show Source | | About « 2.1. The List ADT :: Contents :: 2.3. Linked Lists » 2.2. Array-Based List Implementation¶ 2.2.1. Insert¶ Because the array-based list implementation is defined to store list elements in contiguous cells of the array, the insert, append, and remove methods must maintain this property. Settings Saving... Server Error Resubmit 2.2.2. Append and Remove¶ Settings Saving... Server Error Resubmit Removing an element from the head of the list is similar to insert in that all remaining elements must shift toward the head by one position to fill in the gap. If we want to remove the element at position \\(i\\), then \\(n - i - 1\\) elements must shift toward the head, as shown in the following slideshow. Settings Saving... Server Error Resubmit In the average case, insertion or removal each requires moving half of the elements, which is \\(\\Theta(n)\\). Aside from insert and remove, the only other operations that might require more than constant time are the constructor and clear. The other methods for Class List simply access the current list element or move the current position. They all require \\(\\Theta(1)\\) time. Contact Us || Privacy | | License « 2.1. The List ADT :: Contents :: 2.3. Linked Lists » Contact Us | | Report a bug © Copyright 2011-2023 by OpenDSA Project Contributors and distributed under an MIT license. Last updated on Aug 13, 2024. Created using Sphinx 4.4.0. Summary*: Operating system*: Windows Mac OS Linux iOS Android Other Browser*: Chrome Safari Internet Explorer Opera Other Description*: Attach a screenshot (optional):"
            },
            {
                "role": "user",
                "content": "Can you summarize the key topics covered in this lecture?"
            },
            {
                "role": "assistant",
                "content": "This lecture covers foundational algorithmic concepts including problem formulation, correctness proof using induction, efficiency analysis using asymptotic notation, data structures (arrays, linked lists, BSTs, heaps), and sorting methods (selection sort, merge sort, counting sort). Additionally, it introduces the Word-RAM computational model, providing a theoretical framework for measuring algorithm efficiency."
            }
        ]
    },
    {
        "system": "Hello! I am Kiran, an AI assistant from Tampere University, dedicated to providing you with accurate and insightful assistance. My goal is to help you understand concepts, summarize lectures, and answer questions effectively.",
        "conversations": [
            {
                "role": "host",
                "content": "2.3. Linked Lists — BM40A1500 Data Structures and Algorithms Username Password Forgot your password? Register Username: Password Confirm Password Email: BM40A1500 Data Structures and Algorithms Chapter 2 Linear Data Structures Show Source | | About « 2.2. Array-Based List Implementation :: Contents :: 2.4. Comparison of List Implementations » 2.3. Linked Lists¶ 2.3.1. Linked Lists¶ In this module we present one of the two traditional implementations for lists, usually called a linked list. The linked list uses dynamic memory allocation, that is, it allocates memory for new list elements as needed. The following diagram illustrates the linked list concept. Here there are three nodes that are “linked” together. Each node has two boxes. The box on the right holds a link to the next node in the list. Notice that the rightmost node has a diagonal slash through its link box, signifying that there is no link coming out of this box. Because a list node is a distinct object (as opposed to simply a cell in an array), it is good practice to make a separate list node class. (We can also re-use the list node class to implement linked implementations for the stack and queue data structures. Here is an implementation for list nodes, called the Link class. Objects in the Link class contain an element field to store the element value, and a next field to store a pointer to the next node on the list. The list built from such nodes is called a singly linked list, or a one-way list, because each list node has a single pointer to the next node on the list. The Link class is quite simple. There are two forms for its constructor, one with an initial element value and one without. Member functions allow the link user to get or set the element and link fields. Settings Saving... Server Error Resubmit 2.3.2. Why This Has Problems¶ There are a number of problems with the representation just described. First, there are lots of special cases to code for. For example, when the list is empty we have no element for head, tail, and curr to point to. Implementing special cases for insert and remove increases code complexity, making it harder to understand, and thus increases the chance of introducing bugs. Settings Saving... Server Error Resubmit 2.3.3. A Better Solution¶ Fortunately, there is a fairly easy way to deal with all of the special cases, as well as the problem with deleting the last node. Many special cases can be eliminated by implementing linked lists with an additional header node as the first node of the list. This header node is a link node like any other, but its value is ignored and it is not considered to be an actual element of the list. The header node saves coding effort because we no longer need to consider special cases for empty lists or when the current position is at one end of the list. The cost of this simplification is the space for the header node. However, there are space savings due to smaller code size, because statements to handle the special cases are omitted. We get rid of the remaining special cases related to being at the end of the list by adding a “trailer” node that also never stores a value. The following diagram shows initial conditions for a linked list with header and trailer nodes. Here is what a list with some elements looks like with the header and trailer nodes added. Adding the trailer node also solves our problem with deleting the last node on the list, as we will see when we take a closer look at the remove method’s implementation. 2.3.4. Linked List insertion¶ Settings Saving... Server Error Resubmit Here are some special cases for linked list insertion: Inserting atthe end, and inserting to an empty list. Settings Saving... Server Error Resubmit 2.3.5. Linked List Remove¶ Settings Saving... Server Error Resubmit Implementations for the remaining operations each require \\(\\Theta(1)\\) time. Contact Us || Privacy | | License « 2.2. Array-Based List Implementation :: Contents :: 2.4. Comparison of List Implementations » Contact Us | | Report a bug © Copyright 2011-2023 by OpenDSA Project Contributors and distributed under an MIT license. Last updated on Aug 13, 2024. Created using Sphinx 4.4.0. Summary*: Operating system*: Windows Mac OS Linux iOS Android Other Browser*: Chrome Safari Internet Explorer Opera Other Description*: Attach a screenshot (optional):"
            },
            {
                "role": "user",
                "content": "Can you summarize the key topics covered in this lecture?"
            },
            {
                "role": "assistant",
                "content": "This lecture covers foundational algorithmic concepts including problem formulation, correctness proof using induction, efficiency analysis using asymptotic notation, data structures (arrays, linked lists, BSTs, heaps), and sorting methods (selection sort, merge sort, counting sort). Additionally, it introduces the Word-RAM computational model, providing a theoretical framework for measuring algorithm efficiency."
            }
        ]
    },
    {
        "system": "Hello! I am Kiran, an AI assistant from Tampere University, dedicated to providing you with accurate and insightful assistance. My goal is to help you understand concepts, summarize lectures, and answer questions effectively.",
        "conversations": [
            {
                "role": "host",
                "content": "6.5. Minimal Cost Spanning Trees — BM40A1500 Data Structures and Algorithms Username Password Forgot your password? Register Username: Password Confirm Password Email: BM40A1500 Data Structures and Algorithms Chapter 6 Graphs Show Source | | About « 6.4. All-Pairs Shortest Paths :: Contents :: 6.6. Kruskal’s Algorithm » 6.5. Minimal Cost Spanning Trees¶ The minimal-cost spanning tree (MCST) problem takes as input a connected, undirected graph \\(\\mathbf{G}\\), where each edge has a distance or weight measure attached. The MCST is the graph containing the vertices of \\(\\mathbf{G}\\) along with the subset of \\(\\mathbf{G}\\) ‘s edges that (1) has minimum total cost as measured by summing the values for all of the edges in the subset, and (2) keeps the vertices connected. Applications where a solution to this problem is useful include soldering the shortest set of wires needed to connect a set of terminals on a circuit board, and connecting a set of cities by telephone lines in such a way as to require the least amount of cable. The MCST contains no cycles. If a proposed MCST did have a cycle, a cheaper MCST could be had by removing any one of the edges in the cycle. Thus, the MCST is a free tree with \\(|\\mathbf{V}| - 1\\) edges. The name “minimum-cost spanning tree” comes from the fact that the required set of edges forms a tree, it spans the vertices (i.e., it connects them together), and it has minimum cost. Figure 6.5.1 shows the MCST for an example graph. Figure 6.5.1: A graph and its MCST. All edges appear in the original graph. Those edges drawn with heavy lines indicate the subset making up the MCST. Note that edge \\((C, F)\\) could be replaced with edge \\((D, F)\\) to form a different MCST with equal cost. 6.5.1. Prim’s Algorithm¶ The first of our two algorithms for finding MCSTs is commonly referred to as Prim’s algorithm. Prim’s algorithm is very simple. Start with any Vertex \\(N\\) in the graph, setting the MCST to be \\(N\\) initially. Pick the least-cost edge connected to \\(N\\). This edge connects \\(N\\) to another vertex; call this \\(M\\). Add Vertex \\(M\\) and Edge \\((N, M)\\) to the MCST. Next, pick the least-cost edge coming from either \\(N\\) or \\(M\\) to any other vertex in the graph. Add this edge and the new vertex it reaches to the MCST. This process continues, at each step expanding the MCST by selecting the least-cost edge from a vertex currently in the MCST to a vertex not currently in the MCST. Prim’s algorithm is quite similar to Dijkstra’s algorithm for finding the single-source shortest paths. The primary difference is that we are seeking not the next closest vertex to the start vertex, but rather the next closest vertex to any vertex currently in the MCST. Thus we replace the lines: if (D[w] > (D[v] + G.weight(v, w))) D[w] = D[v] + G.weight(v, w); in Djikstra’s algorithm with the lines: if (D[w] > G.weight(v, w)) D[w] = G.weight(v, w); in Prim’s algorithm. The following code shows an implementation for Prim’s algorithm that searches the distance matrix for the next closest vertex. # Compute shortest distances to the MCST, store them in D. # V[i] will hold the index for the vertex that is i's parent in the MCST def Prim(G, s, D, V): for i in range(G.nodeCount()): # Initialize D[i] = math.inf D[s] = 0 for i in range(G.nodeCount()): # Process the vertices v = minVertex(G, D) # Find next-closest vertex G.setValue(v, VISITED) if D[v] == INFINITY: return # Unreachable if v != s: AddEdgetoMST(V[v], v) for w in G.neighbors(v): if D[w] > G.weight(v, w): D[w] = G.weight(v, w) V[w] = v For each vertex \\(I\\), when \\(I\\) is processed by Prim’s algorithm, an edge going to \\(I\\) is added to the MCST that we are building. Array V[I] stores the previously visited vertex that is closest to Vertex I. This information lets us know which edge goes into the MCST when Vertex \\(I\\) is processed. The implementation above also contains calls to AddEdgetoMST to indicate which edges are actually added to the MCST. Settings Saving... Server Error Resubmit 6.5.2. Prim’s Algorithm Alternative Implementation¶ Alternatively, we can implement Prim’s algorithm using a priority queue to find the next closest vertex, as shown next. As with the priority queue version of Dijkstra’s algorithm, the heap stores DijkElem objects. # Prim's MCST algorithm: priority queue version def PrimPQ(G, s, D, V): E = [None] * G.edgeCount() # Heap for edges E[0] = (0, s) # Initial vertex H = MinHeap(E, 1, G.edgeCount()) for i in range(G.nodeCount()): # Initialize distance D[i] = math.inf D[s] = 0 for i in range(G.nodeCount()): # For each vertex temp = H.removemin() if temp is None: return # Unreachable nodes exist v = temp.value() while G.getValue(v) == VISITED: temp = H.removemin() if temp is None: return # Unreachable nodes exist v = temp.value() G.setValue(v, VISITED) if D[v] == math.inf return # Unreachable if v != s: AddEdgetoMST(V[v], v) # Add edge to MST for w in G.neighbors(v): if D[w] > G.weight(v, w): D[w] = G.weight(v, w) V[w] = v # Where it came from H.insert(D[w], w) Prim’s algorithm is an example of a greedy algorithm. At each step in the for loop, we select the least-cost edge that connects some marked vertex to some unmarked vertex. The algorithm does not otherwise check that the MCST really should include this least-cost edge. This leads to an important question: Does Prim’s algorithm work correctly? Clearly it generates a spanning tree (because each pass through the for loop adds one edge and one unmarked vertex to the spanning tree until all vertices have been added), but does this tree have minimum cost? Theorem: Prim’s algorithm produces a minimum-cost spanning tree. Proof: We will use a proof by contradiction. Let \\(\\mathbf{G} = (\\mathbf{V}, \\mathbf{E})\\) be a graph for which Prim’s algorithm does not generate an MCST. Define an ordering on the vertices according to the order in which they were added by Prim’s algorithm to the MCST: \\(v_0, v_1, ..., v_{n-1}\\). Let edge \\(e_i\\) connect \\((v_x, v_i)\\) for some \\(x < i\\) and \\(i \\leq 1\\). Let \\(e_j\\) be the lowest numbered (first) edge added by Prim’s algorithm such that the set of edges selected so far cannot be extended to form an MCST for \\(\\mathbf{G}\\). In other words, \\(e_j\\) is the first edge where Prim’s algorithm “went wrong.” Let \\(\\mathbf{T}\\) be the “true” MCST. Call \\(v_p (p<j)\\) the vertex connected by edge \\(e_j\\), that is, \\(e_j = (v_p, v_j)\\). Because \\(\\mathbf{T}\\) is a tree, there exists some path in \\(\\mathbf{T}\\) connecting \\(v_p\\) and \\(v_j\\). There must be some edge \\(e'\\) in this path connecting vertices \\(v_u\\) and \\(v_w\\), with \\(u < j\\) and \\(w \\geq j\\). Because \\(e_j\\) is not part of \\(\\mathbf{T}\\), adding edge \\(e_j\\) to \\(\\mathbf{T}\\) forms a cycle. Edge \\(e'\\) must be of lower cost than edge \\(e_j\\), because Prim’s algorithm did not generate an MCST. This situation is illustrated in Figure 6.5.2. However, Prim’s algorithm would have selected the least-cost edge available. It would have selected \\(e'\\), not \\(e_j\\). Thus, it is a contradiction that Prim’s algorithm would have selected the wrong edge, and thus, Prim’s algorithm must be correct. BOX HERE Figure 6.5.2: Prim’s MCST algorithm proof. The left oval contains that portion of the graph where Prim’s MCST and the “true” MCST \\(\\mathbf{T}\\) agree. The right oval contains the rest of the graph. The two portions of the graph are connected by (at least) edges \\(e_j\\) (selected by Prim’s algorithm to be in the MCST) and \\(e'\\) (the “correct” edge to be placed in the MCST). Note that the path from \\(v_w\\) to \\(v_j\\) cannot include any marked vertex \\(v_i, i \\leq j\\), because to do so would form a cycle.¶ Contact Us || Privacy | | License « 6.4. All-Pairs Shortest Paths :: Contents :: 6.6. Kruskal’s Algorithm » Contact Us | | Report a bug © Copyright 2011-2023 by OpenDSA Project Contributors and distributed under an MIT license. Last updated on Aug 13, 2024. Created using Sphinx 4.4.0. Summary*: Operating system*: Windows Mac OS Linux iOS Android Other Browser*: Chrome Safari Internet Explorer Opera Other Description*: Attach a screenshot (optional):"
            },
            {
                "role": "user",
                "content": "Can you summarize the key topics covered in this lecture?"
            },
            {
                "role": "assistant",
                "content": "This lecture covers foundational algorithmic concepts including problem formulation, correctness proof using induction, efficiency analysis using asymptotic notation, data structures (arrays, linked lists, BSTs, heaps), and sorting methods (selection sort, merge sort, counting sort). Additionally, it introduces the Word-RAM computational model, providing a theoretical framework for measuring algorithm efficiency."
            }
        ]
    },
    {
        "system": "Hello! I am Kiran, an AI assistant from Tampere University, dedicated to providing you with accurate and insightful assistance. My goal is to help you understand concepts, summarize lectures, and answer questions effectively.",
        "conversations": [
            {
                "role": "host",
                "content": "5.3. Divide and Conquer: Mergesort — BM40A1500 Data Structures and Algorithms Username Password Forgot your password? Register Username: Password Confirm Password Email: BM40A1500 Data Structures and Algorithms Chapter 5 Algorithm Design Principles Show Source | | About « 5.2. The Greedy Approach :: Contents :: 5.4. Divide and Conquer: Quicksort » 5.3. Divide and Conquer: Mergesort¶ 5.3.1. Mergesort Concepts¶ A natural approach to problem solving is divide and conquer. To use divide and conquer when sorting, we might consider breaking the list to be sorted into pieces, process the pieces, and then put them back together somehow. A simple way to do this would be to split the list in half, sort the halves, and then merge the sorted halves together. This is the idea behind Mergesort. Mergesort is one of the simplest sorting algorithms conceptually, and has good performance both in the asymptotic sense and in empirical running time. Unfortunately, even though it is based on a simple concept, it is relatively difficult to implement in practice. Here is an example implementation of Mergesort in Python: def mergesort(inlist): if len(inlist) <= 1: return inlist l_1 = inlist[len(inlist) // 2 :] # half of the items from inlist l_2 = inlist[: len(inlist) // 2] # other hald of the items return merge(mergesort(l_1), mergesort(l_2)) Here is a visualization that illustrates how Mergesort works. The hardest step to understand about Mergesort is the merge function. The merge function starts by examining the first record of each sublist and picks the smaller value as the smallest record overall. This smaller value is removed from its sublist and placed into the output list. Merging continues in this way, comparing the front records of the sublists and continually appending the smaller to the output list until no more input records remain. Here is an implementation in Python for merge on lists: def merge(l_1, l_2): answer = [] while l_1 or l_2: if not l_1: # l_1 is empty, append rest items from l_2 answer += l_2 break elif not l_2: # l_2 is empty, append rest items from l_1 answer += l_1 break elif l_1[0] <= l_2[0]: answer.append(l_1.pop(0)) else: answer.append(l_2.pop(0)) return answer Here is a visualization for the merge operation. Settings Saving... Server Error Resubmit This visualization provides a running time analysis for Merge Sort. Settings Saving... Server Error Resubmit Contact Us || Privacy | | License « 5.2. The Greedy Approach :: Contents :: 5.4. Divide and Conquer: Quicksort » Contact Us | | Report a bug © Copyright 2011-2023 by OpenDSA Project Contributors and distributed under an MIT license. Last updated on Aug 13, 2024. Created using Sphinx 4.4.0. Summary*: Operating system*: Windows Mac OS Linux iOS Android Other Browser*: Chrome Safari Internet Explorer Opera Other Description*: Attach a screenshot (optional):"
            },
            {
                "role": "user",
                "content": "Can you summarize the key topics covered in this lecture?"
            },
            {
                "role": "assistant",
                "content": "This lecture covers foundational algorithmic concepts including problem formulation, correctness proof using induction, efficiency analysis using asymptotic notation, data structures (arrays, linked lists, BSTs, heaps), and sorting methods (selection sort, merge sort, counting sort). Additionally, it introduces the Word-RAM computational model, providing a theoretical framework for measuring algorithm efficiency."
            }
        ]
    },
    {
        "system": "Hello! I am Kiran, an AI assistant from Tampere University, dedicated to providing you with accurate and insightful assistance. My goal is to help you understand concepts, summarize lectures, and answer questions effectively.",
        "conversations": [
            {
                "role": "host",
                "content": "7.3. NP-Completeness — BM40A1500 Data Structures and Algorithms Username Password Forgot your password? Register Username: Password Confirm Password Email: BM40A1500 Data Structures and Algorithms Chapter 7 NP-completeness Show Source | | About « 7.2. Reductions :: Contents :: 7.4. Examples of NP-complete problems » 7.3. NP-Completeness¶ 7.3.1. Hard Problems¶ There are several ways that a problem could be considered hard. For example, we might have trouble understanding the definition of the problem itself. At the beginning of a large data collection and analysis project, developers and their clients might have only a hazy notion of what their goals actually are, and need to work that out over time. For other types of problems, we might have trouble finding or understanding an algorithm to solve the problem. Understanding spoken English and translating it to written text is an example of a problem whose goals are easy to define, but whose solution is not easy to discover. But even though a natural language processing algorithm might be difficult to write, the program’s running time might be fairly fast. There are many practical systems today that solve aspects of this problem in reasonable time. None of these is what is commonly meant when a computer theoretician uses the word “hard”. Throughout this section, “hard” means that the best-known algorithm for the problem is expensive in its running time. One example of a hard problem is Towers of Hanoi. It is easy to understand this problem and its solution. It is also easy to write a program to solve this problem. But, it takes an extremely long time to run for any “reasonably” large value of \\(n\\). Try running a program to solve Towers of Hanoi for only 30 disks! The Towers of Hanoi problem takes exponential time, that is, its running time is \\(\\Theta(2^n)\\). This is radically different from an algorithm that takes \\(\\Theta(n \\log n)\\) time or \\(\\Theta(n^2)\\) time. It is even radically different from a problem that takes \\(\\Theta(n^4)\\) time. These are all examples of polynomial running time, because the exponents for all terms of these equations are constants. If we buy a new computer that runs twice as fast, the size of problem with complexity \\(\\Theta(n^4)\\) that we can solve in a certain amount of time is increased by the fourth root of two. In other words, there is a multiplicative factor increase, even if it is a rather small one. This is true for any algorithm whose running time can be represented by a polynomial. Consider what happens if you buy a computer that is twice as fast and try to solve a bigger Towers of Hanoi problem in a given amount of time. Because its complexity is \\(\\Theta(2^n)\\), we can solve a problem only one disk bigger! There is no multiplicative factor, and this is true for any exponential algorithm: A constant factor increase in processing power results in only a fixed addition in problem-solving power. There are a number of other fundamental differences between polynomial running times and exponential running times that argues for treating them as qualitatively different. Polynomials are closed under composition and addition. Thus, running polynomial-time programs in sequence, or having one program with polynomial running time call another a polynomial number of times yields polynomial time. Also, all computers known are polynomially related. That is, any program that runs in polynomial time on any computer today, when transferred to any other computer, will still run in polynomial time. There is a practical reason for recognizing a distinction. In practice, most polynomial time algorithms are “feasible” in that they can run reasonably large inputs in reasonable time. In contrast, most algorithms requiring exponential time are not practical to run even for fairly modest sizes of input. One could argue that a program with high polynomial degree (such as \\(n^{100}\\)) is not practical, while an exponential-time program with cost \\(1.001^n\\) is practical. But the reality is that we know of almost no problems where the best polynomial-time algorithm has high degree (they nearly all have degree four or less), while almost no exponential-time algorithms (whose cost is \\((O(c^n))\\) have their constant \\(c\\) close to one. So there is not much gray area between polynomial and exponential time algorithms in practice. For the purposes of this Module, we define a hard algorithm to be one that runs in exponential time, that is, in \\(\\Omega(c^n)\\) for some constant \\(c > 1\\). A definition for a hard problem will be presented soon. 7.3.2. The Theory of NP-Completeness¶ Imagine a magical computer that works by guessing the correct solution from among all of the possible solutions to a problem. Another way to look at this is to imagine a super parallel computer that could test all possible solutions simultaneously. Certainly this magical (or highly parallel) computer can do anything a normal computer can do. It might also solve some problems more quickly than a normal computer can. Consider some problem where, given a guess for a solution, checking the solution to see if it is correct can be done in polynomial time. Even if the number of possible solutions is exponential, any given guess can be checked in polynomial time (equivalently, all possible solutions are checked simultaneously in polynomial time), and thus the problem can be solved in polynomial time by our hypothetical magical computer. Another view of this concept is this: If you cannot get the answer to a problem in polynomial time by guessing the right answer and then checking it, then you cannot do it in polynomial time in any other way. The idea of “guessing” the right answer to a problem—or checking all possible solutions in parallel to determine which is correct—is a called a non-deterministic choice. An algorithm that works in this manner is called a non-deterministic algorithm, and any problem with an algorithm that runs on a non-deterministic machine in polynomial time is given a special name: It is said to be a problem in NP. Thus, problems in NP are those problems that can be solved in polynomial time on a non-deterministic machine. Not all problems requiring exponential time on a regular computer are in NP. For example, Towers of Hanoi is not in NP, because it must print out \\(O(2^n)\\) moves for \\(n\\) disks. A non-deterministic machine cannot “guess” and print the correct answer in less time. On the other hand, consider the TRAVELING SALESMAN problem. Problem TRAVELING SALESMAN (1) Input: A complete, directed graph \\(G\\) with positive distances assigned to each edge in the graph. Output: The shortest simple cycle that includes every vertex. Figure 7.3.1 illustrates this problem. Five vertices are shown, with edges and associated costs between each pair of edges. (For simplicity Figure 7.3.1 shows an undirected graph, assuming that the cost is the same in both directions, though this need not be the case.) If the salesman visits the cities in the order ABCDEA, they will travel a total distance of 13. A better route would be ABDCEA, with cost 11. The best route for this particular graph would be ABEDCA, with cost 9. Figure 7.3.1: An illustration of the TRAVELING SALESMAN problem. Five vertices are shown, with edges between each pair of cities. The problem is to visit all of the cities exactly once, returning to the start city, with the least total cost. We cannot solve this problem in polynomial time with a guess-and-test non-deterministic computer. The problem is that, given a candidate cycle, while we can quickly check that the answer is indeed a cycle of the appropriate form, and while we can quickly calculate the length of the cycle, we have no easy way of knowing if it is in fact the shortest such cycle. However, we can solve a variant of this problem cast in the form of a decision problem. A decision problem is simply one whose answer is either YES or NO. The decision problem form of TRAVELING SALESMAN is as follows. Problem TRAVELING SALESMAN (2) Input: A complete, directed graph \\(G\\) with positive distances assigned to each edge in the graph, and an integer \\(k\\). Output: YES if there is a simple cycle with total distance \\(\\leq k\\) containing every vertex in \\(G\\), and NO otherwise. We can solve this version of the problem in polynomial time with a non-deterministic computer. The non-deterministic algorithm simply checks all of the possible subsets of edges in the graph, in parallel. If any subset of the edges is an appropriate cycle of total length less than or equal to \\(k\\), the answer is YES; otherwise the answer is NO. Note that it is only necessary that some subset meet the requirement; it does not matter how many subsets fail. Checking a particular subset is done in polynomial time by adding the distances of the edges and verifying that the edges form a cycle that visits each vertex exactly once. Thus, the checking algorithm runs in polynomial time. Unfortunately, there are \\(2^{|{\\mathrm E}|}\\) subsets to check, so this algorithm cannot be converted to a polynomial time algorithm on a regular computer. Nor does anybody in the world know of any other polynomial time algorithm to solve TRAVELING SALESMAN on a regular computer, despite the fact that the problem has been studied extensively by many computer scientists for many years. It turns out that there is a large collection of problems with this property: We know efficient non-deterministic algorithms, but we do not know if there are efficient deterministic algorithms. At the same time, we have not been able to prove that any of these problems do not have efficient deterministic algorithms. This class of problems is called NP-complete. What is truly strange and fascinating about NP-complete problems is that if anybody ever finds the solution to any one of them that runs in polynomial time on a regular computer, then by a series of reductions, every other problem that is in NP can also be solved in polynomial time on a regular computer! Define a problem to be NP-hard if any problem in NP can be reduced to \\(X\\) in polynomial time. Thus, \\(X\\) is as hard as any problem in NP. A problem \\(X\\) is defined to be NP-complete if \\(X\\) is in NP, and \\(X\\) is NP-hard. The requirement that a problem be NP-hard might seem to be impossible, but in fact there are hundreds of such problems, including TRAVELING SALESMAN. Another such problem is called K-CLIQUE. Problem K-CLIQUE Input: An arbitrary undirected graph \\(G\\) and an integer \\(k\\). Output: YES if there is a complete subgraph of at least \\(k\\) vertices, and NO otherwise. Settings Saving... Server Error Resubmit Nobody knows whether there is a polynomial time solution for K-CLIQUE, but if such an algorithm is found for K-CLIQUE or for TRAVELING SALESMAN, then that solution can be modified to solve the other, or any other problem in NP, in polynomial time. The primary theoretical advantage of knowing that a problem P1 is NP-complete is that it can be used to show that another problem P2 is NP-complete. This is done by finding a polynomial time reduction of P1 to P2. Because we already know that all problems in NP can be reduced to P1 in polynomial time (by the definition of NP-complete), we now know that all problems can be reduced to P2 as well by the simple algorithm of reducing to P1 and then from there reducing to P2. There is a practical advantage to knowing that a problem is NP-complete. It relates to knowing that if a polynomial time solution can be found for any problem that is NP-complete, then a polynomial solution can be found for all such problems. The implication is that, Because no one has yet found such a solution, it must be difficult or impossible to do; and Effort to find a polynomial time solution for one NP-complete problem can be considered to have been expended for all NP-complete problems. How is NP-completeness of practical significance for typical programmers? Well, if your boss demands that you provide a fast algorithm to solve a problem, they will not be happy if you come back saying that the best you could do was an exponential time algorithm. But, if you can prove that the problem is NP-complete, while they still won’t be happy, at least they should not be mad at you! By showing that their problem is NP-complete, you are in effect saying that the most brilliant computer scientists for the last 50 years have been trying and failing to find a polynomial time algorithm for their problem. Problems that are solvable in polynomial time on a regular computer are said to be in class P. Clearly, all problems in P are solvable in polynomial time on a non-deterministic computer simply by neglecting to use the non-deterministic capability. Some problems in NP are NP-complete. We can consider all problems solvable in exponential time or better as an even bigger class of problems because all problems solvable in polynomial time are solvable in exponential time. Thus, we can view the world of exponential-time-or-better problems in terms of Figure 7.3.2. Figure 7.3.2: Our knowledge regarding the world of problems requiring exponential time or less. Some of these problems are solvable in polynomial time by a non-deterministic computer. Of these, some are known to be NP-complete, and some are known to be solvable in polynomial time on a regular computer. The most important unanswered question in theoretical computer science is whether \\(P = NP\\). If they are equal, then there is a polynomial time algorithm for TRAVELING SALESMAN and all related problems. Because TRAVELING SALESMAN is known to be NP-complete, if a polynomial time algorithm were to be found for this problem, then all problems in NP would also be solvable in polynomial time. Conversely, if we were able to prove that TRAVELING SALESMAN has an exponential time lower bound, then we would know that \\(P \\neq NP\\). Contact Us || Privacy | | License « 7.2. Reductions :: Contents :: 7.4. Examples of NP-complete problems » Contact Us | | Report a bug © Copyright 2011-2023 by OpenDSA Project Contributors and distributed under an MIT license. Last updated on Aug 13, 2024. Created using Sphinx 4.4.0. Summary*: Operating system*: Windows Mac OS Linux iOS Android Other Browser*: Chrome Safari Internet Explorer Opera Other Description*: Attach a screenshot (optional):"
            },
            {
                "role": "user",
                "content": "Can you summarize the key topics covered in this lecture?"
            },
            {
                "role": "assistant",
                "content": "This lecture covers foundational algorithmic concepts including problem formulation, correctness proof using induction, efficiency analysis using asymptotic notation, data structures (arrays, linked lists, BSTs, heaps), and sorting methods (selection sort, merge sort, counting sort). Additionally, it introduces the Word-RAM computational model, providing a theoretical framework for measuring algorithm efficiency."
            }
        ]
    },
    {
        "system": "Hello! I am Kiran, an AI assistant from Tampere University, dedicated to providing you with accurate and insightful assistance. My goal is to help you understand concepts, summarize lectures, and answer questions effectively.",
        "conversations": [
            {
                "role": "host",
                "content": "7.6. Coping with NP-Complete Problems — BM40A1500 Data Structures and Algorithms Username Password Forgot your password? Register Username: Password Confirm Password Email: BM40A1500 Data Structures and Algorithms Chapter 7 NP-completeness Show Source | | About « 7.5. NP-Completeness Proofs :: Contents :: 8.1. Glossary » 7.6. Coping with NP-Complete Problems¶ Finding that your problem is NP-complete might not mean that you can just forget about it. Traveling salesmen need to find reasonable sales routes regardless of the complexity of the problem. What do you do when faced with an NP-complete problem that you must solve? There are several techniques to try. One approach is to run only small instances of the problem. For some problems, this is not acceptable. For example, TRAVELING SALESMAN grows so quickly that it cannot be run on modern computers for problem sizes much over 30 cities, which is not an unreasonable problem size for real-life situations. However, some other problems in NP, while requiring exponential time, still grow slowly enough that they allow solutions for problems of a useful size. Consider the Knapsack problem. We have a dynamic programming algorithm whose cost is \\(\\Theta(nK)\\) for \\(n\\) objects being fit into a knapsack of size \\(K\\). But it turns out that Knapsack is NP-complete. Isn’t this a contradiction? Not when we consider the relationship between \\(n\\) and \\(K\\). How big is \\(K\\)? Input size is typically \\(O(n \\lg K)\\) because the item sizes are smaller than \\(K\\). Thus, \\(\\Theta(nK)\\) is exponential on input size. This dynamic programming algorithm is tractable if the numbers are “reasonable”. That is, we can successfully find solutions to the problem when \\(nK\\) is in the thousands. Such an algorithm is called a pseudo-polynomial time algorithm. This is different from TRAVELING SALESMAN which cannot possibly be solved when \\(n = 100\\) given current algorithms. A second approach to handling NP-complete problems is to solve a special instance of the problem that is not so hard. For example, many problems on graphs are NP-complete, but the same problem on certain restricted types of graphs is not as difficult. For example, while the VERTEX COVER and K-CLIQUE problems are NP-complete in general, there are polynomial time solutions for bipartite graphs (i.e., graphs whose vertices can be separated into two subsets such that no pair of vertices within one of the subsets has an edge between them). 2-SATISFIABILITY (where every clause in a Boolean expression has at most two literals) has a polynomial time solution. Several geometric problems require only polynomial time in two dimensions, but are NP-complete in three dimensions or more. KNAPSACK is considered to run in polynomial time if the numbers (and \\(K\\)) are “small”. Small here means that they are polynomial on \\(n\\), the number of items. In general, if we want to guarantee that we get the correct answer for an NP-complete problem, we potentially need to examine all of the (exponential number of) possible solutions. However, with some organization, we might be able to either examine them quickly, or avoid examining a great many of the possible answers in some cases. For example, dynamic programming attempts to organize the processing of all the subproblems to a problem so that the work is done efficiently. If we need to do a brute-force search of the entire solution space, we can use backtracking to visit all of the possible solutions organized in a solution tree. For example, SATISFIABILITY has \\(2^n\\) possible ways to assign truth values to the \\(n\\) variables contained in the Boolean expression being satisfied. We can view this as a tree of solutions by considering that we have a choice of making the first variable TRUE or FALSE. Thus, we can put all solutions where the first variable is TRUE on one side of the tree, and the remaining solutions on the other. We then examine the solutions by moving down one branch of the tree, until we reach a point where we know the solution cannot be correct (such as if the current partial collection of assignments yields an unsatisfiable expression). At this point we backtrack and move back up a node in the tree, and then follow down the alternate branch. If this fails, we know to back up further in the tree as necessary and follow alternate branches, until finally we either find a solution that satisfies the expression or exhaust the tree. In some cases we avoid processing many potential solutions, or find a solution quickly. In others, we end up visiting a large portion of the \\(2^n\\) possible solutions. Banch-and-Bounds is an extension of backtracking that applies to optimization problems such as TRAVELING SALESMAN where we are trying to find the shortest tour through the cities. We traverse the solution tree as with backtracking. However, we remember the best value found so far. Proceeding down a given branch is equivalent to deciding which order to visit cities. So any node in the solution tree represents some collection of cities visited so far. If the sum of these distances exceeds the best tour found so far, then we know to stop pursuing this branch of the tree. At this point we can immediately back up and take another branch. If we have a quick method for finding a good (but not necessarily best) solution, we can use this as an initial bound value to effectively prune portions of the tree. Another coping strategy is to find an approximate solution to the problem, called an approximation algorithm. There are many approaches to finding approximate solutions. One way is to use a heuristic to solve the problem, that is, an algorithm based on a “rule of thumb” that does not always give the best answer. For example, the TRAVELING SALESMAN problem can be solved approximately by using the heuristic that we start at an arbitrary city and then always proceed to the next unvisited city that is closest. This rarely gives the shortest path, but the solution might be good enough. There are many other heuristics for TRAVELING SALESMAN that do a better job. Some approximation algorithms have guaranteed performance, such that the answer will be within a certain percentage of the best possible answer. For example, consider this simple heuristic for the VERTEX COVER problem: Let \\(M\\) be a maximal (not necessarily maximum) matching in \\(G\\). A matching pairs vertices (with connecting edges) so that no vertex is paired with more than one partner. Maximal means to pick as many pairs as possible, selecting them in some order until there are no more available pairs to select. Maximum means the matching that gives the most pairs possible for a given graph. If OPT is the size of a minimum vertex cover, then \\(|M| \\leq 2 \\cdot \\mbox{OPT}\\) because at least one endpoint of every matched edge must be in any vertex cover. A better example of a guaranteed bound on a solution comes from simple heuristics to solve the BIN PACKING problem. BIN PACKING in its decision form (i.e., asking if the items can be packed in less than \\(k\\) bins) is known to be NP-complete. One simple heuristic for solving this problem is to use a “first fit” approach. We put the first number in the first bin. We then put the second number in the first bin if it fits, otherwise we put it in the second bin. For each subsequent number, we simply go through the bins in the order we generated them and place the number in the first bin that fits. The number of bins used is no more than twice the sum of the numbers, because every bin (except perhaps one) must be at least half full. However, this “first fit” heuristic can give us a result that is much worse than optimal. Consider the following collection of numbers: 6 of \\(1/7 + \\epsilon\\), 6 of \\(1/3 + \\epsilon\\), and 6 of \\(1/2 + \\epsilon\\), where \\(\\epsilon\\) is a small, positive number. Properly organized, this requires 6 bins. But if done wrongly, we might end up putting the numbers into 10 bins. A better heuristic is to use decreasing first fit. This is the same as first fit, except that we keep the bins sorted from most full to least full. Then when deciding where to put the next item, we place it in the fullest bin that can hold it. This is similar to the best fit heuristic for memory management. The significant thing about this heuristic is not just that it tends to give better performance than simple first fit. This decreasing first fit heuristic can be proven to require no more than 11/9 the optimal number of bins. Thus, we have a guarantee on how much inefficiency can result when using the heuristic. The theory of NP-completeness gives a technique for separating tractable from (probably) intractable problems. When faced with a new problem, we might alternate between checking if it is tractable (that is, we try to find a polynomial-time solution) and checking if it is intractable (we try to prove the problem is NP-complete). While proving that some problem is NP-complete does not actually make our upper bound for our algorithm match the lower bound for the problem with certainty, it is nearly as good. Once we realize that a problem is NP-complete, then we know that our next step must either be to redefine the problem to make it easier, or else use one of the “coping” strategies discussed in this section. Contact Us || Privacy | | License « 7.5. NP-Completeness Proofs :: Contents :: 8.1. Glossary » Contact Us | | Report a bug © Copyright 2011-2023 by OpenDSA Project Contributors and distributed under an MIT license. Last updated on Aug 13, 2024. Created using Sphinx 4.4.0. Summary*: Operating system*: Windows Mac OS Linux iOS Android Other Browser*: Chrome Safari Internet Explorer Opera Other Description*: Attach a screenshot (optional):"
            },
            {
                "role": "user",
                "content": "Can you summarize the key topics covered in this lecture?"
            },
            {
                "role": "assistant",
                "content": "This lecture covers foundational algorithmic concepts including problem formulation, correctness proof using induction, efficiency analysis using asymptotic notation, data structures (arrays, linked lists, BSTs, heaps), and sorting methods (selection sort, merge sort, counting sort). Additionally, it introduces the Word-RAM computational model, providing a theoretical framework for measuring algorithm efficiency."
            }
        ]
    },
    {
        "system": "Hello! I am Kiran, an AI assistant from Tampere University, dedicated to providing you with accurate and insightful assistance. My goal is to help you understand concepts, summarize lectures, and answer questions effectively.",
        "conversations": [
            {
                "role": "host",
                "content": "7.4. Examples of NP-complete problems — BM40A1500 Data Structures and Algorithms Username Password Forgot your password? Register Username: Password Confirm Password Email: BM40A1500 Data Structures and Algorithms Chapter 7 NP-completeness Show Source | | About « 7.3. NP-Completeness :: Contents :: 7.5. NP-Completeness Proofs » 7.4. Examples of NP-complete problems¶ 7.4.1. Formula Satisfiability¶ Settings Saving... Server Error Resubmit 7.4.2. Vertex Cover¶ Settings Saving... Server Error Resubmit 7.4.3. Hamiltonian Cycle¶ Settings Saving... Server Error Resubmit 7.4.4. Traveling Salesman¶ Settings Saving... Server Error Resubmit Contact Us || Privacy | | License « 7.3. NP-Completeness :: Contents :: 7.5. NP-Completeness Proofs » Contact Us | | Report a bug © Copyright 2011-2023 by OpenDSA Project Contributors and distributed under an MIT license. Last updated on Aug 13, 2024. Created using Sphinx 4.4.0. Summary*: Operating system*: Windows Mac OS Linux iOS Android Other Browser*: Chrome Safari Internet Explorer Opera Other Description*: Attach a screenshot (optional):"
            },
            {
                "role": "user",
                "content": "Can you summarize the key topics covered in this lecture?"
            },
            {
                "role": "assistant",
                "content": "This lecture covers foundational algorithmic concepts including problem formulation, correctness proof using induction, efficiency analysis using asymptotic notation, data structures (arrays, linked lists, BSTs, heaps), and sorting methods (selection sort, merge sort, counting sort). Additionally, it introduces the Word-RAM computational model, providing a theoretical framework for measuring algorithm efficiency."
            }
        ]
    },
    {
        "system": "Hello! I am Kiran, an AI assistant from Tampere University, dedicated to providing you with accurate and insightful assistance. My goal is to help you understand concepts, summarize lectures, and answer questions effectively.",
        "conversations": [
            {
                "role": "host",
                "content": "3.4. Open Hashing — BM40A1500 Data Structures and Algorithms Username Password Forgot your password? Register Username: Password Confirm Password Email: BM40A1500 Data Structures and Algorithms Chapter 3 Hashing Show Source | | About « 3.3. Sample Hash Functions :: Contents :: 3.5. Bucket Hashing » 3.4. Open Hashing¶ 3.4.1. Open Hashing¶ While the goal of a hash function is to minimize collisions, some collisions are unavoidable in practice. Thus, hashing implementations must include some form of collision resolution policy. Collision resolution techniques can be broken into two classes: open hashing (also called separate chaining) and closed hashing (also called open addressing). (Yes, it is confusing when “open hashing” means the opposite of “open addressing”, but unfortunately, that is the way it is.) The difference between the two has to do with whether collisions are stored outside the table (open hashing), or whether collisions result in storing one of the records at another slot in the table (closed hashing). The simplest form of open hashing defines each slot in the hash table to be the head of a linked list. All records that hash to a particular slot are placed on that slot’s linked list. The following figure illustrates a hash table where each slot points to a linked list to hold the records associated with that slot. The hash function used is the simple mod function. Records within a slot’s list can be ordered in several ways: by insertion order, by key value order, or by frequency-of-access order. Ordering the list by key value provides an advantage in the case of an unsuccessful search, because we know to stop searching the list once we encounter a key that is greater than the one being searched for. If records on the list are unordered or ordered by frequency, then an unsuccessful search will need to visit every record on the list. Given a table of size \\(M\\) storing \\(N\\) records, the hash function will (ideally) spread the records evenly among the \\(M\\) positions in the table, yielding on average \\(N/M\\) records for each list. Assuming that the table has more slots than there are records to be stored, we can hope that few slots will contain more than one record. In the case where a list is empty or has only one record, a search requires only one access to the list. Thus, the average cost for hashing should be \\(\\Theta(1\\)). However, if clustering causes many records to hash to only a few of the slots, then the cost to access a record will be much higher because many elements on the linked list must be searched. Open hashing is most appropriate when the hash table is kept in main memory, with the lists implemented by a standard in-memory linked list. Storing an open hash table on disk in an efficient way is difficult, because members of a given linked list might be stored on different disk blocks. This would result in multiple disk accesses when searching for a particular key value, which defeats the purpose of using hashing. There are similarities between open hashing and Binsort. One way to view open hashing is that each record is simply placed in a bin. While multiple records may hash to the same bin, this initial binning should still greatly reduce the number of records accessed by a search operation. In a similar fashion, a simple Binsort reduces the number of records in each bin to a small number that can be sorted in some other way. Contact Us || Privacy | | License « 3.3. Sample Hash Functions :: Contents :: 3.5. Bucket Hashing » Contact Us | | Report a bug © Copyright 2011-2023 by OpenDSA Project Contributors and distributed under an MIT license. Last updated on Aug 13, 2024. Created using Sphinx 4.4.0. Summary*: Operating system*: Windows Mac OS Linux iOS Android Other Browser*: Chrome Safari Internet Explorer Opera Other Description*: Attach a screenshot (optional):"
            },
            {
                "role": "user",
                "content": "Can you summarize the key topics covered in this lecture?"
            },
            {
                "role": "assistant",
                "content": "This lecture covers foundational algorithmic concepts including problem formulation, correctness proof using induction, efficiency analysis using asymptotic notation, data structures (arrays, linked lists, BSTs, heaps), and sorting methods (selection sort, merge sort, counting sort). Additionally, it introduces the Word-RAM computational model, providing a theoretical framework for measuring algorithm efficiency."
            }
        ]
    },
    {
        "system": "Hello! I am Kiran, an AI assistant from Tampere University, dedicated to providing you with accurate and insightful assistance. My goal is to help you understand concepts, summarize lectures, and answer questions effectively.",
        "conversations": [
            {
                "role": "host",
                "content": "5.8. Finding Prime Numbers — BM40A1500 Data Structures and Algorithms Username Password Forgot your password? Register Username: Password Confirm Password Email: BM40A1500 Data Structures and Algorithms Chapter 5 Algorithm Design Principles Show Source | | About « 5.7. Introduction to Probabilistic Algorithms :: Contents :: 6.1. Graphs Chapter Introduction » 5.8. Finding Prime Numbers¶ How do we tell if a number is prime? One approach is the prime sieve: Test all prime up to \\(\\lfloor\\sqrt{n}\\rfloor\\). This requires up to \\(\\lfloor\\sqrt{n}\\rfloor -1\\) divisions. How does the cost of this algorithm compare to the input size? A problem instance is a single value, and our model syas that size for value \\(n\\) is \\(\\log n\\). Therefore, this is an exponential time algorithm! Note that it is easy to check the number of times 2 divides \\(n\\) when using a binary representation. What about checking for the number of times that 3 divides \\(n\\)? This is not so easy. What if \\(n\\) were represented in trinary? Then it would be easy to check for divisions by 3. In general, is there a polynomial time algorithm? We don’t know of one (and that fact is important to modern cryptography, which relies on the “fact” that factoring large numbers takes a lot of time. But what if we are willing to settle for a probabilistic algorithm? Here are some useful theorems from Number Theory: Prime Number Theorem: The number of primes less than \\(n\\) is (approximately) \\(\\frac{n}{\\ln n}\\). The average distance between primes is \\(\\ln n\\). Prime Factors Distribution Theorem: For large \\(n\\), on average, \\(n\\) has about \\(\\ln \\ln n\\) different prime factors with a standard deviation of \\(\\sqrt{\\ln \\ln n}\\). Note that this is quite small. For \\(2^{32}\\), \\(\\log \\log n = 5\\). To prove that a number is composite, we need only one factor. And, given a (claimed) factor, it is easy to verify whether that claim is true. What does it take to prove that a number is prime? Proving something is prime is much harder than proving that something is composite! Because we need to check a lot more than just one value. Do we need to check all \\(\\sqrt{n}\\) candidates for possible factors of \\(n\\) in order to know if \\(n\\) is prime? It depends on how safe you want to be. (Of course, we actually only need to check primes \\(< \\sqrt{n}\\).) Here are some potential probablistic algorithms that we might use to decide if a value \\(n\\) is prime. Always say that Prime(\\(n\\)) is FALSE. This simple algorithm “usually” works. It only fails \\(1/log n\\) times on average! If you don’t like the notion that for the actual primes values this always fails, than an alternative is to say, with probability \\(1/\\ln n\\), that Prime(\\(n\\)) is TRUE. Even though it is is not sometimes right and sometimes wrong, of course this no better than the previous algorithm. Pick a number \\(m\\) between 2 and \\(\\sqrt{n}\\). Say \\(n\\) is prime if and only if \\(m\\) does not divide \\(n\\). This is not not much help, because it probably did not pick a factor! None of those are really serious probabilistic algorithms to solve the problem. However, using number theory, it is possible to create a cheap test that probabilistically determines a number to be composite (if it is actually composite) 50% of the time. Using this test, we can build an algorithm for prime testing as follows: def Prime(n): for i in range(COMFORT): if not CHEAPTEST(n): return False return True In other words, we can repeatedly try the test, until our number passes enough times for us to be comfortable about claiming that it is prime. Of course, this does nothing to help you find the factors! But there is a nice aspect to this approach. We use large primes for cryptography. But, the numbers used don’t actually need to be prime. They only need to be hard to factor! And those numbers that continually pass the cheap 50/50 test tend to be hard to factor. So, even if a non-prime is used, it will still probably succeed in its intended use. Contact Us || Privacy | | License « 5.7. Introduction to Probabilistic Algorithms :: Contents :: 6.1. Graphs Chapter Introduction » Contact Us | | Report a bug © Copyright 2011-2023 by OpenDSA Project Contributors and distributed under an MIT license. Last updated on Aug 13, 2024. Created using Sphinx 4.4.0. Summary*: Operating system*: Windows Mac OS Linux iOS Android Other Browser*: Chrome Safari Internet Explorer Opera Other Description*: Attach a screenshot (optional):"
            },
            {
                "role": "user",
                "content": "Can you summarize the key topics covered in this lecture?"
            },
            {
                "role": "assistant",
                "content": "This lecture covers foundational algorithmic concepts including problem formulation, correctness proof using induction, efficiency analysis using asymptotic notation, data structures (arrays, linked lists, BSTs, heaps), and sorting methods (selection sort, merge sort, counting sort). Additionally, it introduces the Word-RAM computational model, providing a theoretical framework for measuring algorithm efficiency."
            }
        ]
    },
    {
        "system": "Hello! I am Kiran, an AI assistant from Tampere University, dedicated to providing you with accurate and insightful assistance. My goal is to help you understand concepts, summarize lectures, and answer questions effectively.",
        "conversations": [
            {
                "role": "host",
                "content": "5.7. Introduction to Probabilistic Algorithms — BM40A1500 Data Structures and Algorithms Username Password Forgot your password? Register Username: Password Confirm Password Email: BM40A1500 Data Structures and Algorithms Chapter 5 Algorithm Design Principles Show Source | | About « 5.6. Dynamic Programming :: Contents :: 5.8. Finding Prime Numbers » 5.7. Introduction to Probabilistic Algorithms¶ 5.7.1. Probabilistic Algorithms¶ We now consider how introducing randomness into our algorithms might speed things up, although perhaps at the expense of accuracy. But often we can reduce the possibility for error to be as low as we like, while still speeding up the algorithm. The lower bound for maximum finding in an unsorted list is \\(\\Omega(n)\\). This is the least time needed to be certain that we have found the maximum value. But what if we are willing to relax our requirement for certainty? The first question is: What do we mean by this? There are many aspects to “certainty” and we might relax the requirement in various ways. There are several possible guarantees that we might require from an algorithm that produces \\(X\\) as the maximum value, when the true maximum is \\(Y\\). So far we have assumed that we require \\(X\\) to equal \\(Y\\). This is known as an exact or deterministic algorithm to solve the problem. We could relax this and require only that \\(X\\) ‘s rank is “close to” \\(Y\\) ‘s rank (perhaps within a fixed distance or percentage). This is known as an approximation algorithm. We could require that \\(X\\) is “usually” \\(Y\\). This is known as a probabilistic algorithm. Finally, we could require only that \\(X\\) ‘s rank is “usually” “close” to \\(Y\\) ‘s rank. This is known as a heuristic algorithm. There are also different ways that we might choose to sacrifice reliability for speed. These types of algorithms also have names. Las Vegas Algorithms: We always find the maximum value, and “usually” we find it fast. Such algorithms have a guaranteed result, but do not guarantee fast running time. Monte Carlo Algorithms: We find the maximum value fast, or we don’t get an answer at all (but fast). While such algorithms have good running time, their result is not guaranteed. Here is an example of an algorithm for finding a large value that gives up its guarantee of getting the best value in exchange for an improved running time. This is an example of a probabilistic algorithm, since it includes steps that are affected by random events. Choose \\(m\\) elements at random, and pick the best one of those as the answer. For large \\(n\\), if \\(m \\approx \\log n\\), the answer is pretty good. The cost is \\(m-1\\) compares (since we must find the maximum of \\(m\\) values). But we don’t know for sure what we will get. However, we can estimate that the rank will be about \\(\\frac{mn}{m+1}\\). For example, if \\(n = 1,000,000\\) and \\(m = \\log n = 20\\), then we expect that the largest of the 20 randomly selected values be among the top 5% of the \\(n\\) values. Next, consider a slightly different problem where the goal is to pick a number in the upper half of \\(n\\) values. We would pick the maximum from among the first \\(\\frac{n+1}{2}\\) values for a cost of \\(n/2\\) comparisons. Can we do better than this? Not if we want to guarantee getting the correct answer. But if we are willing to accept near certainty instead of absolute certainty, we can gain a lot in terms of speed. As an alternative, consider this probabilistic algorithm. Pick 2 numbers and choose the greater. This will be in the upper half with probability 3/4 (since it is not in the upper half only when both numbers we choose happen to be in the lower half). Is a probability of 3/4 not good enough? Then we simply pick more numbers! For \\(k\\) numbers, the greatest is in upper half with probability \\(1 - \\frac{1}{2^k}\\), regardless of the number \\(n\\) that we pick from, so long as \\(n\\) is much larger than \\(k\\) (otherwise the chances might become even better). If we pick ten numbers, then the chance of failure is only one in \\(2^{10} = 1024\\). What if we really want to be sure, because lives depend on drawing a number from the upper half? If we pick 30 numbers, we can fail only one time in a billion. If we pick enough numbers, then the chance of picking a small number is less than the chance of the power failing during the computation. Picking 100 numbers means that we can fail only one time in \\(10^{100}\\) which is less than the chance any plausible disaster that you can imagine will disrupt the process. Contact Us || Privacy | | License « 5.6. Dynamic Programming :: Contents :: 5.8. Finding Prime Numbers » Contact Us | | Report a bug © Copyright 2011-2023 by OpenDSA Project Contributors and distributed under an MIT license. Last updated on Aug 13, 2024. Created using Sphinx 4.4.0. Summary*: Operating system*: Windows Mac OS Linux iOS Android Other Browser*: Chrome Safari Internet Explorer Opera Other Description*: Attach a screenshot (optional):"
            },
            {
                "role": "user",
                "content": "Can you summarize the key topics covered in this lecture?"
            },
            {
                "role": "assistant",
                "content": "This lecture covers foundational algorithmic concepts including problem formulation, correctness proof using induction, efficiency analysis using asymptotic notation, data structures (arrays, linked lists, BSTs, heaps), and sorting methods (selection sort, merge sort, counting sort). Additionally, it introduces the Word-RAM computational model, providing a theoretical framework for measuring algorithm efficiency."
            }
        ]
    },
    {
        "system": "Hello! I am Kiran, an AI assistant from Tampere University, dedicated to providing you with accurate and insightful assistance. My goal is to help you understand concepts, summarize lectures, and answer questions effectively.",
        "conversations": [
            {
                "role": "host",
                "content": "7.5. NP-Completeness Proofs — BM40A1500 Data Structures and Algorithms Username Password Forgot your password? Register Username: Password Confirm Password Email: BM40A1500 Data Structures and Algorithms Chapter 7 NP-completeness Show Source | | About « 7.4. Examples of NP-complete problems :: Contents :: 7.6. Coping with NP-Complete Problems » 7.5. NP-Completeness Proofs¶ 7.5.1. NP-Completeness Proofs¶ To start the process of being able to prove problems are NP-complete, we need to prove just one problem \\(H\\) is NP-complete. After that, to show that any problem \\(X\\) is NP-hard, we just need to reduce \\(H\\) to \\(X\\). When doing NP-completeness proofs, it is very important not to get this reduction backwards! If we reduce candidate problem \\(X\\) to known hard problem \\(H\\), this means that we use \\(H\\) as a step to solving \\(X\\). All that means is that we have found a (known) hard way to solve \\(X\\). However, when we reduce known hard problem \\(H\\) to candidate problem \\(X\\), that means we are using \\(X\\) as a step to solve \\(H\\). And if we know that \\(H\\) is hard, that means \\(X\\) must also be hard (because if \\(X\\) were not hard, then neither would \\(H\\) be hard). So a crucial first step to getting this whole theory off the ground is finding one problem that is NP-hard. The first proof that a problem is NP-hard (and because it is in NP, therefore NP-complete) was done by Stephen Cook. For this feat, Cook won the first Turing award, which is the closest Computer Science equivalent to the Nobel Prize. The “grand-daddy” NP-complete problem that Cook used is called SATISFIABILITY (or SAT for short). A Boolean expression is comprised of Boolean variables combined using the operators AND (\\(\\cdot\\)), OR (\\(+\\)), and NOT (to negate Boolean variable \\(x\\) we write \\(\\overline{x}\\)). A literal is a Boolean variable or its negation. A clause is one or more literals OR’ed together. Let \\(E\\) be a Boolean expression over variables \\(x_1, x_2, ..., x_n\\). Then we define Conjunctive Normal Form (CNF) to be a Boolean expression written as a series of clauses that are AND’ed together. For example, \\[E = (x_5 + x_7 + \\overline{x_8} + x_{10}) \\cdot (\\overline{x_2} + x_3) \\cdot (x_1 + \\overline{x_3} + x_6)\\] is in CNF, and has three clauses. Now we can define the problem SAT. Problem SATISFIABILITY (SAT) Input: A Boolean expression \\(E\\) over variables \\(x_1, x_2, ...\\) in Conjunctive Normal Form. Output: YES if there is an assignment to the variables that makes \\(E\\) true, NO otherwise. Cook proved that SAT is NP-hard. Explaining Cook’s proof is beyond the scope of this course. But we can briefly summarize it as follows. Any decision problem \\(F\\) can be recast as some language acceptance problem \\(L\\): \\[F(I) = \\mbox{YES} \\Leftrightarrow L(I') = \\mbox{ACCEPT}.\\] That is, if a decision problem \\(F\\) yields YES on input \\(I\\), then there is a language \\(L\\) containing string \\(I'\\) where \\(I'\\) is some suitable transformation of input \\(I\\). Conversely, if \\(F\\) would give answer NO for input \\(I\\), then \\(I\\) ‘s transformed version \\(I'\\) is not in the language \\(L\\). Turing machines are a simple model of computation for writing programs that are language acceptors. There is a “universal” Turing machine that can take as input a description for a Turing machine, and an input string, and return the execution of that machine on that string. This Turing machine in turn can be cast as a Boolean expression such that the expression is satisfiable if and only if the Turing machine yields ACCEPT for that string. Cook used Turing machines in his proof because they are simple enough that he could develop this transformation of Turing machines to Boolean expressions, but rich enough to be able to compute any function that a regular computer can compute. The significance of this transformation is that any decision problem that is performable by the Turing machine is transformable to SAT. Thus, SAT is NP-hard. To show that a decision problem \\(X\\) is NP-complete, we prove that \\(X\\) is in NP (normally easy, and normally done by giving a suitable polynomial-time, non-deterministic algorithm) and then prove that \\(X\\) is NP-hard. To prove that \\(X\\) is NP-hard, we choose a known NP-complete problem, say \\(A\\). We describe a polynomial-time transformation that takes an arbitrary instance \\(I\\) of \\(A\\) to an instance \\(I'\\) of \\(X\\). We then describe a polynomial-time transformation from \\(SLN'\\) to \\(SLN\\) such that \\(SLN\\) is the solution for \\(I\\). The following modules show a number of known NP-complete problems, and also some proofs that they are NP-complete. The various proofs will link the problems together as shown here: Figure 7.5.1: We will use this sequence of reductions for the NP Complete Proof 7.5.2. Hamiltonian Cycle to Traveling Salesman¶ The following slideshow shows that an instance of Hamiltonian Cycle problem can be reduced to an instance of Traveling Salesman problem in polynomial time. Settings Saving... Server Error Resubmit This reduction can help in providing an NP Completeness proof for the Traveling Salesman problem. Contact Us || Privacy | | License « 7.4. Examples of NP-complete problems :: Contents :: 7.6. Coping with NP-Complete Problems » Contact Us | | Report a bug © Copyright 2011-2023 by OpenDSA Project Contributors and distributed under an MIT license. Last updated on Aug 13, 2024. Created using Sphinx 4.4.0. Summary*: Operating system*: Windows Mac OS Linux iOS Android Other Browser*: Chrome Safari Internet Explorer Opera Other Description*: Attach a screenshot (optional):"
            },
            {
                "role": "user",
                "content": "Can you summarize the key topics covered in this lecture?"
            },
            {
                "role": "assistant",
                "content": "This lecture covers foundational algorithmic concepts including problem formulation, correctness proof using induction, efficiency analysis using asymptotic notation, data structures (arrays, linked lists, BSTs, heaps), and sorting methods (selection sort, merge sort, counting sort). Additionally, it introduces the Word-RAM computational model, providing a theoretical framework for measuring algorithm efficiency."
            }
        ]
    },
    {
        "system": "Hello! I am Kiran, an AI assistant from Tampere University, dedicated to providing you with accurate and insightful assistance. My goal is to help you understand concepts, summarize lectures, and answer questions effectively.",
        "conversations": [
            {
                "role": "host",
                "content": "2.7. Queues — BM40A1500 Data Structures and Algorithms Username Password Forgot your password? Register Username: Password Confirm Password Email: BM40A1500 Data Structures and Algorithms Chapter 2 Linear Data Structures Show Source | | About « 2.6. Linked Stacks :: Contents :: 2.8. Linked Queues » 2.7. Queues¶ 2.7.1. Queue Terminology and Implementation¶ Like the stack, the queue is a list-like structure that provides restricted access to its elements. Queue elements may only be inserted at the back (called an enqueue operation) and removed from the front (called a dequeue operation). Queues operate like standing in line at a movie theater ticket counter. If nobody cheats, then newcomers go to the back of the line. The person at the front of the line is the next to be served. Thus, queues release their elements in order of arrival. In Britain, a line of people is called a “queue”, and getting into line to wait for service is called “queuing up”. Accountants have used queues since long before the existence of computers. They call a queue a “FIFO” list, which stands for “First-In, First-Out”. Here is a sample queue ADT. This section presents two implementations for queues: the array-based queue and the linked queue. 2.7.2. Array-Based Queues¶ The array-based queue is somewhat tricky to implement effectively. A simple conversion of the array-based list implementation is not efficient. Settings Saving... Server Error Resubmit Settings Saving... Server Error Resubmit Settings Saving... Server Error Resubmit 2.7.3. The Circular Queue¶ Settings Saving... Server Error Resubmit Settings Saving... Server Error Resubmit If the value of front is fixed, then \\(n+1\\) different values for rear are needed to distinguish among the \\(n+1\\) states. However, there are only \\(n\\) possible values for rear unless we invent a special case for, say, empty queues. This is an example of the Pigeonhole Principle. The Pigeonhole Principle states that, given \\(n\\) pigeonholes and \\(n+1\\) pigeons, when all of the pigeons go into the holes we can be sure that at least one hole contains more than one pigeon. In similar manner, we can be sure that two of the \\(n+1\\) states are indistinguishable by the \\(n\\) relative values of front and rear. We must seek some other way to distinguish full from empty queues. One obvious solution is to keep an explicit count of the number of elements in the queue, or at least a Boolean variable that indicates whether the queue is empty or not. Another solution is to make the array be of size \\(n+1\\), and only allow \\(n\\) elements to be stored. Which of these solutions to adopt is purely a matter of the implementor’s taste in such affairs. Our choice here is to use an array of size \\(n+1\\). Contact Us || Privacy | | License « 2.6. Linked Stacks :: Contents :: 2.8. Linked Queues » Contact Us | | Report a bug © Copyright 2011-2023 by OpenDSA Project Contributors and distributed under an MIT license. Last updated on Aug 13, 2024. Created using Sphinx 4.4.0. Summary*: Operating system*: Windows Mac OS Linux iOS Android Other Browser*: Chrome Safari Internet Explorer Opera Other Description*: Attach a screenshot (optional):"
            },
            {
                "role": "user",
                "content": "Can you summarize the key topics covered in this lecture?"
            },
            {
                "role": "assistant",
                "content": "This lecture covers foundational algorithmic concepts including problem formulation, correctness proof using induction, efficiency analysis using asymptotic notation, data structures (arrays, linked lists, BSTs, heaps), and sorting methods (selection sort, merge sort, counting sort). Additionally, it introduces the Word-RAM computational model, providing a theoretical framework for measuring algorithm efficiency."
            }
        ]
    },
    {
        "system": "Hello! I am Kiran, an AI assistant from Tampere University, dedicated to providing you with accurate and insightful assistance. My goal is to help you understand concepts, summarize lectures, and answer questions effectively.",
        "conversations": [
            {
                "role": "host",
                "content": "2.8. Linked Queues — BM40A1500 Data Structures and Algorithms Username Password Forgot your password? Register Username: Password Confirm Password Email: BM40A1500 Data Structures and Algorithms Chapter 2 Linear Data Structures Show Source | | About « 2.7. Queues :: Contents :: 2.9. Searching in an Array » 2.8. Linked Queues¶ The linked queue implementation is a straightforward adaptation of the linked list. Here is the linked queue class declaration. 2.8.1. Linked Enqueue¶ Settings Saving... Server Error Resubmit 2.8.2. Linked Dequeue¶ Settings Saving... Server Error Resubmit 2.8.3. Comparison of Array-Based and Linked Queues¶ All member functions for both the array-based and linked queue implementations require constant time. The space comparison issues are the same as for the equivalent stack implementations. Unlike the array-based stack implementation, there is no convenient way to store two queues in the same array, unless items are always transferred directly from one queue to the other. Contact Us || Privacy | | License « 2.7. Queues :: Contents :: 2.9. Searching in an Array » Contact Us | | Report a bug © Copyright 2011-2023 by OpenDSA Project Contributors and distributed under an MIT license. Last updated on Aug 13, 2024. Created using Sphinx 4.4.0. Summary*: Operating system*: Windows Mac OS Linux iOS Android Other Browser*: Chrome Safari Internet Explorer Opera Other Description*: Attach a screenshot (optional):"
            },
            {
                "role": "user",
                "content": "Can you summarize the key topics covered in this lecture?"
            },
            {
                "role": "assistant",
                "content": "This lecture covers foundational algorithmic concepts including problem formulation, correctness proof using induction, efficiency analysis using asymptotic notation, data structures (arrays, linked lists, BSTs, heaps), and sorting methods (selection sort, merge sort, counting sort). Additionally, it introduces the Word-RAM computational model, providing a theoretical framework for measuring algorithm efficiency."
            }
        ]
    },
    {
        "system": "Hello! I am Kiran, an AI assistant from Tampere University, dedicated to providing you with accurate and insightful assistance. My goal is to help you understand concepts, summarize lectures, and answer questions effectively.",
        "conversations": [
            {
                "role": "host",
                "content": "5.4. Divide and Conquer: Quicksort — BM40A1500 Data Structures and Algorithms Username Password Forgot your password? Register Username: Password Confirm Password Email: BM40A1500 Data Structures and Algorithms Chapter 5 Algorithm Design Principles Show Source | | About « 5.3. Divide and Conquer: Mergesort :: Contents :: 5.5. Backtracking, and Branch and Bound » 5.4. Divide and Conquer: Quicksort¶ 5.4.1. Introduction¶ While Mergesort uses the most obvious form of divide and conquer (split the list in half then sort the halves), this is not the only way that we can break down the sorting problem. We saw that doing the merge step for Mergesort when using an array implementation is not so easy. So perhaps a different divide and conquer strategy might turn out to be more efficient? Quicksort is aptly named because, when properly implemented, it is the fastest known general-purpose in-memory sorting algorithm in the average case. It does not require the extra array needed by Mergesort, so it is space efficient as well. Quicksort is widely used, and is typically the algorithm implemented in a library sort routine such as the UNIX qsort function. Interestingly, Quicksort is hampered by exceedingly poor worst-case performance, thus making it inappropriate for certain applications. Before we get to Quicksort, consider for a moment the practicality of using a Binary Search Tree for sorting. You could insert all of the values to be sorted into the BST one by one, then traverse the completed tree using an inorder traversal. The output would form a sorted list. This approach has a number of drawbacks, including the extra space required by BST pointers and the amount of time required to insert nodes into the tree. However, this method introduces some interesting ideas. First, the root of the BST (i.e., the first node inserted) splits the list into two sublists: The left subtree contains those values in the list less than the root value while the right subtree contains those values in the list greater than or equal to the root value. Thus, the BST implicitly implements a “divide and conquer” approach to sorting the left and right subtrees. Quicksort implements this same concept in a much more efficient way. Quicksort first selects a value called the pivot. (This is conceptually like the root node’s value in the BST.) Assume that the input array contains \\(k\\) records with key values less than the pivot. The records are then rearranged in such a way that the \\(k\\) values less than the pivot are placed in the first, or leftmost, \\(k\\) positions in the array, and the values greater than or equal to the pivot are placed in the last, or rightmost, \\(n-k\\) positions. This is called a partition of the array. The values placed in a given partition need not (and typically will not) be sorted with respect to each other. All that is required is that all values end up in the correct partition. The pivot value itself is placed in position \\(k\\). Quicksort then proceeds to sort the resulting subarrays now on either side of the pivot, one of size \\(k\\) and the other of size \\(n-k-1\\). How are these values sorted? Because Quicksort is such a good algorithm, using Quicksort on the subarrays would be appropriate. Quicksort might not seem very “natural” in that it is not an approach that a person is likely to use to sort real objects. But it should not be too surprising that a really efficient sort for huge numbers of abstract objects on a computer would be rather different from our experiences with sorting a relatively few physical objects. Here is an implementation for Quicksort. Parameters i and j define the left and right indices, respectively, for the subarray being sorted. The initial call to quicksort would be quicksort(array, 0, n-1). def quicksort(A, i, j): pivotindex = findpivot(A, i, j) # Pick a pivot A[j], A[pivotindex] = A[pivotindex], A[j] # Stick pivot at the end k = partition(A, i, j-1, A[j]) # k will be the first position in the right sub-array A[j], A[k] = A[k], A[j] # Put pivot in place if k-i > 1: quicksort(A, i, k-1) # Sort left partition if j-k > 1: quicksort(A, k+1, j) # Sort right partition Function partition will move records to the appropriate partition and then return k, the first position in the right partition. Note that the pivot value is initially placed at the end of the array (position j). Thus, partition must not affect the value of array position j. After partitioning, the pivot value is placed in position k, which is its correct position in the final, sorted array. By doing so, we guarantee that at least one value (the pivot) will not be processed in the recursive calls to qsort. Even if a bad pivot is selected, yielding a completely empty partition to one side of the pivot, the larger partition will contain at most \\(n-1\\) records. Selecting a pivot can be done in many ways. The simplest is to use the first key. However, if the input is sorted or reverse sorted, this will produce a poor partitioning with all values to one side of the pivot. It is better to pick a value at random, thereby reducing the chance of a bad input order affecting the sort. Unfortunately, using a random number generator is relatively expensive, and we can do nearly as well by selecting the middle position in the array. Here is a simple findpivot function. def findpivot(A, i, j): return (i + j) // 2 5.4.2. Partition¶ We now turn to function partition. If we knew in advance how many keys are less than the pivot, partition could simply copy records with key values less than the pivot to the low end of the array, and records with larger keys to the high end. Because we do not know in advance how many keys are less than the pivot, we use a clever algorithm that moves indices inwards from the ends of the subarray, swapping values as necessary until the two indices meet. Here is an implementation for the partition step. def partition(A, left, right, pivot): while (left <= right): # Move bounds inward until they meet while (A[left] < pivot): left += 1 while right >= left and A[right] >= pivot: right -= 1 if right > left: # Swap out-of-place values A[left], A[right] = A[right], A[left] Note the check that right >= left in the second inner while loop. This ensures that right does not run off the low end of the partition in the case where the pivot is the least value in that partition. Function partition returns the first index of the right partition (the place where left ends at) so that the subarray bound for the recursive calls to qsort can be determined. Settings Saving... Server Error Resubmit And here is a visualization illustrating the running time analysis of the partition function Settings Saving... Server Error Resubmit 5.4.3. Putting It Together¶ Here is a visualization for the entire Quicksort algorithm. This visualization shows you how the logical decomposition caused by the partitioning process works. In the visualization, the separate sub-partitions are separated out to match the recursion tree. In reality, there is only a single array involved (as you will see in the proficiency exercise that follows the visualization). 5.4.4. Quicksort Analysis¶ This visualization explains the worst-case running time of Quick Sort Settings Saving... Server Error Resubmit This is terrible, no better than Bubble Sort. When will this worst case occur? Only when each pivot yields a bad partitioning of the array. If the pivot values are selected at random, then this is extremely unlikely to happen. When selecting the middle position of the current subarray, it is still unlikely to happen. It does not take many good partitionings for Quicksort to work fairly well. This visualization explains the best-case running time of Quick Sort Settings Saving... Server Error Resubmit Quicksort’s average-case behavior falls somewhere between the extremes of worst and best case. Average-case analysis considers the cost for all possible arrangements of input, summing the costs and dividing by the number of cases. We make one reasonable simplifying assumption: At each partition step, the pivot is equally likely to end in any position in the (sorted) array. In other words, the pivot is equally likely to break an array into partitions of sizes 0 and \\(n-1\\), or 1 and \\(n-2\\), and so on. Given this assumption, the average-case cost is computed from the following equation: \\[{\\bf T}(n) = cn + \\frac{1}{n}\\sum_{k=0}^{n-1}[{\\bf T}(k) + {\\bf T}(n - 1 - k)], \\quad {\\bf T}(0) = {\\bf T}(1) = c.\\] This visualization will help you to understand how this recurrence relation was formed. Settings Saving... Server Error Resubmit This is an unusual situation that the average case cost and the worst case cost have asymptotically different growth rates. Consider what “average case” actually means. We compute an average cost for inputs of size \\(n\\) by summing up for every possible input of size \\(n\\) the product of the running time cost of that input times the probability that that input will occur. To simplify things, we assumed that every permutation is equally likely to occur. Thus, finding the average means summing up the cost for every permutation and dividing by the number of permutations (which is \\(n!\\)). We know that some of these \\(n!\\) inputs cost \\(O(n^2)\\). But the sum of all the permutation costs has to be \\((n!)(O(n \\log n))\\). Given the extremely high cost of the worst inputs, there must be very few of them. In fact, there cannot be a constant fraction of the inputs with cost \\(O(n^2)\\). If even, say, 1% of the inputs have cost \\(O(n^2)\\), this would lead to an average cost of \\(O(n^2)\\). Thus, as \\(n\\) grows, the fraction of inputs with high cost must be going toward a limit of zero. We can conclude that Quicksort will run fast if we can avoid those very few bad input permutations. This is why picking a good pivot is so important. The running time for Quicksort can be improved (by a constant factor), and much study has gone into optimizing this algorithm. Since Quicksort’s worst case behavior arises when the pivot does a poor job of splitting the array into equal size subarrays, improving findpivot seems like a good place to start. If we are willing to do more work searching for a better pivot, the effects of a bad pivot can be decreased or even eliminated. Hopefully this will save more time than was added by the additional work needed to find the pivot. One widely-used choice is to use the “median of three” algorithm, which uses as a pivot the middle of three randomly selected values. Using a random number generator to choose the positions is relatively expensive, so a common compromise is to look at the first, middle, and last positions of the current subarray. However, our simple findpivot function that takes the middle value as its pivot has the virtue of making it highly unlikely to get a bad input by chance, and it is quite cheap to implement. This is in sharp contrast to selecting the first or last record as the pivot, which would yield bad performance for many permutations that are nearly sorted or nearly reverse sorted. A significant improvement can be gained by recognizing that Quicksort is relatively slow when \\(n\\) is small. This might not seem to be relevant if most of the time we sort large arrays, nor should it matter how long Quicksort takes in the rare instance when a small array is sorted because it will be fast anyway. But you should notice that Quicksort itself sorts many, many small arrays! This happens as a natural by-product of the divide and conquer approach. A simple improvement might then be to replace Quicksort with a faster sort for small numbers, say Insertion Sort or Selection Sort. However, there is an even better—and still simpler—optimization. When Quicksort partitions are below a certain size, do nothing! The values within that partition will be out of order. However, we do know that all values in the array to the left of the partition are smaller than all values in the partition. All values in the array to the right of the partition are greater than all values in the partition. Thus, even if Quicksort only gets the values to “nearly” the right locations, the array will be close to sorted. This is an ideal situation in which to take advantage of the best-case performance of Insertion Sort. The final step is a single call to Insertion Sort to process the entire array, putting the records into final sorted order. Empirical testing shows that the subarrays should be left unordered whenever they get down to nine or fewer records. The last speedup to be considered reduces the cost of making recursive calls. Quicksort is inherently recursive, because each Quicksort operation must sort two sublists. Thus, there is no simple way to turn Quicksort into an iterative algorithm. However, Quicksort can be implemented using a stack to imitate recursion, as the amount of information that must be stored is small. We need not store copies of a subarray, only the subarray bounds. Furthermore, the stack depth can be kept small if care is taken on the order in which Quicksort’s recursive calls are executed. We can also place the code for findpivot and partition inline to eliminate the remaining function calls. Note however that by not processing sublists of size nine or less as suggested above, about three quarters of the function calls will already have been eliminated. Thus, eliminating the remaining function calls will yield only a modest speedup. Contact Us || Privacy | | License « 5.3. Divide and Conquer: Mergesort :: Contents :: 5.5. Backtracking, and Branch and Bound » Contact Us | | Report a bug © Copyright 2011-2023 by OpenDSA Project Contributors and distributed under an MIT license. Last updated on Aug 13, 2024. Created using Sphinx 4.4.0. Summary*: Operating system*: Windows Mac OS Linux iOS Android Other Browser*: Chrome Safari Internet Explorer Opera Other Description*: Attach a screenshot (optional):"
            },
            {
                "role": "user",
                "content": "Can you summarize the key topics covered in this lecture?"
            },
            {
                "role": "assistant",
                "content": "This lecture covers foundational algorithmic concepts including problem formulation, correctness proof using induction, efficiency analysis using asymptotic notation, data structures (arrays, linked lists, BSTs, heaps), and sorting methods (selection sort, merge sort, counting sort). Additionally, it introduces the Word-RAM computational model, providing a theoretical framework for measuring algorithm efficiency."
            }
        ]
    },
    {
        "system": "Hello! I am Kiran, an AI assistant from Tampere University, dedicated to providing you with accurate and insightful assistance. My goal is to help you understand concepts, summarize lectures, and answer questions effectively.",
        "conversations": [
            {
                "role": "host",
                "content": "4.1. Introduction — BM40A1500 Data Structures and Algorithms Username Password Forgot your password? Register Username: Password Confirm Password Email: BM40A1500 Data Structures and Algorithms Chapter 4 Recursion and Binary Trees Show Source | | About « 3.7. Deletion :: Contents :: 4.2. Writing a recursive function » 4.1. Introduction¶ An algorithm (or a function in a computer program) is recursive if it invokes itself to do part of its work. Recursion makes it possible to solve complex problems using programs that are concise, easily understood, and algorithmically efficient. Recursion is the process of solving a large problem by reducing it to one or more sub-problems which are identical in structure to the original problem and somewhat simpler to solve. Once the original subdivision has been made, the sub-problems divided into new ones which are even less complex. Eventually, the sub-problems become so simple that they can be then solved without further subdivision. Ultimately, the complete solution is obtained by reassembling the solved components. For a recursive approach to be successful, the recursive “call to itself” must be on a smaller problem than the one originally attempted. In general, a recursive algorithm must have two parts: The base case, which handles a simple input that can be solved without resorting to a recursive call, and The recursive part which contains one or more recursive calls to the algorithm. In every recursive call, the parameters must be in some sense “closer” to the base case than those of the original call. Recursion has no counterpart in everyday, physical-world problem solving. The concept can be difficult to grasp because it requires you to think about problems in a new way. When first learning recursion, it is common for people to think a lot about the recursive process. We will spend some time in these modules going over the details for how recursion works. But when writing recursive functions, it is best to stop thinking about how the recursion works beyond the recursive call. You should adopt the attitude that the sub-problems will take care of themselves, that when you call the function recursively it will return the right answer. You just worry about the base cases and how to recombine the sub-problems. Newcomers who are unfamiliar with recursion often find it hard to accept that it is used primarily as a tool for simplifying the design and description of algorithms. A recursive algorithm does not always yield the most efficient computer program for solving the problem because recursion involves function calls, which are typically more expensive than other alternatives such as a while loop. However, the recursive approach usually provides an algorithm that is reasonably efficient. If necessary, the clear, recursive solution can later be modified to yield a faster implementation. Imagine that someone in a movie theater asks you what row you’re sitting in. You don’t want to count, so you ask the person in front of you what row they are sitting in, knowing that they will tell you a number one less than your row number. The person in front could ask the person in front of them. This will keep happening until word reaches the front row and it is easy to respond: “I’m in row 1!” From there, the correct message (incremented by one each row) will eventually make it’s way back to the person who asked. Imagine that you have a big task. You could just do a small piece of it, and then delegate the rest to some helper, as in this example. Settings Saving... Server Error Resubmit Let’s look deeper into the details of what your friend does when you delegate the work. (Note that we show you this process once now, and once again when we look at some recursive functions. But when you are writing your own recursive functions, you shouldn’t worry about all of these details.) Settings Saving... Server Error Resubmit In order to understand recursion, you need to be able to do two things. First, you have to understand how to read a recursive function. Second, you have to understand how to write a recursive function. Both of these skills require a lot of practice. So we will give you a lot of exercises to do later on. Contact Us || Privacy | | License « 3.7. Deletion :: Contents :: 4.2. Writing a recursive function » Contact Us | | Report a bug © Copyright 2011-2023 by OpenDSA Project Contributors and distributed under an MIT license. Last updated on Aug 13, 2024. Created using Sphinx 4.4.0. Summary*: Operating system*: Windows Mac OS Linux iOS Android Other Browser*: Chrome Safari Internet Explorer Opera Other Description*: Attach a screenshot (optional):"
            },
            {
                "role": "user",
                "content": "Can you summarize the key topics covered in this lecture?"
            },
            {
                "role": "assistant",
                "content": "This lecture covers foundational algorithmic concepts including problem formulation, correctness proof using induction, efficiency analysis using asymptotic notation, data structures (arrays, linked lists, BSTs, heaps), and sorting methods (selection sort, merge sort, counting sort). Additionally, it introduces the Word-RAM computational model, providing a theoretical framework for measuring algorithm efficiency."
            }
        ]
    },
    {
        "system": "Hello! I am Kiran, an AI assistant from Tampere University, dedicated to providing you with accurate and insightful assistance. My goal is to help you understand concepts, summarize lectures, and answer questions effectively.",
        "conversations": [
            {
                "role": "host",
                "content": "4.3. Tracing Recursive Code — BM40A1500 Data Structures and Algorithms Username Password Forgot your password? Register Username: Password Confirm Password Email: BM40A1500 Data Structures and Algorithms Chapter 4 Recursion and Binary Trees Show Source | | About « 4.2. Writing a recursive function :: Contents :: 4.4. Binary Trees » 4.3. Tracing Recursive Code¶ When writing a recursive function, you should think in a top-down manner. Do not worry about how the recursive call solves the sub-problem. Simply accept that it will solve it correctly. Use this result as though you had called some library function, to correctly solve the original problem. When you have to read or trace a recursive function, then you do need to consider how the function is doing its work. Tracing a few recursive functions is a great way to learn how recursion behaves. But after you become comfortable with tracing, you will rarely need to work through so many details again. You will begin to develop confidence about how recursion works. You know that information can be passed in (using a function parameter) from one recursive call to another, on ever smaller problems, until a base case is reached in the winding phase. Then, a return value is passed back as the series of recursive calls unwinds. Sometimes people forget about the “unwinding” phase. Settings Saving... Server Error Resubmit During the winding phase, any parameter passed through the recursive call flows forward until the base case is reached. During the unwinding phase, the return value of the function (if there is one) flows backwards to the calling copy of the function. In the following example, a recursive function to compute factorial has information flowing forward during the winding phase, and backward during the unwinding phase. Settings Saving... Server Error Resubmit The recursive function may have information flow for more than one parameter. For example, a recursive function that sums the values in an array recursively may pass the array itself and the index through the recursive call in the winding phase and returns back the summed value so far in the unwinding phase. Settings Saving... Server Error Resubmit 4.3.1. A Domino Analogy¶ Settings Saving... Server Error Resubmit This recursive model for the domino effect can be used as a template for the solution to all linear recursive functions. Think of tipping over each domino as performing a further step of computation toward the final solution. Remember these rules: 1. Since the first domino has to be tipped over manually, the solution for the base case is computed non-recursively. 2. Before any given domino can be tipped over, all preceding dominos have to be tipped over. 4.3.2. Towers of Hanoi¶ Here is another example of recursion, based on a famous puzzle called “Towers of Hanoi”. The natural algorithm to solve this problem has multiple recursive calls. It cannot be rewritten easily using loops. “Towers of Hanoi” comes from an ancient Vietnamese legend. A group of monks is tasked with moving a tower of 64 disks of different sizes according to certain rules. The legend says that, when the monks will have finished moving all of the disks, the world will end. The Towers of Hanoi puzzle begins with three poles and \\(n\\) rings, where all rings start on the leftmost pole (labeled Pole A). The rings each have a different size, and are stacked in order of decreasing size with the largest ring at the bottom, as shown in part (a) of the figure. The problem is to move the rings from the leftmost pole to the middle pole (labeled Pole B) in a series of steps. At each step the top ring on some pole is moved to another pole. What makes this puzzle interesting is the limitation on where rings may be moved: A ring may never be moved on top of a smaller ring. How can you solve this problem? It is easy if you don’t think too hard about the details. Instead, consider that all rings are to be moved from Pole A to Pole B. It is not possible to do this without first moving the bottom (largest) ring to Pole B. To do that, Pole B must be empty, and only the bottom ring can be on Pole A. The remaining \\(n-1\\) rings must be stacked up in order on Pole C, as shown in part (b) of the figure. How can you do this? Assume that a function \\(X\\) is available to solve the problem of moving the top \\(n-1\\) rings from Pole A to Pole C. Then move the bottom ring from Pole A to Pole B. Finally, again use function \\(X\\) to move the remaining \\(n-1\\) rings from Pole C to Pole B. In both cases, “function \\(X\\)” is simply the Towers of Hanoi function called on a smaller version of the problem. The secret to success is relying on the Towers of Hanoi algorithm to do the work for you. You need not be concerned about the gory details of how the Towers of Hanoi subproblem will be solved. That will take care of itself provided that two things are done. First, there must be a base case (what to do if there is only one ring) so that the recursive process will not go on forever. Second, the recursive call to Towers of Hanoi can only be used to solve a smaller problem, and then only one of the proper form (one that meets the original definition for the Towers of Hanoi problem, assuming appropriate renaming of the poles). Here is an implementation for the recursive Towers of Hanoi algorithm. Function move(start, goal) takes the top ring from Pole start and moves it to Pole goal. If move were to print the values of its parameters, then the result of calling TOHr would be a list of ring-moving instructions that solves the problem. # Compute the moves to solve a Tower of Hanoi puzzle. # Function move does (or prints) the actual move of a disk # from one pole to another. # n: The number of disks # start: The start pole # goal: The goal pole # temp: The other pole def TOHr(n, start, goal, temp): if n == 0: return # Base case TOHr(n-1, start, temp, goal) # Recursive call: n-1 rings move(start, goal) # Move bottom disk to goal TOHr(n-1, temp, goal, start) # Recursive call: n-1 rings This next slideshow explains the solution to the Towers of Hanoi problem. Settings Saving... Server Error Resubmit Contact Us || Privacy | | License « 4.2. Writing a recursive function :: Contents :: 4.4. Binary Trees » Contact Us | | Report a bug © Copyright 2011-2023 by OpenDSA Project Contributors and distributed under an MIT license. Last updated on Aug 13, 2024. Created using Sphinx 4.4.0. Summary*: Operating system*: Windows Mac OS Linux iOS Android Other Browser*: Chrome Safari Internet Explorer Opera Other Description*: Attach a screenshot (optional):"
            },
            {
                "role": "user",
                "content": "Can you summarize the key topics covered in this lecture?"
            },
            {
                "role": "assistant",
                "content": "This lecture covers foundational algorithmic concepts including problem formulation, correctness proof using induction, efficiency analysis using asymptotic notation, data structures (arrays, linked lists, BSTs, heaps), and sorting methods (selection sort, merge sort, counting sort). Additionally, it introduces the Word-RAM computational model, providing a theoretical framework for measuring algorithm efficiency."
            }
        ]
    },
    {
        "system": "Hello! I am Kiran, an AI assistant from Tampere University, dedicated to providing you with accurate and insightful assistance. My goal is to help you understand concepts, summarize lectures, and answer questions effectively.",
        "conversations": [
            {
                "role": "host",
                "content": "4.2. Writing a recursive function — BM40A1500 Data Structures and Algorithms Username Password Forgot your password? Register Username: Password Confirm Password Email: BM40A1500 Data Structures and Algorithms Chapter 4 Recursion and Binary Trees Show Source | | About « 4.1. Introduction :: Contents :: 4.3. Tracing Recursive Code » 4.2. Writing a recursive function¶ Solving a “big” problem recursively means to solve one or more smaller versions of the problem, and using those solutions of the smaller problems to solve the “big” problem. In particular, solving problems recursively means that smaller versions of the problem are solved in a similar way. For example, consider the problem of summing values of an array. What’s the difference between summing the first 50 elements in an array versus summing the first 100 elements? You would use the same technique. You can even use the solution to the smaller problem to help you solve the larger problem. Here are the basic four steps that you need to write any recursive function. Settings Saving... Server Error Resubmit Now le’t see some different ways that we could write Sum recursively. Settings Saving... Server Error Resubmit Example 4.2.1 Our example for summing the first \\(n\\) numbers of an array could have been written just as easily using a loop. Here is an example of a function that is more naturally written using recursion. The following code computes the Fibonacci sequence for a given number. The Fibonacci Sequence is the series of numbers: 1, 1, 2, 3, 5, 8, 13, 21, 34, … Any number in the sequence is found by adding up the two numbers before it. The base cases are that Fibonacci(0) = 1 and Fibonacci(1) = 1. def Fibonacci(n) if n < 2: return 1 return Fibonacci(n - 1) + Fibonacci(n - 2) Contact Us || Privacy | | License « 4.1. Introduction :: Contents :: 4.3. Tracing Recursive Code » Contact Us | | Report a bug © Copyright 2011-2023 by OpenDSA Project Contributors and distributed under an MIT license. Last updated on Aug 13, 2024. Created using Sphinx 4.4.0. Summary*: Operating system*: Windows Mac OS Linux iOS Android Other Browser*: Chrome Safari Internet Explorer Opera Other Description*: Attach a screenshot (optional):"
            },
            {
                "role": "user",
                "content": "Can you summarize the key topics covered in this lecture?"
            },
            {
                "role": "assistant",
                "content": "This lecture covers foundational algorithmic concepts including problem formulation, correctness proof using induction, efficiency analysis using asymptotic notation, data structures (arrays, linked lists, BSTs, heaps), and sorting methods (selection sort, merge sort, counting sort). Additionally, it introduces the Word-RAM computational model, providing a theoretical framework for measuring algorithm efficiency."
            }
        ]
    },
    {
        "system": "Hello! I am Kiran, an AI assistant from Tampere University, dedicated to providing you with accurate and insightful assistance. My goal is to help you understand concepts, summarize lectures, and answer questions effectively.",
        "conversations": [
            {
                "role": "host",
                "content": "7.2. Reductions — BM40A1500 Data Structures and Algorithms Username Password Forgot your password? Register Username: Password Confirm Password Email: BM40A1500 Data Structures and Algorithms Chapter 7 NP-completeness Show Source | | About « 7.1. Limits to Computing :: Contents :: 7.3. NP-Completeness » 7.2. Reductions¶ This module introduces an important concept for understanding the relationships between problems, called reduction. Reduction allows us to solve one problem in terms of another. Equally importantly, when we wish to understand the difficulty of a problem, reduction allows us to make relative statements about upper and lower bounds on the cost of a problem (as opposed to an algorithm or program). Because the concept of a problem is discussed extensively in this chapter, we want notation to simplify problem descriptions. Throughout this chapter, a problem will be defined in terms of a mapping between inputs and outputs, and the name of the problem will be given in all capital letters. Thus, a complete definition of the sorting problem could appear as follows: SORTING Input: A sequence of integers \\(x_0, x_1, x_2, \\ldots, x_{n-1}\\). Output: A permutation \\(y_0, y_1, y_2, \\ldots, y_{n-1}\\) of the sequence such that \\(y_i \\leq y_j\\) whenever \\(i < j\\). Settings Saving... Server Error Resubmit 7.2.1. Example: The Pairing Problem¶ When you buy or write a program to solve one problem, such as sorting, you might be able to use it to help solve a different problem. This is known in software engineering as software reuse. To illustrate this, let us consider another problem. PAIRING Input: Two sequences of integers \\(X = (x_0, x_1, ..., x_{n-1})\\) and \\(Y =(y_0, y_1, ..., y_{n-1})\\). Output: A pairing of the elements in the two sequences such that the least value in \\(X\\) is paired with the least value in \\(Y\\), the next least value in \\(X\\) is paired with the next least value in \\(Y\\), and so on. Figure 7.2.1: An illustration of PAIRING. The two lists of numbers are paired up so that the least values from each list make a pair, the next smallest values from each list make a pair, and so on.¶ Settings Saving... Server Error Resubmit Figure 7.2.1 illustrates PAIRING. One way to solve PAIRING is to use an existing sorting program to sort each of the two sequences, and then pair off items based on their position in sorted order. Technically we say that in this solution, PAIRING is reduced to SORTING, because SORTING is used to solve PAIRING. Notice that reduction is a three-step process. The first step is to convert an instance of PAIRING into two instances of SORTING. The conversion step in this example is not very interesting; it simply takes each sequence and assigns it to an array to be passed to SORTING. The second step is to sort the two arrays (i.e., apply SORTING to each array). The third step is to convert the output of SORTING to the output for PAIRING. This is done by pairing the first elements in the sorted arrays, the second elements, and so on. A reduction of PAIRING to SORTING helps to establish an upper bound on the cost of PAIRING. In terms of asymptotic notation, assuming that we can find one method to convert the inputs to PAIRING into inputs to SORTING “fast enough”, and a second method to convert the result of SORTING back to the correct result for PAIRING “fast enough”, then the asymptotic cost of PAIRING cannot be more than the cost of SORTING. In this case, there is little work to be done to convert from PAIRING to SORTING, or to convert the answer from SORTING back to the answer for PAIRING, so the dominant cost of this solution is performing the sort operation. Thus, an upper bound for PAIRING is in \\(O(n \\log n)\\). It is important to note that the pairing problem does not require that elements of the two sequences be sorted. This is merely one possible way to solve the problem. PAIRING only requires that the elements of the sequences be paired correctly. Perhaps there is another way to do it? Certainly if we use sorting to solve PAIRING, the algorithms will require \\(\\Omega(n \\log n)\\) time. But, another approach might conceivably be faster. 7.2.2. Reduction and Finding a Lower Bound¶ There is another use of reductions aside from applying an old algorithm to solve a new problem (and thereby establishing an upper bound for the new problem). That is to prove a lower bound on the cost of a new problem by showing that it could be used as a solution for an old problem with a known lower bound. Assume we can go the other way and convert SORTING to PAIRING “fast enough”. What does this say about the minimum cost of PAIRING? We know that the lower bound for SORTING in the worst and average cases is in \\(\\Omega(n \\log n)\\). In other words, the best possible algorithm for sorting requires at least \\(n \\log n\\) time. Assume that PAIRING could be done in \\(O(n)\\) time. Then, one way to create a sorting algorithm would be to convert SORTING into PAIRING, run the algorithm for PAIRING, and finally convert the answer back to the answer for SORTING. Provided that we can convert SORTING to/from PAIRING “fast enough”, this process would yield an \\(O(n)\\) algorithm for sorting! Because this contradicts what we know about the lower bound for SORTING, and the only flaw in the reasoning is the initial assumption that PAIRING can be done in \\(O(n)\\) time, we can conclude that there is no \\(O(n)\\) time algorithm for PAIRING. This reduction process tells us that PAIRING must be at least as expensive as SORTING and so must itself have a lower bound in \\(\\Omega(n \\log n)\\). To complete this proof regarding the lower bound for PAIRING, we need now to find a way to reduce SORTING to PAIRING. This is easily done. Take an instance of SORTING (i.e., an array \\(A\\) of \\(n\\) elements). A second array \\(B\\) is generated that simply stores \\(i\\) in position \\(i\\) for \\(0 \\leq i < n\\). Pass the two arrays to PAIRING. Take the resulting set of pairs, and use the value from the \\(B\\) half of the pair to tell which position in the sorted array the \\(A\\) half should take; that is, we can now reorder the records in the \\(A\\) array using the corresponding value in the \\(B\\) array as the sort key and running a simple \\(\\Theta(n)\\) Binsort. The conversion of SORTING to PAIRING can be done in \\(O(n)\\) time, and likewise the conversion of the output of PAIRING can be converted to the correct output for SORTING in \\(O(n)\\) time. Thus, the cost of this “sorting algorithm” is dominated by the cost for PAIRING. 7.2.3. The Reduction Template¶ Consider any two problems for which a suitable reduction from one to the other can be found. The first problem takes an arbitrary instance of its input, which we will call I, and transforms I to a solution, which we will call SLN. The second problem takes an arbitrary instance of its input, which we will call I’, and transforms I’ to a solution, which we will call SLN’. We can define reduction more formally as a three-step process: Transform an arbitrary instance of the first problem to an instance of the second problem. In other words, there must be a transformation from any instance I of the first problem to an instance I’ of the second problem. Apply an algorithm for the second problem to the instance I’, yielding a solution SLN’. Transform SLN’ to the solution of I, known as SLN. Note that SLN must in fact be the correct solution for I for the reduction to be acceptable. Figure 7.2.2: The general process for reduction shown as a “blackbox” diagram.¶ Figure 7.2.2 shows a graphical representation of the general reduction process, showing the role of the two problems, and the two transformations. Next is a slideshow that shows the steps for the reduction of SORTING to PAIRING. Settings Saving... Server Error Resubmit It is important to note that the reduction process does not give us an algorithm for solving either problem by itself. It merely gives us a method for solving the first problem given that we already have a solution to the second. More importantly for the topics to be discussed in the remainder of this chapter, reduction gives us a way to understand the bounds of one problem in terms of another. Specifically, given efficient transformations, the upper bound of the first problem is at most the upper bound of the second. Conversely, the lower bound of the second problem is at least the lower bound of the first. 7.2.4. Two Multiplication Examples¶ As a second example of reduction, consider the simple problem of multiplying two \\(n\\)-digit numbers. The standard long-hand method for multiplication is to multiply the last digit of the first number by the second number (taking \\(\\Theta(n)\\) time), multiply the second digit of the first number by the second number (again taking \\(\\Theta(n)\\) time), and so on for each of the \\(n\\) digits of the first number. Finally, the intermediate results are added together. Note that adding two numbers of length \\(M\\) and \\(N\\) can easily be done in \\(\\Theta(M + N)\\) time. Because each digit of the first number is multiplied against each digit of the second, this algorithm requires \\(\\Theta(n^2)\\) time. Asymptotically faster (but more complicated) algorithms are known, but none is so fast as to be in \\(O(n)\\). Next we ask the question: Is squaring an \\(n\\)-digit number as difficult as multiplying two \\(n\\)-digit numbers? We might hope that something about this special case will allow for a faster algorithm than is required by the more general multiplication problem. However, a simple reduction proof serves to show that squaring is “as hard” as multiplying. The key to the reduction is the following formula: \\[X \\times Y = \\frac{(X + Y)^2 - (X - Y)^2}{4}.\\] The significance of this formula is that it allows us to convert an arbitrary instance of multiplication to a series of operations involving three addition/subtractions (each of which can be done in linear time), two squarings, and a division by 4. This is because \\[(X + Y)^2 - (X - Y)^2 = X^2 + 2XY + Y^2 - (X^2 - 2XY + Y^2) = 4XY\\] Note that the division by 4 can be done in linear time (simply convert to binary, shift right by two digits, and convert back). This reduction shows that if a linear time algorithm for squaring can be found, it can be used to construct a linear time algorithm for multiplication. Our next example of reduction concerns the multiplication of two \\(n \\times n\\) matrices. For this problem, we will assume that the values stored in the matrices are simple integers and that multiplying two simple integers takes constant time (because multiplication of two int variables takes a fixed number of machine instructions). The standard algorithm for multiplying two matrices is to multiply each element of the first matrix’s first row by the corresponding element of the second matrix’s first column, then adding the numbers. This takes \\(\\Theta(n)\\) time. Each of the \\(n^2\\) elements of the solution are computed in similar fashion, requiring a total of \\(\\Theta(n^3)\\) time. Faster algorithms are known (see Strassen’s algorithm), but none are so fast as to be in \\(O(n^2)\\). Now, consider the case of multiplying two symmetric matrices. A symmetric matrix is one in which entry \\(ij\\) is equal to entry \\(ji\\); that is, the upper-right triangle of the matrix is a mirror image of the lower-left triangle. Is there something about this restricted case that allows us to multiply two symmetric matrices faster than in the general case? The answer is no, as can be seen by the following reduction. Assume that we have been given two \\(n \\times n\\) matrices \\(A\\) and \\(B\\). We can construct a \\(2n \\times 2n\\) symmetric matrix from an arbitrary matrix \\(A\\) as follows: \\[\\begin{split}\\left[ \\begin{array}{cc} 0 &A\\\\ A^{\\rm T}& 0 \\end{array} \\right].\\end{split}\\] Here 0 stands for an \\(n \\times n\\) matrix composed of zero values, \\(A\\) is the original matrix, and \\(A^{\\rm T}\\) stands for the transpose of matrix \\(A\\). 1 Note that the resulting matrix is now symmetric. We can convert matrix \\(B\\) to a symmetric matrix in a similar manner. If symmetric matrices could be multiplied “quickly” (in particular, if they could be multiplied together in \\(\\Theta(n^2)\\) time), then we could find the result of multiplying two arbitrary \\(n \\times n\\) matrices in \\(\\Theta(n^2)\\) time by taking advantage of the following observation: \\[\\begin{split}\\left[ \\begin{array}{cc} 0&A\\\\ A^{\\rm T}&0 \\end{array} \\right] \\left[ \\begin{array}{cc} 0&B^{\\rm T}\\\\ B&0 \\end{array} \\right] = \\left[ \\begin{array}{cc} AB&0\\\\ 0&A^{\\rm T}B^{\\rm T} \\end{array} \\right].\\end{split}\\] In the above formula, \\(AB\\) is the result of multiplying matrices \\(A\\) and \\(B\\) together. The following slideshow illustrates this reduction process. Settings Saving... Server Error Resubmit 1 The transpose operation takes position \\(ij\\) of the original matrix and places it in position \\(ji\\) of the transpose matrix. This can easily be done in \\(n^2\\) time for an \\(n \\times n\\) matrix. 7.2.5. Bounds Theorems¶ We will use the following notation: \\(\\leq_{O(g(n))}\\) means that a reduction can be done with transformations that cost \\(O(g(n))\\). Lower Bound Theorem}: If \\(P_1 \\leq_{O(g(n))} P_2\\), then there is a lower bound of \\(\\Omega(h(n))\\) on the time complexity of \\(P_1\\), and \\(g(n) = o(h(n))\\), then there is a lower bound of \\(\\Omega(h(n))\\) on \\(P_2\\). (Notice little-oh, not big-Oh.) Example: SORTING \\(\\leq_{O(n)}\\) PAIRING, because \\(g(n) = n\\), \\(h(n) = n \\log n\\), and \\(g(n) = o(h(n))\\). The Lower Bound Theorem gives us an \\(\\Omega(n \\log n)\\) lower bound on PAIRING. This also goes the other way. Upper Bound Theorem: If \\(P_2\\) has time complexity \\(O(h(n))\\) and \\(P_1 \\leq_{O(g(n))} P_2\\), then \\(P_1\\) has time complexity \\(O(g(n) + h(n))\\). So, given good transformations, both problems take at least \\(\\Omega(P_1)\\) and at most \\(O(P_2)\\). 7.2.6. The Cost of Making a Simple Polygon¶ SIMPLE POLYGON: Given a set of \\(n\\) points in the plane, find a simple polygon with those points as vertices. (Here, “simple” means that no lines cross.) We will show that SORTING \\(\\leq_{O(n)}\\) SIMPLE POLYGON. We start with an instance of SORTING: \\(\\{x_1, x_2, \\cdots, x_n\\}\\). In linear time, find \\(M = \\max|x_i|\\). Let \\(C\\) be a circle centered at the origin, of radius \\(M\\). We will generate an instance of SIMPLE POLYGON by replacing each value in the array to be sorted with a corresponding point defined as \\[\\{(x_1, \\sqrt{M^2 - x_i^2}), \\cdots, (x_n, \\sqrt{M^2 - x_n^2})\\}.\\] Figure 7.2.3: Input to SORTING: the values 5, -3, 2, 0, 10. When converted to points, they fall on a circle as shown. It is an important fact that all of these points fall on \\(C\\). Furthermore, when we find a simple polygon, the points all fall along the circle in sort order. This is because the only simple polygon having all of its points on \\(C\\) as vertices is the convex one. Therefore, by the Lower Bound Theorem, SIMPLE POLYGON is in \\(\\Omega(n \\log n)\\). Contact Us || Privacy | | License « 7.1. Limits to Computing :: Contents :: 7.3. NP-Completeness » Contact Us | | Report a bug © Copyright 2011-2023 by OpenDSA Project Contributors and distributed under an MIT license. Last updated on Aug 13, 2024. Created using Sphinx 4.4.0. Summary*: Operating system*: Windows Mac OS Linux iOS Android Other Browser*: Chrome Safari Internet Explorer Opera Other Description*: Attach a screenshot (optional):"
            },
            {
                "role": "user",
                "content": "Can you summarize the key topics covered in this lecture?"
            },
            {
                "role": "assistant",
                "content": "This lecture covers foundational algorithmic concepts including problem formulation, correctness proof using induction, efficiency analysis using asymptotic notation, data structures (arrays, linked lists, BSTs, heaps), and sorting methods (selection sort, merge sort, counting sort). Additionally, it introduces the Word-RAM computational model, providing a theoretical framework for measuring algorithm efficiency."
            }
        ]
    },
    {
        "system": "Hello! I am Kiran, an AI assistant from Tampere University, dedicated to providing you with accurate and insightful assistance. My goal is to help you understand concepts, summarize lectures, and answer questions effectively.",
        "conversations": [
            {
                "role": "host",
                "content": "Search — BM40A1500 Data Structures and Algorithms Username Password Forgot your password? Register Username: Password Confirm Password Email: BM40A1500 Data Structures and Algorithms Contents Search Please activate JavaScript to enable the search functionality. Searching for multiple words only shows matches that contain all words. Contact Us || Privacy | | License Contents Contact Us | | Report a bug © Copyright 2011-2023 by OpenDSA Project Contributors and distributed under an MIT license. Last updated on Aug 13, 2024. Created using Sphinx 4.4.0. Summary*: Operating system*: Windows Mac OS Linux iOS Android Other Browser*: Chrome Safari Internet Explorer Opera Other Description*: Attach a screenshot (optional):"
            },
            {
                "role": "user",
                "content": "Can you summarize the key topics covered in this lecture?"
            },
            {
                "role": "assistant",
                "content": "This lecture covers foundational algorithmic concepts including problem formulation, correctness proof using induction, efficiency analysis using asymptotic notation, data structures (arrays, linked lists, BSTs, heaps), and sorting methods (selection sort, merge sort, counting sort). Additionally, it introduces the Word-RAM computational model, providing a theoretical framework for measuring algorithm efficiency."
            }
        ]
    },
    {
        "system": "Hello! I am Kiran, an AI assistant from Tampere University, dedicated to providing you with accurate and insightful assistance. My goal is to help you understand concepts, summarize lectures, and answer questions effectively.",
        "conversations": [
            {
                "role": "host",
                "content": "2.5. Stacks — BM40A1500 Data Structures and Algorithms Username Password Forgot your password? Register Username: Password Confirm Password Email: BM40A1500 Data Structures and Algorithms Chapter 2 Linear Data Structures Show Source | | About « 2.4. Comparison of List Implementations :: Contents :: 2.6. Linked Stacks » 2.5. Stacks¶ 2.5.1. Stack Terminology and Implementation¶ The stack is a list-like structure in which elements may be inserted or removed from only one end. While this restriction makes stacks less flexible than lists, it also makes stacks both efficient (for those operations they can do) and easy to implement. Many applications require only the limited form of insert and remove operations that stacks provide. In such cases, it is more efficient to use the simpler stack data structure rather than the generic list. For example, the freelist is really a stack. Despite their restrictions, stacks have many uses. Thus, a special vocabulary for stacks has developed. Accountants used stacks long before the invention of the computer. They called the stack a “LIFO” list, which stands for “Last-In, First-Out.” Note that one implication of the LIFO policy is that stacks remove elements in reverse order of their arrival. The accessible element of the stack is called the top element. Elements are not said to be inserted, they are pushed onto the stack. When removed, an element is said to be popped from the stack. Here is a simple stack ADT. As with lists, there are many variations on stack implementation. The two approaches presented here are the array-based stack and the linked stack, which are analogous to array-based and linked lists, respectively. 2.5.2. Array-Based Stacks¶ The array-based stack implementation is essentially a simplified version of the array-based list. The only important design decision to be made is which end of the array should represent the top of the stack. Settings Saving... Server Error Resubmit Settings Saving... Server Error Resubmit 2.5.3. Pop¶ Settings Saving... Server Error Resubmit Contact Us || Privacy | | License « 2.4. Comparison of List Implementations :: Contents :: 2.6. Linked Stacks » Contact Us | | Report a bug © Copyright 2011-2023 by OpenDSA Project Contributors and distributed under an MIT license. Last updated on Aug 13, 2024. Created using Sphinx 4.4.0. Summary*: Operating system*: Windows Mac OS Linux iOS Android Other Browser*: Chrome Safari Internet Explorer Opera Other Description*: Attach a screenshot (optional):"
            },
            {
                "role": "user",
                "content": "Can you summarize the key topics covered in this lecture?"
            },
            {
                "role": "assistant",
                "content": "This lecture covers foundational algorithmic concepts including problem formulation, correctness proof using induction, efficiency analysis using asymptotic notation, data structures (arrays, linked lists, BSTs, heaps), and sorting methods (selection sort, merge sort, counting sort). Additionally, it introduces the Word-RAM computational model, providing a theoretical framework for measuring algorithm efficiency."
            }
        ]
    },
    {
        "system": "Hello! I am Kiran, an AI assistant from Tampere University, dedicated to providing you with accurate and insightful assistance. My goal is to help you understand concepts, summarize lectures, and answer questions effectively.",
        "conversations": [
            {
                "role": "host",
                "content": "2.6. Linked Stacks — BM40A1500 Data Structures and Algorithms Username Password Forgot your password? Register Username: Password Confirm Password Email: BM40A1500 Data Structures and Algorithms Chapter 2 Linear Data Structures Show Source | | About « 2.5. Stacks :: Contents :: 2.7. Queues » 2.6. Linked Stacks¶ 2.6.1. Linked Stack Implementation¶ The linked stack implementation is quite simple. Elements are inserted and removed only from the head of the list. A header node is not used because no special-case code is required for lists of zero or one elements. Here is a visual representation for the linked stack. 2.6.2. Linked Stack Push¶ Settings Saving... Server Error Resubmit 2.6.3. Linked Stack Pop¶ Settings Saving... Server Error Resubmit 2.6.4. Comparison of Array-Based and Linked Stacks¶ All operations for the array-based and linked stack implementations take constant time, so from a time efficiency perspective, neither has a significant advantage. Another basis for comparison is the total space required. The analysis is similar to that done for list implementations. The array-based stack must declare a fixed-size array initially, and some of that space is wasted whenever the stack is not full. The linked stack can shrink and grow but requires the overhead of a link field for every element. When implementing multiple stacks, sometimes you can take advantage of the one-way growth of the array-based stack by using a single array to store two stacks. One stack grows inward from each end as illustrated by the figure below, hopefully leading to less wasted space. However, this only works well when the space requirements of the two stacks are inversely correlated. In other words, ideally when one stack grows, the other will shrink. This is particularly effective when elements are taken from one stack and given to the other. If instead both stacks grow at the same time, then the free space in the middle of the array will be exhausted quickly. Contact Us || Privacy | | License « 2.5. Stacks :: Contents :: 2.7. Queues » Contact Us | | Report a bug © Copyright 2011-2023 by OpenDSA Project Contributors and distributed under an MIT license. Last updated on Aug 13, 2024. Created using Sphinx 4.4.0. Summary*: Operating system*: Windows Mac OS Linux iOS Android Other Browser*: Chrome Safari Internet Explorer Opera Other Description*: Attach a screenshot (optional):"
            },
            {
                "role": "user",
                "content": "Can you summarize the key topics covered in this lecture?"
            },
            {
                "role": "assistant",
                "content": "This lecture covers foundational algorithmic concepts including problem formulation, correctness proof using induction, efficiency analysis using asymptotic notation, data structures (arrays, linked lists, BSTs, heaps), and sorting methods (selection sort, merge sort, counting sort). Additionally, it introduces the Word-RAM computational model, providing a theoretical framework for measuring algorithm efficiency."
            }
        ]
    },
    {
        "system": "Hello! I am Kiran, an AI assistant from Tampere University, dedicated to providing you with accurate and insightful assistance. My goal is to help you understand concepts, summarize lectures, and answer questions effectively.",
        "conversations": [
            {
                "role": "host",
                "content": "4.9. 2-3 Trees — BM40A1500 Data Structures and Algorithms Username Password Forgot your password? Register Username: Password Confirm Password Email: BM40A1500 Data Structures and Algorithms Chapter 4 Recursion and Binary Trees Show Source | | About « 4.8. Balanced Trees :: Contents :: 4.10. The AVL Tree » 4.9. 2-3 Trees¶ This section presents a data structure called the 2-3 tree. The 2-3 tree is not a binary tree, but instead its shape obeys the following definition: A node contains one or two keys. Every internal node has either two children (if it contains one key) or three children (if it contains two keys). Hence the name. All leaves are at the same level in the tree, so the tree is always height balanced. In addition to these shape properties, the 2-3 tree has a search tree property analogous to that of a BST. For every node, the values of all descendants in the left subtree are less than the value of the first key, while values in the center subtree are greater than or equal to the value of the first key. If there is a right subtree (equivalently, if the node stores two keys), then the values of all descendants in the center subtree are less than the value of the second key, while values in the right subtree are greater than or equal to the value of the second key. To maintain these shape and search properties requires that special action be taken when nodes are inserted and deleted. The 2-3 tree has the advantage over the BST in that the 2-3 tree can be kept height balanced at relatively low cost. Here is an example 2-3 tree. Figure 4.9.1: An example of a 2-3 tree. Nodes are shown as rectangular boxes with two key fields. (These nodes actually would contain complete records or pointers to complete records, but the figures will show only the keys.) Internal nodes with only two children have an empty right key field. Leaf nodes might contain either one or two keys. From the defining rules for 2-3 trees we can derive relationships between the number of nodes in the tree and the depth of the tree. A 2-3 tree of height \\(k\\) has at least \\(2^{k-1}\\) leaves, because if every internal node has two children it degenerates to the shape of a complete binary tree. A 2-3 tree of height \\(k\\) has at most \\(3^{k-1}\\) leaves, because each internal node can have at most three children. Searching for a value in a 2-3 tree is similar to searching in a BST. Search begins at the root. If the root does not contain the search key \\(K\\), then the search progresses to the only subtree that can possibly contain \\(K\\). The value(s) stored in the root node determine which is the correct subtree. For example, if searching for the value 30 in the tree of Figure 4.9.1, we begin with the root node. Because 30 is between 18 and 33, it can only be in the middle subtree. Searching the middle child of the root node yields the desired record. If searching for 15, then the first step is again to search the root node. Because 15 is less than 18, the first (left) branch is taken. At the next level, we take the second branch to the leaf node containing 15. If the search key were 16, then upon encountering the leaf containing 15 we would find that the search key is not in the tree. Insertion into a 2-3 tree is similar to insertion into a BST to the extent that the new record is placed in the appropriate leaf node. Unlike BST insertion, a new child is not created to hold the record being inserted, that is, the 2-3 tree does not grow downward. The first step is to find the leaf node that would contain the record if it were in the tree. If this leaf node contains only one value, then the new record can be added to that node with no further modification to the tree, as illustrated in the following visualization. Settings Saving... Server Error Resubmit If we insert the new record into a leaf node \\(L\\) that already contains two records, then more space must be created. Consider the two records of node \\(L\\) and the record to be inserted without further concern for which two were already in \\(L\\) and which is the new record. The first step is to split \\(L\\) into two nodes. Thus, a new node—call it \\(L'\\)—must be created from free store. \\(L\\) receives the record with the least of the three key values. \\(L'\\) receives the greatest of the three. The record with the middle of the three key value is passed up to the parent node along with a pointer to \\(L'\\). This is called a promotion. The promoted key is then inserted into the parent. If the parent currently contains only one record (and thus has only two children), then the promoted record and the pointer to \\(L'\\) are simply added to the parent node. If the parent is full, then the split-and-promote process is repeated. Here is an example of a a simple promotion. Settings Saving... Server Error Resubmit Here is an illustration for what happens when promotions require the root to split, adding a new level to the tree. Note that all leaf nodes continue to have equal depth. Settings Saving... Server Error Resubmit When deleting a record from the 2-3 tree, there are three cases to consider. The simplest occurs when the record is to be removed from a leaf node containing two records. In this case, the record is simply removed, and no other nodes are affected. The second case occurs when the only record in a leaf node is to be removed. The third case occurs when a record is to be removed from an internal node. In both the second and the third cases, the deleted record is replaced with another that can take its place while maintaining the correct order, similar to removing a node from a BST. If the tree is sparse enough, there is no such record available that will allow all nodes to still maintain at least one record. In this situation, sibling nodes are merged together. The delete operation for the 2-3 tree is excessively complex and will not be described further. Instead, a complete discussion of deletion will be postponed until the next section, where it can be generalized for a particular variant of the B-tree. The 2-3 tree insert and delete routines do not add new nodes at the bottom of the tree. Instead they cause leaf nodes to split or merge, possibly causing a ripple effect moving up the tree to the root. If necessary the root will split, causing a new root node to be created and making the tree one level deeper. On deletion, if the last two children of the root merge, then the root node is removed and the tree will lose a level. In either case, all leaf nodes are always at the same level. When all leaf nodes are at the same level, we say that a tree is height balanced. Because the 2-3 tree is height balanced, and every internal node has at least two children, we know that the maximum depth of the tree is \\(\\log n\\). Thus, all 2-3 tree insert, find, and delete operations require \\(\\Theta(\\log n)\\) time. Click here for another visualization that will let you construct and interact with a 2-3 tree. Actually, this visualization is for a data structure that is more general than just a 2-3 tree. To see how a 2-3 would behave, be sure to use the “Max Degree = 3” setting. This visualization was written by David Galles of the University of San Francisco as part of his Data Structure Visualizations package. Contact Us || Privacy | | License « 4.8. Balanced Trees :: Contents :: 4.10. The AVL Tree » Contact Us | | Report a bug © Copyright 2011-2023 by OpenDSA Project Contributors and distributed under an MIT license. Last updated on Aug 13, 2024. Created using Sphinx 4.4.0. Summary*: Operating system*: Windows Mac OS Linux iOS Android Other Browser*: Chrome Safari Internet Explorer Opera Other Description*: Attach a screenshot (optional):"
            },
            {
                "role": "user",
                "content": "Can you summarize the key topics covered in this lecture?"
            },
            {
                "role": "assistant",
                "content": "This lecture covers foundational algorithmic concepts including problem formulation, correctness proof using induction, efficiency analysis using asymptotic notation, data structures (arrays, linked lists, BSTs, heaps), and sorting methods (selection sort, merge sort, counting sort). Additionally, it introduces the Word-RAM computational model, providing a theoretical framework for measuring algorithm efficiency."
            }
        ]
    },
    {
        "system": "Hello! I am Kiran, an AI assistant from Tampere University, dedicated to providing you with accurate and insightful assistance. My goal is to help you understand concepts, summarize lectures, and answer questions effectively.",
        "conversations": [
            {
                "role": "host",
                "content": "Algorithm efficiency: introduction 1. What is algorithm efficiency? 2. Pseudocode analysis 1. What is algorithm efficiency? Two quantities of interest: • How much time does the algorithm require? • How much computer memory does the algorithm require? Focus on time. Q: For some given computational task what do we really want? A: We want to know beforehand how much time the task will take. Software/hardware factors which will aﬀect the time: • a particular algorithm for doing the task • a particular programming language • a particular implementation of the algorithm in the programming language with error checks, data types, objects, etc. • particular set of input data • a particular compiler with all compiler options • a particular computer with a particular CPU, cache memory, clock frequency, etc. • a particular environment: what else is computer doing? Too difficult to predict beforehand. We can measure the computational time afterwards, once all factors have been fixed. Does this help? Not very often. Suppose one or more factors change. Q: What can we do? A: Analyze pseudocode. Pseudocode ignores most of the above factors. What factors are left? • a particular algorithm • a particular set of input data 2. Pseudocode analysis Q: What is pseudocode analysis? A: counting the number of ’simple operations’ and seeing how this depends on the ’size’ of the input data Q: What is a ’simple operation’? A: Any operation that does not depend on size of input data. Simple operations: • arithmetic operations: +, -, *, / • if-statement, else-statement • one iteration of for or while or for-each • variable assignment • accessing a single item in memory • a single call to a procedure (NOT the execution of the procedure itself) RAM-model of computer: all simple operations take the same time Example What is the ’size’ of input data? • if input is array, then usually length of array • if input polynomial, then usually degree of polynomial • if input is a number, then often the number of bits needed to represent number • if input is binary tree, then sometimes the height of the tree, sometimes the number of nodes (vertices) Goal of pseudocode analysis is f(n) f(n) = total count of simple operations for input data of size n Asymptotic analysis: only consider those parts f(n) that grows the fastest as n increases Example j most number of times line 8 executed Count of times line 8 executed in INSERTSORT Q: Can pseudocode analysis help with our original task of predicting how long some com- putation task will take? A: No. Q: What can we do with pseudocode analysis? A: Compare alternative algorithms. Tämä teos on lisensoitu Creative Commons Nimeä-EiKaupallinen- EiMuutoksia 4.0 Kansainvälinen -lisenssillä. Tarkastele lisenssiä osoitteessa http://creativecommons.org/licenses/by-nc-nd/4.0/. tekijä: Frank Cameron This work is licensed under the Creative Commons Attribution- NonCommercial-NoDerivatives 4.0 International License. To view a copy of this license, visit http://creativecommons.org/licenses/by-nc-nd/4.0/. made by Frank Cameron"
            },
            {
                "role": "user",
                "content": "Can you summarize the key topics covered in this lecture?"
            },
            {
                "role": "assistant",
                "content": "This lecture covers foundational algorithmic concepts including problem formulation, correctness proof using induction, efficiency analysis using asymptotic notation, data structures (arrays, linked lists, BSTs, heaps), and sorting methods (selection sort, merge sort, counting sort). Additionally, it introduces the Word-RAM computational model, providing a theoretical framework for measuring algorithm efficiency."
            }
        ]
    },
    {
        "system": "Hello! I am Kiran, an AI assistant from Tampere University, dedicated to providing you with accurate and insightful assistance. My goal is to help you understand concepts, summarize lectures, and answer questions effectively.",
        "conversations": [
            {
                "role": "host",
                "content": "Algorithms, pseudocode, programming code; overview 1. Algorithm 2. Three presentation levels 3. Pseudocode 1. Algorithm At start: some problem (questions) we want to solve, given some material/information At end: solution (answers) to the problem Definition of algorithm (Levitin A. The design and analysis of algorithms) An algorithm is a sequence of unambiguous instructions for solving a problem i.e. for obtaining a required output for any legitimate input in a finite amount of time. Example Start: you are inside with socks on; a (suitable) pair of your shoes exist inside End: both shoes on correct feet and you are ready to go outside An algorithm for putting on shoes: 1. Find the left shoe. 2. Find the right shoe. 3. Put the left shoe on your left foot. 4. Put the right shoe on your right foot. 5. If your left shoe has laces and the laces need to be tied, then tie the laces. 6. If your right shoe has laces and the laces need to be tied, then tie the laces. 7. If your left shoe has straps and the straps need to be fasten, then fasten them. 8. If your right shoe has straps and the straps need to be fasten, then fasten them. 9. Your shoes are on. In computer science: Algorithm starting point: problem with well speciﬁed starting (input) data X Algorithm ending point: well speciﬁed output data Y that solves problem Algorithm: how do we compute Y from X? Unambiguous instructions: • each instruction is given in suﬃcient detail so that the device performing the algorithm cannot misunderstand it • each instruction has only one interpretation • the order of the instructions has only one interpretation, which cannot be misunderstood • the conditions for terminating the algorithm cannot be misunderstood (algorithm must terminate!) Legitimate input • what input is acceptable/unacceptable must be well specifed • what form/format the input can be given in must be well specifed Finite amount of time - algorithm must terminate! - conditions under which algorithm terminates must be well specified 2. Three presentation levels Three levels of presentation for some algorithm: • description level: person to person • pseudocode level: technical person (programmer) to technical person (programmer) • program code level: programmer/computer to computer 0111001011000101 description pseudocode programming language low on technicalities high on technicalities or Pseudocode level • not a programming language ready for compiling and/or execution • parameters and variables used and deﬁned • use typical control structures and blocks, e.g. if-then-else-end, for-end, while-end, for each-end • no error handling, checking on inputs, modularity, etc. • avoid program language speciﬁcs e.g. ++ for incrementaion of integer counter Description level • describe what each step/stage of algorithm does • can be mixture of ordinary language, math, very simple code Progam code level • some recognized language C++, Java, Python, Matlab, etc. • intended for compilation/execution • syntax requirements • other speciﬁcation must be met: error handling, checking validity of input, etc. Example 3. Pseudocode What we can ignore: • error handling • variable types (often) • ﬁle I/O, formatting • syntax (to a certain extent) • hardware • programming language What we can focus on: • algorithm computations/operations • essential variables/parameters Suppose Jussi knows Finnish, English, Java and python. Suppose Mio knows Japanese, Korean, C++ and C#. How can they communicate efficiently an algorithm? Pseudocode speciﬁcations • indentation is used to indicate blocks • control-blocks similar to C++: if-then-else-end, for-end, while-end, • control block used to iterate through elements in a set/list for each-end • comments within “/* */” • assignment made with single equals “=”, testing made with double equals “==” • variables are local to a given procedure (function) • element in i’th location in array L is L[i] • elements in locations i, i + 1, . . . j in array L are L[i..j] • an object contains compound data; an object has attributes, e.g. person : object name person.ﬁrstname : one object attribute person.lastname : another object attribute • return transfers control back to calling procedure immediately • NIL is a reference (pointer) to nothing • use 8to present a free-form instruction, e.g. 8check that all numbers in array L are even These specifications are not universal! Tämä teos on lisensoitu Creative Commons Nimeä-EiKaupallinen- EiMuutoksia 4.0 Kansainvälinen -lisenssillä. Tarkastele lisenssiä osoitteessa http://creativecommons.org/licenses/by-nc-nd/4.0/. tekijä: Frank Cameron This work is licensed under the Creative Commons Attribution- NonCommercial-NoDerivatives 4.0 International License. To view a copy of this license, visit http://creativecommons.org/licenses/by-nc-nd/4.0/. made by Frank Cameron"
            },
            {
                "role": "user",
                "content": "Can you summarize the key topics covered in this lecture?"
            },
            {
                "role": "assistant",
                "content": "This lecture covers foundational algorithmic concepts including problem formulation, correctness proof using induction, efficiency analysis using asymptotic notation, data structures (arrays, linked lists, BSTs, heaps), and sorting methods (selection sort, merge sort, counting sort). Additionally, it introduces the Word-RAM computational model, providing a theoretical framework for measuring algorithm efficiency."
            }
        ]
    },
    {
        "system": "Hello! I am Kiran, an AI assistant from Tampere University, dedicated to providing you with accurate and insightful assistance. My goal is to help you understand concepts, summarize lectures, and answer questions effectively.",
        "conversations": [
            {
                "role": "host",
                "content": "The A* algorithm and the shortest path problem 1. Background and example 2. A* algortihm 3. Remarks 1. Background and example Example Shortest path from s = F to t = H? A modified Dijkstra's algorithm can be used to solve the single-pair shortest path problem. Example (cont'd) i = order in which node handled x.d = length of shortest path from s = F to x = edge of current shortest path tree Shortest path from s = F to t = H using DIJKSTAPAIR. Remarks 2. A* algortihm Note: g(s,x) is the same as x.d in Dijkstra Example (contd) For graph shown, using the coordinates and above Euclidean distance: Example (contd) i = order in which node handled x.e = lower bound estimate of length of shortest path from s = F to t = H passing through x = edge of current shortest path tree Shortest path from s = F to t = H using ASTAR continue ... 3. Remarks Tämä teos on lisensoitu Creative Commons Nimeä-EiKaupallinen- EiMuutoksia 4.0 Kansainvälinen -lisenssillä. Tarkastele lisenssiä osoitteessa http://creativecommons.org/licenses/by-nc-nd/4.0/. tekijä: Frank Cameron This work is licensed under the Creative Commons Attribution- NonCommercial-NoDerivatives 4.0 International License. To view a copy of this license, visit http://creativecommons.org/licenses/by-nc-nd/4.0/. made by Frank Cameron"
            },
            {
                "role": "user",
                "content": "Can you summarize the key topics covered in this lecture?"
            },
            {
                "role": "assistant",
                "content": "This lecture covers foundational algorithmic concepts including problem formulation, correctness proof using induction, efficiency analysis using asymptotic notation, data structures (arrays, linked lists, BSTs, heaps), and sorting methods (selection sort, merge sort, counting sort). Additionally, it introduces the Word-RAM computational model, providing a theoretical framework for measuring algorithm efficiency."
            }
        ]
    },
    {
        "system": "Hello! I am Kiran, an AI assistant from Tampere University, dedicated to providing you with accurate and insightful assistance. My goal is to help you understand concepts, summarize lectures, and answer questions effectively.",
        "conversations": [
            {
                "role": "host",
                "content": "Verifying asymptotic performance COMP.CS.300 Data structures and algorithms 1 Matti Rintala (matti.rintala@tuni.fi) Verifying asymptotic performance •Pseudocode/designing – Estimate of asymptotic performance •Implementation/coding – As efficient implementation as possible – Hopefully has the asympotic performance that was estimated •Testing – Does the performance of the implementation correspond to the design?"
            },
            {
                "role": "user",
                "content": "Can you summarize the key topics covered in this lecture?"
            },
            {
                "role": "assistant",
                "content": "This lecture covers foundational algorithmic concepts including problem formulation, correctness proof using induction, efficiency analysis using asymptotic notation, data structures (arrays, linked lists, BSTs, heaps), and sorting methods (selection sort, merge sort, counting sort). Additionally, it introduces the Word-RAM computational model, providing a theoretical framework for measuring algorithm efficiency."
            }
        ]
    },
    {
        "system": "Hello! I am Kiran, an AI assistant from Tampere University, dedicated to providing you with accurate and insightful assistance. My goal is to help you understand concepts, summarize lectures, and answer questions effectively.",
        "conversations": [
            {
                "role": "host",
                "content": "Algorithm efficiency of Mergesort and Quicksort 1. Mergesort 2. Quicksort 1. Mergesort Pseudocode from another lecture: Mergesort: a divide and conquer algorithm Assumption: input array to MERGE A[1..n], where n = 2x, for integer x, x  2. Observations: - simple operations in for-loop in lines 5-7: x - simple operations in lines 8-21: x Assumption: input array to MERGESORT: A[1..2x], L = 1, R = 2x. Q: How often is MERGE executed in MERGESORT and with what arguments? On each recurssion level all elements of A[1..2x] are merged. Q: How many recursion levels in MERGESORT? A: x. Sum all operations in MERGE: Using n = 2x or log2 n = x: Q: How about when n 2x? A: Same efficiency. 2. Quicksort Pseudocode from another lecture: Quicksort: a divide and conquer algorithm Assumption: input array is A[1..n], where n = 2x - 1, for integer x, x  2. Q: What is runtime efficiency of PARTITION when input is A[1..(2x - 1)], L = 1, R = (2x - 1). A: Runtime efficiency is x - 1x Assumption: input to QUICKSORT is A[1..(2x - 1)], L = 1, R = (2x - 1). Q: How often is PARTITION executed in QUICKSORT and with what arguments? A: Depends on pivot location (value of k). Assumption: pivot is always in middle k = L + R)/2 This assumption corresponds to best case (lower bound on runtime efficiency). NOTE: arrays in last row are actually not partitioned, but they are used in a call to QUICKSORT. Q: How many recursion levels when pivot is always in middle? A: x Sums of all array sizes in previous table: Worst case for Quicksort in lecture: Algorithm design techniques: randomization Efficiency result for both MERGESORT and QUICKSORT: Are MERGESORT and QUICKSORT equally efficient? - yes (theoretically) when considering best case for QUICKSORT - in practice QUICKSORT usually better, but it requires 'tuning' Tämä teos on lisensoitu Creative Commons Nimeä-EiKaupallinen- EiMuutoksia 4.0 Kansainvälinen -lisenssillä. Tarkastele lisenssiä osoitteessa http://creativecommons.org/licenses/by-nc-nd/4.0/. tekijä: Frank Cameron This work is licensed under the Creative Commons Attribution- NonCommercial-NoDerivatives 4.0 International License. To view a copy of this license, visit http://creativecommons.org/licenses/by-nc-nd/4.0/. made by Frank Cameron"
            },
            {
                "role": "user",
                "content": "Can you summarize the key topics covered in this lecture?"
            },
            {
                "role": "assistant",
                "content": "This lecture covers foundational algorithmic concepts including problem formulation, correctness proof using induction, efficiency analysis using asymptotic notation, data structures (arrays, linked lists, BSTs, heaps), and sorting methods (selection sort, merge sort, counting sort). Additionally, it introduces the Word-RAM computational model, providing a theoretical framework for measuring algorithm efficiency."
            }
        ]
    },
    {
        "system": "Hello! I am Kiran, an AI assistant from Tampere University, dedicated to providing you with accurate and insightful assistance. My goal is to help you understand concepts, summarize lectures, and answer questions effectively.",
        "conversations": [
            {
                "role": "host",
                "content": "Algorithm efficiency and asymptotic analysis: Big-Oh, Big-Omega, Big-Theta 1. Introduction to asymptotic analysis 2. Big-Oh 3. Big-Omega and Big-Theta 4. Analysis of insertion sort 1. Introduction to asymptotic analysis Suppose algorithm X exists and X has input data whose size is n. Q: What is the goal of the asymptotic analysis of X produce? A: To describe how the running time of X depends on n. Remark: ’running time’ is traditional and misleading since • asymptotic analysis used mostly for analyzing pseudocode • pseudocode cannot be ’run’ executed • no ’times’ can be measured from executing pseudocode We will still use the traditional expression of 'running time'. Q: Why ’asymptotic’? A: We are interested in the behaviour of X as n becomes ’large’. Let running time function f(n) be count of simple operations needed when X computes from start to finish. More precise first question: Q: What is the goal of the asymptotic analysis of X produce? A: To describe how f(n) depends on n when n gets 'large'. Example When is n large? Asymptotic: we can ignore all terms in f(n) except the fastest growing Example function f(n) fastest growing term function of interest for asymptotic analysis For some algorithm we might have Remarks on asymptotic analysis • other names: run time analysis, time-complexity analysis, growth rate analysis • for two alternative algorithms X and Y, if fX(n) grows slower than fY(n), then X is ’better’ • we want lower and upper bounds on f(n) 2. Big-Oh Definition Plot of big-oh Example (contd) function f(n) fastest growing term O(g(n)) for f(n) Example (contd) Example Consider the following functions: O(g(n)) functions that belong to O(g(n)) functions that do not belong to O(g(n)) Q: In previous example Are they all correct? Which is the best in other words provides the most information? A: They are all correct. The most information is provided by Principle: smallest upper bound conveys the most information Typical O()-classes encountered in algorithm analysis Big-Oh is the most frequently quoted asymptotic result for diﬀerent routines/procedures: • C++ STL https://alyssaq.github.io/stl-complexities/ https://github.com/gibsjose/cpp-cheat-sheet/blob/master/Data%20Structures%20and% 20Algorithms.md • Java Collections https://gist.github.com/psayre23/c30a821239f4818b0709 • python https://wiki.python.org/moin/TimeComplexity 3. Big-Omega and Big-Theta Definition Big-Omega Plot of big-Omega significance. Example (contd) Q: In the previous example we could have written Are they all correct? Which is the best in other words provides the most information? A: They are all correct. The most information is provided by Principle: largest lower bound conveys the most information Order of big-Omega classes: Big-Theta Definition Example (contd) Plot of big-Theta significance. Interpretations for big-oh, big-omega when f(n) is running time function of some algorithm. 4. Analysis of insertion sort We will form two running time functions for INSERTSORT by counting simple operations. Simple operations: • arithmetic operations: +, -, *, / • if-statement, else-statement • one iteration of for or while or for-each • variable assignment • accessing a single item in memory • a single call to a procedure (NOT the execution of the procedure itself) Assumption: each simple operation takes the same amount of time In forming the counts we ignore nature of input data. line lower bound on count upper bound on count Tämä teos on lisensoitu Creative Commons Nimeä-EiKaupallinen- EiMuutoksia 4.0 Kansainvälinen -lisenssillä. Tarkastele lisenssiä osoitteessa http://creativecommons.org/licenses/by-nc-nd/4.0/. tekijä: Frank Cameron This work is licensed under the Creative Commons Attribution- NonCommercial-NoDerivatives 4.0 International License. To view a copy of this license, visit http://creativecommons.org/licenses/by-nc-nd/4.0/. made by Frank Cameron"
            },
            {
                "role": "user",
                "content": "Can you summarize the key topics covered in this lecture?"
            },
            {
                "role": "assistant",
                "content": "This lecture covers foundational algorithmic concepts including problem formulation, correctness proof using induction, efficiency analysis using asymptotic notation, data structures (arrays, linked lists, BSTs, heaps), and sorting methods (selection sort, merge sort, counting sort). Additionally, it introduces the Word-RAM computational model, providing a theoretical framework for measuring algorithm efficiency."
            }
        ]
    },
    {
        "system": "Hello! I am Kiran, an AI assistant from Tampere University, dedicated to providing you with accurate and insightful assistance. My goal is to help you understand concepts, summarize lectures, and answer questions effectively.",
        "conversations": [
            {
                "role": "host",
                "content": "Balanced binary search trees 1. Background and motivation 2. Rotations 3. AVL trees 4. Red-black trees 1. Background and motivation Two numbers associated with any binary tree: Always true: Example a perfect binary tree with h = 2 and n = 7 Example best case: h is as small as possible worst case: h is as large as possible Conclusions - to avoid O(n) runtime efficiency, we want BSTs to be balanced - a mechanism is needed to help balance an unbalanced BST 2. Rotations Left rotation BST property preserved: Consider heights Example Q: Why would we want to perform rotations? A: To decrease the height of the BST and hence improve performance. Height is not reduced after rotation. Height was reduced after rotation. Right rotation Right-left double rotation Left-right double rotation ... left to viewer 3. AVL trees When computing an AVL-tree node attributes: Example Node.diff in red Typical code when forming AVL trees: Detailed example from Wikipedia: By Bruno Schalch - Own work, CC BY-SA 4.0, https://commons.wikimedia.org/w/index.php? curid=64250599 Conclusion: search (and other operations) in AVL are O(log2(n)) 4. Red-black trees Example Is the BST a red-black tree? No. It does not have property 3. Yes. Yes. No. It does not have property 2. For each node x, we get the following from the properties: Example Add a node whose key is 6 to the following red-black tree Stage 1 search for correct location and try to add red node Stage 2: recoloring Stage 3: rotation Stage 4: recoloring root For a red-black BST: Conclusion: search (and other operations) in red-black tree are O(log2(n)) Visualization of data structures: https://www.cs.usfca.edu/~galles/visualization/Algorithms.html Tämä teos on lisensoitu Creative Commons Nimeä-EiKaupallinen- EiMuutoksia 4.0 Kansainvälinen -lisenssillä. Tarkastele lisenssiä osoitteessa http://creativecommons.org/licenses/by-nc-nd/4.0/. tekijä: Frank Cameron This work is licensed under the Creative Commons Attribution- NonCommercial-NoDerivatives 4.0 International License. To view a copy of this license, visit http://creativecommons.org/licenses/by-nc-nd/4.0/. made by Frank Cameron"
            },
            {
                "role": "user",
                "content": "Can you summarize the key topics covered in this lecture?"
            },
            {
                "role": "assistant",
                "content": "This lecture covers foundational algorithmic concepts including problem formulation, correctness proof using induction, efficiency analysis using asymptotic notation, data structures (arrays, linked lists, BSTs, heaps), and sorting methods (selection sort, merge sort, counting sort). Additionally, it introduces the Word-RAM computational model, providing a theoretical framework for measuring algorithm efficiency."
            }
        ]
    },
    {
        "system": "Hello! I am Kiran, an AI assistant from Tampere University, dedicated to providing you with accurate and insightful assistance. My goal is to help you understand concepts, summarize lectures, and answer questions effectively.",
        "conversations": [
            {
                "role": "host",
                "content": "Algorithm efficiency and asymptotic analysis: best case, worst case, average case 1. Best case, worst case, average case 2. Binary search 3. Insertion sort 1. Best case, worst case, average case Suppose algorithm X exists and X has input data whose size is n. Suppose the running time function of X is f(n). Q: With regard to the input data, what is the best case for algorithm X ? A: The best case input data is that for which the asymptotic growth of f(n) is as least as small as that of any other input data. Q: With regard to the input data, what is the worst case for algorithm X ? A: The worst case input data is that for which the asymptotic growth of f(n) is as least as large as that of any other input data. For best cases and the worst cases one should always be able to provide examples. Focus on worst case (usually): - gives a guarantee on how bad f(n) can be - worst case can often occur e.g. searching for data that does not occur Average case is more difficult - there is no one average case, rather proabilities are associated with input data - requires probabilitistic analysis to obtain asymptotic results - often must make assumptions on probability distribution of input data 2. Binary search Iterative pseudocode for binary search Input data assumptions: - sorted input array A[1..n] from smallest to largest - L = 1, R = n and n > 1. Q: What is best case input data? A: Line 9 while-loop is only executed once. When possible: first iteration of while-loop results in A[mid] = key Running time function for best case: f(n) = Q: Is best case possible? A: Yes. key = -6 Big-Omega and Big-Oh for best case? Big-Theta for best case? Q: What is worst case input data? A: Line 9 while-loop is executed as many times as necessary, until L >= R. Let k be necessary number of iterations. n ____________________________________________________________________________ _ k until L >= R Q: Is worst case possible? A: Yes. e.g. key < A[1] Big-Omega and Big-Oh for worst case? Running time function for worst case: f(n) = Big-Theta for worst case? 3. Insertion sort Previously for entire procedure we obtained lower bound and upper bound on running time: Q: What is best case input data? A: Case where f(n) is fL. Q: Is best case possible? A: Yes. If input array A[1..n] is already sorted from smallest to largest. Big-Omega and Big-Oh for best case? Big-Theta for best case? Q: What is worst case input data? A: Case where f(n) is fU. Q: Is worst case possible? A: Yes. If input array A[1..n] is in reverse order from largest to smallest. Big-Omega and Big-Oh for worst case? Big-Theta for worst case? Tämä teos on lisensoitu Creative Commons Nimeä-EiKaupallinen- EiMuutoksia 4.0 Kansainvälinen -lisenssillä. Tarkastele lisenssiä osoitteessa http://creativecommons.org/licenses/by-nc-nd/4.0/. tekijä: Frank Cameron This work is licensed under the Creative Commons Attribution- NonCommercial-NoDerivatives 4.0 International License. To view a copy of this license, visit http://creativecommons.org/licenses/by-nc-nd/4.0/. made by Frank Cameron"
            },
            {
                "role": "user",
                "content": "Can you summarize the key topics covered in this lecture?"
            },
            {
                "role": "assistant",
                "content": "This lecture covers foundational algorithmic concepts including problem formulation, correctness proof using induction, efficiency analysis using asymptotic notation, data structures (arrays, linked lists, BSTs, heaps), and sorting methods (selection sort, merge sort, counting sort). Additionally, it introduces the Word-RAM computational model, providing a theoretical framework for measuring algorithm efficiency."
            }
        ]
    },
    {
        "system": "Hello! I am Kiran, an AI assistant from Tampere University, dedicated to providing you with accurate and insightful assistance. My goal is to help you understand concepts, summarize lectures, and answer questions effectively.",
        "conversations": [
            {
                "role": "host",
                "content": "Breadth first search 1. Background 2. Data structures 3. Procedure 4. Results and interpretation 1. Background At start: we have a digraph G = (V,E) and a starting node (source node) s from the digraph. Goal: we want to know all nodes that are reachable from s. One way to do this is by performing a graph search. If all other nodes are reachable, then the search is also a traversal. Reminders New: Example x adjacent to x reachable from x paths from D to C length Results from Breadth-first-search (BFS): 2. Data structures In a graph search a node is either discovered or undiscovered. Q: What do we mean when we say a node x has been discovered? A: We mean that a path from s to x has been found. In any graph search - start with all nodes undiscovered, except s - progress is made by moving along edges and discovering nodes that have been undiscovered - graph searches differ in the order in which they move along edges In BFS the status of a node is monitored by assigning it a color: - a white node is undiscovered - a gray node has been discovered, but it may have adjacent nodes that are undiscovered (white) - a black node has been discovered and all nodes adjacent to it have been discovered (either gray or black) Note: progression of a node's color: white  gray  black To perform BFS, for each node we have the following attributes: In BFS we maintain a queue of gray nodes. A queue is a one-dimensional data structure that has two ends: the head (front end) and the tail (back end). A queue is said to function on a first-in-first-out (FIFO) basis. Example tail head operation queue Start with empty queue: Q: ENQUEUE( Q, 7 ) ENQUEUE( Q, 2 ) DEQUEUE( Q ) ENQUEUE( Q, 4 ) tail head tail head Let Q be a queue. There are two basic operations: tail head tail head 3. Procedure Description of BFS: Q: How do we know that there are still nodes that are reachable from s? Pseudocode: Remarks 1. The entire graph G is given as an argument. This is intended to represent the set of vertices V and the adjacency sets x.Adj for each node. 2. Initialization is done in forEach loop of line 5. 3. At line 12, s is the only element in Q. Hence in the first iteration of the while loop, x = s. 4. In each iteration of the while loop - x is removed from Q - all nodes adjacent to x are investigated in the forEach loop of line 16 - x is eventually colored black 5. A node can only be added once to Q. Example Execute BREADTH-FIRST_SEARCH with s = D while-loop iteration item A.d B.d C.d D.d F.d H.d A. B. C. D. F. H. A.color B.color C.color D.color F.color H.color 0 Q 1 NIL NIL NIL NIL NIL NIL white white white gray white white white gray white black gray white NIL D NIL NIL D NIL 2 white black gray black gray white NIL D B NIL D NIL 3 NIL D B NIL D NIL white black gray black black white 4 gray black black black black white C D B NIL D NIL 4. Results and interpretation Final results: x A B C D F H x.d x. x.color Q: How do we know that x.d is the length of the shortest path from s to x, s, x)? A: The nodes are put into the queue Q in increasing order of the their distance. 3 1 2 0 1  C D B NIL D NIL black black black black black white From example: Q: How do we know what nodes are reachable from s? A1: If x is reachable from s then x.color is not white. A2: If x is reachable from s then x.d is not . Q: How can we construct the BF-tree? A: The root is s. For each node x, we can construct the reverse path (from x to s) using parent attribute x. Remark: The paths from s to x is the shortest length path. (There may be other paths that are as short, but none that are shorter.) Q: Can we use Breadth-first-search on an undirected graph? A: Yes. Remarks for undirected graph: - if x belongs to y.Adj, then y belongs to x.Adj - if x is reachable from s, then s is reachable from x - x.d is the distance both from s to x and from x to s x A B C D F H x. C D B NIL D NIL BF-tree from example: Tämä teos on lisensoitu Creative Commons Nimeä-EiKaupallinen- EiMuutoksia 4.0 Kansainvälinen -lisenssillä. Tarkastele lisenssiä osoitteessa http://creativecommons.org/licenses/by-nc-nd/4.0/. tekijä: Frank Cameron This work is licensed under the Creative Commons Attribution- NonCommercial-NoDerivatives 4.0 International License. To view a copy of this license, visit http://creativecommons.org/licenses/by-nc-nd/4.0/. made by Frank Cameron"
            },
            {
                "role": "user",
                "content": "Can you summarize the key topics covered in this lecture?"
            },
            {
                "role": "assistant",
                "content": "This lecture covers foundational algorithmic concepts including problem formulation, correctness proof using induction, efficiency analysis using asymptotic notation, data structures (arrays, linked lists, BSTs, heaps), and sorting methods (selection sort, merge sort, counting sort). Additionally, it introduces the Word-RAM computational model, providing a theoretical framework for measuring algorithm efficiency."
            }
        ]
    },
    {
        "system": "Hello! I am Kiran, an AI assistant from Tampere University, dedicated to providing you with accurate and insightful assistance. My goal is to help you understand concepts, summarize lectures, and answer questions effectively.",
        "conversations": [
            {
                "role": "host",
                "content": "Binary search trees 1. Background 2. Procedures 3. STL containers 1. Background In a binary search tree, each node x has a key: x.key (common interpretation: x.key is an integer) Example Is the tree a BST or not? Example Is the BST height-balanced or not? 2. Procedures Search Example B-TREE-SEARCH( , 19 ) Min Example Max B-TREE-MAX( ) Insert Example INSERT( , ) Successor Predecessor Example SUCCESSOR( ) PREDECCESSOR( ) Inorder traversal Example order of parts: left subtree, root, right subtree output list: 3, 8, 10, 14, 15, 16, 17, 18, 20 The delete procedure is tricky and ill not be presented. 3. STL containers From https://en.cppreference.com/w/cpp/container/map Tämä teos on lisensoitu Creative Commons Nimeä-EiKaupallinen- EiMuutoksia 4.0 Kansainvälinen -lisenssillä. Tarkastele lisenssiä osoitteessa http://creativecommons.org/licenses/by-nc-nd/4.0/. tekijä: Frank Cameron This work is licensed under the Creative Commons Attribution- NonCommercial-NoDerivatives 4.0 International License. To view a copy of this license, visit http://creativecommons.org/licenses/by-nc-nd/4.0/. made by Frank Cameron"
            },
            {
                "role": "user",
                "content": "Can you summarize the key topics covered in this lecture?"
            },
            {
                "role": "assistant",
                "content": "This lecture covers foundational algorithmic concepts including problem formulation, correctness proof using induction, efficiency analysis using asymptotic notation, data structures (arrays, linked lists, BSTs, heaps), and sorting methods (selection sort, merge sort, counting sort). Additionally, it introduces the Word-RAM computational model, providing a theoretical framework for measuring algorithm efficiency."
            }
        ]
    },
    {
        "system": "Hello! I am Kiran, an AI assistant from Tampere University, dedicated to providing you with accurate and insightful assistance. My goal is to help you understand concepts, summarize lectures, and answer questions effectively.",
        "conversations": [
            {
                "role": "host",
                "content": "Sharing data among data structures COMP.CS.300 Data structures and algorithms 1 Matti Rintala (matti.rintala@tuni.fi) Value semantics (C++) Reference semantics (Python...) Case 1: no problems yet? name: Jussi age: 32 name: Katya age: 45 name: Samir age: 21 name: Alice age: 31 name: Äijä age: 65 Case 2: Duplicating data? name: Jussi age: 32 name: Katya age: 45 name: Samir age: 21 name: Alice age: 31 name: Äijä age: 65 Case 3: Sharing data? name: Jussi age: 32 name: Katya age: 45 name: Samir age: 21 name: Alice age: 31 nimi: Äijä age: 65 Case 4: Referring to data? Alternatives with indirect referral •Pointers (references) •Smart pointers (memory management) •Indices (if data in a vector, etc.) •Iterators (if data in a data structure) •Any other way to easily access the data! •(Note. danger of reference invalidation!)"
            },
            {
                "role": "user",
                "content": "Can you summarize the key topics covered in this lecture?"
            },
            {
                "role": "assistant",
                "content": "This lecture covers foundational algorithmic concepts including problem formulation, correctness proof using induction, efficiency analysis using asymptotic notation, data structures (arrays, linked lists, BSTs, heaps), and sorting methods (selection sort, merge sort, counting sort). Additionally, it introduces the Word-RAM computational model, providing a theoretical framework for measuring algorithm efficiency."
            }
        ]
    },
    {
        "system": "Hello! I am Kiran, an AI assistant from Tampere University, dedicated to providing you with accurate and insightful assistance. My goal is to help you understand concepts, summarize lectures, and answer questions effectively.",
        "conversations": [
            {
                "role": "host",
                "content": "Algorithm design strategies: decrease and conquer 1. Design strategies 2. Decrease and conquer 1. Design strategies Some algorithm design techniques: • decrease and conquer (incremental) • divide and conquer • transform and conquer • dynamic programming (solve all subproblems) • greedy • randomize These are design categories, not algorithms themselves. 2. Decrease and conquer strategy How does it work? • at start: all input data is unprocessed • at each iteration part of input data is processed (completely) • after each iteration, the amount of processed data increases • algortihm stops when all data is processed Example Linear search through array for particular element Search for 24 in array Example Find largest number in numerical array Example Insertion sort: a method for sorting elements in a list/array Sort numbers in array from smallest to largest Strengths • clear, intuitive • easy to implement (small amount of code) • works well for small amounts of data Weakness • often better (=more eﬃcient) alternatives for larger amounts of data Tämä teos on lisensoitu Creative Commons Nimeä-EiKaupallinen- EiMuutoksia 4.0 Kansainvälinen -lisenssillä. Tarkastele lisenssiä osoitteessa http://creativecommons.org/licenses/by-nc-nd/4.0/. tekijä: Frank Cameron This work is licensed under the Creative Commons Attribution- NonCommercial-NoDerivatives 4.0 International License. To view a copy of this license, visit http://creativecommons.org/licenses/by-nc-nd/4.0/. made by Frank Cameron"
            },
            {
                "role": "user",
                "content": "Can you summarize the key topics covered in this lecture?"
            },
            {
                "role": "assistant",
                "content": "This lecture covers foundational algorithmic concepts including problem formulation, correctness proof using induction, efficiency analysis using asymptotic notation, data structures (arrays, linked lists, BSTs, heaps), and sorting methods (selection sort, merge sort, counting sort). Additionally, it introduces the Word-RAM computational model, providing a theoretical framework for measuring algorithm efficiency."
            }
        ]
    },
    {
        "system": "Hello! I am Kiran, an AI assistant from Tampere University, dedicated to providing you with accurate and insightful assistance. My goal is to help you understand concepts, summarize lectures, and answer questions effectively.",
        "conversations": [
            {
                "role": "host",
                "content": "Depth first search 1. Background 2. Data structures 3. Procedure 4. Results and interpretation 1. Background At start: we have a digraph G = (V,E) and a starting node (source node) s from the digraph. Goal: we want to know all nodes that are reachable from s. One way to do this is by performing a graph search. If all other nodes are reachable, then the search is also a traversal. Reminders Example Results from Depth-first-search (DFS): 2. Data structures In DFS a node can be in one of 4 states: (i) undetected, (ii) detected and undiscovered, (iii) discovered or (iv) discovered and handled. Q: What do we mean when we say a node x has been discovered? A: We mean that a path from s to x has been found. some cycles: x adjacent to x reachable from x Q: What do we mean when we say a node x has been detected and undiscovered? A: We mean that x is adjacent to a node that has been discovered, but that x has not yet been discovered. Q: What do we mean when we say a node x has been discovered and handled? A: We mean that x has been discovered and all nodes adjacent to x have been discovered. In DFS - start with all nodes undetected and undiscovered, except s - progress is made by moving along edges and discovering nodes that have been undetected and/or undiscovered - the status of a node is monitored using a stack and colors If x is undetected, then it is white and it is not on the stack. If x is detected but undiscovered, then it is white and it is on the stack. If x is discovered, then it is gray and it is on the stack. If x is discovered and handled, then it is black and it is not on the stack. Note: progression of a node: (white, not on stack)  (white, on stack)  (gray, on stack)  (black, not on stack) To perform DFS, for each node we have the following attributes: A stack is a one-dimensional data structure that has top. A stack S has two basic operations; PUSH(S,x): places (pushes) item x onto top of stack POP(S): removes and returns item from top of stack In DFS we maintain a stack of gray and white nodes. A stack is said to function on a last-in-first-out (LIFO) basis. Example top operation stack Start with empty stack: S: PUSH( S, 7 ) PUSH( S, 2 ) POP( S ) PUSH( S, 4 ) top top top top 3. Procedure Description of DFS: When we perform step 3, we are moving forward. When we perform step 5, we are moving backward (or backtracking). Example: starting node: s = D x x x discovered (gray) x discovered and handled (black) 3. forward: (C,A) 1. forward: (D,B) 2. forward: (B,C) 4. backward: (C,A) 5. forward: (C,F) 6. backward: (C,F) 7. backward: (B,C) 8. backward: (D,B) 9. D is handled Pseudocode: Remarks 1. Choose any node x from the stack S. The path from s to x can always be found from nodes lower down in the stack S. 2. All gray nodes in the stack S form a linear path starting from s out to the gray node of the greatest 'depth'. 3. The parent of a node may be set several times at line 21. The final time when it is set corresponds to two possible situations: - When moving forward: x is colored gray at one iteration of the while-loop and y is colored gray on the following iteration. - When moving backwards: we POP a white node x at line 12 that is adjacent to the deepest gray node z currently on the stack. 4. In each iteration of the while loop - Either x is white and it is colored gray, or x is gray and it is colored black - A node will be added to the stack as a gray node only once. - A black node is never added to the stack. Example Execute DEPTH-FIRST-SEARCH with s = D while-loop iteration item A. B. C. D. F. H. A.color B.color C.color D.color F.color H.color 0 S 1 NIL NIL NIL NIL NIL NIL white white white white white white x cycle edge D B,F,D D NIL D NIL NIL D NIL white white white gray white white B C,B,F,D white gray white gray white white NIL D B NIL D NIL 2 3 A,F,C,B,F,D C C D B NIL C NIL white gray gray gray white white 4 A,F,C,B,F,D black node gray node A C D B NIL C NIL gray gray gray gray white white (A,D), (A,B) while-loop iteration item A. B. C. D. F. H. A.color B.color C.color D.color F.color H.color S x cycle edge 4 A,F,C,B,F,D black node gray node A C D B NIL C NIL gray gray gray gray white white (A,D), (A,B) 5 F,C,B,F,D A C D B NIL C NIL black gray gray gray white white 6 F,C,B,F,D F C D B NIL C NIL black gray gray gray gray white (F,B) 7 C,B,F,D F C D B NIL C NIL black gray gray gray black white 4. Results and interpretation Final results: x A B C D F H x. x.color C D B NIL C NIL black black black black black white edges causing cycles: (A,D), (A,B), (F,B) Q: How do we know what nodes are reachable from s? A: If x is reachable from s then x.color is black. Q: How can we produce the DF-tree? A: The DF-tree can be made using the parent attributes x. of the nodes. DF-tree: Q: How do we know that the edges found at line 23 produce cycles? A: This is a consequence of the observation that the gray nodes in the stack form a linear path from s. s gray nodes in S edge to gray node Q: What do we obtain if we remove all the cycle edges found in line 23 from the original graph? A: We obtain an acyclic graph. edges causing cycles: (A,D), (A,B), (F,B) s Why is this not possible for gray nodes in S? Q: Can we use Depth-first-search on an undirected graph? A: Yes. Remarks for undirected graph: - if x belongs to y.Adj, then y belongs to x.Adj - if x is reachable from s, then s is reachable from x - all edges in graph are either tree edges or edges that cause (undirected) cycles Tämä teos on lisensoitu Creative Commons Nimeä-EiKaupallinen- EiMuutoksia 4.0 Kansainvälinen -lisenssillä. Tarkastele lisenssiä osoitteessa http://creativecommons.org/licenses/by-nc-nd/4.0/. tekijä: Frank Cameron This work is licensed under the Creative Commons Attribution- NonCommercial-NoDerivatives 4.0 International License. To view a copy of this license, visit http://creativecommons.org/licenses/by-nc-nd/4.0/. made by Frank Cameron"
            },
            {
                "role": "user",
                "content": "Can you summarize the key topics covered in this lecture?"
            },
            {
                "role": "assistant",
                "content": "This lecture covers foundational algorithmic concepts including problem formulation, correctness proof using induction, efficiency analysis using asymptotic notation, data structures (arrays, linked lists, BSTs, heaps), and sorting methods (selection sort, merge sort, counting sort). Additionally, it introduces the Word-RAM computational model, providing a theoretical framework for measuring algorithm efficiency."
            }
        ]
    },
    {
        "system": "Hello! I am Kiran, an AI assistant from Tampere University, dedicated to providing you with accurate and insightful assistance. My goal is to help you understand concepts, summarize lectures, and answer questions effectively.",
        "conversations": [
            {
                "role": "host",
                "content": "Dijkstra's algorithm and the shortest path problem 1. Background 2. Data structures 3. Procedures 4. Priority queue issues 1. Background At start: a weighted digraph G = (V,E) and a starting node (source node) s from the digraph Goal: find shortest path from s to all nodes that are reachable from s Weight of edge (x,y) is w((x,y)). Weight w((x,y)) is interpreted as a distance. All weights are assumed to be non-negative. This problem is sometimes called the single-source shortest path problem. Example Paths from F to H and their total lengths path <F, D, H> <F, A, B, H> <F, A, C, D, H> total length Dynamic programming principle used in solving shortest path problem: Dynamic programming is not 'programming' in some computer language: C++, Java, etc. Dynamic programming is an algorithm design technique. Dijkstra's algorithm: solves solves single source shortest path problem using the dynamic programming follows BFS with addition of priority queue to take into account weights of edges produces shortest path tree To perform Dijkstra's algorithm, for each node we use the following attributes: Significance of colors: x.colour = white: node x not yet discovered x.colour = gray: node x discovered, but shortest path not yet found x.colour = black: node x discovered and shortest path from s has been found  2. Data structures For each edge Dijkstra needs weight w((x,y)). Dijkstra's algorithm maintains a priority queue Q containing pairs (x, x.d) for gray nodes. In Dijkstra: node x priority = x.d. Two basic operations: Example operation Q Start with empty priority queue: Q: INSERT( Q, a, 6 ) INSERT( Q, b, 8 ) INSERT( Q, b, 5 ) EXTRACT-MIN( Q ) 3. Procedures Dijkstra's algorithm uses procedure RELAX for detecting when a shorter path has been found. Remarks 1. The pseudocode is very similar to BREADTH-FIRST-SEARCH. The main difference is that a priority queue is used in place of an ordinary queue. 2. Lines 23-26 are needed for updating Q when a shorter path is found. 3. Because of line 25 the same node may have several elements in Q. Example Execute DIJKSTRA with s = F. while-loop iteration item A.d B.d C.d D.d F.d H.d A. B. C. D. F. H. 0 Q 1 NIL NIL NIL NIL NIL NIL 2 3 4 black node gray node x F NIL F F NIL NIL F A A F NIL NIL F A A F NIL D F C A F NIL C while-loop iteration item A.d B.d C.d D.d F.d H.d A. B. C. D. F. H. Q 4 black node gray node x F C A F NIL C 6 F C A F NIL C 8 F C A F NIL C Final results: x A B C D F H x.d x. 2 7 5 4 0 10 F C A F NIL C Shortest path tree 4. Priority queue issues A heap can be used as a priority queue. need min-heap heap does allow efficient changing of priorities: hence same node may appear several times EXTRACT-MIN will always remove element with smallest priority in C++ STL max-heap is std::priority_queue< > in C++ STL with <length, node> pair: std::priority_queue< std::pair<int, Node*> > to use max-heap as min-heap multiply priority by -1 NOT allow Alternative to heap for priority queue: balanced binary search tree. in C++ STL balanced binary serach tree is std::set< > using with <length, node> pair: std::set< std::pair<int, Node*> > to change priority: remove old <length, node> pair and insert new <length, node> pair Tämä teos on lisensoitu Creative Commons Nimeä-EiKaupallinen- EiMuutoksia 4.0 Kansainvälinen -lisenssillä. Tarkastele lisenssiä osoitteessa http://creativecommons.org/licenses/by-nc-nd/4.0/. tekijä: Frank Cameron This work is licensed under the Creative Commons Attribution- NonCommercial-NoDerivatives 4.0 International License. To view a copy of this license, visit http://creativecommons.org/licenses/by-nc-nd/4.0/. made by Frank Cameron"
            },
            {
                "role": "user",
                "content": "Can you summarize the key topics covered in this lecture?"
            },
            {
                "role": "assistant",
                "content": "This lecture covers foundational algorithmic concepts including problem formulation, correctness proof using induction, efficiency analysis using asymptotic notation, data structures (arrays, linked lists, BSTs, heaps), and sorting methods (selection sort, merge sort, counting sort). Additionally, it introduces the Word-RAM computational model, providing a theoretical framework for measuring algorithm efficiency."
            }
        ]
    },
    {
        "system": "Hello! I am Kiran, an AI assistant from Tampere University, dedicated to providing you with accurate and insightful assistance. My goal is to help you understand concepts, summarize lectures, and answer questions effectively.",
        "conversations": [
            {
                "role": "host",
                "content": "Algorithm design strategies: divide and conquer 1. Introduction 2. An application: tiling a grid 3. Data structures 1. Introduction How does divide and conquer (D&C) work? 1. Divide the problem into subproblems: • subproblems should be roughly the same size • subproblems should be smaller versions of original problem 2. Continue dividing until the subproblem size is small enough to have a simple solution. 3. Combine the solutions of the subproblems. Connection to recursion: • subproblems should be smaller versions of original problem (recursion case) • stop dividing when subproblem has simple solution (base case) However... An algorithm using D&C does not have to use recursion. A recursive algorithm does not necessarily use D&C. Why use D&C? • nature of the problem being handled • more eﬃcient algorithms (possibly) • data structure is intentionally D&C friendly 2. An application: tiling a grid Situation: • there is a 2^n ×2^n grid of squares • one square is already ﬁlled • suﬃcient supply of L-shaped trominoes to ﬁll entire grid Problem: How to place trominoes to ﬁll entire grid? L-shaped tromino 4x4 grid When n=1 One filled grid for n=2 at start at end Divide and conquer: Diﬃculty: One quadrant is diﬀerent, since it has one ﬁlled square. How can we use a tromino to make all quadrants have this same property? Example at start at end 3. Data structures How suitable is a data structure for D&C? Array: a part of an array is still an array • D&C is used in both merge-sort and quicksort for sorting fixed set of elements • divide is used in binary search, if array is in sorted • cannot be used for inserting new element or deleting old element Binary search tree (BST): both the left and the right subtrees of a node are BSTs • D&C used in traversing BST to get sorted list (inorder-traversal) • D&C used in computing height of BST • divide used in inserting new element, deleting old element, searching for an element Tämä teos on lisensoitu Creative Commons Nimeä-EiKaupallinen- EiMuutoksia 4.0 Kansainvälinen -lisenssillä. Tarkastele lisenssiä osoitteessa http://creativecommons.org/licenses/by-nc-nd/4.0/. tekijä: Frank Cameron This work is licensed under the Creative Commons Attribution- NonCommercial-NoDerivatives 4.0 International License. To view a copy of this license, visit http://creativecommons.org/licenses/by-nc-nd/4.0/. made by Frank Cameron"
            },
            {
                "role": "user",
                "content": "Can you summarize the key topics covered in this lecture?"
            },
            {
                "role": "assistant",
                "content": "This lecture covers foundational algorithmic concepts including problem formulation, correctness proof using induction, efficiency analysis using asymptotic notation, data structures (arrays, linked lists, BSTs, heaps), and sorting methods (selection sort, merge sort, counting sort). Additionally, it introduces the Word-RAM computational model, providing a theoretical framework for measuring algorithm efficiency."
            }
        ]
    },
    {
        "system": "Hello! I am Kiran, an AI assistant from Tampere University, dedicated to providing you with accurate and insightful assistance. My goal is to help you understand concepts, summarize lectures, and answer questions effectively.",
        "conversations": [
            {
                "role": "host",
                "content": "The runtime efficiency of three graph algorithms 1. Background 2. BFS 3. DFS 4. Dijkstra 1. Background At start: a digraph G = (V,E) possibly with weights on edges Adjacency sum result 2. BFS THREE LOOPS 3. DFS THREE LOOPS 4. Dijkstra THREE LOOPS The priority queue could also be implemented using a balanced binary search tree. The DIJKSTRA2 procedure assumes this. This efficiency assumes that the priority queue in Dijkstra is implemented using a heap. Tämä teos on lisensoitu Creative Commons Nimeä-EiKaupallinen- EiMuutoksia 4.0 Kansainvälinen -lisenssillä. Tarkastele lisenssiä osoitteessa http://creativecommons.org/licenses/by-nc-nd/4.0/. tekijä: Frank Cameron This work is licensed under the Creative Commons Attribution- NonCommercial-NoDerivatives 4.0 International License. To view a copy of this license, visit http://creativecommons.org/licenses/by-nc-nd/4.0/. made by Frank Cameron"
            },
            {
                "role": "user",
                "content": "Can you summarize the key topics covered in this lecture?"
            },
            {
                "role": "assistant",
                "content": "This lecture covers foundational algorithmic concepts including problem formulation, correctness proof using induction, efficiency analysis using asymptotic notation, data structures (arrays, linked lists, BSTs, heaps), and sorting methods (selection sort, merge sort, counting sort). Additionally, it introduces the Word-RAM computational model, providing a theoretical framework for measuring algorithm efficiency."
            }
        ]
    },
    {
        "system": "Hello! I am Kiran, an AI assistant from Tampere University, dedicated to providing you with accurate and insightful assistance. My goal is to help you understand concepts, summarize lectures, and answer questions effectively.",
        "conversations": [
            {
                "role": "host",
                "content": "Graphs, trees and binary trees 1. Graphs 2. Trees 3. Binary trees 1. Graphs Example NOTE: multiple edges not allowed Example Example vertex x vertices adjacent to x vertices to which x is adjacent in-degree out-degree Example vertex x vertices adjacent to x degree The graph has no isolated vertices. Example Example 2. Trees Undirected trees Example not acyclic, so not a tree no path from d to e, so not a tree is a tree Rooted trees Example NOTE: usually omit arrows when drawing rooted tree and root is at the top no vertex has a path to all other vertices, so not a rooted tree two paths from d to a, so not a rooted tree is a rooted tree; root is d Example OR 3. Binary trees Example vertex left child right child parent Example b has only one child, so not a full binary tree is a full binary tree, but not complete since leaves b and e are not at same depth is a complete binary tree Tämä teos on lisensoitu Creative Commons Nimeä-EiKaupallinen- EiMuutoksia 4.0 Kansainvälinen -lisenssillä. Tarkastele lisenssiä osoitteessa http://creativecommons.org/licenses/by-nc-nd/4.0/. tekijä: Frank Cameron This work is licensed under the Creative Commons Attribution- NonCommercial-NoDerivatives 4.0 International License. To view a copy of this license, visit http://creativecommons.org/licenses/by-nc-nd/4.0/. made by Frank Cameron"
            },
            {
                "role": "user",
                "content": "Can you summarize the key topics covered in this lecture?"
            },
            {
                "role": "assistant",
                "content": "This lecture covers foundational algorithmic concepts including problem formulation, correctness proof using induction, efficiency analysis using asymptotic notation, data structures (arrays, linked lists, BSTs, heaps), and sorting methods (selection sort, merge sort, counting sort). Additionally, it introduces the Word-RAM computational model, providing a theoretical framework for measuring algorithm efficiency."
            }
        ]
    },
    {
        "system": "Hello! I am Kiran, an AI assistant from Tampere University, dedicated to providing you with accurate and insightful assistance. My goal is to help you understand concepts, summarize lectures, and answer questions effectively.",
        "conversations": [
            {
                "role": "host",
                "content": "Implementing Graphs in C++ Background Graphs not available in STL (too many different possibilities) Assumptions - nodes exist along with accompanying data - all nodes already stored in own container - directed graph Part of London subway system Example Nodes Nodes and edges x x's adjacency set std::unordered_map<int, Node> allNodes; struct Node { // All the data stored in the node int id; std::string name; // other node data ... std::vector<Node*> to_neighbours; }; Node struct has - unique node identifier - a name - node's adjacency set (pointers) Implementation 1 Suitable when - only need to move forward along edges - edges added or deleted infrequently Implementation 2 struct Node { // All the data stored in the node int id; std::string name; // other node data ... std::unordered_set<Node*> to_neighbours; }; Node struct has - unique node identifier - a name - node's adjacency set (pointers) Suitable when - only need to move forward along edges - edges added or deleted frequently struct Node { // All the data stored in the node int id; std::string name; // other node data ... std::vector<Node*> to_neighbours; std::vector<Node*> from_neighbours; }; Node struct has - unique node identifier - a name - node's adjacency set (pointers) - to what other nodes this node is adjacent (pointers) Implementation 3 Suitable when - need to move both forward and backward along edges - edges added or deleted infrequently Implementation 4 enum Color { WHITE, GRAY, BLACK }; struct Node { // All the data stored in the node int id; std::string name; int d; // Distance Node* parent; // π Color color; // node's color std::vector<Node*> to_neighbours; }; Node struct has - unique node identifier - a name - node's adjacency set (pointers) - a distance - a parent node - a color Suitable when - only need to move forward along edges - edges added or deleted infrequently - some graph search/traversal algorithm is performed (BFS, DFS) - memory use not strict (d, parent, color always included) Implementation 5 enum Color { WHITE, GRAY, BLACK }; struct SearchInfo { int d; // Distance Node* path_back; // π Color color; // Color }; struct Node { // All the data stored in the node int id; std::string name; SearchInfo* search_info; // Must be created! std::vector<Node*> to_neighbours; }; Node struct has - unique node identifier - a name - node's adjacency set (pointers) - pointer to extra data Suitable when - only need to move forward along edges - edges added or deleted infrequently - some graph search/traversal algorithm is performed (BFS, DFS) - memory use strict (create extra data when needed, delete after) Implementation 6 enum Color { WHITE, GRAY, BLACK }; struct SearchInfo { int d; // Distance int parent; // π Color color; // Color }; vector<Node> allNodes; vector<SearchInfo> allSearchInfos; struct Node { // All the data stored in the node int id; std::string name; std::vector<int> to_neighbours; }; Node struct has - unique node identifier - a name - node's adjacency set (indices) Suitable when - only need to move forward along edges - edges added or deleted infrequently - some graph search/traversal algorithm is performed (BFS, DFS) - memory use strict (create extra vector when needed, delete after) Tämä teos on lisensoitu Creative Commons Nimeä-EiKaupallinen- EiMuutoksia 4.0 Kansainvälinen -lisenssillä. Tarkastele lisenssiä osoitteessa http://creativecommons.org/licenses/by-nc-nd/4.0/. tekijä: Frank Cameron This work is licensed under the Creative Commons Attribution- NonCommercial-NoDerivatives 4.0 International License. To view a copy of this license, visit http://creativecommons.org/licenses/by-nc-nd/4.0/. made by Frank Cameron"
            },
            {
                "role": "user",
                "content": "Can you summarize the key topics covered in this lecture?"
            },
            {
                "role": "assistant",
                "content": "This lecture covers foundational algorithmic concepts including problem formulation, correctness proof using induction, efficiency analysis using asymptotic notation, data structures (arrays, linked lists, BSTs, heaps), and sorting methods (selection sort, merge sort, counting sort). Additionally, it introduces the Word-RAM computational model, providing a theoretical framework for measuring algorithm efficiency."
            }
        ]
    },
    {
        "system": "Hello! I am Kiran, an AI assistant from Tampere University, dedicated to providing you with accurate and insightful assistance. My goal is to help you understand concepts, summarize lectures, and answer questions effectively.",
        "conversations": [
            {
                "role": "host",
                "content": "Hash tables 1. Background and terminology 2. Resolving collisions 3. Hash function 4. Rehashing 5. Runtime efficiency and amortized analysis 1. Background and terminology Example A database of cars n = number of records stored m = number of buckets Example If two (or more) keys get mapped to same bucket ... Collision! In C++ STL unordered_set and unordered_map are hash tables. 2. Resolving collisions 3. Hash functions Example Sensible policy: use existing (default) hash functions and focus on providing good keys. Hash functions and STL: STL will not accept a struct as a key. In such cases it may be necessary to form a hash function. Two examples of forming a hash function from a struct. Example I Example II If your key is really composed of two or more attributes (elements), then search (Google) for help to obtain hash function. 4. Rehashing However, user can set these. 5. Runtime efficiency and amortized analysis Example Assumptions: Operation sequence: Total number of elements in hash table: Total operations count: Amortized efficiency per operation: Tämä teos on lisensoitu Creative Commons Nimeä-EiKaupallinen- EiMuutoksia 4.0 Kansainvälinen -lisenssillä. Tarkastele lisenssiä osoitteessa http://creativecommons.org/licenses/by-nc-nd/4.0/. tekijä: Frank Cameron This work is licensed under the Creative Commons Attribution- NonCommercial-NoDerivatives 4.0 International License. To view a copy of this license, visit http://creativecommons.org/licenses/by-nc-nd/4.0/. made by Frank Cameron"
            },
            {
                "role": "user",
                "content": "Can you summarize the key topics covered in this lecture?"
            },
            {
                "role": "assistant",
                "content": "This lecture covers foundational algorithmic concepts including problem formulation, correctness proof using induction, efficiency analysis using asymptotic notation, data structures (arrays, linked lists, BSTs, heaps), and sorting methods (selection sort, merge sort, counting sort). Additionally, it introduces the Word-RAM computational model, providing a theoretical framework for measuring algorithm efficiency."
            }
        ]
    },
    {
        "system": "Hello! I am Kiran, an AI assistant from Tampere University, dedicated to providing you with accurate and insightful assistance. My goal is to help you understand concepts, summarize lectures, and answer questions effectively.",
        "conversations": [
            {
                "role": "host",
                "content": "A heap is a binary tree with certain properties. Assumption: each vertex (node) in the binary tree has a value (a key) associated with it Implementing a heap using an array 1. Storing a heap binary tree as an array 2. Heap algorithms using array storage 3. Heap sort 1. Storing a heap binary tree as an array To be a heap a binary tree must have the following properties: property 1: the parent's key is at least as large as the keys of its children property 2: all depths, except possibly the largest, have the maximum number of nodes property 3: at the largest depth, any missing nodes (leaves) are at the right end From binary tree to array We have lost the pointers to children and parents! Q: When a binary tree is stored as an array, how can we ﬁnd the children and/or parent of a given node i? A: We store the nodes in a particular order and use the array indices (locations). Array indices and locations in binary tree. corresponding array Example heap as binary tree corresponding array For node whose index is x in array: LEFT(x) = 2x , RIGHT(x) = 2x + 1, PARENT(x) = x/2 Assumption: heap array is A NOTES 1. key of root is always A[1] 2. A.heapsize is number of elements in heap 3. A.length must be at least A.heapsize 4. assume functions LEFT, RIGHT and PARENT are available Example Consider swapping key of node1 with its parent key. Assume A[x] corresponds to node1. Using pointers Using array Q: Why store a heap as an array? A: It is more efficient both in terms of running time and storage space to store a heap as an array rather than using a binary tree with pointers. 2. Heap algorithms using array storage HEAP-INSERT: allows insertion of a new node into an existing heap HEAPIFY: for a binary tree (or subtree) for which some node x lacks property 1, restores this property for all nodes below node x thereby making a heap BUILD-HEAP: for a binary tree lacking property 1 at any (and possibly all) nodes, restores this property thereby making a heap HEAP-EXTRACT-MAX: removes the root from a heap, restores the heap properties and returns the maximum key 3. Heap sort A heap can be used to produce a sorted array using procedure HEAPSORT. Example Starting array: NOTES 1. Heapsort needs no extra storage space. 2. Heapsort's runtime efficiency is the same as Mergesort's runtime efficiency. 3 8 4 5 6 2 11 2 6 4 5 3 8 11 Tämä teos on lisensoitu Creative Commons Nimeä-EiKaupallinen- EiMuutoksia 4.0 Kansainvälinen -lisenssillä. Tarkastele lisenssiä osoitteessa http://creativecommons.org/licenses/by-nc-nd/4.0/. tekijä: Frank Cameron This work is licensed under the Creative Commons Attribution- NonCommercial-NoDerivatives 4.0 International License. To view a copy of this license, visit http://creativecommons.org/licenses/by-nc-nd/4.0/. made by Frank Cameron"
            },
            {
                "role": "user",
                "content": "Can you summarize the key topics covered in this lecture?"
            },
            {
                "role": "assistant",
                "content": "This lecture covers foundational algorithmic concepts including problem formulation, correctness proof using induction, efficiency analysis using asymptotic notation, data structures (arrays, linked lists, BSTs, heaps), and sorting methods (selection sort, merge sort, counting sort). Additionally, it introduces the Word-RAM computational model, providing a theoretical framework for measuring algorithm efficiency."
            }
        ]
    },
    {
        "system": "Hello! I am Kiran, an AI assistant from Tampere University, dedicated to providing you with accurate and insightful assistance. My goal is to help you understand concepts, summarize lectures, and answer questions effectively.",
        "conversations": [
            {
                "role": "host",
                "content": "The heap data structure 1. Heap background 2. Heap algorithms 3. Runtime efficiencies 1. Heap background A heap is a binary tree with certain properties. Assumption: each vertex (node) in the binary tree has a value (a key) associated with it To be a heap a binary tree must have the following properties: property 1: the parent's key is at least as large as the keys of its children property 2: all depths, except possibly the largest, have the maximum number of nodes property 3: at the largest depth, any missing nodes (leaves) are at the right end Example Is the binary tree a heap? Notes 1. Property 1 is required for a max-heap. In a min-heap, the parent's key is at most as large as the keys of its children. 2. Given a set of keys there may be several binary trees that satisfy the heap properties. Hence a heap is not unique. 3. For each node, both its right subtree and its left subtree are heaps. (Hence: suitable for recursion). 4. In a max-heap, the maximum key is always at the root. Both subtrees are heaps. Heap parameters Relationships: A priority queue is often implemented using a heap. Priority queue: - a collection of items each having a priority - allows easy access to the item with the highest priority - allows the item with the highest priority to be removed - allows new item to be added to 2. Heap algorithms Assume the following are always available: node.parent = pointer to parent of node or NIL if node is heapRoot node.left = pointer to left child of node or NIL if there is no left child node.right = pointer to right child of node or NIL if there is no right child Four algorithms: HEAP-INSERT: allows insertion of a new node into an existing heap HEAPIFY: for a binary tree (or subtree) lacking property 1 at the root, restores this property thereby making a heap BUILD-HEAP: for a binary tree lacking property 1 at any (and possibly all) nodes, restores this property thereby making a heap HEAP-EXTRACT-MAX: removes the root from a heap, restores the heap properties and returns the maximum key node.key = key value associated with node heapRoot = root node of heap (represents entire heap) height = the height of the heap HEAP-INSERT-pseudocode NOTE: In a SWAP a node retains its key, but its pointers are updated. Example heap at start: node to add stage line new heap HEAPIFY-pseudocode Example starting binary tree node1: stage line computation or new heap BUILD-HEAP-pseudocode Example starting binary tree stage line new binary tree HEAP-EXTRACT-MAX-pseudocode Example heap at start: stage line new heap or computation 3. Runtime efficiencies What are the runtime efficiencies of HEAP-INSERT, HEAPIFY, BUILD-HEAP and HEAP- EXTRACT-MAX? HEAP-INSERT efficiency Q: How many iterations in while- loop? A: At most one more than height of the starting heap (so h+1). runtime efficiency of HEAP-INSERT: HEAPIFY efficiency Q: How many iterations in while-loop? A: At most height of the starting heap (so h). runtime efficiency of HEAPIFY: Q: What is the runtime efficiency of HEAP-EXTRACT-MAX? A: The same as that of HEAPIFY. HEAP-EXTRACT-MAX efficiency runtime efficiency of HEAP-EXTRACT-MAX: BUILDHEAP efficiency Assumption: starting heap is a complete binary tree, so n = 2h+1 - 1. Sum of all simple operations: It can be shown: Conclusion: runtime efficiency of BUILDHEAP: Tämä teos on lisensoitu Creative Commons Nimeä-EiKaupallinen- EiMuutoksia 4.0 Kansainvälinen -lisenssillä. Tarkastele lisenssiä osoitteessa http://creativecommons.org/licenses/by-nc-nd/4.0/. tekijä: Frank Cameron This work is licensed under the Creative Commons Attribution- NonCommercial-NoDerivatives 4.0 International License. To view a copy of this license, visit http://creativecommons.org/licenses/by-nc-nd/4.0/. made by Frank Cameron"
            },
            {
                "role": "user",
                "content": "Can you summarize the key topics covered in this lecture?"
            },
            {
                "role": "assistant",
                "content": "This lecture covers foundational algorithmic concepts including problem formulation, correctness proof using induction, efficiency analysis using asymptotic notation, data structures (arrays, linked lists, BSTs, heaps), and sorting methods (selection sort, merge sort, counting sort). Additionally, it introduces the Word-RAM computational model, providing a theoretical framework for measuring algorithm efficiency."
            }
        ]
    },
    {
        "system": "Hello! I am Kiran, an AI assistant from Tampere University, dedicated to providing you with accurate and insightful assistance. My goal is to help you understand concepts, summarize lectures, and answer questions effectively.",
        "conversations": [
            {
                "role": "host",
                "content": "COMP.CS.300 24 2.3 Implementing algorithms In the real world you need to be able to use theoretical knowledge in practise. For example: apply a given sorting algorithm ins a certain programming problem • numbers are rarely sorted alone, we sort structures with – a key – satellite data • the key sets the order ⇒it is used in the comparisons • the satellite data is not used in the comparison, but it must be moved around together with the key COMP.CS.300 25 The INSERTION-SORT algorithm from the previous chapter would change as follows if there were some satellite data used: 1 for j := 2 to A.length do 2 temp := A[j] 3 i := j −1 4 while i > 0 and A[i].key > temp.key do 5 A[i + 1] := A[i] 6 i := i −1 7 A[i + 1] := temp • An array of pointers to structures should be used with a lot of satellite data. The sorting is done with the pointers and the structures can then be moved directly to their correct locations. COMP.CS.300 26 The programming language and the problem to be solved also often dictate other implementation details, for example: • Indexing starts from 0 (in pseudocode often from 1) • Is indexing even used, or some other method of accessing data (or do we use arrays or some other data structures) • (C++) Is the data really inside the array/datastructure, or somewhere else at the end of a pointer (in which case the data doesn’t have to be moved and sharing it is easier). Many other programming languages always use pointers/references, so you don’t have to choose. • If you refer to the data indirectly from elsewhere, does it happen with – Pointers (or references) – Smart pointers (C++, shared_ptr) – Iterators (if the data is inside a datastructure) – Index (if the data is inside an array) – Search key (if the data is insde a data structure with fast search) COMP.CS.300 27 • Is recursion implemented really as recursion or as iteration • Are algorithm \"parameters\" in pseudocode really parameters in code, or just variables COMP.CS.300 28 In order to make an executable program, additional information is needed to implement INSERTION-SORT • an actual programming language must be used with its syntax for deﬁning variables and functions • a main program that takes care of reading the input, checking its legality and printing the results is also needed – it is common that the main is longer than the actual algorithm COMP.CS.300 29 The implementation of the program described abowe in C++: #include <iostream> #include <vector> typedef std::vector<int> Array; void insertionSort( Array & A ) { int key, i; unsigned int j; for( j = 1; j < A.size(); ++j ) { key = A.at(j); i = j-1; while( i >= 0 && A.at(i) > key ) { A.at(i+1) = A.at(i); --i; } A.at(i+1) = key; } } int main() { unsigned int i; // getting the amount of elements std::cout << \"Give the size of the array 0...: \"; std::cin >> i; COMP.CS.300 30 Array A(i); // creating the array // reading in the elements for( i = 0; i < A.size(); ++i ) { std::cout << \"Give A[\" << i+1 << \"]: \"; std::cin >> A.at(i); } insertionSort( A ); // sorting // print nicely for( i = 0; i < A.size(); ++i ) { if( i % 5 == 0 ) { std::cout << std::endl; } else { std::cout << \" \"; } std::cout << A.at(i); } std::cout << std::endl; } COMP.CS.300 31 The program code is signiﬁcantly longer than the pseudocode. It is also more difﬁcult to see the central characteristics of the algorithm. This course concentrates on the principles of algorithms and data structures. Therefore using program code doesn’t serve the goals of the course. ⇒From now on, program code implementations are not normally shown."
            },
            {
                "role": "user",
                "content": "Can you summarize the key topics covered in this lecture?"
            },
            {
                "role": "assistant",
                "content": "This lecture covers foundational algorithmic concepts including problem formulation, correctness proof using induction, efficiency analysis using asymptotic notation, data structures (arrays, linked lists, BSTs, heaps), and sorting methods (selection sort, merge sort, counting sort). Additionally, it introduces the Word-RAM computational model, providing a theoretical framework for measuring algorithm efficiency."
            }
        ]
    },
    {
        "system": "Hello! I am Kiran, an AI assistant from Tampere University, dedicated to providing you with accurate and insightful assistance. My goal is to help you understand concepts, summarize lectures, and answer questions effectively.",
        "conversations": [
            {
                "role": "host",
                "content": "Insertion sort 1. Introduction 2. Algorithm presentation 3. Operation counts: best, worst, average 1. Introduction Insertion sort is a sorting algorithm. Sorting: placing elements in order (smallest to largest or largest to smallest) Assumption: for two elements x, y, only three possible cases: (i) x < y, (ii) x = y or (iii) x > y Insertion-sort: • can be used to sort items/elements in an array or list • an example of decrease and conquer • easy to understand/code • one of many diﬀerent sorting algorithms 2. Algorithm presentation Example Example A[1..n] before line 6 j key A[1..n] after line 10 3. Operation counts: best, worst, average How many times will each line be executed? • for-loop goes through A.length −1 elements; line 6 (or 5 or 10) executed A.length −1 times • how many times line 8 executed depends on initial order of A[1..n] Best Is it possible we never need to execute line 8? What conditions? Worst Consider j = n (element A[n]). Most number of times line 8 must be executed? Consider j = n-1 (element A[n-1]). Most number of times line 8 must be executed? Average Consider j = n (element A[n]). Best assumption on how many numbers in A[1..(n-1)] are greater than A[n]? Consider j = n-1 (element A[n-1]). Best assumption on how many numbers in A[1..(n-2)] are greater than A[n-1]? j most number of times line 8 executed average number of times line 8 executed Tämä teos on lisensoitu Creative Commons Nimeä-EiKaupallinen- EiMuutoksia 4.0 Kansainvälinen -lisenssillä. Tarkastele lisenssiä osoitteessa http://creativecommons.org/licenses/by-nc-nd/4.0/. tekijä: Frank Cameron This work is licensed under the Creative Commons Attribution- NonCommercial-NoDerivatives 4.0 International License. To view a copy of this license, visit http://creativecommons.org/licenses/by-nc-nd/4.0/. made by Frank Cameron"
            },
            {
                "role": "user",
                "content": "Can you summarize the key topics covered in this lecture?"
            },
            {
                "role": "assistant",
                "content": "This lecture covers foundational algorithmic concepts including problem formulation, correctness proof using induction, efficiency analysis using asymptotic notation, data structures (arrays, linked lists, BSTs, heaps), and sorting methods (selection sort, merge sort, counting sort). Additionally, it introduces the Word-RAM computational model, providing a theoretical framework for measuring algorithm efficiency."
            }
        ]
    },
    {
        "system": "Hello! I am Kiran, an AI assistant from Tampere University, dedicated to providing you with accurate and insightful assistance. My goal is to help you understand concepts, summarize lectures, and answer questions effectively.",
        "conversations": [
            {
                "role": "host",
                "content": "COMP.CS.300 Tietorakenteet ja algoritmit 1 1 COMP.CS.300 Tietorakenteet ja algoritmit 1 COMP.CS.300 Tietorakenteet ja algoritmit 1 2 Lähteet Luentomoniste pohjautuu vahvasti prof. Antti Valmarin vanhaan luentomonisteeseen Tietorakenteet ja algoritmit. Useimmat algoritmit ovat peräisin kirjasta Introduction to Algorithms; Thomas H. Cormen, Charles E. Leiserson, Ronald L. Rivest, Clifford Stein. Lisäksi luentomonistetta koottaessa on käytetty seuraavia kirjoja: • Introduction to The Design & Analysis of Algorithms; Anany Levitin • Olioiden ohjelmointi C++:lla; Matti Rintala, Jyke Jokinen • Tietorakenteet ja Algoritmit; Ilkka Kokkarinen, Kirsti Ala-Mutka • The C++ Standard Library; Nicolai M. Josuttis COMP.CS.300 Tietorakenteet ja algoritmit 1 3 1 Johdanto Mietitään ensin hiukan syitä tietorakenteiden ja algoritmien opiskelulle Algorithms in the world COMP.CS.300 Tietorakenteet ja algoritmit 1 4 1.1 Miksi? Mitkä ovat sinun elämääsi eniten vaikuttavat algoritmit? Picture: Chris Watt COMP.CS.300 Tietorakenteet ja algoritmit 1 5 Tietokoneohjelmia ei ole olemassa ilman algoritmeja • algoritmeihin törmäät esimerkiksi seuraavissa sovelluksissa: COMP.CS.300 Tietorakenteet ja algoritmit 1 6 COMP.CS.300 Tietorakenteet ja algoritmit 1 7 aina, kun käytät tietokonetta, käytät myös algoritmeja. COMP.CS.300 Tietorakenteet ja algoritmit 1 8 1.2 Kaikki pennillä? Via: The Washington Post COMP.CS.300 Tietorakenteet ja algoritmit 1 9 Tietorakenteita tarvitaan ohjelmissa käsiteltävän tiedon tallettamiseen ja sen käsittelyn mahdollistamiseen ja helpottamiseen • tietorakenteita on monia eivätkä ne kaikki sovi kaikkiin tilanteisiin ⇒ohjelmoijan pitää osata valita tilanteeseen sopivin ⇒vaihtoehtojen käyttäytyminen, vahvuudet ja heikkoudet on tunnettava Modernit ohjelmointikielet tarjoavat valmiina helppokäyttöisiä tietorakenteita. Näiden ominaisuuksien sekä käyttöön vaikuttavien rajoitteiden tuntemiseksi tarvitaan perustietorakenneosaamista COMP.CS.300 Tietorakenteet ja algoritmit 1 10 COMP.CS.300 Tietorakenteet ja algoritmit 1 11 Kuinka moni on turhautunut ohjelman tai esimerkiksi kännykän hitauteen? • toiminnallisuus on toki ensisijaisen tärkeää kaikille ohjelmille, mutta tehokkuus ei ole merkityksetön sivuseikka • on tärkeää huomioida ja miettiä ratkaisujaan myös ajan- ja muistinkäytön kannalta • valmiin kirjaston käyttö näyttää usein suoraviivaisemmalta kuin onkaan Näitä asioita käsitellään tällä kurssilla COMP.CS.300 Tietorakenteet ja algoritmit 1 12 2 Käsitteitä ja merkintöjä Tässä luvussa esitellään kurssilla käytettävää käsitteistöä ja merkintätapoja. Luvussa käsitellään myös pseudokoodiesityksen ja ohjelmointikielisen koodin eroja. Esimerkkinä käytetään järjestämisalgoritmia INSERTION-SORT. COMP.CS.300 Tietorakenteet ja algoritmit 1 13 2.1 Tavoitteet Kurssin keskeisenä tavoitteena on antaa opiskelijalle käyttöön peruskoneisto kuhunkin ohjelmointitehtävään sopivan ratkaisun valitsemiseen ja omien ratkaisujen tehokkuuden arvioimiseen karkealla tasolla. • Kurssilla keskitytään tilanteeseen sopivan tietorakenteen valintaan. • Lisäksi käsitellään käytännön tilanteissa usein vastaan tulevia ongelmatyyppejä ja algoritmeja, joilla ne voi ratkaista. COMP.CS.300 Tietorakenteet ja algoritmit 1 14 • Kurssilla keskitytään lähinnä ns. hyviin algoritmeihin. • Painotus on siis siinä, miten algoritmin ajankulutus kasvaa syötekoon kasvaessa, eikä niinkään yksityiskohtien optimoinnissa. COMP.CS.300 Tietorakenteet ja algoritmit 1 15 2.2 Peruskäsitteistöä Tietorakenne COMP.CS.300 Tietorakenteet ja algoritmit 1 16 Tapa tallettaa ja järjestää tietoa: • tietoa pystytään lisäämään ja hakemaan algoritmien avulla. • talletettua tietoa voidaan muokata • tietorakenteita on useamman tasoisia: tietorakenne voi koostua toisista tietorakenteista COMP.CS.300 Tietorakenteet ja algoritmit 1 17 Algoritmi: Kuva 1: kuva: Aldo Cortesi COMP.CS.300 Tietorakenteet ja algoritmit 1 18 • Joukko ohjeita tai askeleita jonkin ongelman ratkaisemiseksi • Hyvin määritelty laskentamenetelmä, joka saa syötteenään alkion tai joukon alkioita ja ja tuottaa tuloksenaan alkion tai joukon alkioita • Tapa tuottaa syötteestä tulos COMP.CS.300 Tietorakenteet ja algoritmit 1 19 • hyvin määritelty = – jokainen askel on kuvattu niin tarkasti, että lukija (ihminen tai kone) osaa suorittaa sen – jokainen askel on määritelty yksikäsitteisesti – samat vaatimukset pätevät askelten suoritusjärjestykselle – suorituksen tulee päättyä äärellisen askelmäärän jälkeen COMP.CS.300 Tietorakenteet ja algoritmit 1 20 Algoritmi ratkaisee jonkin hyvin määritellyn (laskenta)tehtävän. • laskentatehtävä määrittelee, missä suhteessa tulosten tulee olla annettuihin syötteisiin • esimerkiksi: – taulukon järjestäminen syötteet: jono lukuja a1, a2, . . . , an tulokset: luvut a1, a2, . . . , an suuruusjärjestyksessä pienin ensin – lentoyhteyksien etsiminen syötteet: lentoreittiverkosto eli kaupunkeja joiden välillä lentoyhteyksiä tulokset: Lentojen numerot, yhteyden tiedot ja hinta. COMP.CS.300 Tietorakenteet ja algoritmit 1 21 • laskentatehtävän esiintymä eli instanssi saadaan antamalla tehtävän syötteille lailliset arvot – järjestämistehtävän instanssiesimerkki: 31, 41, 59, 26, 41, 58 Algoritmi on oikea (correct), jos se pysähtyy ja antaa oikeat tulokset aina kun sille on annettu laillinen syöte. COMP.CS.300 Tietorakenteet ja algoritmit 1 22 • algoritmin tai laskentatehtävän määritelmä saa kieltää osan muodollisesti mahdollisista syötteistä COMP.CS.300 Tietorakenteet ja algoritmit 1 23 algoritmi voi olla virheellinen kolmella tavalla – antaa väärän lopputuloksen – kaatuu kesken suorituksen – ei koskaan lopeta virheellinenkin algoritmi on joskus hyvin käyttökelpoinen, jos virhetiheys hallitaan! – esim. luvun testaus alkuluvuksi COMP.CS.300 Tietorakenteet ja algoritmit 1 24 Periaatteessa mikä tahansa menetelmä kelpaa algoritmien esittämiseen, kunhan tulos on tarkka ja yksikäsitteinen. • yleensä algoritmit toteutetaan tietokoneohjelmina tai laitteistoina • käytännön toteutuksessa on otettava huomioon monia insinöörinäkökohtia – sopeuttaminen käyttötilanteeseen – syötteiden laillisuuden tarkistukset – virhetilanteiden käsittely – ohjelmointikielen rajoitukset – laitteiston ja kielen aiheuttamat nopeus- ja tarkoituksenmukaisuusnäkökohdat – ylläpidettävyys ⇒modulaarisuus jne. ⇒algoritmin idea hukkuu helposti toteutusyksityiskohtien alle COMP.CS.300 Tietorakenteet ja algoritmit 1 25 Tällä kurssilla keskitytään algoritmien ideoihin ja algoritmit esitetään useimmiten pseudokoodina ilman laillisuustarkistuksia, virheiden käsittelyä yms. Otetaan esimerkiksi pienten taulukoiden järjestämiseen soveltuva algoritmi INSERTION-SORT: Kuva 2: kuva: Wikipedia COMP.CS.300 Tietorakenteet ja algoritmit 1 26 • periaate: – toiminnan aikana taulukon alkuo- sa on järjestyksessä ja loppuosa ei – osien raja lähtee liikkeelle paikko- jen 1 ja 2 välistä ja etenee askel ker- rallaan taulukon loppuun • kullakin siirtoaskeleella etsitään taulu- kon alkuosasta kohta, johon loppuo- san ensimmäinen alkio kuuluu – uudelle alkiolle raivataan tilaa siirtämällä isompia alkioita askel eteenpäin – lopuksi alkio sijoitetaan paikalleen ja alkuosaa kasvatetaan pykälällä 59 41 31 58 41 26 59 41 58 41 26 59 41 31 31 26 59 58 41 41 31 26 58 59 41 41 31 26 58 41 59 41 31 26 58 41 COMP.CS.300 Tietorakenteet ja algoritmit 1 27 Kurssilla käytetyllä pseudokoodiesityksellä INSERTION-SORT näyttää tältä: INSERTION-SORT(A) (syöte saadaan taulukossa A) 1 for j := 2 to A.length do (siirretään osien välistä rajaa) 2 key := A[j] (otetaan alkuosan uusi alkio käsittelyyn) 3 i := j −1 4 while i > 0 and A[i] > key do(etsitään uudelle alkiolle oikea paikka) 5 A[i + 1] := A[i] (raivataan uudelle alkiolle tilaa) 6 i := i −1 7 A[i + 1] := key (asetetaan uusi alkio oikealle paikalleen) • for- yms. rakenteellisten lauseiden rajaus osoitetaan sisennyksillä • (kommentit) kirjoitetaan sulkuihin kursiivilla • sijoitusoperaattorina on “:=” (“=” on yhtäsuuruuden vertaaminen) • ▷-merkkillä varustettu rivi antaa ohjeet vapaamuotoisesti COMP.CS.300 Tietorakenteet ja algoritmit 1 28 • tietueen (tai olion) kenttiä osoitetaan pisteen avulla – esim. opiskelija.nimi, opiskelija.numero • osoittimen x osoittaman tietueen kenttiä osoitetaan merkin →avulla – esim. x→nimi, x→numero • ellei toisin sanota, kaikki muuttujat ovat paikallisia • taulukoilla ja / tai osoittimilla kootun kokonaisuuden nimi tarkoittaa viitettä ko. kokonaisuuteen – tuollaiset isommat tietorakenteethan aina käytännössä kannattaa välittää viiteparametreina • yksittäisten muuttujien osalta aliohjelmat käyttävät arvonvälitystä (kuten C++-ohjelmatkin oletuksena) • osoitin tai viite voi kohdistua myös ei minnekään: NIL COMP.CS.300 Tietorakenteet ja algoritmit 1 29 2.3 Algoritmien toteutuksesta Käytännön toteutuksissa teoriaa tulee osata soveltaa. Esimerkki: järjestämisalgoritmin sopeuttaminen käyttötilanteeseen. • harvoin järjestetään pelkkiä lukuja; yleensä järjestetään tietueita, joissa on – avain (key) – oheisdataa (satellite data) • avain määrää järjestyksen ⇒sitä käytetään vertailuissa • oheisdataa ei käytetä vertailuissa, mutta sitä on siirreltävä samalla kuin avaintakin COMP.CS.300 Tietorakenteet ja algoritmit 1 30 Edellisessä kappaleessa esitelty INSERTION-SORTissa muuttuisi seuraavalla tavalla, jos siihen lisättäisi oheisdata: 1 for j := 2 to A.length do 2 temp := A[j] 3 i := j −1 4 while i > 0 and A[i].key > temp.key do 5 A[i + 1] := A[i] 6 i := i −1 7 A[i + 1] := temp • jos oheisdataa on paljon, kannattaa järjestää taulukollinen osoittimia tietueisiin ja siirtää lopuksi tietueet suoraan paikoilleen COMP.CS.300 Tietorakenteet ja algoritmit 1 31 Jotta tulokseksi saataisi ajokelpoinen ohjelma, joka toteuttaa INSERTION-SORT:n tarvitaan vielä paljon enemmän. • täytyy ottaa käyttöön oikea ohjelmointikieli muuttujien määrittelyineen ja aliohjelmineen • tarvitaan pääohjelma, joka hoitaa syötteenluvun ja sen laillisuuden tutkimisen ja vastauksen tulostamisen – on tavallista, että pääohjelma on selvästi algoritmia pidempi COMP.CS.300 Tietorakenteet ja algoritmit 1 32 Ohjelmointikieli määrää usein myös muita asioita, esim: • Indeksointi alkaa 0:sta (pseudokoodissa usein 1:stä) • Käytetäänkö edes indeksointia (tai taulukoita, tai...) • (C++) Onko data oikeasti tietorakenteen sisässä, vai osoittimen päässä (jolloin dataa ei tarvitse siirtää ja sen jakaminen on helpompaa) • Jos dataan viitataan epäsuorasti muualta, tapahtuuko se – Osoittimella – Älyosoittimella (esim. shared_ptr) – Iteraattorilla (jos data tietorakenteessa) – Indeksillä (jos data vektorissa tms.) – Hakuavaimella (jos data tietorakenteessa, josta haku nopeaa) • Toteutetaanko rekursio iteroinnilla vai ei (riippuu myös ongelmasta) • Ovatko algoritmin \"parametrit\"oikeasti parametreja, vai vain muuttujia tms. COMP.CS.300 Tietorakenteet ja algoritmit 1 33 Otetaan esimerkiksi edellä kuvatun ohjelman toteutus C++:lla: #include <iostream> #include <vector> typedef std::vector<int> Taulukko; void insertionSort( Taulukko & A ) { int key = 0; int i = 0; for( Taulukko::size_type j = 1; j < A.size(); ++j ) { key = A.at(j); i = j-1; while( i >= 0 && A.at(i) > key ) { A.at(i+1) = A.at(i); --i; } A.at(i+1) = key; } } int main() { unsigned int i; // haetaan järjestettävien määrä std::cout << \"Anna taulukon koko 0...: \"; std::cin >> i; COMP.CS.300 Tietorakenteet ja algoritmit 1 34 Taulukko A(i); // luodaan taulukko // luetaan järjestettävät for( i = 0; i < A.size(); ++i ) { std::cout << \"Anna A[\" << i+1 << \"]: \"; std::cin >> A.at(i); } insertionSort( A ); // järjestetään // tulostetaan siististi for( i = 0; i < A.size(); ++i ) { if( i % 5 == 0 ) { std::cout << std::endl; } else { std::cout << \" \"; } std::cout << A.at(i); } std::cout << std::endl; } COMP.CS.300 Tietorakenteet ja algoritmit 1 35 Ohjelmakoodi on huomattavasti pseudokoodia pidempi ja algoritmille ominaisten asioiden hahmottaminen on siitä paljon vaikeampaa. Tämä kurssi keskittyy algoritmien ja tietorakenteiden periaatteisiin, joten ohjelmakoodi ei palvele tarkoituksiamme. ⇒Tästä eteenpäin toteutuksia ohjelmointikielillä ei juurikaan esitetä."
            },
            {
                "role": "user",
                "content": "Can you summarize the key topics covered in this lecture?"
            },
            {
                "role": "assistant",
                "content": "This lecture covers foundational algorithmic concepts including problem formulation, correctness proof using induction, efficiency analysis using asymptotic notation, data structures (arrays, linked lists, BSTs, heaps), and sorting methods (selection sort, merge sort, counting sort). Additionally, it introduces the Word-RAM computational model, providing a theoretical framework for measuring algorithm efficiency."
            }
        ]
    },
    {
        "system": "Hello! I am Kiran, an AI assistant from Tampere University, dedicated to providing you with accurate and insightful assistance. My goal is to help you understand concepts, summarize lectures, and answer questions effectively.",
        "conversations": [
            {
                "role": "host",
                "content": "COMP.CS.300 Tietorakenteet ja algoritmit 1 52 4 Tehokkuus ja algoritmien suunnittelu Tässä luvussa pohditaan tehokkuuden käsitettä ja esitellään kurssilla käytetty kertaluokkanotaatio, jolla kuvataan algoritmin asymptoottista käyttäytymistä eli tapaa, jolla algoritmin resurssien kulutus muuttuu syötekoon kasvaessa. t n COMP.CS.300 Tietorakenteet ja algoritmit 1 53 4.1 Kertaluokat Algoritmin analysoinnilla tarkoitetaan sen kuluttamien resurssien määrän arvioimista Tyypillisesti analysoidaan syötekoon kasvun vaikutusta algoritmin resurssien kulutukseen Useimmiten meitä kiinnostaa algoritmin ajankäytön kasvu syötteen koon kasvaessa – Voimme siis tarkastella ajankäyttöä irrallaan toteutusympäristöstä – Itse asiassa voimme kuvata periaatteessa minkä tahansa peräkkäisiä operaatioita sisältävän toiminnan ajankulutusta COMP.CS.300 Tietorakenteet ja algoritmit 1 54 – Algoritmin ajankäyttö: Algoritmin suorittamien \"askelten\" suorituskertojen määrä – Askel: syötekoosta riippumattoman operaation viemä aika. – Emme välitä siitä, kuinka monta kertaa jokin operaatio suoritetaan kunhan se tehdään vain vakiomäärä kertoja. – Tutkimme kuinka monta kertaa algoritmin suorituksen aikana kukin rivi suoritetaan ja laskemme nämä määrät yhteen. COMP.CS.300 Tietorakenteet ja algoritmit 1 55 – Yksinkertaistamme vielä tulosta poistamalla mahdolliset vakiokertoimet ja alemman asteen termit. ⇒Näin voidaan tehdä, koska syötekoon kasvaessa riittävästi alemman asteen termit käyvät merkityksettömiksi korkeimman asteen termin rinnalla. ⇒Menetelmä ei luonnollisestikaan anna luotettavia tuloksia pienillä syöteaineistoilla, mutta niillä ohjelmat ovat tyypillisesti riittävän tehokkaita joka tapauksessa. – Kutsumme näin saatua tulosta algoritmin ajan kulutuksen kertaluokaksi, jota merkitään kreikkalaisella kirjaimella Θ (äännetään “theeta”). f(n) = 23n2 + 2n + 15 ⇒f ∈Θ(n2) f(n) = 1 2n lg n + n ⇒f ∈Θ(n lg n) COMP.CS.300 Tietorakenteet ja algoritmit 1 56 Esimerkki 1: taulukon alkioiden summaus 1 for i := 1 to A.length do 2 summa := summa + A[i] – jos taulukon A pituus (syötekoko) on n, rivi 1 suoritetaan n + 1 kertaa – rivi 2 suoritetaan n kertaa – ajankulutus kasvaa siis n:n kasvaessa seuraavalla tavalla: n aika = 2n + 1 1 3 10 21 100 201 1000 2001 10000 20001 ⇒n:n arvo hallitsee ajankulutusta COMP.CS.300 Tietorakenteet ja algoritmit 1 57 – suoritamme edellä sovitut yksinkertaistukset: poistamme vakiokertoimen ja alemman asteen termin: f(n) = 2n + 1 ⇒n ⇒saamme tulokseksi Θ(n) ⇒kulutettu aika riippuu lineaarisesti syötteen koosta COMP.CS.300 Tietorakenteet ja algoritmit 1 58 Esimerkki 2: alkion etsintä järjestämättömästä taulukosta 1 for i := 1 to A.length do 2 if A[i] = key then 3 return i – tässä tapauksessa suoritusaika riippuu syöteaineiston koon lisäksi sen koostumuksesta eli siitä, mistä kohtaa taulukkoa haluttu alkio löytyy – On tutkittava erikseen: paras, huonoin ja keskimääräinen tapaus COMP.CS.300 Tietorakenteet ja algoritmit 1 59 – paras tapaus: Kuva 6: Etsintä: paras tapaus, löytyy ensimmäisestä alkiosta ⇒alkio löytyy vakioajassa eli ajankulutus on Θ(1) COMP.CS.300 Tietorakenteet ja algoritmit 1 60 – huonoin tapaus Kuva 7: Etsintä: huonoin tapaus, löytyy viimeisestä tai ei ollenkaan rivi 1 suoritetaan n + 1 kertaa ja rivi 2 n kertaa ⇒suoritusaika on lineaarinen eli Θ(n). COMP.CS.300 Tietorakenteet ja algoritmit 1 61 – keskimääräinen tapaus: täytyy tehdä jonkinlainen oletus tyypillisestä eli keskimääräisestä aineistosta: * alkio on taulukossa todennäköisyydellä p (0 ≤p ≤1) * ensimmäinen haettu alkio löytyy taulukon jokaisesta kohdasta samalla todennäköisyydellä – voimme laskea suoraan todennäköisyyslaskennan avulla, kuinka monta vertailua keskimäärin joudutaan tekemään COMP.CS.300 Tietorakenteet ja algoritmit 1 62 – todennäköisyys sille, että alkio ei löydy taulukosta on 1 - p ⇒joudutaan tekemään n vertailua (huonoin tapaus) – todennäköisyys sille, että alkio löytyy kohdasta i, on p/n ⇒joudutaan tekemään i vertailua – odotusarvoinen tarvittavien vertailujen määrä saadaan siis seuraavasti: [1 · p n + 2 · p n + · · · + i · p n · · · + n · p n] + n · (1 −p) COMP.CS.300 Tietorakenteet ja algoritmit 1 63 – oletamme, että alkio varmasti löytyy taulukosta eli p = 1, saamme tulokseksi (n+1)/2 eli Θ(n) ⇒koska myös tapaus, jossa alkio ei löydy taulukosta, on ajankäytöltään lineaarinen, voimme olla varsin luottavaisia sen suhteen, että keskimääräinen ajankäyttö on kertaluokassa Θ(n) – kannattaa kuitenkin muistaa, että läheskään aina kaikki syötteet eivät ole yhtä todennäköisiä ⇒jokaista tapausta on syytä tutkia erikseen COMP.CS.300 Tietorakenteet ja algoritmit 1 64 Esimerkki 3: kahden taulukon yhteisen alkion etsintä 1 for i := 1 to A.length do 2 for j := 1 to B.length do 3 if A[i] = B[j] then 4 return A[i] – rivi 1 suoritetaan 1 .. (n + 1) kertaa – rivi 2 suoritetaan 1 .. (n · (n + 1)) kertaa – rivi 3 suoritetaan 1 .. (n · n) kertaa – rivi 4 suoritetaan korkeintaan kerran COMP.CS.300 Tietorakenteet ja algoritmit 1 65 – nopeimmillaan algoritmi on siis silloin kun molempien taulukoiden ensimmäinen alkio on sama ⇒parhaan tapauksen ajoaika on Θ(1) – pahimmassa tapauksessa taulukoissa ei ole ainuttakaan yhteistä alkiota tai ainoastaan viimeiset alkiot ovat samat ⇒tällöin suoritusajaksi tulee neliöllinen eli 2n2 + 2n + 1 = Θ(n2) – keskimäärin voidaan olettaa, että molempia taulukoita joudutaan käymään läpi noin puoleen väliin ⇒tällöin suoritusajaksi tulee Θ(n2) (tai Θ(nm) mikäli taulukot ovat eri mittaisia) COMP.CS.300 Tietorakenteet ja algoritmit 1 66 Palataan INSERTION-SORTiin. Sen ajankäyttö: INSERTION-SORT( A ) (syöte saadaan taulukossa A) 1 for j := 2 to A.length do (siirretään osien välistä rajaa) 2 key := A[ j ] (otetaan alkuosan uusi alkio käsittelyyn) 3 i := j −1 4 while i > 0 and A[ i ] > key do (etsitään uudelle alkiolle oikea paikka) 5 A[ i + 1 ] := A[ i ] (raivataan uudelle alkiolle tilaa) 6 i := i −1 7 A[ i + 1 ] := key (asetetaan uusi alki o oikealle paikalleen) – rivi 1 suoritetaan n kertaa – rivit 2 ja 3 suoritetaan n - 1 kertaa – rivi 4 suoritetaan vähintään n - 1, enintään (2 + 3 + 4 + · · · + n - 2) kertaa – rivit 5 ja 6 suoritetaan vähintään 0, enintään (1 + 2 + 3 + 4 + · · · + n - 3) kertaa COMP.CS.300 Tietorakenteet ja algoritmit 1 67 – parhaassa tapauksessa, kun taulukko on valmiiksi järjestyksessä, koko algoritmi siis kuluttaa vähintään Θ(n) aikaa – huonoimmassa tapauksessa, kun taulukko on käänteisessä järjestyksessä, aikaa taas kuluu Θ(n2) – keskimääräisen tapauksen selvittäminen on jälleen vaikeampaa: * oletamme, että satunnaisessa järjestyksessä olevassa taulukossa olevista elementtipareista puolet ovat keskenään epäjärjestyksessä. ⇒vertailuja joudutaan tekemään puolet vähemmän kuin pahimmassa tapauksessa, jossa kaikki elementtiparit ovat keskenään väärässä järjestyksessä ⇒keskimääräinen ajankulutus on pahimman tapauksen ajankäyttö jaettuna kahdella: [(n - 1)n]/ 4 = Θ(n2)"
            },
            {
                "role": "user",
                "content": "Can you summarize the key topics covered in this lecture?"
            },
            {
                "role": "assistant",
                "content": "This lecture covers foundational algorithmic concepts including problem formulation, correctness proof using induction, efficiency analysis using asymptotic notation, data structures (arrays, linked lists, BSTs, heaps), and sorting methods (selection sort, merge sort, counting sort). Additionally, it introduces the Word-RAM computational model, providing a theoretical framework for measuring algorithm efficiency."
            }
        ]
    },
    {
        "system": "Hello! I am Kiran, an AI assistant from Tampere University, dedicated to providing you with accurate and insightful assistance. My goal is to help you understand concepts, summarize lectures, and answer questions effectively.",
        "conversations": [
            {
                "role": "host",
                "content": "COMP.CS.300 Tietorakenteet ja algoritmit 1 40 3.3 Suunnitteluperiaate: Hajota ja hallitse Suunnitteluperiaate hajota ja hallitse on todennäköisesti periaatteista kuuluisin. Se toimii useiden tunnettujen tehokkaiden algoritmien periaatteena Perusidea: • ongelma jaetaan alkuperäisen kaltaisiksi, mutta pienemmiksi osaongelmiksi. • pienet osaongelmat ratkaistaan suoraviivaisesti • suuremmat osaongelmat jaetaan edelleen pienempiin osiin • lopuksi osaongelmien ratkaisut kootaan alkuperäisen ongelman ratkaisuksi COMP.CS.300 Tietorakenteet ja algoritmit 1 41 Hajota ja hallitse on usein rekursiivinen rakenteeltaan: algoritmi kutsuu itseään osaongelmille Pienten osaongelmien ratkaisemiseksi voidaan myös hyödyntää toista algoritmia COMP.CS.300 Tietorakenteet ja algoritmit 1 42 3.4 QUICKSORT Ongelman jakaminen pienemmiksi osaongelmiksi • Valitaan jokin taulukon alkioista jakoalkioksi eli pivot-alkioksi. • Muutetaan taulukon alkioiden järjestystä siten, että kaikki jakoalkiota pienemmät tai yhtäsuuret alkiot ovat taulukossa ennen jakoalkiota ja suuremmat alkiot sijaitsevat jakoalkion jälkeen. • Jatketaan alku ja loppuosien jakamista pienemmiksi, kunnes ollaan päästy 0:n tai 1:n kokoisiin osataulukoihin. COMP.CS.300 Tietorakenteet ja algoritmit 1 43 QUICKSORT-algoritmi QUICKSORT( A, left, right ) 1 if left < right then (triviaalitapaukselle ei tehdä mitään) 2 p := PARTITION( A, left, right )(muuten jaetaan alkuosaan ja loppuosaan) 3 QUICKSORT( A, left, p −1 ) (järjestetään jakoalkiota pienemmät) 4 QUICKSORT( A, p + 1, right ) (järjestetään jakoalkiota suuremmat) Kuva 3: Jako pienempiin ja suurempiin COMP.CS.300 Tietorakenteet ja algoritmit 1 44 Pienet osaongelmat: • 0:n tai 1:n kokoiset osataulukot ovat valmiiksi järjestyksessä. Järjestyksessä olevien osataulukoiden yhdistäminen: • Kun alkuosa ja loppuosa on järjestetty on koko (osa)taulukko automaattisesti järjestyksessä. – kaikki alkuosan alkiothan ovat loppuosan alkioita pienempiä, kuten pitääkin Ositus- eli partitiointialgoritmi jakaa taulukon paikallaan vaaditulla tavalla. PARTITION( A, left, right ) 1 p := A[ right ] (otetaan pivotiksi viimeinen alkio) 2 i := left −1 (merkitään i:llä pienten puolen loppua) 3 for j := left to right −1 do (käydään läpi toiseksi viimeiseen alkioon asti) 4 if A[ j ] ≤p (jos A[ j] kuuluu pienten puolelle...) 5 i := i + 1 (... kasvatetaan pienten puolta...) 6 exchange A[ i ] ↔A[ j ] (... ja siirretään A[j] sinne) 7 exchange A[ i + 1 ] ↔A[ right ] (sijoitetaan pivot pienten ja isojen puolten väliin) 8 return i + 1 (palautetaan pivot-alkion sijainti) COMP.CS.300 Tietorakenteet ja algoritmit 1 45 3.5 MERGESORT Erinomainen esimerkki hajota ja hallitse -periaatteesta on MERGE-SORT järjestämisalgoritmi: 1. Taulukko jaetaan kahteen osaan A[1..⌊n/2⌋] ja A[⌊n/2⌋+ 1..n]. 2. Järjestetään puolikkaat rekursiivisesti 3. Limitetään järjestetyt puolikkaat järjestetyksi taulukoksi COMP.CS.300 Tietorakenteet ja algoritmit 1 46 • MERGE-SORT -ALGORITMI MERGE-SORT( A, left, right ) 1 if left < right then (jos taulukossa on alkioita...) 2 mid := ⌊( left + right )/2⌋ (... jaetaan se kahtia) 3 MERGE-SORT( A, left, mid ) (järjestetään alkuosa...) 4 MERGE-SORT( A, mid + 1, right ) (... ja loppuosa) 5 MERGE( A, left, mid, right ) (limitetään osat siten, että järjestys säilyy) COMP.CS.300 Tietorakenteet ja algoritmit 1 47 Hajota: Kuva 4: Jako osaongelmiin COMP.CS.300 Tietorakenteet ja algoritmit 1 48 Hallitse: Kuva 5: Osaongelmien ratkaisujen limitys COMP.CS.300 Tietorakenteet ja algoritmit 1 49 Eli: – jaetaan järjestettävä taulukko kah- teen osaan – jatketaan edelleen osien jakamista kahtia, kunnes osataulukot ovat 0 tai 1 alkion kokoisia – 0 ja 1 kokoiset taulukot ovat valmiiksi järjestyksessä eivätkä vaadi mitään toimenpiteitä – lopuksi yhdistetään järjestyksessä olevat osataulukot limittämällä – huomaa, että rekursiivinen algorit- mi ei toimi kuvan tavalla molemmat puolet rinnakkain 6 8 5 3 6 6 1 8 5 3 6 6 1 8 3 5 6 1 5 6 1 3 6 8 5 6 1 3 6 8 5 3 6 6 8 1 5 6 1 3 6 8 COMP.CS.300 Tietorakenteet ja algoritmit 1 50 – limityksen suorittava MERGE-algoritmi: MERGE( A, left, mid, right ) 1 for i := left to right do (käydään koko alue läpi...) 2 B[ i ] := A[ i ] (... ja kopioidaan se aputaulukkoon) 3 i := left (asetetaan i osoittamaan valmiin osan loppua) 4 j := left; k := mid + 1 (asetetaan j ja k osoittamaan osien alkuja) 5 while j ≤mid and k ≤right do (käydään läpi, kunnes jompikumpi osa loppuu) 6 if B[ j ] ≤B[ k ] then (jos alkuosan ensimmäinen alkio on pienempi...) 7 A[ i ] := B[ j ] (... sijoitetaan se tulostaulukkoon...) 8 j := j + 1 (... ja siirretään alkuosan alkukohtaa) 9 else (muuten...) 10 A[ i ] := B[ k ] (... sijoitetaan loppuosan alkio tulostaulukkoon...) 11 k := k + 1 (... ja siirretään loppuosan alkukohtaa) 12 i := i + 1 (siirretään myös valmiin osan alkukohtaa) 13 if j > mid then 14 k := 0 15 else 16 k := mid −right 17 for j := i to right do (siirretään loput alkiot valmiin osan loppuun) 18 A[ j ] := B[ j + k ] MERGE limittää taulukot käyttäen “pala kerrallaan” -menetelmää. COMP.CS.300 Tietorakenteet ja algoritmit 1 51 Tuottaako hajota ja hallitse tehokkaamman ratkaisun kuin pala kerrallaan? Ei aina, mutta tarkastellaksemme tilannetta tarkemmin, meidän täytyy tutustua algoritmin analyysiin"
            },
            {
                "role": "user",
                "content": "Can you summarize the key topics covered in this lecture?"
            },
            {
                "role": "assistant",
                "content": "This lecture covers foundational algorithmic concepts including problem formulation, correctness proof using induction, efficiency analysis using asymptotic notation, data structures (arrays, linked lists, BSTs, heaps), and sorting methods (selection sort, merge sort, counting sort). Additionally, it introduces the Word-RAM computational model, providing a theoretical framework for measuring algorithm efficiency."
            }
        ]
    },
    {
        "system": "Hello! I am Kiran, an AI assistant from Tampere University, dedicated to providing you with accurate and insightful assistance. My goal is to help you understand concepts, summarize lectures, and answer questions effectively.",
        "conversations": [
            {
                "role": "host",
                "content": "COMP.CS.300 Tietorakenteet ja algoritmit 1 75 5 Kertaluokkamerkinnät Tässä luvussa käsitellään asymptoottisessa analyysissa käytettyjä matemaattisia merkintätapoja Määritellään tarkemmin Θ, sekä kaksi muuta saman sukuista merkintää O ja Ω. COMP.CS.300 Tietorakenteet ja algoritmit 1 76 5.1 Asymptoottinen aika-analyysi Edellisessä luvussa yksinkertaistimme suoritusajan lauseketta melkoisesti: • jätimme jäljelle ainoastaan eniten merkitsevän termin • poistimme sen edestä vakiokertoimen ⇒kuvastavat algoritmin käyttäytymistä, kun syötekoko kasvaa kohti ääretöntä • siis kuvaavat asymptoottista suorituskykyä ⇒antavat käyttökelpoista tietoa vain jotain rajaa suuremmilla syötteillä • todettiin, että usein raja on varsin alhaalla ⇒Θ- yms. merkintöjen mukaan nopein on myös käytännössä nopein, paitsi aivan pienillä syötteillä COMP.CS.300 Tietorakenteet ja algoritmit 1 77 Θ-merkintä Kuva 8: Θ-merkintä COMP.CS.300 Tietorakenteet ja algoritmit 1 78 eli matemaattisesti • olkoon g(n) funktio luvuilta luvuille Θ(g(n)) on niiden funktioiden f(n) joukko, joille on olemassa positiiviset vakiot c1, c2 ja n0 siten, että aina kun n ≥n0, niin 0 ≤c1 · g(n) ≤f(n) ≤c2 · g(n) – Θ(g(n)) on joukko funktioita ⇒tulisi kirjoittaa esim. f(n) ∈Θ(g(n)) ⇒ohjelmistotieteessä vakiintunut käytäntö kuitenkin on käyttää = -merkintää COMP.CS.300 Tietorakenteet ja algoritmit 1 79 Funktion f(n) kuuluvuuden kertaluokkaan Θ(g(n)), voi siis todistaa etsimällä jotkin arvot vakioille c1, c2 ja n0 ja osoittamalla, että funktion arvo pysyy n:n arvoilla n0:sta alkaen arvojen c1g(n) ja c2g(n) välillä (eli suurempana tai yhtäsuurena kuin c1g(n) ja pienempänä tai yhtäsuurena, kuin c2g(n)). Esimerkki: 3n2 + 5n −20 = Θ(n2) • valitaan c1 = 3, c2 = 4 ja n0 = 4 • 0 ≤3n2 ≤3n2 + 5n −20 ≤4n2 kun n ≥4, koska silloin 0 ≤5n −20 ≤n2 • yhtä hyvin olisi voitu valita c1 = 2, c2 = 6 ja n0 = 7 tai c1 = 0,000 1, c2 = 1 000 ja n0 = 1 000 • tärkeää on vain, että voidaan valita jotkut positiiviset, ehdot täyttävät c1, c2 ja n0 COMP.CS.300 Tietorakenteet ja algoritmit 1 80 Tärkeä tulos: jos ak > 0, niin aknk + ak−1nk−1 + · · · + a2n2 + a1n + a0 = Θ(nk) • toisin sanoen, jos polynomin eniten merkitsevän termin kerroin on positiivinen, Θ-merkintä sallii kaikkien muiden termien sekä e.m. kertoimen abstrahoinnin pois Vakiofunktiolle pätee c = Θ(n0) = Θ(1) • Θ(1) ei kerro, minkä muuttujan suhteen funktioita tarkastellaan ⇒sitä saa käyttää vain kun muuttuja on asiayhteyden vuoksi selvä ⇒yleensä algoritmien tapauksessa on COMP.CS.300 Tietorakenteet ja algoritmit 1 81 O-merkintä Kuva 9: O-merkintä COMP.CS.300 Tietorakenteet ja algoritmit 1 82 O-merkintä on muuten samanlainen kuin Θ-merkintä, mutta se rajaa funktion ainoastaan ylhäältä. ⇒asymptoottinen yläraja Määritelmä: O(g(n)) on niiden funktioiden f(n) joukko, joille on olemassa positiiviset vakiot c ja n0 siten, että aina kun n ≥n0, niin 0 ≤f(n) ≤c · g(n) COMP.CS.300 Tietorakenteet ja algoritmit 1 83 • pätee: jos f(n) = Θ(g(n)), niin f(n) = O(g(n)) • päinvastainen ei aina päde: n2 = O(n3), mutta n2 ̸= Θ(n3) • tärkeä tulos: jos k ≤m, niin nk = O(nm) • jos hitaimman tapauksen suoritusaika on O(g(n)), niin jokaisen tapauksen suoritusaika on O(g(n)) COMP.CS.300 Tietorakenteet ja algoritmit 1 84 Usein algoritmin hitaimman (ja samalla jokaisen) tapauksen suoritusajalle saadaan pelkällä vilkaisulla jokin O-merkinnällä ilmoitettava yläraja. Usein vain yläraja onkin kiinnostava ⇒O-merkinnällä on suuri käytännön merkitys COMP.CS.300 Tietorakenteet ja algoritmit 1 85 Esimerkki: INSERTION-SORT rivi kerta-aika for j := 2 to A.length do O(n) key := A[j] · O(1) i := j −1 · O(1) while i > 0 and A[i] > key do · O(n) A[i + 1] := A[i] · · O(1) i := i −1 · · O(1) A[i + 1] := key · O(1) Jolloin pahimmalle tapaukselle saadaan suoritusaika O(n) · O(n) · O(1) = O(n2) COMP.CS.300 Tietorakenteet ja algoritmit 1 86 Ω-merkintä (äännetään “iso oomega”) Kuva 10: Ω-merkintä COMP.CS.300 Tietorakenteet ja algoritmit 1 87 Ω-merkintä on muuten täysin samanlainen kuin Θ-merkintä, mutta se rajaa funktion vain alhaalta. ⇒asymptoottinen alaraja määritelmä: Ω(g(n)) on niiden funktioiden f(n) joukko, joille on olemassa positiiviset vakiot c ja n0 siten, että aina kun n ≥n0, niin 0 ≤c · g(n) ≤f(n) COMP.CS.300 Tietorakenteet ja algoritmit 1 88 • määritelmistä seuraa tärkeä tulos: f(n) = Θ(g(n)) jos ja vain jos f(n) = O(g(n)) ja f(n) = Ω(g(n)). • jos nopeimman tapauksen suoritusaika on Ω(g(n)), niin jokaisen tapauksen suoritusaika on Ω(g(n)) Käytännön hyötyä Ω-merkinnästä on lähinnä tilanteissa, joissa jonkin ratkaisuvaihtoehdon parhaan tapauksenkin tehokkuus on epätyydyttävä, jolloin ratkaisu voidaan hylätä välittömästi. COMP.CS.300 Tietorakenteet ja algoritmit 1 89 Merkintöjen keskinäiset suhteet f(n) = Ω(g(n)) ja f(n) = O(g(n)) ⇐⇒f(n) = Θ(g(n)) Kertaluokkamerkinnöillä on samankaltaisia ominaisuuksia kuin lukujen vertailuilla: f(n) = O(g(n)) a ≤b f(n) = Θ(g(n)) a = b f(n) = Ω(g(n)) a ≥b Eli, jos f(n):n korkeimman asteen termi, josta on poistettu vakiokerroin ≤g(n): n vastaava, f(n) = O(g(n)) jne. Yksi merkittävä ero: reaaliluvuille pätee aina tasan yksi kaavoista a < b, a = b ja a > b, mutta vastaava ei päde kertaluokkamerkinnöille. ⇒Kaikkia funktioita ei pysty mielekkäällä tavalla vertaamaan toisiinsa kertaluokkamerkintöjen avulla (esim. n ja n1+sin n). COMP.CS.300 Tietorakenteet ja algoritmit 1 90 Hieman yksinkertaisten: • Jos algoritmi on Ω(g(n)), sen resurssien kulutus on ainakin kertaluokassa g(n). – vrt. kirja maksaa ainakin noin kympin. • Jos algoritmi on O(g(n)), sen resurssien kulutus on korkeintaan kertaluokassa g(n). – vrt. kirja maksaa korkeintaan noin kympin. • Jos algoritmi on Θ(g(n)), sen resurssien kulutus on aina kertaluokassa g(n). – vrt. kirja maksaa suunnilleen kympin. COMP.CS.300 Tietorakenteet ja algoritmit 1 91 Kaikkien algoritmien kaikkien tapausten suoritusajalle ei välttämättä voida antaa mitään ratkaisua Θ-notaatiolla. Esimerkkinä Insertion-Sort: • paras tapaus on Ω(n), mutta ei Ω(n2) • pahin tapaus on O(n2), mutta ei O(n) ⇒kaikille tapauksille yhteistä Θ-arvoa ei voida määrittää COMP.CS.300 Tietorakenteet ja algoritmit 1 92 Esimerkki Otetaan funktio f(n) = 3n2 + 5n + 2. Suoritetaan sille aiemmin sovitut yksinkertaistukset: • alemman asteen termit pois • vakiokertoimet pois ⇒f(n) = Θ(n2) Vakuuttuaksemme asiasta etsim- me kertoimet c1 ja c2: 3n2 ≤3n2 + 5n + 2 ≤4n2, kun n ≥6 ⇒c1 = 3, c2 = 4 ja n0 = 6 toimivat ⇒f(n) = O(n2) ja Ω(n2) ⇒f(n) = Θ(n2) 0 5 10 15 0 100 200 300 400 500 600 700 800 f(n) n² 0 5 10 15 0 100 200 300 400 500 600 700 800 n0 4n² 3n² COMP.CS.300 Tietorakenteet ja algoritmit 1 93 Selvästi kerroin c2 = 4 toimii myös kun g(n) = n3, sillä kun n ≥6, n3 > n2 ⇒f(n) = O(n3) • sama pätee kun g(n) = n4... Ja alapuolella kerroin c1 = 3 toimii myös kun g(n) = n lg n, sillä kun n ≥ 6, n2 > n lg n ⇒f(n) = Ω(n lg n) • sama pätee kun g(n) = n tai g(n) = lg n 0 5 10 15 0 100 200 300 400 500 600 700 800 n0 4n² 3n² 3n 3nlog(n) 4n³ COMP.CS.300 Tietorakenteet ja algoritmit 1 94 5.2 Suorituskykykäsitteitä Tähän mennessä algoritmien suorituskykyä on arvioitu lähinnä suoritusajan näkökulmasta. Muitakin vaihtoehtoja kuitenkin on: • Voidaan mitata myös esimerkiksi muistinkulutusta tai kaistanleveyttä. Lisäksi käytännössä tulee ottaa huomioon ainakin seuraavat seikat: • Millä mittayksiköllä resurssien kulutusta mitataan? • Miten syötekoko määritellään? • Mitataanko huonoimman, parhaan vai keskimääräisen tapauksen resurssien kulutusta? • Millaiset syötekoot tulevat kysymykseen? • Riittääkö kertaluokkamerkintöjen tarkkuus vai tarvitaanko tarkempaa tietoa? COMP.CS.300 Tietorakenteet ja algoritmit 1 95 Ajoajan mittayksiköt Valittaessa ajoajan mittayksikköä “askelta” pyritään yleensä mahdollisimman koneriippumattomaan ratkaisuun: • Todelliset aikayksiköt kuten sekunti eivät siis kelpaa. • Vakiokertoimet käyvät merkityksettömiksi. ⇒Jäljelle jää kertaluokkamerkintöjen tarkkuustaso. ⇒askeleeksi voidaan katsoa mikä tahansa enintään vakioajan vievä operaatio. • Vakioaikaiseksi tulkitaan mikä tahansa operaatio, jonka ajankulutus on riippumaton syötekoosta. • Tällöin on olemassa jokin syötteen koosta riippumaton aikamäärä, jota operaation kesto ei milloinkaan ylitä. • Yksittäisiä askeleita ovat esimerkiksi yksittäisen muuttujan sijoitus, if-lauseen ehdon testaus etc. • Askeleen rajauksen kanssa ei tarvitse olla kovin tarkka, koska Θ(1) + Θ(1) = Θ(1). COMP.CS.300 Tietorakenteet ja algoritmit 1 96 Muistin käytön mittayksiköt Tarkkoja yksiköitä ovat lähes aina bitti, tavu (8 bittiä) ja sana (jos sen pituus tunnetaan). Eri tyyppien muistin käyttö on usein tunnettu, joskin se vaihtelee vähän eri tietokoneiden ja kielten välillä. • kokonaisluku on yleensä 16 tai 32 bittiä • merkki on yleensä 1 tavu = 8 bittiä • osoitin on yleensä 4 tavua = 32 bittiä • taulukko A[1 . . . n] on usein n · <alkion koko> ⇒Tarkka muistin käytön arvioiminen on usein mahdollista, joskin huolellisuutta vaativaa. COMP.CS.300 Tietorakenteet ja algoritmit 1 97 Kertaluokkamerkinnät ovat käteviä silloin, kun tarkka tavujen laskeminen ei ole vaivan arvoista. Jos algoritmi säilyttää yhtäaikaa koko syöteaineiston, niin arvioinnissa kannattaa erottaa syöteaineiston kuluttama muisti muusta muistin tarpeesta. • Θ(1) lisämuistin tarve vs. Θ(n) lisämuistin tarve • Kannattaa kuitenkin huomata, että esimerkiksi merkkijonon etsintä syötetiedostosta ei talleta yhtäaikaa koko syöteaineistoa, vaan selaa sen läpi. COMP.CS.300 Tietorakenteet ja algoritmit 1 98 6 Pikalajittelu ja satunnaistaminen MERGE-SORTilla osaongelmiin jako oli helppoa ja ratkaisujen yhdistämisessä nähtiin paljon työtä. Tutustutaan seuraavaksi keskimäärin erittäin nopeaan järjestämisalgoritmiin QUICKSORT, jossa työ tehdään jakovaiheessa. QUICKSORTin kautta kohdataan uusi suunnitteluperiaate: satunnaistaminen. COMP.CS.300 Tietorakenteet ja algoritmit 1 99 6.1 QUICKSORT Ongelman jakaminen pienemmiksi osaongelmiksi • Valitaan jokin taulukon alkioista jakoalkioksi eli pivot-alkioksi. • Muutetaan taulukon alkioiden järjestystä siten, että kaikki jakoalkiota pienemmät tai yhtäsuuret alkiot ovat taulukossa ennen jakoalkiota ja suuremmat alkiot sijaitsevat jakoalkion jälkeen. • Jatketaan alku ja loppuosien jakamista pienemmiksi, kunnes ollaan päästy 0:n tai 1:n kokoisiin osataulukoihin. COMP.CS.300 Tietorakenteet ja algoritmit 1 100 Kertaus: QUICKSORT-algoritmi QUICKSORT( A, left, right ) 1 if left < right then (triviaalitapaukselle ei tehdä mitään) 2 p := PARTITION( A, left, right )(muuten jaetaan alkuosaan ja loppuosaan) 3 QUICKSORT( A, left, p −1 ) (järjestetään jakoalkiota pienemmät) 4 QUICKSORT( A, p + 1, right ) (järjestetään jakoalkiota suuremmat) Kuva 11: Jako pienempiin ja suurempiin COMP.CS.300 Tietorakenteet ja algoritmit 1 101 Pienet osaongelmat: • 0:n tai 1:n kokoiset osataulukot ovat valmiiksi järjestyksessä. Järjestyksessä olevien osataulukoiden yhdistäminen: • Kun alkuosa ja loppuosa on järjestetty on koko (osa)taulukko automaattisesti järjestyksessä. – kaikki alkuosan alkiothan ovat loppuosan alkioita pienempiä, kuten pitääkin Ositus- eli partitiointialgoritmi jakaa taulukon paikallaan vaaditulla tavalla. PARTITION( A, left, right ) 1 p := A[ right ] (otetaan pivotiksi viimeinen alkio) 2 i := left −1 (merkitään i:llä pienten puolen loppua) 3 for j := left to right −1 do (käydään läpi toiseksi viimeiseen alkioon asti) 4 if A[ j ] ≤p (jos A[ j] kuuluu pienten puolelle...) 5 i := i + 1 (... kasvatetaan pienten puolta...) 6 exchange A[ i ] ↔A[ j ] (... ja siirretään A[j] sinne) 7 exchange A[ i + 1 ] ↔A[ right ] (sijoitetaan pivot pienten ja isojen puolten väliin) 8 return i + 1 (palautetaan pivot-alkion sijainti) COMP.CS.300 Tietorakenteet ja algoritmit 1 102 Kuinka nopeasti PARTITION toimii? • For-silmukka tekee n - 1 kierrosta, kun n on right - left • Kaikki muut operaatiot ovat vakioaikaisia. ⇒Sen suoritusaika on Θ(n). Kuten MERGE-SORTilla QUICKSORTin analyysi ei ole yhtä suoraviivainen rekursion vuoksi COMP.CS.300 Tietorakenteet ja algoritmit 1 103 • Koska PARTITIONIA ja rekursiivista kutsua lukuunottamatta kaikki QUICKSORTIN operaatiot ovat vakioaikaisia, keskitymme tarkastelemaan PARTITIONIN instanssien käyttämää aikaa. 1 1 1 1 1 n n n 1 1 1 1 1 2 2 n − n − 1 n − n − 1 COMP.CS.300 Tietorakenteet ja algoritmit 1 104 • Kokonaisaika on edellisen kuvan esittämän puun solmujen aikojen summa. • 1 kokoiselle taulukolle suoritus on vakioaikaista. • Muille suoritus on lineaarista osataulukon kokoon nähden. ⇒Kokonaisaika on siis Θ(solmujen numeroiden summa). COMP.CS.300 Tietorakenteet ja algoritmit 1 105 Pahimman tapauksen suoritusaika • Solmun numero on aina pienempi kuin isäsolmun numero, koska jakoalkio on jo oikealla paikallaan, eikä se kuulu kumpaankaan järjestettävään osa- taulukkoon ⇒puussa voi siis olla kerroksia enintään n kappa- letta • pahin tapaus realisoituu, kun jakoalkioksi valitaan aina pienin tai suurin alkio – näin tapahtuu esimerkiksi valmiiksi järjestetyllä taulukolla • solmujen numeroiden summa on n + n - 1 + · · · + 2 + 1 ⇒QUICKSORTIN suoritusaika on O(n2) n−1 n−2 k 2 1 n COMP.CS.300 Tietorakenteet ja algoritmit 1 106 Paras tapaus realisoituu kun taulukko jakautuu aina tasan. • Alla oleva kuva näyttää kuinka osataulukoiden koot pienenevät. – harmaat ruudut ovat jo oikealla paikallaan olevia alkioita • Jokaisella tasolla tehtävä työ on kertaluokassa Θ(n). – tästä saamme pessimistisen arvion suorituspuun korkeudelle parhaassa tapauksessa ⇒O(lg n) ⇒Parhaan tapauksen suoritusajan yläraja on O(n lg n). O(lg n) O(n) O(n) O(n) O(n) O(n) COMP.CS.300 Tietorakenteet ja algoritmit 1 107 QUICKSORTIN kohdalla parhaan ja huonoimman tapauksen suoritusajat eroavat toisistaan selvästi. • Olisi mielenkiintoista tietää keskimääräinen suoritusaika. • Sen analysoiminen menee tämän kurssin tavoitteiden ulkopuolelle, mutta on osoitettu, että jos aineisto on tasajakautunutta, keskimääräinen suoritusaika on Θ(n lg n). • Keskimääräinen suoritusaika on siis varsin hyvä. COMP.CS.300 Tietorakenteet ja algoritmit 1 108 QUICKSORTIIN liittyy kuitenkin se kiusallinen seikka, että sen pahimman tapauksen suoritus on hidasta ja sen esiintyminen on käytännössä varsin todennäköistä. • On helppoa kuvitella tilanteita, joissa aineisto on jo järjestyksessä tai melkein järjestyksessä. ⇒Tarvitaan keino, jolla pahimman tapauksen systemaattisen esiintymisen riskiä saadaan pienennettyä. Satunnaistaminen on osoittautunut tässä varsin tehokkaaksi. COMP.CS.300 Tietorakenteet ja algoritmit 1 109 QUICKSORTIN etuja ja haittoja Etuja: • järjestää taulukon keskimäärin erittäin tehokkaasti – ajoaika on keskimäärin Θ(n lg n) – vakiokerroin on pieni • tarvitsee vain vakiomäärän lisämuistia • sopii hyvin virtuaalimuistiympäristöön • käyttää tehokkaasti välimuistia Haittoja: • ajoaika on hitaimmillaan Θ(n2) • hitaimman tapauksen tuottava syöte on kiusallisen tavallinen ilman satunnaistamista • rekursiivisuus ⇒tarvitsee lisämuistia pinolle • epävakaus"
            },
            {
                "role": "user",
                "content": "Can you summarize the key topics covered in this lecture?"
            },
            {
                "role": "assistant",
                "content": "This lecture covers foundational algorithmic concepts including problem formulation, correctness proof using induction, efficiency analysis using asymptotic notation, data structures (arrays, linked lists, BSTs, heaps), and sorting methods (selection sort, merge sort, counting sort). Additionally, it introduces the Word-RAM computational model, providing a theoretical framework for measuring algorithm efficiency."
            }
        ]
    },
    {
        "system": "Hello! I am Kiran, an AI assistant from Tampere University, dedicated to providing you with accurate and insightful assistance. My goal is to help you understand concepts, summarize lectures, and answer questions effectively.",
        "conversations": [
            {
                "role": "host",
                "content": "COMP.CS.300 Tietorakenteet ja algoritmit 1 119 7 C++:n standardikirjasto Tässä luvussa käsitellään C++:n standardikirjaston tietorakenteita ja algoritmeja. Tarkoituksena on käsitellä sellaisia asioita, joihin tulee kiinnittää huomiota, jotta kirjastoa tulisi käyttäneeksi tarkoituksenmukaisesti ja tehokkaasti. COMP.CS.300 Tietorakenteet ja algoritmit 1 120 7.1 Yleistä C++:n standardikirjastosta Standardikirjasto standardoitiin C++-kielen mukana syksyllä 1998, ja sitä on jonkin verran laajennettu myöhemmissä versioissa. Uusin standardiversio on C++17 vuodelta 2017. Kirjasto sisältää tärkeimmät perustietorakenteet ja algoritmit. • useimmat tämän aineiston alkupuolen tietorakenteet ja algoritmit mukana muodossa tai toisessa Rajapinnat ovat harkittuja, joustavia, geneerisiä ja tyyppiturvallisia. Rajapintojen tarjoamien operaatioiden tehokkuudet on ilmaistu O-merkinnällä. Kirjaston geneerisyys on toteutettu käännösaikaisella mekanismilla: C++:n malli (template) COMP.CS.300 Tietorakenteet ja algoritmit 1 121 Tietorakennekurssin kannalta kiinnostavin standardikirjaston elementti on ns. STL (Standard Template Library): säiliöt eli kirjaston tarjoamat tietorakenteet, geneeriset algoritmit sekä iteraattorit, joiden avulla säiliöiden alkioita käsitellään. Kuva 12: STL:n osaset C++11:n mukana tulleet lambdat ovat myös keskeisiä COMP.CS.300 Tietorakenteet ja algoritmit 1 122 7.2 Iteraattorit Kaikki standardikirjaston tietora- kenteet näyttäytyvät meille var- sin samanlaisina mustina laati- koina, joista oikeastaan tiedäm- me ainoastaan, että ne sisältä- vät tallettamiamme alkioita ja että ne toteuttavat tietyt raja- pintafunktiot. Pystymme käsittelemään säiliöi- den sisältöä ainoastaan noi- den rajapintafunktioiden sekä iteraattorien avulla. Alkio2 Alkio3 Alkio4 Alkio5 Iteraattori Iteraattori Iteraattori Alkio1 Säiliö lukeminen muuttaminen x q siirto p COMP.CS.300 Tietorakenteet ja algoritmit 1 123 Iteraattorit ovat kahvoja tai “kirjanmerkkejä” tietorakenteen alkioihin. • kukin iteraattori osoittaa joko tietorakenteen alkuun, loppuun tai kahden alkion väliin. • säiliöiden rajapinnassa on yleensä funktiot begin() ja end(), jotka palauttavat säiliön alkuun ja loppuun osoittavat iteraattorit • iteraattorin läpi pääsee käsiksi sen oikealla puolella olevaan alkioon, paitsi jos kysymyksessä on käänteisiteraattori (reverse iterator), joilloin sen läpi käsitellään vasemmanpuoleista alkiota • käänteisiteraattorille myös siirto-operaatiot toimivan käänteisesti, esimerkiksi ++ siirtää iteraattoria pykälän vasemmalle • begin():ä ja end():ä vastaavat käänteisiteraattorit saa rbegin():llä ja rend():llä COMP.CS.300 Tietorakenteet ja algoritmit 1 124 • nimensä mukaisesti iteraattoria voi siirtää säiliön sisällä, ja sen avulla säiliön voi käydä läpi • iteraattorin avulla voi lukea ja kirjoittaa • säiliöön lisättävien ja siitä poistettavien alkioiden sijainti yleensä ilmaistaan iteraattoreiden avulla Kullakin säiliöllä on oma iteraattorityyppinsä. • eri säiliöt tarjoavat erilaisia mahdollisuuksia siirtää iteraattoria nopeasti paikasta toiseen (vrt. taulukon/listan mielivaltaisen alkion lukeminen) • suunnitteluperiaatteena on ollut, että kaikkien iteraattoreille tehtävien operaatioiden tulee onnistua vakioajassa, jotta geneeriset algoritmit toimisivat luvatulla tehokkuudella riippumatta siitä mikä iteraattori niille annetaan • iteraattorit voidaan jakaa kategorioihin sen mukaan, millaisia vakioaikaisia operaatioita ne pystyvät tarjoamaan COMP.CS.300 Tietorakenteet ja algoritmit 1 125 Syöttöiteraattorin (input itera- tor) avulla voi vain lukea al- kioita, mutta ei muuttaa niitä • iteraattorin osoittaman al- kion arvon voi lukea (*p) • iteraattorin osoittaman al- kion kentän voi lukea tai kutsua sen jäsenfunktiota (p->) • iteraattoria voi siirtää as- keleen eteenpäin (++p tai p++) • iteraattoreita voi sijoittaa ja vertailla toisiinsa (p=q, p==q, p!=) input iterator * (luku), ==, !=, ++, =, −> syöttöiteraattori output iterator tulostusiteraattori * (kirjoitus), ==, !=, ++, =, −> forward iterator * (luku/kirjoitus) eteenpäin−iteraattori bidirectional iterator −− kaksisuuntainen iteraattori random access iterator +=, −=, +, −, [], <, >, <=, >= hajasaanti−iteraattori deque vector set multiset map multimap list T[] Tulostusiteraattori (output iterator) on kuten syöttöiteraattori, mutta sen avulla voi vain muuttaa alkioita. (*p=x) COMP.CS.300 Tietorakenteet ja algoritmit 1 126 Eteenpäin-iteraattori (forward iterator) on yhdistelmä syöttö- ja tulostusiteraattorien rajapinnoista. Kaksisuuntainen iteraattori (bidirectional iterator) osaa lisäksi siirtyä yhden askeleen kerrallaan taaksepäin. (--p tai p--) Hajasaanti-iteraattori (random access iterator) on kuin kaksisuuntainen iteraattori, mutta sitä voi lisäksi siirtää mielivaltaisen määrän eteen- tai taaksepäin. • iteraattoria voi siirtää n askelta eteen- tai taaksepäin (p+=n, p-=n, q=p+n, q=p-n) • iteraattorista n:n alkion päässä olevan alkion pystyy lukemaan ja sitä pystyy muokkaamaan (p[n]) • kahden iteraattorin välisen etäisyyden pystyy selvittämään (p-q) • iteraattoreiden keskinäistä suuruusjärjestystä voi vertailla, iteraattori on toista “pienempi”, jos sen paikka on säiliössä ennen tätä (p<q, p<=q, p>q, p>=q) COMP.CS.300 Tietorakenteet ja algoritmit 1 127 Iteraattorit ovat osoitinabstraktio →Iteraattorien operaatioiden syntaksi muistuttaa selvästi C++:n osoitinsyntaksia. Iteraattorit saadaan otettua käyttöön komennolla #include <iterator> Oikean tyyppinen iteraattori saadaan luotua esimerkiksi seuraavalla syntaksilla. säiliö<talletettava tyyppi>::iterator p; Iteraattoreiden kanssa avainsana auto on käyttökelpoinen: auto p = begin( säiliö ); // →std::vector<std::string>::iterator Säiliöihin tehtävät poistot ja lisäykset saattavat mitätöidä säiliöön jo osoittavia iteraattoreita. • ominaisuus on säiliö-kohtainen, joten sen yksityiskohdat käsitellään säiliöiden yhteydessä COMP.CS.300 Tietorakenteet ja algoritmit 1 128 Tavallisten iteraattorien lisäksi STL tarjoaa joukon iteraattorisovittimia. • niiden avulla voidaan muunnella geneeristen algoritmien toiminnallisuutta • edellä mainitut käänteisiteraattorit (reverse iterator) ovat iteraattorisovittimia • lisäysiteraattorit (insert iterator/inserter) ovat tärkeitä iteraattorisovittimia. – ne ovat tulostusiteraattoreita, jotka lisäävät alkioita halutulle paikalle säiliöön kopioimisen sijasta – säiliön alkuun lisäävän iteraattorin saa funktiokutsulla front_inserter(säiliö) – säiliön loppuun lisäävän iteraattorin saa funktiokutsulla back_inserter(säiliö) – annetun iteraattorin kohdalle lisäävän iteraattorin saa funktiokutsulla inserter(säiliö, paikka) COMP.CS.300 Tietorakenteet ja algoritmit 1 129 • virtaiteraattorit (stream iterator) ovat syöttö- ja tulostusiteraattoreita, jotka käyttävät säiliöiden sijaista C++:n tiedostovirtoja – cin virrasta haluttua tyyppiä lukevan syöttöiteraattorin saa syntaksilla istream_iterator<tyyppi> (cin) – cout virtaan haluttua tyyppiä pilkuilla erotettuna tulostavan tulostusiteraattorin saa syntaksilla ostream_iterator<tyyppi> (cout, ’,’) • siirtoiteraattorit (move iterator) muuttavat alkion kopioinnin iteraattorin avulla alkion siirtämiseksi. COMP.CS.300 Tietorakenteet ja algoritmit 1 130 7.3 Säiliöt Standardikirjaston säiliöt kuuluvat pääsääntöisesti kahteen kategoriaan rajapintojensa puolesta: • sarjat (sequence) – alkioita pystyy hakemaan niiden järjestysnumeron perusteella – alkioita pystyy lisäämään ja poistamaan halutusta kohdasta – alkioita pystyy selaamaan järjestyksessä • assosiatiiviset säiliöt (associative container) – alkiot sijoitetaan säiliöön avaimen määräämään kohtaan – talletettavien alkioiden avainten arvoja pitää pystyä vertaamaan toisiinsa oletusarvoisesti operaattorilla < järjestetyissä säiliöissä • Rajapintojen tarjoamista jäsenfunktiosta näkee, mikä säiliön mielekäs käyttötarkoitus on COMP.CS.300 Tietorakenteet ja algoritmit 1 131 Kirjaston säiliöt: Säiliötyyppi Kirjasto Sarjat array vector deque list (forward_list) Assosiatiiviset map set Järjestämättömät unordered_map assosiatiiviset unordered_set Säiliösovittimet queue stack COMP.CS.300 Tietorakenteet ja algoritmit 1 132 Säiliöt noudattavat arvon välitystä. • säiliö ottaa talletettavasta datasta kopion • säiliö palauttaa kopioita sisältämästään tiedosta ⇒säiliön ulkopuolella tehtävät muutokset eivät vaikuta säiliön sisältämään dataan • kaikilla säiliöihin talletettavilla alkioilla tulee olla kopiorakentaja ja sijoitusoperaattori. – perustyypeillä sellaiset ovat valmiina • itse määriteltyä tyyppiä olevat alkiot kannattaa tallettaa osoittimen päähän – näinhän kannattaisi tehdä tehokkuussyistä joka tapauksessa COMP.CS.300 Tietorakenteet ja algoritmit 1 133 • C++11 tarjoaa kätevän työkalun muistinhallinnan helpottamiseksi shared_pointer tilanteissa, joissa useampi taho tarvitsee resurssia – sisältää sisäänrakennetun viitelaskurin ja tuhoaa alkion kun viitelaskuri nollautuu – deleteä ei tarvitse eikä saakaan kutsua – luominen: auto pi = std::make_shared<Olio>(params); – näppärä erityisesti jos halutaan tehdä kahden avaimen mukaan järjestetty tietorakenne: * sijoitetaan oheisdata shared_pointerin päähän * sijoitetaan osoittimet kahteen eri säiliöön kahden eri avaimen mukaan COMP.CS.300 Tietorakenteet ja algoritmit 1 134 Sarjat: Taulukko array<tyyppi> on vakiokokoinen taulukko. • Luodaan std::array<tyyppi, koko> a = {arvo, arvo,...}; • Indeksointi jäsenfunktiolla .at() tai []-operaatiolla. Funktioilla front() ja back() voidaan käsitellä ensimmäistä ja viimeistä alkiota. • Tarjoaa iteraattorit ja käänteisiteraattorit • empty(), size() ja max_size() • Funktion data() avulla päästään suoraan käsiksi sisällä olevaan taulukkoon Taulukon operaatiot ovat vakioaikaisia, mutta fill() ja swap() ovat O(n) COMP.CS.300 Tietorakenteet ja algoritmit 1 135 Vektori vector<tyyppi> on lopustaan joustavarajainen taulukko • Luodaan vector<int> v {arvo, arvo, ...}; • Vakioaikainen indeksointi .at(), [] sekä (tasatusti) vakioaikainen lisäys push_back() ja poisto pop_back() vektorin lopussa • alkion lisääminen muualle insert():illä ja poistaminen erase:lla on lineaarista, O(n) • emplace_back(args); rakentaa alkion suoraan vektoriin • Vektorin kokoa voi kasvattaa funktiolla .resize(koko, alkuarvo); – alkuarvo on vapaaehtoinen – tarvittaessa vektori varaa lisää muistia automaattisesti – muistia voi varata myös ennakkoon: .reserve(koko), .capacity() • iteraattorien mitätöitymistä tapahtuu seuraavissa tilanteissa – mikäli vectorille ei ole etukäteen varattu riittävää tilaa, voi mikä tahansa lisäys aiheuttaa kaikkien iteraattoreiden mitätöitymisen COMP.CS.300 Tietorakenteet ja algoritmit 1 136 – poistot aiheuttavat mitätöitymisen ainoastaan poistopaikan jälkeen tuleville iteraattoreille – lisäykset keskelle aiheuttavat aina lisäyspaikan jälkeen tulevien iteraattoreiden mitätöitymisen • vector<bool>:ille on määritelty erikoistoteutus, joka poikkeaa siitä mitä yleinen toteutus tekisi muistinkäytön tehostamiseksi – tavoite: mahdollistaa 1 bitti / alkio, kun tavallinen toteutus luultavasti veisi 1 tavu / alkio eli 8 bittiä / alkio COMP.CS.300 Tietorakenteet ja algoritmit 1 137 Pakka deque<tyyppi> on molemmista päistään avoin taulukko • luodaan deque<tyyppi> d {arvo, arvo, arvo...}; • Rajapinta vektorin kanssa yhtenevä, mutta tarjoaa tehokkaan (O(1) tasattu suoritusaika) lisäyksen ja poiston molemmissa päissä: .push_front(alkio), .emplace_front(args), .pop_front() • iteraattorien mitätöitymistä tapahtuu seuraavissa tilanteissa – kaikki lisäykset voivat mitätöidä iteraattorit – poistot keskelle mitätöivät kaikki iteraattorit – kaikki paitsi päihin kohdistuvat lisäys- ja poisto-operaatiot voivat mitätöidä viitteet ja osoittimet COMP.CS.300 Tietorakenteet ja algoritmit 1 138 Lista on säiliö, joka tukee kaksisuuntaista iterointia • luodaan list<tyyppi> l {arvo, arvo, arvo }; • lisäys ja poisto kaikkialla vakioaikaista, indeksointioperaatiota ei ole • lisäys ja poisto eivät mitätöi iteraattoreita ja viitteitä (paitsi tietysti poistettuihin alkioihin) • listalla on monenlaisia erikoispalveluja – .splice(paikka, toinen_lista) siirtää toisen listan nykyisen osaksi paikan eteen, O(1). – .splice(paikka, lista, alkio) siirtää alkion toisesta tai samasta listasta paikan eteen, O(1). – .splice(paikka, lista, alku, loppu) siirtää välin alkiot paikan eteen, O(1) tai lineaarinen – .merge(toinen_lista) ja .sort(), vakaa, keskimäärin O(nlogn) – .reverse(), lineaarinen COMP.CS.300 Tietorakenteet ja algoritmit 1 142 Assosiatiiviset säiliöt: Joukko set<tyyppi> ja monijoukko multiset<tyyppi> on dynaaminen joukko, josta voi • etsiä, lisätä ja poistaa logaritmisessa ajassa • selata suuruusjärjestyksessä tasatussa vakioajassa siten että läpikäynti alusta loppuun on aina lineaarinen operaatio • alkioilla on oltava suuruusjärjestys “<” • voi määritellä erikseen joko osana tyyppiä tai rakentajan parametrina • tutkii yhtäsuuruuden kaavalla ¬(x<y ∨y<x) ⇒“<” määriteltävä järkevästi ja tehokkaaksi Monijoukossa sama alkio voi olla moneen kertaan, joukossa ei COMP.CS.300 Tietorakenteet ja algoritmit 1 143 Luodaan std::set<tyyppi> s {arvo, arvo, arvo...}; • alkion arvon muuttaminen on pyritty estämään – sen sijaan pitää poistaa vanha alkio ja lisätä uusi • mielenkiintoisia operaatioita: – .find(alkio) etsii alkion (monijoukolle ensimmäisen monesta), tai palauttaa .end() jollei löydä – .lower_bound(alkio) etsii ensimmäisen, joka on ≥alkio – .upper_bound(alkio) etsii ensimmäisen, joka on > alkio – .equal_range(alkio) palauttaa make_pair( .lower_bound(alkio), .upper_bound(alkio) ), mutta selviää yhdellä etsinnällä (joukolle ko. välin leveys on 0 tai 1) – joukoille insert palauttaa parin (paikka, lisättiin), koska alkiota ei saa lisätä, jos se on jo joukossa • standardi lupaa, että iteraattorit eivät vanhene lisäyksessä ja poistossa (paitsi tietysti poistettuihin alkioihin kohdistuneet) COMP.CS.300 Tietorakenteet ja algoritmit 1 144 Kuvaus map<avaimen_tyyppi, alkion_tyyppi> ja monikuvaus multimap<avaimen_tyyppi, alkion_tyyppi> • alkiot avain-oheisdatapareja – parin tyyppi on pair<tyyppi1, tyyppi2> – parin voi tehdä funktiolla make_pair:illä – parin kentät saa operaatioilla .first(), .second() • luodaan std::map m<avain_tyyppi, alkio_tyyppi> m {{avain1, arvo1}, {avain2, arvo2}, avain3, arvo3},...}; esim. std::map<std::string,int> anim { {\"bear\",4}, {\"giraffe\",2}, {\"tiger\",7} }; • map:ia voi poikkeuksellisesti indeksoida avaimen avulla O(logn) – Jos avainta ei löydy, lisää arvoparin avain-arvo rakenteeseen • nytkään iteraattorit eivät vanhene lisäyksessä ja poistossa COMP.CS.300 Tietorakenteet ja algoritmit 1 145 Hajautustaulu, Unordered set/multiset, joka sisältää joukon alkioita ja unordered map/multimap, joka sisältää joukon alkioita, jotka assosioidaan avainarvojoukolle. • unordered map/set muistuttavat rajapinnaltaan mapia ja setia • tärkeimmät erot: – alkiot eivät ole järjestyksessä (unordered) – lisäys, poisto ja etsintä ovat keskimäärin vakioaikaisia ja pahimmassa tapauksessa lineaarisia – tarjoavat hajautuksen kannalta olennaisia funktioita, kuten rehash(koko), load_factor(), hash_function() ja bucket_size(). COMP.CS.300 Tietorakenteet ja algoritmit 1 146 • hajautustalun kokoa kasvatetaan automaattisesti, jotta lokeroiden keskimääräinen täyttöaste saadaan pidettyä sovitun rajan alapuolella – hajautustaulun koon muuttaminen (rehashing) on keskimäärin lineaarinen pahimmillaan neliöllinen operaatio – koon muuttaminen mitätöi kaikki iteraattorit, muttei osoittimia eikä viitteitä COMP.CS.300 Tietorakenteet ja algoritmit 1 147 Lisäksi Standardikirjastosta löytyy joitakin muita säiliöitä: Bittivektori bitset<bittien_määrä> • #include<bitset> • tarkoitettu kiinteän kokoisten binääristen bittisarjojen käsittelyyn • tarjoaa tyypillisiä binäärisiä operaatioita (AND, OR, XOR, NOT) Merkkijonot string • #include<string> • vaikka C++:n merkkijonot on optimoitu muuhun tarkoitukseen eikä niitä yleensä ajatella säiliöinä, ne ovat muun lisäksi säiliöitäkin • säilövät merkkejä, mutta saadaan säilömään muutakin • niillä on mm. iteraattorit, [. . . ], .at(. . . ), .size(), .capacity() ja swap • merkkijonot voivat kasvaa hyvin suuriksi ja varaavat tarvittaessa automaattisesti lisää muistia COMP.CS.300 Tietorakenteet ja algoritmit 1 148 • merkkijonojen muokkausoperaatioiden (katenointi, poisto) kanssa kannattaa olla varovainen, koska niissä suoritetaan muistinvarausta ja kopiointia, minkä vuoksi ne ovat pitkille merkkijonoille varsin raskaita • usein on muutenkin järkevää sijoittaa merkkijonot esimerkiksi osoittimen päähän sijoitettaessa niitä säiliöihin, jottei niitä turhaan kopioitaisi • samasta syystä merkkijonot tulee välittää viiteparametreina Säiliöiden lisäksi STL tarjoaa joukon säiliösovittimia, jotka eivät itsessään ole säiliöitä, mutta joiden avulla säiliön rajapinnan saa “sovitettua toiseen muottiin”: COMP.CS.300 Tietorakenteet ja algoritmit 1 149 Pino stack<alkion_tyyppi, säiliön_tyyppi> • tarjoaa normaalien luokka-operaatioiden lisäksi vain – pino-operaatiot, .push(. . . ), .top(), .pop() – koon kyselyt .size() ja .empty() – vertailut “==”, “<” jne. • .pop() ei palauta mitään, ylimmän alkion saa tarkasteltavaksi .top():illa • pinon ylintä alkiota voi muuttaa paikallaan: pino.top() = 35; • kurssin kannalta kiinnostavaa on, että käyttäjä voi valita taulukkoon tai listaan perustuvan toteutuksen – mikä tahansa säiliö, joka tarjoaa back(), push_back() ja pop_back() käy, erityisesti vector, list ja deque. – stack<tyyppi> perus_pino; (deque) – stack<tyyppi, list<tyyppi> > lista_pino; COMP.CS.300 Tietorakenteet ja algoritmit 1 150 Jono queue<alkion_tyyppi, säiliön_tyyppi> • jono-operaatiot .push(. . . ), .pop(), .front(), .back()(!) • mikä tahansa säiliö, joka tarjoaa front(), back(), push_back() ja pop_front käy • muuten kutakuinkin samanlainen kuin pino Prioriteettijono priority_queue<alkion_tyyppi,säiliön_tyyppi> • lähes täysin samanlainen rajapinta kuin jonolla • toteutus kekona • mikä tahansa säiliö, jolla front(), push_back() ja pop_back() ja hajasaanti-iteraattoreita tukeva käy, erityisesti vector (oletus) ja deque • alkioilla eri järjestys: .top() palauttaa suurimman • yhtäsuurista palauttaa minkä vain • ylintä alkiota ei voi muuttaa top:in avulla paikallaan • kuten assosiatiivisilla säiliöillä, järjestämisperiaatteen voi antaa <>-parametrina tai rakentajan parametrina (strict weak ordering) COMP.CS.300 Tietorakenteet ja algoritmit 1 151 tieto- lisäys lisäys 1. alkion n:s alkio tietyn suurimman rakenne loppuun muualle poisto poisto (indeks.) etsintä poisto array O(1) O(n)[2] vector O(1) O(n) O(n) O(n)[1] O(1) O(n)[2] O(n)[3] list O(1) O(1) O(1) O(1) O(n) O(n) O(n)[3] deque O(1) O(n)[4] O(1) O(n)[1] O(1) O(n)[2] O(n)[3] stack[9] O(1) O(1)[5] queue[9] O(1)[6] O(1)[7] priority O(log n) O(log n)[8] queue[9] [10] set O(log n) O(log n) O(log n) O(n) O(log n) O(log n) (multiset) map O(log n) O(log n) O(log n) O(n) O(log n) O(log n) (multimap) unordered_ O(n) O(n) O(n) O(n) (multi)set ≈Θ(1) ≈Θ(1) ≈Θ(1) O(n) unordered_ O(n) O(n) O(n) O(n) (multi)map ≈Θ(1) ≈Θ(1) ≈Θ(1) O(n) COMP.CS.300 Tietorakenteet ja algoritmit 1 152 [1] vakioaikainen viimeiselle alkiolle, muuten lineaarinen [2] logaritminen jos tietorakenne on järjestetty, muuten lineaarinen [3] vakioaikainen jos tietorakenne on järjestetty, muuten lineaarinen [4] vakioaikainen ensimmäiselle alkiolle, muuten lineaarinen [5] mahdollinen vain viimeiselle alkiolle [6] vain alkuun lisääminen on mahdollista [7] vain lopusta poistaminen on mahdollista [8] kysyminen vakioajassa, poistaminen logaritmisessa ajassa [9] säiliösovitin [10] lisäys tapahtuu automaattisesti kekojärjestyksen mukaiselle paikalle"
            },
            {
                "role": "user",
                "content": "Can you summarize the key topics covered in this lecture?"
            },
            {
                "role": "assistant",
                "content": "This lecture covers foundational algorithmic concepts including problem formulation, correctness proof using induction, efficiency analysis using asymptotic notation, data structures (arrays, linked lists, BSTs, heaps), and sorting methods (selection sort, merge sort, counting sort). Additionally, it introduces the Word-RAM computational model, providing a theoretical framework for measuring algorithm efficiency."
            }
        ]
    },
    {
        "system": "Hello! I am Kiran, an AI assistant from Tampere University, dedicated to providing you with accurate and insightful assistance. My goal is to help you understand concepts, summarize lectures, and answer questions effectively.",
        "conversations": [
            {
                "role": "host",
                "content": "COMP.CS.300 Tietorakenteet ja algoritmit 1 153 7.4 Geneeriset algoritmit Standardikirjasto tarjoaa useimmat tähän mennessä käsitellyistä algoritmeista. Algoritmit on kaikki toteutettu funktiomalleina, jotka saavat kaikki tarvitsemansa tiedon käsiteltävistä säiliöistä parametrien avulla. Algoritmeille ei kuitenkaan koskaan anneta parametrina kokonaisia säiliöitä vaan ainoastaan iteraattoreita niihin. • algoritmeilla voidaan käsitellä myös säiliön osia kokonaisten säiliöiden sijasta • algoritmi voi saada parametrinaan iteraattoreita erityyppisiin säiliöihin, jolloin yhdellä funktiokutsulla voidaan yhdistää esimerkiksi vectorin ja listan sisällöt ja tallettaa tulos joukkoon • algoritmien toimintaa voidaan muuttaa iteraattorisovittimien avulla • ohjelmoija voi toteuttaa omiin tietorakenteisiinsa iteraattorit, jonka jälkeen algoritmit toimivat myös niille COMP.CS.300 Tietorakenteet ja algoritmit 1 154 Kaikkia algoritmeja ei kuitenkaan pystytä suorittamaan kaikille tietorakenteille tehokkaasti. ⇒osa algoritmeista hyväksyy parametreikseen vain tietyn iteraattorikategorian iteraattoreita. • tämä takaa algoritmien tehokkuuden, koska kaikki iteraattorin tarjoamat operaatiot ovat vakioaikaisia • jos iteraattori on väärää tyyppiä, annetaan käännösaikainen virhe-ilmoitus ⇒jos algoritmille annetaan tietorakenne, jolle sitä ei voida toteuttaa tehokkaasti, se ei edes käänny Standardikirjaston algoritmit ovat kirjastossa algorithm. Lisäksi standardi määrittelee C-kielen algoritmikirjaston cstdlib. jakaa algoritmit kolmee pääryhmään: muuttamattomat sarjalliset operaatiot, muuttavat sarjalliset operaatiot ja järjestäminen sekä siihen liittyvät operaatiot. Seuraavaksi lyhyt kuvaus joistakin kurssin kannalta kiinnostavimmista algoritmeista (näiden lisäksi on vielä COMP.CS.300 Tietorakenteet ja algoritmit 1 155 runsaasti suoraviivaisia selaamiseen yms. perustuvia algoritmeja): COMP.CS.300 Tietorakenteet ja algoritmit 1 156 Puolitushaku • binary_search(eka, loppu, arvo) kertoo onko arvo järjestetyssä jononpätkässä – eka ja loppu ovat iteraattoreita, jotka osoittavat etsittävän alueen alkuun ja loppuun, muttei välttämättä säiliön alkuun ja loppuun • samaa arvoa voi olla monta peräkkäin ⇒lower_bound ja upper_bound palauttavat sen alueen rajat, jolla on arvoa – alaraja on, yläraja ei ole mukana alueessa • rajat saa myös pariksi yhdistettynä yhdellä etsinnällä: equal_range • vertaa BIN-SEARCH sivu 74 COMP.CS.300 Tietorakenteet ja algoritmit 1 157 Järjestämisalgoritmit • sort(alku, loppu) ja stable_sort(alku, loppu) • sortin suoritusaika O(nlogn) ja stable_sortin O(nlogn) jos tarpeeksi lisämuistia on saatavilla, muuten O(nlog2n) • järjestelyalgoritmit vaativat parametreikseen hajasaanti-iteraattorit ⇒eivät toimi listoille, mutta niissä on oma sort (ja ei-kopioiva merge) jäsenfunktiona • löytyy myös järjestäminen, joka lopettaa, kun halutun mittainen alkuosa on järjestyksessä: partial_sort(alku, keski, loppu) • lisäksi is_sorted(alku, loppu) ja is_sorted_until(alku, loppu) nth_element( eka, ¨ann¨as, loppu ) • etsii alkion, joka järjestetyssä säiliössä olisi kohdalla ¨ann¨as • muistuttaa algoritmia RANDOMIZED-SELECT • iteraattoreiden tulee olla hajasaanti-iteraattoreita COMP.CS.300 Tietorakenteet ja algoritmit 1 158 Ositus (partitiointi) • partition(eka, loppu, ehtofunktio) epävakaa, erikseen stable_partition. • stable_partition(eka, loppu, ehtofunktio) vakaa, mutta hitaampi ja/tai varaa enemmän muistia • järjestää välillä eka - loppu olevat alkiot siten, että ensin tulevat alkiot, joille ehtofunktio palauttaa true ja sitten, ne joille se palauttaa false. • vrt. QUICK-SORTn yhteudessä esitelty PARTITION • partition on tehokkuudeltaan lineaarinen • lisäksi is_partitioned ja partition_point COMP.CS.300 Tietorakenteet ja algoritmit 1 159 merge( alku1, loppu1, alku2, loppu2, maali) • Algoritmi limittää välien alku1 - loppu1 ja alku2 - loppu2 alkiot ja kopioi ne suuruusjärjestyksessä iteraattorin maali päähän • algoritmi edellyttää, että alkiot yhdistettävillä väleillä ovat järjestyksessä • vertaa sivun 45 MERGE • algoritmi on lineaarinen • alku- ja loppu-iteraattorit ovat syöttöiteraattoreita ja maali on tulostusiteraattori Keot • STL:stä löytyy myös vastineet luvun 3.1 kekoalgoritmeille • push_heap( eka, loppu) HEAP-INSERT • pop_heap( eka, loppu ) vaihtaa huippualkion viimeiseksi (eli paikkaan loppu −1) ja ajaa HEAPIFY:n osalle eka . . . loppu −1 – vrt. HEAP-EXTRACT-MAX • make_heap( eka, loppu) BUILD-HEAP COMP.CS.300 Tietorakenteet ja algoritmit 1 160 • sort_heap( eka, loppu ) HEAPSORT • lisäksi is_heap ja is_heap_until • iteraattoreiden tulee olla hajasaanti-iteraattoreita Joukko-operaatiot • C++:n standardikirjasto sisältää tätä tukevia funktioita • includes( eka1, loppu1, eka2, loppu2 ) osajoukko ⊆ • set_union( eka1, loppu1, eka2, loppu2, tulos ) unioni ∪ • set_intersection(. . . ) leikkaus ∩ • set_difference(. . . ) erotus - • set_symmetric_difference(. . . ) • alku- ja loppu-iteraattorit ovat syöttöiteraattoreita ja tulos on tulostusiteraattori find_first_of( eka1, loppu1, eka2, loppu2 ) • lopussa voi lisäksi olla tutkittavia alkioita rajaava ehto COMP.CS.300 Tietorakenteet ja algoritmit 1 161 • etsii ensimmäisestä jonosta ensimmäisen alkion, joka on myös toisessa jonossa • jono voi olla taulukko, lista, joukko, . . . • yksinkertainen toteutus on hitaimmillaan Θ(nm), missä n ja m ovat jonojen pituudet • toinen jono selataan jokaiselle ensimmäisen jonon alkiolle – hitain tapaus kun ei löydy ⇒hidasta, jos molemmat jonot pitkiä • toteutus saataisiin yksinkertaiseksi, nopeaksi ja muistia säästäväksi vaatimalla, että jonot ovat järjestyksessä HUOM! Mikään STL:n algoritmi ei automaattisesti tee säiliöihin lisäyksiä eikä poistoja, vaan ainoastaan muokkaa olemassa olevia alkioita. • esimerkiksi merge ei toimi, jos sille annetaan tulostusiteraattoriksi iteraattori tyhjän säiliön alkuun • jos tulostusiteraattorin halutaan tekevän lisäyksiä kopioinnin sijasta, tulee käyttää iteraattorisovitinta lisäysiteraattori COMP.CS.300 Tietorakenteet ja algoritmit 1 162 7.5 Lambdat: [](){} Algoritmikirjaston yhteydessä on paljon tilanteita, joissa on tarve välittää funktiolle toiminnallisuutta – esim. find_if, for_each, sort Lambdat ovat nimettömiä, määrittelemättömän tyyppisiä funktion kaltaisia. Ne ottavat parametreja, palauttavat paluuarvon ja pystyvät viittaamaan luontiympäristönstä muuttujiin sekä muuttamaan niitä. Syntaksi: [ympäristö](parametrit)->paluutyyppi {runko} – Jos lambda ei viittaa ympäristöönsä ympäristö on tyhjä – parametrit voi puuttua – jos ->paluutyyppiä ei ole annettu, se on void. Yksittäisestä return-lauseesta se voidaan päätellä – esim. [](int x, int y){ return x+y;} for_each( v.begin(), v.end(),[] (int val) {cout<<val<<endl;}); std::cin >> raja; //paikallinen muuttuja std:find_if(v.begin(), v.end(),[raja](int a){return a<raja;}); COMP.CS.300 Tietorakenteet ja algoritmit 1 163 STL:n algoritmeja voi ajatella nimettyinä erityissilmukoina, joiden runko lambda on bool kaikki = true; for (auto i : v) { if (i%10 != 0) { kaikki = false; break; } } if (kaikki) {...} if (std::all_of(v.begin(), v.end(), [](int i){return i%10==0;}){...}"
            },
            {
                "role": "user",
                "content": "Can you summarize the key topics covered in this lecture?"
            },
            {
                "role": "assistant",
                "content": "This lecture covers foundational algorithmic concepts including problem formulation, correctness proof using induction, efficiency analysis using asymptotic notation, data structures (arrays, linked lists, BSTs, heaps), and sorting methods (selection sort, merge sort, counting sort). Additionally, it introduces the Word-RAM computational model, providing a theoretical framework for measuring algorithm efficiency."
            }
        ]
    },
    {
        "system": "Hello! I am Kiran, an AI assistant from Tampere University, dedicated to providing you with accurate and insightful assistance. My goal is to help you understand concepts, summarize lectures, and answer questions effectively.",
        "conversations": [
            {
                "role": "host",
                "content": "COMP.CS.300 Tietorakenteet ja algoritmit 1 169 8.1 Puu Ennen kuin käydään käsiksi kekoon, määritellään sen tueksi käsite puu. Puu on: lehtiä sisäsolmuja juuri • rakenne, joka koostuu solmuista, joilla on mielivaltainen määrä lapsia. COMP.CS.300 Tietorakenteet ja algoritmit 1 170 • Binääripuussa lasten määrä on rajoitettu välille 0–2. Tällöin lapset nimetään vasen (left) ja oikea (right) • solmu on lapsiensa isä (parent) • lapseton solmu on lehti (leaf), ja muut solmut ovat sisäsolmuja (internal node) • puussa on korkeintaan yksi solmu, jolla ei ole isää. Isätön solmu on puun juuri (root). – kaikki muut solmut ovat juuren lapsia, lastenlapsia jne. COMP.CS.300 Tietorakenteet ja algoritmit 1 171 • puun rakenne on rekursiivinen: kunkin solmun jälkeläiset muodostavat puun alipuun, jonka juuri kyseinen solmu on Kuva 13: Binääripuun rekursiivisuus • puun solmun korkeus (height) on pisimmän solmusta COMP.CS.300 Tietorakenteet ja algoritmit 1 172 suoraan alas lehteen vievän polun pituus – pituus lasketaan kaarien mukaan, jolloin lehden korkeus on 0 • puun korkeus on sen juuren korkeus • puu on täydellisesti tasapainotettu (completely balanced), jos sen juuren lasten määräämien alipuiden korkeudet eroavat toisistaan enintään yhdellä, ja alipuut on täydellisesti tasapainotettu • n-solmuisen puun korkeus on vähintään ⌊lg n⌋ja korkeintaan n - 1 (logaritmin kantaluku riippuu lasten maksimimäärästä) ⇒O(n) ja Ω(lg n) COMP.CS.300 Tietorakenteet ja algoritmit 1 173 Puun solmut voidaan käsitellä monessa eri järjestyksessä. • esijärjestys (preorder) eli ensin käsi- tellään juuri, sitten rekursiivisesti lap- set. – kutsu: PREORDER-TREE-WALK(T.root) – esimerkin käsittelyjärjestys on 18, 13, 8, 5, 3, 6, 9, 15, 14, 25, 22, 23, 30, 26, 33, 32, 35 PREORDER-TREE-WALK(x) 1 if x ̸= NIL then 2 käsittele alkio x 3 for child in x→children do 4 PREORDER-TREE-WALK(child) 13 25 33 lehtiä sisäsolmuja juuri 14 18 9 23 5 15 3 8 35 32 26 22 30 6 COMP.CS.300 Tietorakenteet ja algoritmit 1 174 • välijärjestys (inorder) – välijärjestys koskee lähinnä bi- nääripuuta, siinä käsitellään en- sin rekursiivisesti vasen lapsi, sitten juuri ja lopuksi rekursiivisesti oikea lapsi – esimerkissä 3, 5, 6, 8, 9, 13, 14, 15, 18, 22, 23, 25, 26, 30, 32, 33, 35 INORDER-TREE-WALK(x) 1 if x ̸= NIL then 2 INORDER-TREE-WALK(x→left) 3 käsittele alkio x 4 INORDER-TREE-WALK(x→right) 13 25 33 lehtiä sisäsolmuja juuri 14 18 9 23 5 15 3 8 35 32 26 22 30 6 COMP.CS.300 Tietorakenteet ja algoritmit 1 175 • jälkijärjestys (postorder), eli ensin käsitellään rekursiivisesti lapset, lo- puksi vasta juuri – esimerkissä 3, 6, 5, 9, 8, 14, 15, 13, 23, 22, 26, 32, 35, 33, 30, 25, 18 POSTORDER-TREE-WALK(x) 1 if x ̸= NIL then 2 for child in x→children do 3 POSTORDER-TREE-WALK(child) 4 käsittele alkio x 13 25 33 lehtiä sisäsolmuja juuri 14 18 9 23 5 15 3 8 35 32 26 22 30 6 COMP.CS.300 Tietorakenteet ja algoritmit 1 176 Puun läpikäynnin ajankäyttö: • ajoaika Θ(n), algoritmit kutsuvat itseään kahdesti joka solmussa: kerran vasemmalle ja kerran oikealle lapselle • lisämuistin tarve = Θ(rekursion maksimisyvyys) = Θ(h + 1) = Θ(h)"
            },
            {
                "role": "user",
                "content": "Can you summarize the key topics covered in this lecture?"
            },
            {
                "role": "assistant",
                "content": "This lecture covers foundational algorithmic concepts including problem formulation, correctness proof using induction, efficiency analysis using asymptotic notation, data structures (arrays, linked lists, BSTs, heaps), and sorting methods (selection sort, merge sort, counting sort). Additionally, it introduces the Word-RAM computational model, providing a theoretical framework for measuring algorithm efficiency."
            }
        ]
    },
    {
        "system": "Hello! I am Kiran, an AI assistant from Tampere University, dedicated to providing you with accurate and insightful assistance. My goal is to help you understand concepts, summarize lectures, and answer questions effectively.",
        "conversations": [
            {
                "role": "host",
                "content": "COMP.CS.300 Tietorakenteet ja algoritmit 1 139 Tasattu eli amortisoitu ajoaika Vektori on joustavarajainen taulukko eli sen kokoa kasvatetaan tarvittaessa. • kun uusi alkio ei enää mahdu taulukkoon, varataan uusi suurempi ja siirretään kaikki alkiot sinne • taulukko ei koskaan kutistu ⇒muistin varaus ei vähene muuten kuin kopioimalla vektoriin kokonaan uusi sisältö ⇒Alkion lisäämisen vectorin loppuun sanottiin olevan tasatusti (amortisoidusti) vakioaikaista. COMP.CS.300 Tietorakenteet ja algoritmit 1 140 • Amortisoidusti lähestyttäessä suoritusaikaa tarkastellaan kokonaisuutena, tutkitaan operaatiosarjojen suoritusaikaa yksittäisten operaatioiden sijaan – jokaista kallista muistinvarausta vaativaa lisäysoperaatiota edeltää kalliin operaation hintaan suoraan verrannollinen määrä halpoja lisäysoperaatioita – kalliin operaation kustannus voidaan jakaa tasan halvoille operaatioille – tällöin halvat operaatiot ovat edelleen vakioaikaisia, tosin vakiokertoimen verran hitaampia kuin oikeasti – kallis operaatio voidaan maksaa säästöillä ⇒kaikki lisäysoperaatiot vektorin loppuun ovat tasatusti vakioaikaisia COMP.CS.300 Tietorakenteet ja algoritmit 1 141 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 Tämä voidaan todentaa vaikkapa kirjanpito- menetelmällä: • laskutetaan jokaisesta lisäyksestä kolme ra- haa • yksi raha käytetään lisäyksen todellisiin kus- tannuksiin • yksi raha laitetaan säästöön lisätyn alkion i kohdalle • yksi raha laitetaan säästöön alkion i −1 2 · vector.capacity() kohdalle • kun tulee tarve laajentaa taulukkoa, jokai- sella alkiolla on yksi raha säästössä, ja kallis kopiointi voidaan maksaa niillä COMP.CS.300 Tietorakenteet ja algoritmit 1 110 6.2 Suunnitteluperiaate: Satunnaistaminen Satunnaista on eräs algoritmien suunnitteluperiaatteista. • Sen avulla voidaan usein estää huonoimpien tapausten patologinen ilmeneminen. • Parhaan ja huonoimman tapauksen suoritusajat eivät useinkaan muutu, mutta niiden esiintymistodennäköisuus käytännössä laskee. • Huonot syötteet ovat täsmälleen yhtä todennäköisiä kuin mitkä tahansa muut syötteet riippumatta alkuperäisestä syötteiden jakaumasta. • Satunnaistaminen voidaan suorittaa joko ennen algoritmin suoritusta satunnaistamalla sen saama syöteaineisto tai upottamalla satunnaistaminen algoritmin sisälle. – jälkimmäisellä tavalla päästään usein parempaan tulokseen – usein se on myös helpompaa kuin syötteen esikäsittely COMP.CS.300 Tietorakenteet ja algoritmit 1 111 • Satunnaistaminen on hyvä ratkaisu yleensä silloin, kun – algoritmi voi jatkaa suoritustaan monella tavalla – on vaikea arvata etukäteen, mikä tapa on hyvä – suuri osa tavoista on hyviä – muutama huono arvaus hyvien joukossa ei haittaa paljoa • Esimerkiksi QUICKSORT voi valita jakoarvoksi minkä tahansa taulukon alkion – hyviä valintoja ovat kaikki muut, paitsi lähes pienimmät ja lähes suurimmat taulukossa olevat arvot – on vaikea arvata valintaa tehdessä, onko ko. arvo lähes pienin / suurin – muutama huono arvaus silloin tällöin ei turmele QUICKSORTin suorituskykyä ⇒satunnaistaminen sopii QUICKSORTille COMP.CS.300 Tietorakenteet ja algoritmit 1 112 Satunnaistamisen avulla voidaan tuottaa algoritmi RANDOMIZED-QUICKSORT, joka käyttää satunnaistettua PARTITIONIA. • Ei valita jakoarvoksi aina A[ right ]:tä, vaan valitaan jakoarvo satunnaisesti koko osataulukosta. • Jotta PARTITION ei menisi rikki, sijoitetaan jakoarvo silti kohtaan right taulukkoa ⇒Nyt jako on todennäköisesti melko tasainen riippumatta siitä, mikä syöte saatiin ja mitä taulukolle on jo ehditty tehdä. COMP.CS.300 Tietorakenteet ja algoritmit 1 113 RANDOMIZED-PARTITION( A, left, right ) 1 p := RANDOM(left, right) (valitaan satunnainen alkio pivotiksi) 2 exchange A[ right ] ↔A[ p ] (asetetaan se taulukon viimeiseksi) 3 return PARTITION( A, left, right ) (kutsutaan tavallista partitiointia) RANDOMIZED-QUICKSORT( A, left, right ) 1 if left < right then 2 p := RANDOMIZED-PARTITION( A, left, right ) 3 RANDOMIZED-QUICKSORT( A, left, p −1 ) 4 RANDOMIZED-QUICKSORT( A, p + 1, right ) RANDOMIZED-QUICKSORTIN ajoaika on keskimäärin Θ(n lg n) samoin kuin tavallisenkin QUICKSORTIN. COMP.CS.300 Tietorakenteet ja algoritmit 1 114 • RANDOMIZED-QUICKSORTILLE kuitenkin varmasti pätee keskimääräisen ajankäytön analyysin yhteydessä tehtävä oletus, jonka mukaan pivot-alkio on osataulukon pienin, toiseksi pienin jne. aina samalla todennäköisyydellä. • Tavalliselle QUICKSORTILLE tämä pätee ainoastaan, jos aineisto on tasaisesti jakautunutta. ⇒RANDOMIZED-QUICKSORT on yleisessä tapauksessa tavallista QUICKSORTIA parempi. COMP.CS.300 Tietorakenteet ja algoritmit 1 115 QUICKSORTIA voidaan tehostaa myös muilla keinoilla: • Voidaan järjestää pienet osataulukot pienille taulukoille tehokkaalla algoritmilla (esim. INSERTIONSORT) avulla. – voidaan myös jättää ne vain järjestämättä ja järjestää taulukko lopuksi INSERTIONSORTIN avulla • Jakoarvo voidaan valita esimerkiksi kolmen satunnaisesti valitun alkion mediaanina. • On jopa mahdollista käyttää aina mediaanialkiota jakoalkiona. COMP.CS.300 Tietorakenteet ja algoritmit 1 116 Mediaani on mahdollista etsiä nopeasti niin sanotun laiskan QUICKSORTIN avulla. • Jaetaan taulukko “pienten alkioiden” alaosaan ja “suurten alkioiden” yläosaan kuten QUICKSORTissa. • Lasketaan, kumpaan osaan i:s alkio kuuluu, ja jatketaan rekursiivisesti sieltä. • Toiselle osalle ei tarvitse tehdä enää mitään. RANDOMIZED-SELECT( A, left, right, goal ) 1 if left = right then (jos osataulukko on yhden kokoinen...) 2 return A[ left ] (... palautetaan ainoa alkio) 3 p := RANDOMIZED-PARTITION( A, left, right ) (jaetaan taulukko pieniin ja isoihin) 4 k := p −left + 1 (lasketaan monesko jakoalkio on) 5 if i = k then (jos jakoalkio on taulukon i:s alkio...) 6 return A[ p ] (...palautetaan se) 7 else if i < k then (jatketaan etsintää pienten puolelta) 8 return RANDOMIZED-SELECT( A, left, p −1, goal ) 9 else (jatketaan etsintää suurten puolelta) 10 return RANDOMIZED-SELECT( A, p + 1, right, goal −k ) COMP.CS.300 Tietorakenteet ja algoritmit 1 117 RANDOMIZED-SELECTIN suoritusajan alaraja: • Jälleen kaikki muu on vakioaikaista paitsi RANDOMIZED-PARTITION ja rekursiivinen kutsu. • Parhaassa tapauksessa RANDOMIZED-PARTITIONIN valitsema jakoalkio on taulukon i:s alkio, ja ohjelman suoritus loppuu. • RANDOMIZED-PARTITION ajetaan kerran koko taulukolle. ⇒Algoritmin suoritusaika on Ω(n). RANDOMIZED-SELECTIN suoritusajan yläraja: • RANDOMIZED-PARTITION sattuu aina valitsemaan pienimmän tai suurimman alkion, ja i:s alkio jää suuremmalle puoliskolle • työmäärä pienenee vain yhdellä askeleella joka rekursiotasolla. ⇒Algoritmin suoritusaika on O(n2). COMP.CS.300 Tietorakenteet ja algoritmit 1 118 Keskimääräisen tapauksen ajoaika on kuitenkin O(n). Algoritmi löytyy esimerkiksi STL:stä nimellä nth_element. Algoritmi on mahdollista muuttaa myös toimimaan aina lineaarisessa ajassa."
            },
            {
                "role": "user",
                "content": "Can you summarize the key topics covered in this lecture?"
            },
            {
                "role": "assistant",
                "content": "This lecture covers foundational algorithmic concepts including problem formulation, correctness proof using induction, efficiency analysis using asymptotic notation, data structures (arrays, linked lists, BSTs, heaps), and sorting methods (selection sort, merge sort, counting sort). Additionally, it introduces the Word-RAM computational model, providing a theoretical framework for measuring algorithm efficiency."
            }
        ]
    },
    {
        "system": "Hello! I am Kiran, an AI assistant from Tampere University, dedicated to providing you with accurate and insightful assistance. My goal is to help you understand concepts, summarize lectures, and answer questions effectively.",
        "conversations": [
            {
                "role": "host",
                "content": "COMP.CS.300 Tietorakenteet ja algoritmit 1 177 8.2 Suunnitteluperiaate: Muunna ja hallitse Muunna ja hallitse on suunnitteluperiaate, joka • Ensin muokkaa ongelman instanssia muotoon, joka on helpompi ratkaista – muunnosvaihe • Sitten ongelma voidaan ratkaista – hallintavaihe Ongelman instanssi voidaan muuntaa kolmella eri tavalla: • Yksinkertaistaminen (Instance simpliﬁcation): yksinkertaisempi tai kätevämpi instanssi samasta ongelmasta • Esitystavan muutos (Representation change): saman instanssin toinen esitystapa • Ongelman muunnos (Problem reduction): ratkaistaan sellaisen ongelman, jolle algoritmi on jo valmiina, instanssi COMP.CS.300 Tietorakenteet ja algoritmit 1 178 Keko Taulukko A[1 . . . n] on keko, jos A[i] ≥A[2i] ja A[i] ≥A[2i + 1] aina kun 1 ≤i ≤⌊n 2⌋(ja 2i + 1 ≤n). Tämä on helpompi ymmärtää, kun tulkitaan keko täydellisesti tasapainotetuksi binääripuuksi, jonka • juuri on talletettu taulukon paik- kaan 1 • paikkaan i talletetun solmun lapset (jos olemassa) on talletet- tu paikkoihin 2i ja 2i + 1 • paikkaan i talletetun solmun isä on talletettu paikkaan ⌊i 2⌋ 12 15 14 7 17 5 10 8 2 7 COMP.CS.300 Tietorakenteet ja algoritmit 1 179 Tällöin jokaisen solmun arvo on suurempi tai yhtä suuri kuin sen lasten arvot. Kekopuun jokainen kerros on täysi, paitsi ehkä alin, joka on täytetty vasemmasta reunasta alkaen. COMP.CS.300 Tietorakenteet ja algoritmit 1 180 Jotta kekoa olisi helpompi ajatella puuna, määrittelemme isä- ja lapsisolmut löytävät aliohjelmat. • ne ovat toteutettavissa hyvin tehokkaasti bittisiirtoina • kunkin suoritusaika on aina Θ(1) PARENT(i) return ⌊i/2⌋ LEFT(i) return 2i RIGHT(i) return 2i + 1 COMP.CS.300 Tietorakenteet ja algoritmit 1 181 ⇒Nyt keko-ominaisuus voidaan lausua seuraavasti: A[PARENT(i)] ≥A[i] aina kun 2 ≤i ≤A.heapsize • A.heapsize kertoo keon koon (myöhemmin nähdään, ettei se aina ole välttämättä sama kuin taulukon koko) Keko-ominaisuudesta seuraa, että keon suurin alkio on aina keon juuressa, siis taulukon ensimmäisessa lokerossa. Jos keon korkeus on h, sen solmujen määrä on välillä 2h . . . 2h+1 −1. ⇒Jos keossa n solmua, sen korkeus on Θ(lg n). COMP.CS.300 Tietorakenteet ja algoritmit 1 182 Alkion lisääminen kekoon ylhäältä: • oletetaan, että A[1 . . . n] on muuten keko, mutta keko- ominaisuus ei päde kekopuun juurelle – toisin sanoen A[1] < A[2] tai A[1] < A[3] 12 10 9 8 3 4 5 6 7 2 1 15 14 7 5 9 8 10 2 7 • ongelma saadaan siirrettyä alemmas puussa valitsemal- la juuren lapsista suurempi, ja vaihtamalla se juuren kanssa – jotta keko-ominaisuus ei ha- joaisi, pitää valita lapsista suu- rempi - siitähän tulee toisen lapsen uusi isä 12 10 9 8 3 4 5 6 7 2 1 9 7 14 5 10 8 15 7 2 COMP.CS.300 Tietorakenteet ja algoritmit 1 183 • sama voidaan tehdä alipuulle, jonka juureen ongelma siirtyi, ja sen alipuulle jne. kunnes ongel- ma katoaa – ongelma katoaa viimeistään kun saavutetaan lehti ⇒puu muuttuu keoksi 12 10 9 8 3 4 5 6 7 2 1 14 7 5 10 15 8 9 7 2 COMP.CS.300 Tietorakenteet ja algoritmit 1 184 Sama pseudokoodina: HEAPIFY( A, i ) (i kertoo paikan, jonka alkio saattaa olla liian pieni) 1 repeat (toistetaan, kunnes keko on ehjä) 2 old_i := i (otetaan i:n arvo talteen) 3 l := LEFT( i ) 4 r := RIGHT( i ) 5 if l ≤A.heapsize and A[ l ] > A[ i ] then (vasen lapsi on suurempi kuin i) 6 i := l 7 if r ≤A.heapsize and A[ r ] > A[ i ] then (oikea lapsi on vielä suurempi) 8 i := r 9 if i ̸= old_i then (jos suurempi lapsi löytyi...) 10 exchange A[ old_i ] ↔A[ i ] (...siirretään rike alaspäin) 11 until i = old_i (jos keko oli jo ehjä, lopetetaan) • Suoritus on vakioaikaista kun rivin 11 ehto toteutuu heti ensimmäisellä kerralla kun sinne päädytään: Ω(1). • Pahimmassa tapauksessa uusi alkio joudutaan siirtämään lehteen asti koko korkeuden verran. ⇒Suoritusaika on O(h) = O(lg n). COMP.CS.300 Tietorakenteet ja algoritmit 1 185 Keon rakentaminen • seuraava algoritmi järjestää taulukon uudelleen niin, että siitä tulee keko: BUILD-HEAP(A) 1 A.heapsize := A.length (koko taulukosta tehdään keko) 2 for i := ⌊A.length/2⌋downto 1 do (käydään taulukon alkupuolisko läpi) 3 HEAPIFY(A, i) (kutsutaan Heapifyta) • Lähdetään käymään taulukkoa läpi lopusta käsin ja kutsutaan HEAPIFYTA kaikille alkioille. – ennen HEAPIFY-funktion kutsua keko-ominaisuus pätee aina i:n määräämälle alipuulle, paitsi että paikan i alkio on mahdollisesti liian pieni – yhden kokoisia alipuita ei tarvitse korjata, koska niissä keko-ominaisuus pätee triviaalisti – HEAPIFY(A, i):n jälkeen i:n määräämä alipuu on keko ⇒HEAPIFY(A, 1):n jälkeen koko taulukko on keko COMP.CS.300 Tietorakenteet ja algoritmit 1 186 • BUILD-HEAP ajaa for-silmukan ⌊n 2⌋kertaa ja HEAPIFY on Ω(1) ja O(lg n), joten – nopein suoritusaika on ⌊n 2⌋· Ω(1) + Θ(n) = Ω(n) – ohjelma ei voi koskaan käyttää enempää aikaa kuin ⌊n 2⌋· O(lg n) + Θ(n) = O(n lg n) • Näin saamamme hitaimman tapauksen suoritusaika on kuitenkin liian pessimistinen: COMP.CS.300 Tietorakenteet ja algoritmit 1 187 – HEAPIFY on O(h), missä h on kekopuun korkeus – i:n muuttuessa myös puun korkeus vaihtelee kerros h HEAPIFY-suoritusten määrä alin 0 0 toinen 1 ⌊n 4⌋ kolmas 2 ⌊n 8⌋ ... ... ... ylin ⌊lg n⌋ 1 – siis pahimman tapauksen suoritusaika onkin n 4 · 1 + n 8 · 2 + n 16 · 3 + · · · = n 2 · P∞ i=1 i 2i = n 2 · 2 = n ⇒O(n) ⇒BUILD-HEAPIN suoritusaika on aina Θ(n) COMP.CS.300 Tietorakenteet ja algoritmit 1 188 8.3 Järjestäminen keon avulla Taulukon alkioiden ärjestäminen voidaan toteuttaa tehokkaasti kekoa hyödyntäen: HEAPSORT( A ) 1 BUILD-HEAP( A ) (muutetaan taulukko keoksi) 2 for i := A.length downto 2 do (käydään taulukko läpi lopusta alkuun) 3 exchange A[ 1 ] ↔A[ i ] (siirretään keon suurin alkio keon viimeiseksi) 4 A.heapsize := A.heapsize −1 (siirretään suurin alkio keon ulkopuolelle) 5 HEAPIFY( A, 1 ) (korjataan keko, joka on muuten kunnossa, mutta...) (... sen ensimmäinen alkio saattaa olla liian pieni) Esitetäänpä sama kuvien avulla: • ensin taulukko muutetaan keoksi • esimerkistä on helppo havaita, et- tei operaatio ole kovin raskas – keko-ominaisuus on selvästi jär- jestystä heikompi 9 12 8 14 7 15 9 9 9 12 12 12 8 8 8 7 7 7 14 14 14 15 15 15 COMP.CS.300 Tietorakenteet ja algoritmit 1 189 • kuvassa voi nähdä kuinka järjeste- tyn loppuosan koko kasvaa, kun- nes koko taulukko on järjestyksessä • kasvatusten välillä keko-osuus kor- jataan • korjaus näyttää tällaisessa pienes- sä esimerkissä tarpeettoman moni- mutkaiselta – korjaukseen ei suurillakaan taulu- koilla kulu kovin montaa askelta, ainoastaan logaritminen määrä 7 8 14 9 12 15 15 15 15 15 15 12 8 7 9 14 7 14 14 15 9 12 8 7 14 15 8 14 9 12 7 14 14 14 14 12 12 12 12 7 9 8 9 9 7 8 8 7 8 7 9 14 12 9 8 12 7 9 8 8 7 9 12 12 14 7 8 9 15 15 15 15 COMP.CS.300 Tietorakenteet ja algoritmit 1 190 HEAPSORTIN suoritusaika koostuu seuraavista osista. • BUILD-HEAP rivillä 1 suoritetaan kerran: Θ(n) • for-silmukan sisältö suoritetaan n - 1 kertaa – rivien 3 ja 4 operaatiot ovat vakioaikaisia – HEAPIFY käyttää aikaa Ω(1) ja O(lg n) ⇒Saadaan yhteensä Ω(n) ja O(n lg n) • alaraja on tarkka – jos kaikki alkiot ovat samanarvoisia, keon korjaustoimenpiteitä ei tarvita koskaan ja HEAPIFY on aina vakioaikainen • myös yläraja on tarkka – tämän osoittaminen on hieman hankalampaa ja tyydymmekin myöhemmin saatavaan tulokseen vertailuun perustuvan järjestämisen nopeudesta COMP.CS.300 Tietorakenteet ja algoritmit 1 191 Huom! Edelliset suoritusaikalaskelmat olettavat, että keon pohjana käytettävällä tietorakenteella on vakioaikainen indeksointi. • Kekoa kannattaa käyttää ainoastaan silloin! HEAPSORTIN etuja ja haittoja Etuja: • järjestää taulukon paikallaan • ei koskaan käytä enempää kuin Θ(n lg n) aikaa Haittoja: • suoritusajan vakiokerroin on suurehko • epävakaus – samanarvoisten alkioiden keskinäinen järjestys ei säily COMP.CS.300 Tietorakenteet ja algoritmit 1 192 8.4 Prioriteettijono Prioriteettijono (priority queue) on tietorakenne, joka pitää yllä joukkoa S alkioita, joista jokaiseen liittyy avain (key), ja sallii seuraavat operaatiot: • INSERT(S, x) lisää alkion x joukkoon S • MAXIMUM(S) palauttaa sen alkion, jonka avain on suurin – jos monella eri alkiolla on sama, suurin avain, valitsee vapaasti minkä tahansa niistä • EXTRACT-MAX(S) poistaa ja palauttaa sen alkion, jonka avain on suurin • vaihtoehtoisesti voidaan toteuttaa operaatiot MINIMUM(S) ja EXTRACT-MIN(S) – samassa jonossa on joko vain maksimi- tai vain minimioperaatiot! COMP.CS.300 Tietorakenteet ja algoritmit 1 193 Prioriteettijonoilla on monia käyttökohteita • tehtävien ajoitus käyttöjärjestelmässä – uusia tehtäviä lisätään komennolla INSERT – kun edellinen tehtävä valmistuu tai keskeytetään, seuraava valitaan komennolla EXTRACT-MAX • tapahtumapohjainen simulointi – jono tallettaa tulevia (= vielä simuloimattomia) tapahtumia – avain on tapahtuman tapahtumisaika – tapahtuma voi aiheuttaa uusia tapahtumia ⇒lisätään jonoon operaatiolla INSERT – EXTRACT-MIN antaa seuraavan simuloitavan tapahtuman • lyhimmän reitin etsintä kartalta – simuloidaan vakionopeudella ajavia, eri reitit valitsevia autoja, kunnes ensimmäinen perillä – prioriteettijonoa tarvitaan käytännössä myöhemmin esiteltävässä lyhimpien polkujen etsintäalgoritmissa COMP.CS.300 Tietorakenteet ja algoritmit 1 194 Prioriteettijonon voisi käytännössä toteuttaa järjestämättömänä tai järjestettynä taulukkona, mutta se olisi tehotonta. • järjestämättömässä taulukossa MAXIMUM ja EXTRACT-MAX ovat hitaita • järjestetyssä taulukossa INSERT on hidas Sen sijaan keon avulla prioriteettijonon voi toteuttaa tehokkaasti. • Joukon S alkiot talletetaan kekoon A. • MAXIMUM( S ) on hyvin helppo, ja toimii ajassa Θ(1). HEAP-MAXIMUM( A ) 1 if A.heapsize < 1 then (tyhjästä keosta ei löydy maksimia) 2 error “heap underﬂow” 3 return A[ 1 ] (muuten palautetaan taulukon ensimmäinen alkio) COMP.CS.300 Tietorakenteet ja algoritmit 1 195 • EXTRACT-MAX(S) voidaan toteuttaa korjaamalla keko poiston jälkeen HEAPIFYN avulla. • HEAPIFY dominoi algoritmin ajoaikaa: O(lg n). HEAP-EXTRACT-MAX( A ) 1 if A.heapsize < 1 then (tyhjästä keosta ei löydy maksimia) 2 error “heap underﬂow” 3 max := A[ 1 ] (suurin alkio löytyy taulukon alusta) 4 A[ 1 ] := A[ A.heapsize ] (siirretään viimeinen alkio juureen) 5 A.heapsize := A.heapsize −1 (pienennetään keon kokoa) 6 HEAPIFY( A, 1 ) (korjataan keko) 7 return max COMP.CS.300 Tietorakenteet ja algoritmit 1 196 • INSERT(S, x) lisää uuden alkion kekoon asettamalla sen uudeksi lehdeksi, ja nostamalla sen suuruutensa mukaiselle paikalle. – se toimii kuten HEAPIFY, mutta alhaalta ylöspäin – lehti joudutaan nostamaan pahimmassa tapauksessa juureen asti: ajoaika O(lg n) HEAP-INSERT( A, key ) 1 A.heapsize := A.heapsize + 1 (kasvatetaan keon kokoa) 2 i := A.heapsize (lähdetään liikkeelle taulukon lopusta) 3 while i > 1 and A[ PARENT(i) ] < key do (edetään kunnes ollaan juuressa tai ...) (...kohdassa jonka isä on avainta suurempi) 4 A[ i ] := A[ PARENT(i) ] (siirretään isää alas päin) 5 i := PARENT(i) (siirrytään ylöspäin) 6 A[ i ] := key (asetetaan avain oikealle paikalleen) ⇒Keon avulla saadaan jokainen prioriteettijonon operaatio toimimaan ajassa O(lg n). COMP.CS.300 Tietorakenteet ja algoritmit 1 197 Prioriteettijonoa voidaan ajatella abstraktina tietotyyppinä, johon kuuluu talletettu data (joukko S) ja operaatiot (INSERT, MAXIMUM,EXTRACT-MAX. • käyttäjälle kerrotaan ainoastaan operaatioiden nimet ja merkitykset, muttei toteutusta • toteutus kapseloidaan esimerkiksi pakkaukseksi (Ada), luokaksi (C++) tai itsenäiseksi tiedostoksi (C) ⇒Toteutusta on helppo ylläpitää, korjata ja tarvittaessa vaihtaa toiseen, ilman että käyttäjien koodiin tarvitsee koskea."
            },
            {
                "role": "user",
                "content": "Can you summarize the key topics covered in this lecture?"
            },
            {
                "role": "assistant",
                "content": "This lecture covers foundational algorithmic concepts including problem formulation, correctness proof using induction, efficiency analysis using asymptotic notation, data structures (arrays, linked lists, BSTs, heaps), and sorting methods (selection sort, merge sort, counting sort). Additionally, it introduces the Word-RAM computational model, providing a theoretical framework for measuring algorithm efficiency."
            }
        ]
    },
    {
        "system": "Hello! I am Kiran, an AI assistant from Tampere University, dedicated to providing you with accurate and insightful assistance. My goal is to help you understand concepts, summarize lectures, and answer questions effectively.",
        "conversations": [
            {
                "role": "host",
                "content": "COMP.CS.300 Tietorakenteet ja algoritmit 1 263 13 Graaﬁt Seuraavaksi tutustutaan tietorakenteeseen, jonka muodostavat pisteet ja niiden välille muodostetut yhteydet – graaﬁin. Keskitymme myös tyypillisimpiin tapoihin etsiä tietoa graaﬁsta eli graaﬁalgoritmeihin. Kuva: Flickr, Rachel D COMP.CS.300 Tietorakenteet ja algoritmit 1 264 13.1 Graaﬁen esittäminen tietokoneessa Graaﬁon ohjelmistotekniikassa keskeinen rakenne, joka koostuu solmuista (vertex, node) ja niitä yhdistävistä kaarista (edge, arc). Graaﬁvoi olla suuntaamaton (undirected) tai suunnattu (directed). COMP.CS.300 Tietorakenteet ja algoritmit 1 265 Kuva 18: Kuva: Getty Images • kaaviokuvat voidaan usein ajatella graafeiksi • asioiden välisiä suhteita voi usein esittää graafeina • monet tehtävät voidaan palauttaa graaﬁtehtäviksi COMP.CS.300 Tietorakenteet ja algoritmit 1 266 Kuva 19: Kuva: Wikimedia Commons, User:Jpatokal COMP.CS.300 Tietorakenteet ja algoritmit 1 267 Kuva 20: Kuva: Flickr, yaph,http://exploringdata.github.io/vis/footballers-search-relations/ COMP.CS.300 Tietorakenteet ja algoritmit 1 268 Kuva 21: Kuva: Flickr, Andy Lamb COMP.CS.300 Tietorakenteet ja algoritmit 1 269 Kuva 22: Kuva: Tokyo Metro COMP.CS.300 Tietorakenteet ja algoritmit 1 270 Matematiikassa graaﬁG ilmoitetaan usein parina G = (V, E). • V = solmujen joukko • E = kaarien joukko • tällöin samojen solmujen välillä saa yleensä olla vain yksi kaari molempiin suuntiin – aina tämä ei kuitenkaan käytännön sovelluksessa riitä – esimerkiksi juna-aikatauluja esittävässä graaﬁssa kaupunkien välillä on yleensä useampia vuoroja – tällaista graaﬁa kutsutaan monigraaﬁksi (multigraph) Kuva: HSL COMP.CS.300 Tietorakenteet ja algoritmit 1 271 • jos solmujen välillä sallitaan vain yksi kaari suuntaansa ⇒ E ⊆V 2 – suunnatulla graaﬁlla |E| voi vaihdella välillä 0, . . . , |V |2 – laskettaessa graaﬁalgoritmien suoritusaikoja oletamme tämän Graaﬁalgoritmin suorituskyky ilmoitetaan yleensä sekä |V |:n että |E|:n funktiona – helppouden vuoksi jätämme itseisarvomerkit pois kertaluokkamerkintöjen sisällä ts. O(V E) = O(|V | · |E|) COMP.CS.300 Tietorakenteet ja algoritmit 1 272 Graaﬁn esittämiseen tietokoneessa on kaksi perusmenetelmää kytkentälista (adjacency list) ja kytkentämatriisi (adjacency matrix). Näistä kahdesta kytkentälistaesitys on yleisempi, ja tällä kurssilla keskitytään siihen. • Solmut on talletettu johonkin tietorakenteeseen (valittu tietorakenne riippuu siitä, mitä oheisdataa solmut sisältävät, miten niitä pitää pystyä hakemaan, lisäämään jne.) • Yksinkertaisimmillaan jokaisessa solmussa on tietorakenne, jossa on tallessa mihin solmuihin tästä solmusta on kaari. – Valittu tietorakenne riippuu siitä, paljonko kaaria arvellaan olevan, lisätäänkö/poistetaanko niitä jatkuvasti, täytyykö tiettyä kaarta pystyä hakemaan nopeasti jne.) – Tieto kohdesolmusta voidaan tallettaa osoittimena, solmun indeksinä (jos solmut indeksoitavassa tietorakenteessa) tms. – Solmujen järjestyksellä kytkentälistassa ei yleensä ole väliä COMP.CS.300 Tietorakenteet ja algoritmit 1 273 • Graaﬁn kytkentälistojen yh- teiskoko on – |E|, jos graaﬁon suunnat- tu, 2·|E|, jos graaﬁon suun- taamaton ⇒kytkentälistaesityksen muistin kulutus on O(max(V, E)) = O(V + E) • Tiedon “onko kaarta solmusta v solmuun u” haku edellyttää yhden kytkentälistan läpi selaamista mikä vie hitaimmillaan aikaa Θ(V ) (ellei solmusta lähteviä kaaria talleteta johonkin tietorakenteeseen, josta niistä pystytään nopeasti hakemaan kohdekaaren perusteella) • Jos kaari on painotettu tai siihen liittyy oheisdataa, täytyy kaaresta tallettaa kohdesolmun lisäksi myös oheisdata (esim. struct, jossa kohdesolmu ja oheisdata) • Joskus on tarpeen tallettaa myös tieto solmuun tulevista kaarista samaan tapaan (esim. solmujen ja kaarien poistamisen helpottamiseksi) COMP.CS.300 Tietorakenteet ja algoritmit 1 274 Kytkentämatriisiesityksen avulla edelliseen kysymykseen pystytään vastaamaan helposti. • kytkentämatriisi on |V | × |V | -matriisi A, jonka alkio aij on – 0, jos solmusta i ei ole kaarta solmuun j – 1, jos solmusta i on kaari solmuun j • edellisen esimerkin graaﬁen kytkentämatriisit ovat 1 2 3 4 5 1 2 3 4 5 1 0 1 0 1 1 1 0 1 0 1 0 2 1 0 0 1 0 2 0 1 0 0 0 3 0 0 0 0 0 3 0 0 0 0 0 4 1 1 0 0 1 4 0 1 0 0 1 5 1 0 0 1 0 5 1 0 0 1 0 COMP.CS.300 Tietorakenteet ja algoritmit 1 275 • muistin kulutus on aina Θ(V 2) – jokainen alkio tarvitsee vain bitin muistia, joten useita alkioita voidaan tallettaa yhteen sanaan ⇒vakiokerroin saadaan aika pieneksi • kytkentämatriisiesitystä kannattaa käyttää lähinnä vain hyvin tiheiden graaﬁen yhteydessä. COMP.CS.300 Tietorakenteet ja algoritmit 1 276 Tarkastellaan tarkemmin kytkentälistaesitystavan toteutusta: • käytännön sovelluksissa solmuun kertyy usein monenlaista tehtävän tai algoritmin vaatimaa tietoa – nimi – bitti, joka kertoo, onko solmussa käyty – osoitin, joka kertoo, mistä solmusta tähän solmuun viimeksi tultiin – . . . ⇒solmusta kannattaa tehdä itsenäinen alkio, jossa on tarpeelliset kentät • yleensä sama pätee myös kaariin COMP.CS.300 Tietorakenteet ja algoritmit 1 277 • pääperiaate: – talleta jokainen asia yhteen kertaan – ota käyttöön osoittimet, joilla voit helposti kulkea haluamiisi suuntiin COMP.CS.300 Tietorakenteet ja algoritmit 1 278 13.2 Yleistä graaﬁalgoritmeista Käsitteitä: • askel = siirtyminen solmusta toiseen yhtä kaarta pitkin – suunnatussa graaﬁssa askel on otettava kaaren suuntaan • solmun v2 etäisyys (distance) solmusta v1 on lyhimmän v1:stä v2:een vievän polun pituus. – jokaisen solmun etäisyys itsestään on 0 – merkitään δ(v1, v2) – suunnatussa graaﬁssa on mahdollista (ja tavallista), että δ(v1, v2) ̸= δ(v2, v1) – jos v1:stä ei ole polkua v2:een, niin δ(v1, v2) = ∞ COMP.CS.300 Tietorakenteet ja algoritmit 1 279 Algoritmien ymmärtämisen helpottamiseksi annamme usein solmuille värit. • valkoinen = solmua ei ole vielä löydetty • harmaa = solmu on löydetty, mutta ei loppuun käsitelty • musta = solmu on löydetty ja loppuun käsitelty • solmun väri muuttuu järjestyksessä valkoinen →harmaa → musta • värikoodaus on lähinnä ajattelun apuväline, eikä sitä välttämättä tarvitse toteuttaa täysin, yleensä riittää tietää, onko solmu löydetty vai ei. – usein tämäkin informaatio on pääteltävissä nopeasti muista kentistä COMP.CS.300 Tietorakenteet ja algoritmit 1 280 Monet graaﬁalgoritmit perustuvat graaﬁn tai sen osan läpikäyntiin tietyssä järjestyksessä. • perusläpikäyntijärjestyksiä on kaksi, leveyteen ensin -haku (BFS) ja syvyyteen ensin -haku (DFS). • läpikäynnillä tarkoitetaan algoritmia, jossa – käydään kerran graaﬁn tai sen osan jokaisessa solmussa – kuljetaan kerran graaﬁn tai sen osan jokainen kaari Hakualgoritmit käyttävät lähtökohtana jotain annettua graaﬁn solmua, lähtösolmua (source) ja etsivät kaikki ne solmut, joihin pääsee lähtösolmusta nollalla tai useammalla askeleella. COMP.CS.300 Tietorakenteet ja algoritmit 1 281 13.3 Leveyteen ensin -haku (breadth-ﬁrst) Leveyteen ensin -hakua voi käyttää esimerkiksi • kaikkien solmujen etäisyyden määrittämiseen lähtösolmusta • (yhden) lyhimmän polun löytämiseen lähtösolmusta jokaiseen solmuun Leveyteen ensin -haun nimi tulee siitä, että se tutkii tutkitun ja tuntemattoman graaﬁn osan välistä rajapintaa koko ajan koko sen leveydeltä. Solmujen kentät: • v→d = jos solmu v on löydetty niin sen etäisyys s:tä, muutoin ∞ • v→π = osoitin solmuun, josta haku ensi kerran tuli v:hen, löytämättömille solmuille NIL • v→colour = solmun v väri • v→Adj = solmun v naapurisolmujen joukko COMP.CS.300 Tietorakenteet ja algoritmit 1 282 Algoritmin käyttämä tietorakenne Q on jono (noudattaa FIFO-jonokuria). BFS(s) (algoritmi saa parametrinaan aloitussolmun s) 1 ▷alussa kaikkien solmujen kentät ovat arvoiltaan colour = WHITE, d = ∞, π = NIL 2 s→colour := GRAY (merkitään alkutila löydetyksi) 3 s→d := 0 (etäisyys alkutilasta alkutilaan on 0) 4 PUSH(Q, s) (työnnetään alkutila jonoon) 5 while Q ̸= ∅do (toistetaan niin kauan kun tiloja riittää) 6 u := POP(Q) (vedetään jonosta seuraava tila) 7 for each v ∈u→Adj do (käydään u:n naapurit läpi) 8 if v→colour = WHITE then (jos solmua ei ole vielä löydetty . . . ) 9 v→colour := GRAY (. . . merkitään se löydetyksi) 10 v→d := u→d + 1 (kasvatetaan etäisyyttä yhdellä) 11 v→π := u (tilaan v tultiin tilan u kautta) 12 PUSH(Q, v) (työnnetään tila jonoon odottamaan käsittelyä) 13 u→colour := BLACK (merkitään tila u käsitellyksi) Kaikkia algoritmin käyttämiä solmun kenttiä ei välttämättä käytännön toteutuksessa tarvita, vaan osan arvoista voi päätellä toisistaan. COMP.CS.300 Tietorakenteet ja algoritmit 1 283 Alla olevassa kuvassa graaﬁn solmut on numeroitu siinä järjestyksessä, jossa BFS löytää ne. Solmujen etäisyys alkutilasta on merkitty solmun viereen ja haun kulkureitti tummennettu. 8 1 1 alkutila 1 2 9 3 2 2 2 5 4 6 7 3 3 4 COMP.CS.300 Tietorakenteet ja algoritmit 1 284 Suoritusaika solmujen (V ) ja kaarien (E) määrien avulla ilmaistuna: • ennen algoritmin kutsumista solmut pitää alustaa – järkevässä ratkaisussa tämä on tehtävissä ajassa O(V ) • rivillä 7 algoritmi selaa solmun lähtökaaret – onnistuu käyttämällämme kytkentälistaesityksellä solmun kaarien määrään nähden lineaarisessa ajassa • kukin jono-operaatio vie vakiomäärän aikaa • while-silmukan kierrosten määrä – vain valkoisia solmuja laitetaan jonoon – samalla solmun väri muuttuu harmaaksi ⇒kukin solmu voi mennä jonoon korkeintaan kerran ⇒while-silmukka pyörähtää siis korkeintaan O(V ) määrän kertoja COMP.CS.300 Tietorakenteet ja algoritmit 1 285 • for-silmukan kierrosten määrä – algoritmi kulkee jokaisen kaaren korkeintaan kerran molempiin suuntiin ⇒for-silmukka käydään läpi yhteensä korkeintaan O(E) kertaa ⇒koko algoritmin suoritusaika on siis O(V + E) COMP.CS.300 Tietorakenteet ja algoritmit 1 286 Algoritmin lopetettua π-osoittimet määrittelevät puun, joka sisältää löydetyt solmut, ja jonka juurena on lähtösolmu s. • leveyteen ensin -puu (breadth-ﬁrst tree) • π-osoittimet määräävät puun kaaret “takaperin” – osoittavat juurta kohti – v→π = v:n edeltäjä (predecessor) eli isä (parent) • kaikki lähtösolmusta saavutettavissa olevat solmut kuuluvat puuhun • puun polut ovat mahdollisimman lyhyitä polkuja s:stä löydettyihin solmuihin COMP.CS.300 Tietorakenteet ja algoritmit 1 287 Lyhimmän polun tulostaminen • kun BFS on asettanut π-osoittimet kohdalleen, lyhin polku lähtösolmusta s solmuun v voidaan tulostaa seuraavasti: PRINT-PATH(G, s, v) 1 if v = s then (rekursion pohjatapaus) 2 print s 3 else if v→π = NIL then (haku ei ole saavuttanut solmua v lainkaan) 4 print “ei polkua” 5 else 6 PRINT-PATH(G, s, v→π) (rekursiokutsu . . . ) 7 print v (. . . jonka jälkeen suoritetaan tulostus) • ei-rekursiivisen version voi tehdä esim. – kokoamalla solmujen numerot taulukkoon kulkemalla π-osoittimia pitkin, ja tulostamalla taulukon sisällön takaperin – kulkemalla polku kahdesti, ja kääntämällä π-osoittimet takaperin kummallakin kertaa (jälkimmäinen käännös ei tarpeen, jos π-osoittimet saa turmella) COMP.CS.300 Tietorakenteet ja algoritmit 1 288 13.4 Syvyyteen ensin -haku (depth-ﬁrst) Syvyyteen ensin -haku on toinen perusläpikäyntijärjestyksistä. Siinä missä leveyteen ensin -haku tutkii koko hakurintamaa sen koko leveydeltä, syvyyteen ensin -haku menee yhtä polkua eteen päin niin kauan kuin se on mahdollista. • polkuun hyväksytään vain solmuja, joita ei ole aiemmin nähty • kun algoritmi ei enää pääse eteenpäin, se peruuttaa juuri sen verran kuin on tarpeen uuden etenemisreitin löytämiseksi, ja lähtee sitä pitkin • algoritmi lopettaa, kun se peruuttaa viimeisen kerran takaisin lähtösolmuun, eikä löydä enää sieltäkään tutkimattomia kaaria Algoritmi muistuttaa huomattavasti leveyteen ensin -haun pseudokoodia. COMP.CS.300 Tietorakenteet ja algoritmit 1 289 Merkittäviä eroja on oikeastaan vain muutama: • jonon sijasta käsittelyvuoroaan odottavat tilat talletetaan pinoon • algoritmi ei löydä lyhimpiä polkuja, vaan ainoastaan jonkin polun – tästä syystä esimerkkipseudokoodia on yksinkertaistettu jättämällä π-kentät pois Algoritmin käyttämä tietorakenne S on pino (noudattaa LIFO-jonokuria). COMP.CS.300 Tietorakenteet ja algoritmit 1 290 DFS(s) (algoritmi saa parametrinaan aloitussolmun s) 1 ▷alussa kaikkien (käsittelemättömien) solmujen värikenttä colour = WHITE 2 PUSH(S, s) (työnnetään alkutila pinoon) 3 while S ̸= ∅do (jatketaan niin kauan kun pinossa on tavaraa) 4 u := POP(S) (vedetään pinosta viimeisin sinne lisätty tila) 5 if u→colour = WHITE then (jos solmua ei ole vielä käsitelty . . . ) 6 u→colour := GRAY (merkitään tila käsittelyssä olevaksi) 7 PUSH(S, u) (työnnetään taas pinoon (mustaksi värjäys)) 8 for each v ∈u→Adj do (käydään u:n naapurit läpi) 9 if v→colour = WHITE then (jos solmua ei ole vielä käsitelty . . . ) 10 PUSH(S, v) (. . . työnnetään se pinoon odottamaan käsittelyä) 11 else if v→colour = GRAY then (harmaa solmu! Sykli löytynyt! . . . ) 12 ???? (käsittele sykli, jos se kiinnostaa) 13 else 14 u→colour := BLACK (kaikki lapset käsitelty, solmu on valmis) Jos halutaan tutkia koko graaﬁ, voidaan kutsua syvyyteen ensin -hakua kertaalleen kaikista vielä tutkimattomista solmuista. • tällöin solmuja ei väritetä valkoisiksi kutsukertojen välillä COMP.CS.300 Tietorakenteet ja algoritmit 1 291 Rivin 5 perään voitaisi lisätä operaatio, joka kaikille graaﬁn alkioille halutaan tehdä. Voidaan esimerkiksi • tutkia onko tila maalitila, ja lopettaa jos on • ottaa talteen solmuun liittyvää oheisdataa • muokata solmuun liittyvää oheisdataa COMP.CS.300 Tietorakenteet ja algoritmit 1 292 Suoritusaika voidaan laskea samoin kuin leveyteen ensin -haun yhteydessä: • ennen algoritmin kutsumista solmut pitää alustaa – järkevässä ratkaisussa tämä on tehtävissä ajassa O(V ) • rivillä 6 algoritmi selaa solmun lähtökaaret – onnistuu käyttämällämme kytkentälistaesityksellä solmun kaarien määrään nähden lineaarisessa ajassa • kukin pino-operaatio vie vakiomäärän aikaa • while-silmukan kierrosten määrä – vain valkoisia solmuja laitetaan pinoon – samalla solmun väri muuttuu harmaaksi ⇒kukin solmu voi mennä pinoon korkeintaan kerran ⇒while-silmukka pyörähtää siis korkeintaan O(V ) määrän kertoja COMP.CS.300 Tietorakenteet ja algoritmit 1 293 • for-silmukan kierrosten määrä – algoritmi kulkee jokaisen kaaren korkeintaan kerran molempiin suuntiin ⇒for-silmukka käydään läpi yhteensä korkeintaan O(E) kertaa ⇒koko algoritmin suoritusaika on siis O(V + E) COMP.CS.300 Tietorakenteet ja algoritmit 1 294 DFS on myös mahdollista toteuttaa rekursiivisesti, jolloin algoritmin pinona toimii funktioiden kutsupino. • Rekursiivinen versio on itse asiassa jonkin verran yksinkertaisempi kuin iteratiivinen versio! • (Keksitkö, miksi?) Huom! Ennen algoritmin kutsumista kaikki solmut tulee alustaa valkoisiksi! DFS(u) 1 u→colour := GRAY (merkitään tila löydetyksi) 2 for each v ∈u→Adj do (käydään kaikki u:n naapurit läpi) 3 if v→colour = WHITE then (jos ei olla vielä käyty v:ssä. . . ) 4 DFS(v) (. . . jatketaan etsintää rekursiivisesti tilasta v) 5 else if v→colour = GRAY then (jos on jo käyty, muttei loppuun käsitelty . . . ) 6 ▷silmukka on löytynyt (. . . silmukka on löytynyt) 7 u→colour := BLACK (merkitään tila käsitellyksi) COMP.CS.300 Tietorakenteet ja algoritmit 1 295 Suoritusaika: • rekursiivinen kutsu tehdään ainoastaan valkoisille solmuille • funktion alussa solmu väritetään harmaaksi ⇒DFS:ää kutsutaan korkeintaan O(V ) kertaa • kuten aiemmassakin versiossa for-silmukka kiertää korkeintaan kaksi kierrosta kutakin graaﬁn kaarta kohden koko algoritmin suorituksen aikana ⇒siis for-silmukan kierroksia tulee korkeintaan O(E) kappaletta • muut operaatioista ovat vakioaikaisia ⇒koko algoritmin suoritusaika on edelleen O(V + E) COMP.CS.300 Tietorakenteet ja algoritmit 1 296 Leveyteen ensin -haku vai syvyyteen ensin -haku: • lyhimmän polun etsimiseen täytyy käyttää leveyteen ensin -hakua • jos graaﬁn esittämä tilavaruus on hyvin suuri, käyttää leveyteen ensin -haku yleensä huomattavasti enemmän muistia – syvyyteen ensin -haun pinon koko pysyy yleensä pienempänä kuin leveyteen ensin -haun jonon koko – useissa sovelluksissa esimerkiksi tekoälyn alalla jonon koko estää leveyteen ensin -haun käytön • mikäli graaﬁn koko on ääretön, ongelmaksi nousee se, ettei syvyyteen ensin -haku välttämättä löydä ikinä maalitilaa, eikä edes lopeta ennen kuin muisti loppuu – näin tapahtuu, jos algoritmi lähtee tutkimaan hedelmätöntä äärettömän pitkää haaraa – tätä ongelmaa ei kuitenkaan esiinny äärellisten graaﬁen yhteydessä COMP.CS.300 Tietorakenteet ja algoritmit 1 297 • syvyyteen ensin -haun avulla voi ratkaista joitakin monimutkaisempia ongelmia, kuten graaﬁn silmukoiden etsintä – harmaat solmut muodostavat lähtösolmusta nykyiseen solmuun vievän polun – mustista solmuista pääsee vain mustiin ja harmaisiin solmuihin ⇒jos nykyisestä solmusta pääsee harmaaseen solmuun, niin graaﬁssa on silmukka"
            },
            {
                "role": "user",
                "content": "Can you summarize the key topics covered in this lecture?"
            },
            {
                "role": "assistant",
                "content": "This lecture covers foundational algorithmic concepts including problem formulation, correctness proof using induction, efficiency analysis using asymptotic notation, data structures (arrays, linked lists, BSTs, heaps), and sorting methods (selection sort, merge sort, counting sort). Additionally, it introduces the Word-RAM computational model, providing a theoretical framework for measuring algorithm efficiency."
            }
        ]
    },
    {
        "system": "Hello! I am Kiran, an AI assistant from Tampere University, dedicated to providing you with accurate and insightful assistance. My goal is to help you understand concepts, summarize lectures, and answer questions effectively.",
        "conversations": [
            {
                "role": "host",
                "content": "COMP.CS.300 Tietorakenteet ja algoritmit 1 298 14 Lyhimmät painotetut polut BFS löytää lyhimmän polun lähtösolmusta graaﬁn saavutettaviin solmuihin. Se ei kuitenkaan enää suoriudu tehtävästä, jos kaarien läpi kulkeminen maksaa askelta enemmän. Tässä luvussa käsitellemme lyhimpien painotettujen polkujen etsintää graaﬁsta, jonka kaaripainot ovat positiivisia ja voivat poiketa ykkösestä. • negatiivisten kaaripainojen hallitsemiseen tarvitaan monimutkaisempia algoritmeja, esimerkiksi Bellman-Ford algoritmi COMP.CS.300 Tietorakenteet ja algoritmit 1 299 14.1 Lyhin polku Graaﬁn kaarilla voi olla ominaisuus nimeltä paino(weight). • paino voi edustaa vaikkapa reitin pituutta tai tilasiirtymän kustannusta • Graaﬁn G = (V, E) painofunktio w : E →R kaarilta reaalilukupainoille • Polun p = ⟨v0, v1, ..., vk⟩paino w(p) on sen muodostavien kaarten painojen summa w(p) = Pk i=1 w(vi−1, vi). Määritelmä: lyhimmän polun paino δ(u, v): δ(u, v) = ( min{w(p) : u p⇝v} jos on polkuu:sta v:hen, ∞ muuten. ja siten lyhin polku u:sta v:hen mikä tahansa polku p, jolle w(p) = δ(u, v) COMP.CS.300 Tietorakenteet ja algoritmit 1 300 Tämä mutkistaa lyhimmän reitin etsintää merkittävästi. • lyhin reitti on se lähtösolmusta etsittyyn solmuun kulkeva polku, jonka kaarien painojen summa on mahdollisimman pieni • jos jokaisen kaaren paino on 1, tehtävä voidaan ratkaista käymällä lähtösolmusta saavutettavissa oleva graaﬁn osa läpi leveyteen ensin -järjestyksessä • jos painot saattavat olla < 0, voi olla, että tehtävään ei ole ratkaisua, vaikka polkuja olisi olemassakin – jos graaﬁssa on silmukka, jonka kaaripainojen summa on negatiivinen saadaan mielivaltaisen pieni painojen summa kiertämällä silmukkaa tarpeeksi monta kertaa COMP.CS.300 Tietorakenteet ja algoritmit 1 301 14.2 Dijkstran algoritmi Suunnatun, painotetun graaﬁn G = (V, E), jossa kaaripainot ovat ei-negatiivisia, lyhimmät painotetut polut lähtösolmusta voi etsiä Dijkstran algoritmilla. • etsii lyhimmät polut lähtösolmusta s kaikkiin saavutettaviin solmuihin, painottaen kaarien pituuksia w:n mukaan • valitsee joka tilanteessa tutkittavakseen lyhimmän polun, jota se ei ole vielä tutkinut ⇒se on siis ahne algoritmi • oletus: w(u, v) ≥0 ∀(u, v) ∈E COMP.CS.300 Tietorakenteet ja algoritmit 1 302 DIJKSTRA(s, w) (algoritmi saa parametrinaan aloitussolmun s) 1 ▷alussa kaikkien solmujen kentät ovat arvoiltaan colour = WHITE, d = ∞, π = NIL 2 s→colour := GRAY (merkitään alkutila löydetyksi) 3 s→d := 0 (etäisyys alkutilasta alkutilaan on 0) 4 PUSH(Q, s) (työnnetään alkutila prioriteettijonoon) 5 while Q ̸= ∅do (jatketaan niin kauan kun solmuja riittää) 6 u := EXTRACT-MIN(Q) (otetaan prioriteettijonosta seuraava tila) 7 for each v ∈u→Adj do (käydään u:n naapurit läpi) 8 if v→colour = WHITE then (jos solmussa ei ole käyty . . . ) 9 v→colour := GRAY (. . . merkitään se löydetyksi) 10 PUSH(Q, v) (työnnetään tila jonoon odottamaan käsittelyä) 11 RELAX(u, v, w) 12 u→colour := BLACK (merkitään tila u käsitellyksi) RELAX(u, v, w) 1 if v→d > u→d + w(u, v) then (jos löydettiin uusi lyhyempi reitti tilaan v...) 2 v→d := u→d + w(u, v) (...pienennetään v:n etäisyyttä lähtösolmusta) 3 v→π := u (merkitään, että v:n tultiin u:sta) COMP.CS.300 Tietorakenteet ja algoritmit 1 303 Algoritmin käyttämä tietorakenne Q on prioriteettijono (luentomonisteen kohta 3.2). w sisältää kaikkien kaarien painot. Dijkstran algoritmi käyttää apufunktiota RELAX • kaaren (u, v) relaksointi (relaxation) testaa, voiko lyhintä löydettyä v:hen vievää polkua parantaa reitittämällä sen loppupää u:n kautta, ja tarvittaessa tekee niin Muilta osin algoritmi muistuttaa huomattavasti leveyteen ensin -hakua. • se löytää lyhimmät polut kasvavan pituuden mukaisessa järjestyksessä • kun solmu u otetaan Q:sta, sen painotettu etäisyys s:stä varmistuu u→d:ksi – jos prioriteettijonosta otettu tila on maalitila, voidaan algoritmin suoritus lopettaa saman tien COMP.CS.300 Tietorakenteet ja algoritmit 1 304 Alla olevassa kuvassa nähdään Dijkstran algoritmi tilanteessa, jossa mustalla ympyröidyt solmut on käsitelty. Alkutila 5 1 3 2 4 3 7 3 8 0 1 4 i i i 2 COMP.CS.300 Tietorakenteet ja algoritmit 1 305 Suoritusaika: • while-silmukka käy enintään O(V ) kierrosta ja for-silmukka yhteensä enintään O(E) kierrosta • prioriteettijonon voi toteuttaa tehokkaasti keon avulla tai vähemmän tehokkaasti listan avulla kekototeutuksella:listatoteutuksella: rivi 4: Θ(1) Θ(1) (tyhjään tietorakenteeseen lisääminen) rivi 5: Θ(1) Θ(1) (onko prioriteettijono tyhjä) rivi 6: O(lg V ) O(V ) (Extract-Min) rivi 10: Θ(1) Θ(1) (valkoisen solmun prioriteetti on ääretön, joten sen oikea paikka on keon lopussa) rivi 11: O(lg V ) Θ(1) (relaksoinnissa solmun prioriteetti voi muuttua) • käytettäessä kekototeutusta jokaisella while- ja for-silmukan kierroksella suoritetaan yksi O(lg V ) aikaa kuluttava operaatio ⇒algoritmin suoritusaika on O((V + E) lg V ) COMP.CS.300 Tietorakenteet ja algoritmit 1 306 14.3 A*-algoritmi Dijkstran algoritmi etsi lyhimmän painotetun reitin kartoittamalla solmuja lyhimmästä reitistä alkaen reitin pituusjärjestyksessä. Eli: Dijkstra käyttää hyväkseen vain jo kuljetuista kaarista saatavaa tietoa. A*-algoritmi tehostaa tätä lisäämällä heuristiikan (=oletuksen) lyhimmästä mahdollisesta etäisyydestä maaliin. (Esim. maantiereitin haussa etäisyys linnuntietä). • etsii lyhimmän painotetun polun lähtösolmusta s annettuun maalisolmuun g. Ei kartoita lyhintä reittiä kaikkiin solmuihin (kuten Dijkstra), vain maalisolmuun. • edellyttää, että painot ovat ei-negatiivisia (kuten Dijkstrakin) • edellyttää, että jokaiselle solmulle voidaan laskea sen minimietäisyys maalista (ts. löytynyt lyhin reitti ei voi olla lyhempi). • valitsee joka tilanteessa tutkittavakseen ei-tutkitun solmun, jossa (lyhin etäisyys lähdöstä solmuun + arvioitu minimietäisyys maaliin) on pienin. COMP.CS.300 Tietorakenteet ja algoritmit 1 307 Ainoa ero A*:n ja Dijkstran välillä on relaksointi (ja se, että A* kannattaa lopettaa heti maalisolmun löydyttyä, koska se ei kartoita lyhimpiä etäisyyksiä kaikkiin solmuihin). RELAX-A*(u, v, w) 1 if v→d > u→d + w(u, v) then (jos löydettiin uusi lyhyempi reitti tilaan v...) 2 v→d := u→d + w(u, v) (...uusi pituus tähän saakka...) 3 v→de := v→d + min_est(v, g) (...ja minimiarvio koko reitistä) 4 v→π := u (merkitään, että v:n tultiin u:sta) A*:n käyttämässä prioriteettijonossa käytetään prioriteettina koko reitin pituusarviota v→de. (Dijkstran algoritmi on A*:n erikoistapaus, jossa min_est(a, b) on aina 0.) COMP.CS.300 Tietorakenteet ja algoritmit 1 308 14.4 Kevyin virittävä puu Graaﬁn G = (V, E) kevyin virittävä puu on sen asyklinen aligraaﬁ, joka yhdistää kaikki graaﬁn solmut niin, että aligraaﬁn kaarien painojen summa on pienin mahdollinen. Puun löytämiseksi on kaksi algoritmia: Primin ja Kruskalin Prim muistuttaa Dijkstran algoritmin kun taas Kruskal lähestyy ongelmaa luomalla metsän, jossa on puu jokaiselle puulle ja sitten yhdistämällä näitä puuksi"
            },
            {
                "role": "user",
                "content": "Can you summarize the key topics covered in this lecture?"
            },
            {
                "role": "assistant",
                "content": "This lecture covers foundational algorithmic concepts including problem formulation, correctness proof using induction, efficiency analysis using asymptotic notation, data structures (arrays, linked lists, BSTs, heaps), and sorting methods (selection sort, merge sort, counting sort). Additionally, it introduces the Word-RAM computational model, providing a theoretical framework for measuring algorithm efficiency."
            }
        ]
    },
    {
        "system": "Hello! I am Kiran, an AI assistant from Tampere University, dedicated to providing you with accurate and insightful assistance. My goal is to help you understand concepts, summarize lectures, and answer questions effectively.",
        "conversations": [
            {
                "role": "host",
                "content": "COMP.CS.300 Tietorakenteet ja algoritmit 1 231 11.1 Hajautustaulu Hajautustaulun ideana on tiivistää dynaamisen joukon avainten arvoalue pienemmäksi hajautusfunktion (hash function) h avulla, siten että ne voidaan tallettaa taulukkoon. • taulukon etuna on sen tarjoama tehokas vakioaikainen indeksointi 123456 121212 201304 111222 201304 13579 567135 N=30000 h(k) 201304 123456 201304 121212 121212 13579 567135 111222 COMP.CS.300 Tietorakenteet ja algoritmit 1 232 Avainten arvoalueen tiivistämisestä seuraa kuitenkin ongelma: törmäykset. • useampi kuin yksi alkio voi hajautua samaan hajautustaulun lokeroon Tavallisin tapa ratkaista ongelma on ketjuttaminen (chaining). • samaan lokeroon hajautuvat alkiot talletetaan listoihin • muitakin ratkaisutapoja on – avoimen osoituksen käsittelytavalla alkio laitetaan sekundääriseen lokeroon, mikäli primäärinen ei ole vapaana – joissakin tapauksissa avainten arvoalue on niin pieni, että arvoalueen tiivistämistä ei tarvita, eikä siis synny törmäyksiäkään * tällainen suoraosoitustaulu (direct-access table) on hyvin yksinkertainen ja tehokas – tällä kurssilla kuitenkin käsitellään ainoastaan ketjutettuja hajautustauluja COMP.CS.300 Tietorakenteet ja algoritmit 1 233 Alla oleva kuva esittää ketjutettua hajautustaulua, jonka avaimet on hajautettu etukirjaimen mukaan viereisen taulukon avulla. h(k) alkukirjain 0 H P X 1 A I Q Y 2 B J R Z 3 C K S Ä 4 D L T Ö 5 E M U Å 6 F N V 7 G O W Onko tämä hyvä hajautus? • Ei. Katsotaan seuraavaksi, miksei. COMP.CS.300 Tietorakenteet ja algoritmit 1 234 Ketjutettu hajautustaulu tarjoaa ainoastaan sanakirjan operaatiot, mutta ne ovat hyvin yksinkertaisia: CHAINED-HASH-SEARCH(T, k) ▷etsi listasta T[h(k)] alkio, jonka avain on k CHAINED-HASH-INSERT(T, x) ▷lisää x listan T[h(x→key)] alkuun CHAINED-HASH-DELETE(T, x) ▷poista x listasta T[h(x→key)] COMP.CS.300 Tietorakenteet ja algoritmit 1 235 Suoritusajat: • lisäys: Θ(1) • etsintä: hitaimmassa tapauksessa Θ(n) • poisto: jos lista kaksisuuntainen, niin Θ(1); yksisuuntaisella hitaimmillaan Θ(n), koska poistettavan edeltäjä on ensin etsittävä listasta – käytännössä ero ei kuitenkaan ole kovin merkittävä, koska yleensä poistettava alkio joudutaan joka tapauksessa etsimään listasta Ketjutetun hajautustaulun operaatioiden keskimääräiset suoritusajat riippuvat listojen pituuksista. COMP.CS.300 Tietorakenteet ja algoritmit 1 236 • huonoimmassa tapauksessa kaikki alkiot joutuvat samaan listaan jolloin suoritusajat ovat Θ(n) • keskimääräisen tapauksen selville saamiseksi käytämme seuraavia merkintöjä: – m = hajautustaulun koko – n = alkioiden määrä taulussa – α = n m = täyttöaste (load factor) eli listan keskimääräinen pituus • lisäksi keskimääräisen suoritusajan arvioimiseksi on tehtävä oletus siitä, miten hyvin h hajauttaa alkiot – jos esim. h(k) = nimen alkukirjaimen 3 ylintä bittiä, niin kaikki osuvat samaan listaan – usein oletetaan että, jokaisella alkiolla on yhtä suuri todennäköisyys osua mihin tahansa lokeroon – tasainen hajautus (simple uniform hashing) – oletetaan myös, että h(k):n laskenta kuluttaa Θ(1) aikaa COMP.CS.300 Tietorakenteet ja algoritmit 1 237 • jos etsitään alkiota, jota ei ole taulussa, niin joudutaan selaamaan koko lista läpi ⇒joudutaan tutkimaan keskimäärin α alkiota ⇒suoritusaika keskimäärin Θ(1 + α) • jos oletetaan, että listassa oleva avain on mikä tahansa listan alkio samalla todennäköisyydellä, joudutaan listaa selamaan etsinnän yhteydessä keskimäärin puoleen väliin siinäkin tapauksessa, että avain löytyy listasta ⇒suoritusaika keskimäärin Θ(1 + α 2) = Θ(1 + α) • jos täyttöaste pidetään alle jonkin kiinteän rajan (esim. α < 50 %), niin Θ(1 + α) = Θ(1) ⇒ketjutetun hajautustaulun kaikki operaatiot voi toteuttaa keskimäärin ajassa Θ(1) – tämä edellyttää, että hajautustaulun koko on samaa luokkaa kuin sinne talletettavien alkioiden määrä COMP.CS.300 Tietorakenteet ja algoritmit 1 238 Laskiessamme keskimääräistä suoritusaikaa oletimme, että hajautusfunktio hajauttaa täydellisesti. Ei kuitenkaan ole mitenkään itsestään selvää, että näin tapahtuu. Hajautusfunktion laatu on kriittisin tekijä hajautustaulun suorituskyvyn muodostumisessa. Hyvän hajautusfunktion ominaisuuksia: • hajautusfunktion on oltava deterministinen – muutoin kerran tauluun pantua ei välttämättä enää koskaan löydetä! • tästä huolimatta olisi hyvä, että hajautusfunktion arvo olisi mahdollisimman “satunnainen” – kuhunkin lokeroon tulisi osua mahdollisimman tarkasti 1 m avaimista COMP.CS.300 Tietorakenteet ja algoritmit 1 239 • valitettavasti täysin tasaisesti hajottavan hajautusfunktion teko on useinmiten mahdotonta – eri arvojen esiintymistodennäköisyydet aineistossa ovat yleensä tuntemattomia – aineisto ei yleensä ole tasaisesti jakautunut * lähes mikä tahansa järkevä hajautusfunktio jakaa tasaisesti jakautuneen aineiston täydellisesti • yleensä hajautusfunktio pyritään muodostamaan siten, että se sotkee tehokkaasti kaikki syöteaineistossa luultavasti esiintyvät säännönmukaisuudet – esimerkiksi nimien tapauksessa ei katsota yksittäisiä kirjaimia, vaan otetaan jotenkin huomioon nimen kaikki bitit COMP.CS.300 Tietorakenteet ja algoritmit 1 240 • esittelemme kaksi yleistä usein hyvin toimivaa hajautusfunktion luontimenetelmää • oletamme, että avaimet ovat luonnollisia lukuja 0, 1, 2, . . . – jollei näin ole, avain voidaan usein tulkita luonnolliseksi luvuksi – esim. nimen saa luvuksi muuttamalla kirjaimet numeroiksi ASCII-koodiarvon mukaan, ja laskemalla ne sopivasti painottaen yhteen COMP.CS.300 Tietorakenteet ja algoritmit 1 241 Hajautusfunktion luonti jakomenetelmällä on yksinkertaista ja nopeaa. • h(k) = k mod m • sitä kannattaa kuitenkin käyttää vain, jos m:n arvo on sopiva • esim. jos m = 2b jollekin b ∈N = {0, 1, 2, . . .}, niin h(k) = k:n b alinta bittiä ⇒funktio ei edes katso kaikkia k:n bittejä ⇒funktio todennäköisesti hajauttaa huonosti, jos avaimet ovat peräisin binäärijärjestelmästä COMP.CS.300 Tietorakenteet ja algoritmit 1 242 • samasta syystä tulee välttää m:n arvoja muotoa m = 10b, jos avaimet ovat peräisin kymmenjärjestelmän luvuista • jos avaimet ovat muodostetut tulkitsemalla merkkijono 128-järjestelmän luvuksi, niin m = 127 on huono valinta, koska silloin saman merkkijonon kaikki permutaatiot osuvat samaan lokeroon • hyviä m:n arvoja ovat yleensä alkuluvut, jotka eivät ole lähellä 2:n potensseja – esim. halutaan ≈700 listaa ⇒701 kelpaa • kannattaa tarkistaa kokeilemalla pienellä “oikealla” aineistolla, hajauttaako funktio avaimet tehokkaasti COMP.CS.300 Tietorakenteet ja algoritmit 1 243 Hajautusfunktion luonti kertomenetelmällä ei aseta suuria vaatimuksia m:n arvolle. • valitaan vakio A siten, että 0 < A < 1 • h(k) = ⌊m(kA −⌊kA⌋)⌋ • jos m = 2b, koneen sanapituus on w, ja k ja 2w · A mahtuvat yhteen sanaan, niin h(k) voidaan laskea helposti seuraavasti: h(k) = ⌊(((2w · A) · k) mod 2w) 2w−b ⌋ • mikä arvo A:lle tulisi valita? – kaikki A:n arvot toimivat ainakin jollain lailla – kuulemma A ≈ √ 5−1 2 toimii usein aika hyvin"
            },
            {
                "role": "user",
                "content": "Can you summarize the key topics covered in this lecture?"
            },
            {
                "role": "assistant",
                "content": "This lecture covers foundational algorithmic concepts including problem formulation, correctness proof using induction, efficiency analysis using asymptotic notation, data structures (arrays, linked lists, BSTs, heaps), and sorting methods (selection sort, merge sort, counting sort). Additionally, it introduces the Word-RAM computational model, providing a theoretical framework for measuring algorithm efficiency."
            }
        ]
    },
    {
        "system": "Hello! I am Kiran, an AI assistant from Tampere University, dedicated to providing you with accurate and insightful assistance. My goal is to help you understand concepts, summarize lectures, and answer questions effectively.",
        "conversations": [
            {
                "role": "host",
                "content": "COMP.CS.300 Tietorakenteet ja algoritmit 1 244 12 Binäärihakupuu http://imgur.com/L77FY5X Tässä luvussa käsitellään erilaisia yleisiä puurakenteita. • Ensin opitaan, millainen rakenne on binäärihakupuu, • ja tasapainotetaan binäärihakupuu muuttamalla se puna-mustaksi puuksi. • Sitten tutustutaan monihaaraisiin puihin: merkkijonopuu Trie ja B-puu. • Lopuksi vilkaistaan splay- ja AVL-puita. COMP.CS.300 Tietorakenteet ja algoritmit 1 245 12.1 Tavallinen binäärihakupuu Kertauksena: Binääripuu (binary tree) on äärellinen solmuista (node) koostuva rakenne, joka on joko • tyhjä, tai • sisältää yhden solmun nimeltä juuri (root), sekä kaksi binääripuuta nimeltä vasen alipuu (left subtree) ja oikea alipuu (right subtree). Kuva 14: Kertaus: Binääripuu COMP.CS.300 Tietorakenteet ja algoritmit 1 246 Lisäksi määritellään: • Lapseton solmu: lehti (leaf). • Muut solmut sisäsolmuja. • Solmu on lastensa isä (pa- rent) ja solmun esi-isiä (ancestor) ovat solmu itse, solmun isä, tämän isä jne. • Jälkeläinen (descendant) vastaavasti. 13 25 33 lehtiä sisäsolmuja juuri 14 18 9 23 5 15 3 8 35 32 26 22 30 6 COMP.CS.300 Tietorakenteet ja algoritmit 1 247 Binäärihakupuu (binary search tree) on binääripuu, jonka kaikille solmuille x pätee: Jos l on mikä tahansa solmu x:n vasemmassa alipuussa ja r mikä tahansa solmu x:n oikeassa alipuussa, niin l.key ≤x.key ≤r.key • Edellisen sivun binääripuu on binäärihakupuu • Luvussa 8.2 esitelty kekorakenne on binääripuu muttei binäärihakupuu Useinmiten binäärihakupuu esitetään linkitettynä rakenteena, jossa jokaisessa alkiossa on kentät avain (key), vasen lapsi (left), oikea lapsi (right) ja vanhempi (p (parent)). Lisäksi alkiolla on oheisdataa. COMP.CS.300 Tietorakenteet ja algoritmit 1 248 Kuva 15: Hakupuita COMP.CS.300 Tietorakenteet ja algoritmit 1 249 Avaimen haku binäärihakupuusta : • koko puusta haku R-TREE-SEARCH(T.root, k) • palauttaa osoittimen x solmuun, jolle x→key = k, tai NIL, jos tällaista solmua ei ole R-TREE-SEARCH(x, k) 1 if x = NIL or k = x→key then 2 return x (etsitty avain löytyi) 3 if k < x→key then (jos etsitty on pienempi kuin avain...) 4 return R-TREE-SEARCH(x→left, k) (...etsitään vasemmasta alipuusta) 5 else (muuten...) 6 return R-TREE-SEARCH(x→right, k) (...etsitään oikeasta alipuusta) COMP.CS.300 Tietorakenteet ja algoritmit 1 250 Algoritmi suunnistaa juuresta alaspäin huonoimmassa tapauksessa pisimmän polun päässä olevaan lehteen asti. • suoritusaika O(h), missä h on puun korkeus • lisämuistin tarve O(h), rekursion vuoksi Saman voi tehdä myös ilman rekursiota, mikä on suositeltavaa. • tällöin lisämuistin tarve on vain Θ(1) • ajoaika on yhä O(h) TREE-SEARCH(x, k) 1 while x ̸= NIL and k ̸= x→key do (kunnes avain on löytynyt tai ollaan lehdessä) 2 if k < x→key then (jos etsitty on pienempi kuin avain...) 3 x := x→left (...siirrytään vasemmalle) 4 else (muuten...) 5 x := x→right (...siirrytään oikealle) 6 return x (palautetaan tulos) COMP.CS.300 Tietorakenteet ja algoritmit 1 251 Minimi ja maksimi: • minimi löydetään menemällä vasemmalle niin kauan kun se on mahdollista TREE-MINIMUM(x) 1 while x→left ̸= NIL do 2 x := x→left 3 return x • maksimi löydetään vastaavasti menemällä oikealle niin kauan kun se on mahdollista TREE-MAXIMUM(x) 1 while x→right ̸= NIL do 2 x := x→right 3 return x • molempien ajoaika on O(h) ja lisämuistin tarve Θ(1) COMP.CS.300 Tietorakenteet ja algoritmit 1 252 Solmun seuraajaa ja edeltäjää kannattaa etsiä binäärihakupuusta puun rakenteen avulla mieluummin kuin avainten arvojen perusteella. • tällöin kaikki alkiot saadaan käytyä niiden avulla läpi, vaikka puussa olisi yhtä suuria avaimia ⇒tarvitaan siis algoritmi, joka etsii annettua solmua välijärjestyksessä seuraavan solmun • sellainen voidaan rakentaa algoritmin TREE-MINIMUM avulla Kuva 16: Solmun seuraaja? COMP.CS.300 Tietorakenteet ja algoritmit 1 253 Binäärihakupuun solmun seuraaja on joko: • oikean alipuun pienin alkio • tai solmusta juureen vievällä polulla ensimmäinen kohdattu solmu, jonka vasempaan alipuuhun solmu kuuluu jos edellä mainittuja solmuja ei löydy, on kysymyksessä puun suurin solmu Kuva 17: Seuraajat COMP.CS.300 Tietorakenteet ja algoritmit 1 254 TREE-SUCCESSOR(x) 1 if x→right ̸= NIL then (jos oikea alipuu löytyy...) 2 return TREE-MINIMUM(x→right) (...etsitään sen minimi) 3 y := x→p (muuten lähdetään kulkemaan kohti juurta) 4 while y ̸= NIL and x = y→right do (kunnes ollaan tultu vasemmasta lapsesta) 5 x := y 6 y := y→p 7 return y (palautetaan löydetty solmu) • huomaa, että avainten arvoja ei edes katsota! • vrt. seuraajan löytäminen järjestetystä listasta • ajoaika O(h), lisämuistin tarve Θ(1) • TREE-PREDECESSOR voidaan toteuttaa vastaavalla tavalla COMP.CS.300 Tietorakenteet ja algoritmit 1 255 TREE-SUCCESSORIN ja TREE-MINIMUMIN avulla voidaan rakentaa toinen tapa selata puu läpi välijärjestyksessä. TREE-SCAN-ALL(T) 1 if T.root ̸= NIL then 2 x := TREE-MINIMUM(T.root) (aloitetaan selaus puun minimistä) 3 else 4 x := NIL 5 while x ̸= NIL do (selataan niin kauan kun seuraajia löytyy) 6 käsittele alkio x 7 x := TREE-SUCCESSOR(x) • jokainen kaari kuljetaan kerran molempiin suuntiin ⇒TREE-SCAN-ALL selviää ajassa Θ(n), vaikka kutsuukin TREE-SUCCESSORia n kertaa COMP.CS.300 Tietorakenteet ja algoritmit 1 256 • lisämuistin tarve Θ(1) ⇒TREE-SCAN-ALL on asymptoottisesti yhtä nopea, ja muistinkulutukseltaan asymptoottisesti parempi kuin INORDER-TREE-WALK – vakiokertoimissa ei suurta eroa ⇒kannattaa valita TREE-SCAN-ALL, jos tietueissa on p-kentät • TREE-SCAN-ALL sallii useat yhtäaikaiset selaukset, INORDER-TREE-WALK ei COMP.CS.300 Tietorakenteet ja algoritmit 1 257 Lisäys binäärihakupuuhun: TREE-INSERT(T, z) (z osoittaa käyttäjän varaamaa alustettua tietuetta) 1 y := NIL; x := T.root (aloitetaan juuresta) 2 while x ̸= NIL do (laskeudutaan kunnes kohdataan tyhjä paikka) 3 y := x (otetaan potentiaalinen isä-solmu talteen) 4 if z→key < x→key then (siirrytään oikealle tai vasemmalle) 5 x := x→left 6 else 7 x := x→right 8 z→p := y (sijoitetaan löydetty solmu uuden solmun isäksi) 9 if y = NIL then 10 T.root := z (puun ainoa solmu on juuri) 11 else if z→key < y→key then (sijoitetaan uusi solmu isänsä vasemmaksi . . . ) 12 y→left := z 13 else (. . . tai oikeaksi lapseksi) 14 y→right := z 15 z→left := NIL; z→right := NIL Algoritmi suunnistaa juuresta lehteen; uusi solmu sijoitetaan aina lehdeksi. ⇒ajoaika O(h), lisämuistin tarve Θ(1) COMP.CS.300 Tietorakenteet ja algoritmit 1 258 Poisto on monimutkaisempaa, koska se voi kohdistua sisäsolmuun: TREE-DELETE(T, z) (z osoittaa poistettavaa solmua) 1 if z→left = NIL or z→right = NIL then (jos z:lla on vain yksi lapsi . . . ) 2 y := z (. . . asetetaan z poistettavaksi tietueeksi) 3 else 4 y := TREE-SUCCESSOR(z) (muuten poistetaan z:n seuraaja) 5 if y→left ̸= NIL then (otetaan talteen poistettavan ainoa lapsi) 6 x := y→left 7 else 8 x := y→right 9 if x ̸= NIL then (jos lapsi on olemassa . . . ) 10 x→p := y→p (. . . linkitetään se poistettavan tilalle) 11 if y→p = NIL then (jos poistettava oli juuri . . . ) 12 T.root := x (. . . merkitään x uudeksi juureksi) 13 else if y = y→p→left then (sijoitetaan x poistettavan tilalle . . . ) 14 y→p→left := x (. . . sen isän vasemmaksi . . . ) 15 else 16 y→p→right := x (. . . tai oikeaksi lapseksi) 17 if y ̸= z then (jos poistettiin joku muu kuin z . . . ) 18 z→key := y→key (. . . vaihdetaan poistetun ja z:n datat) 19 z→satellitedata := y→satellitedata 20 return y (palautetaan osoitin poistettuun solmuun) COMP.CS.300 Tietorakenteet ja algoritmit 1 259 Huom! Rivillä 5 todellakin tiedetään, että y:llä on korkeintaan yksi lapsi. • jos z:lla on vain yksi lapsi, y on z • jos rivillä 4 kutsutaan TREE-SUCCESSORIA tiedetään, että z:lla on oikea alipuu, jonka minimi on y – minimillä ei voi olla vasenta lasta Algoritmi näyttää monimutkaiselta, mutta rivin 4 TREE-SUCCESSORIA lukuunottamatta kaikki operaatiot ovat vakioaikaisia. ⇒ajoaika on siis O(h) ja lisämuistin tarve Θ(1) Siis kaikki dynaamisen joukon perusoperaatiot saadaan binäärihakupuulla toimimaan ajassa O(h) ja lisämuistilla Θ(1): SEARCH, INSERT, DELETE, MINIMUM, MAXIMUM, SUCCESSOR ja PREDECESSOR COMP.CS.300 Tietorakenteet ja algoritmit 1 260 Binäärihakupuita - kuten muitakin tietorakenteita - voi sovittaa uusiin tehtäviin lisäämällä uusia tehtävän kannalta olennaista tietoa sisältäviä kenttiä. • tällöin perusoperaatiot tulee muokata ylläpitämään myös uusien kenttien sisältöä • esimerkiksi lisäämällä solmuihin kenttä, joka kertoo solmun virittämän alipuun koon – saadaan toteutettua algoritmi, joka palauttaa puun korkeuteen nähden lineaarisessa ajassa i:nnen alkion – saadaan toteutettua algoritmi, joka puun korkeuteen nähden lineaarisessa ajassa kertoo, monesko kysytty alkio on suuruusjärjestyksessä – ilman ylimääräistä kenttää algoritmit täytyisi toteuttaa huomattavasti tehottomammin puun alkioiden määrään nähden lineaarisessa ajassa COMP.CS.300 Tietorakenteet ja algoritmit 1 261 12.2 Kuinka korkeita binäärihakupuut yleensä ovat? Kaikki dynaamisen joukon perusoperaatiot saatiin binäärihakupuulla toimimaan ajassa O(h). ⇒puun korkeus on tehokkuuden kannalta keskeinen tekijä. Jos oletetaan, että alkiot on syötetty satunnaisessa järjestyksessä, ja jokainen järjestys on yhtä todennäköinen, suoraan INSERTillä rakennetun binäärihakupuun korkeus on keskimäärin Θ(lg n). ⇒kaikki operaatiot keskimäärin Θ(lg n) COMP.CS.300 Tietorakenteet ja algoritmit 1 262 Valitettavasti lopputulos on surkeampi, jos avai- met syötetään (lähes) suuruusjärjestyksessä, ku- ten viereisestä kuvasta voi havaita. • korkeus on n -1, surkeaa! Ongelmaa ei pystytä ratkaisemaan järkevästi esimerkiksi satunnaistamalla, jos halutaan säilyt- tää kaikki dynaamisen joukon operaatiot. Puu pitää siis tasapainottaa. Siihen palataan myöhemmin. 7 6 5 1 4 3 2 COMP.CS.300 Tietorakenteet ja algoritmit 1 309 15 Tasapainotetut puurakenteet Binäärihakupuu toteuttaa kaikki dynaamisen joukon operaatiot O(h) ajassa Kääntöpuolena on, että puu voi joskus litistyä listaksi, jolloin tehokkuus menetetään (O(n)) Tässä luvussa käsitellään tapoja pitää huolta siitä, ettei litistymistä käy Ensin opitaan tasapainoitus puna-mustan puun invarianttia ylläpitämällä Lopuksi vilkaistaan muista tasapainotetuista binäärihakupuista Splay- ja AVL-puita COMP.CS.300 Tietorakenteet ja algoritmit 1 310 15.1 Puna-musta binäärihakupuu Puna-mustat puut ovat tasapainotettuja binäärihakupuita. Ne tekevät lisäysten ja poistojen yhteydessä tasapainotustoimenpiteitä, jotka takaavat, ettei haku ole koskaan tehoton vaikka alkiot olisikin lisätty puuhun epäsuotuisassa järjestyksessä. • puna-musta puu ei voi koskaan litistyä listaksi, kuten perusbinäärihakupuu Kuva 23: Punamustapuu (via Wikipedia, ©Colin M.L. Burnett (CC BY-SA 3.0)) COMP.CS.300 Tietorakenteet ja algoritmit 1 311 Puna-mustien puiden perusidea: • jokaisessa solmussa on yksi lisäbitti: väri (colour) – arvot punainen ja musta • muut kentät ovat vanhat tutut key, left, right ja p – jätämme oheisdatan näyttämättä, jotta pääideat eivät hukkuisi yksityiskohtien taakse • värikenttien avulla ylläpidetään puna-mustan puun invarianttia, joka takaa, että puun korkeus on aina kertaluokassa Θ(lg n) COMP.CS.300 Tietorakenteet ja algoritmit 1 312 Puna-mustien puiden invariantti: 1. Jos solmu on punainen, niin sillä joko • ei ole lapsia, tai • on kaksi lasta, ja ne molemmat ovat mustia. 2. Jokaiselle solmulle pätee: jokainen solmusta alas 1- tai 0-lapsiseen solmuun vievä polku sisältää saman määrän mustia solmuja. 3. Juuri on musta. Solmun x musta-korkeus (black-height) bh(x) on siitä alas 1- tai 0-lapsiseen solmuun vievällä polulla olevien mustien solmujen määrä. • invariantin osan 3 mukaisesti jokaisen solmun mustakorkeus on yksikäsitteinen • jokaisella vaihtoehtoisella polulla on sama määrä mustia solmuja • koko puun mustakorkeus on sen juuren mustakorkeus COMP.CS.300 Tietorakenteet ja algoritmit 1 313 Puna-mustan puun maksimikorkeus • merkitään korkeus = h ja solmujen määrä = n • kunkin juuresta lehteen vievän polun solmuista vähintään puolet (⌊h 2⌋+ 1) ovat mustia (invariantin osat 1 ja 3) • jokaisella juuresta lehteen vievällä polulla on saman verran mustia solmuja (invariantin osa 2) ⇒ainakin ⌊h 2⌋+ 1 ylintä tasoa täysiä ⇒n ≥2 h 2 ⇒h ≤2 lg n Invariantti siis todellakin takaa puun korkeuden pysymisen logaritmisena puun alkioiden määrään nähden. ⇒Dynaamisen joukon operaatiot SEARCH, MINIMUM, MAXIMUM, SUCCESSOR ja PREDECESSOR saadaan toimimaan puna-mustille puille ajassa O(lg n). • binäärihakupuulle operaatiot toimivat ajassa O(h), ja puna-musta puu on binäärihakupuu, jolle h = Θ(lg n) COMP.CS.300 Tietorakenteet ja algoritmit 1 314 Puna-mustien puiden ylläpitämiseen ei kuitenkaan voida käyttää samoja lisäys- ja poistoalgoritmeja kuin tavallisilla binäärihakupuilla, koska ne saattavat rikkoa invariantin. Niiden sijaan käytetään algoritmeja RB-INSERT ja RB-DELETE. • operaatiot RB-INSERT ja RB-DELETE perustuvat kiertoihin (rotation) • kiertoja on kaksi: vasemmalle ja oikealle • ne muuttavat puun rakennetta, mutta säilyttävät binäärihakupuiden perusominaisuuden kaikille solmuille x y C A B C B A x y x y • kierto vasemmalle – olettaa, että solmut x ja y ovat olemassa • kierto oikealle vastaavasti – left ja right vaihtaneet paikkaa COMP.CS.300 Tietorakenteet ja algoritmit 1 315 LEFT-ROTATE(T, x) 1 y := x→right; x→right := y→left 2 if y→left ̸= NIL then 3 y→left→p := x 4 y→p := x→p 5 if x→p = NIL then 6 T.root := y 7 else if x = x→p→left then 8 x→p→left := y 9 else 10 x→p→right := y 11 y→left := x; x→p := y • molempien kiertojen ajoaika on Θ(1) • ainoastaan osoittimia muutetaan COMP.CS.300 Tietorakenteet ja algoritmit 1 316 Lisäyksen perusidea • ensin uusi solmu lisätään kuten tavalliseen binäärihakupuuhun • sitten lisätty väritetään punaiseksi • mitä puna-mustien puiden perusominaisuuksia näin tehty lisäys voi rikkoa? COMP.CS.300 Tietorakenteet ja algoritmit 1 317 • Invariantin osa – 1 rikkoutuu lisätyn solmun osalta, jos sen isä on punainen; muuten se ei voi rikkoutua. – 2 ei rikkoudu, koska minkään solmun alla olevien mustien solmujen määrät ja sijainnit eivät muutu, ja lisätyn alla ei ole solmuja. – 3 rikkoutuu, jos puu oli alun perin tyhjä. COMP.CS.300 Tietorakenteet ja algoritmit 1 318 • korjataan puu seuraavasti: – ominaisuutta 2 pilaamatta siirretään 1:n rike ylöspäin kunnes se katoaa – lopuksi 3 korjataan värittämällä juuri mustaksi (ei voi pilata ominaisuuksia 1 ja 2) • 1:n rike = sekä solmu että sen isä ovat punaisia • siirto tapahtuu värittämällä solmuja ja tekemällä kiertoja COMP.CS.300 Tietorakenteet ja algoritmit 1 319 RB-INSERT(T, x) 1 TREE-INSERT(T, x) 2 x→colour := RED (suoritetaan silmukkaa kunnes rike on hävinnyt tai ollaan saavutettu juuri) 3 while x ̸= T.root and x→p→colour = RED do 4 if x→p = x→p→p→left then 5 y := x→p→p→right 6 if y ̸= NIL and y→colour = RED then (siirretään rikettä ylöspäin) 7 x→p→colour := BLACK 8 y→colour := BLACK 9 x→p→p→colour := RED 10 x := x→p→p 11 else (siirto ei onnistu →korjataan rike) 12 if x = x→p→right then 13 x := x→p; LEFT-ROTATE(T, x) 14 x→p→colour := BLACK 15 x→p→p→colour := RED 16 RIGHT-ROTATE(T, x→p→p) 17 else . . . ▷sama kuin rivit 5. . . 16 paitsi “left” ja “right” vaihtaneet paikkaa 30 T.root→colour := BLACK (väritetään juuri mustaksi) COMP.CS.300 Tietorakenteet ja algoritmit 1 320 x−>p−>p x−>p x y C B x A A C B Ominaisuuden 1 rikkeen siirto ylöspäin: • solmu x ja sen isä ovat molemmat punai- sia. • myös solmun x setä on punainen ja isoisä musta. ⇒rike siirretään ylöspäin värittämällä sekä x:n setä että isä mustiksi ja isoisä punaiseksi. Korjauksen jälkeen: • ominaisuus 1 saattaa olla edelleen rikki – solmu x ja sen isä saattavat molemmat olla punaisia • ominaisuus 2 ei rikkoudu – kaikkien polkujen mustien solmujen määrä pysyy samana • ominaisuus 3 saattaa rikkoutua – jos ollaan noustu juureen asti, se on saatettu värittää punaiseksi COMP.CS.300 Tietorakenteet ja algoritmit 1 321 x−>p−>p x−>p y x x x−>p x−>p−>p y C D B A D C A B Mikäli punaista setää ei ole olemassa, rikettä ei voi siirtää ylöspäin vaan se täytyy poistaa käyttäen monimutkaisem- paa menetelmää: • Varmistetaan ensin, että x on isänsä vasen lapsi tekemällä tarvittaessa kier- to vasemmalle. COMP.CS.300 Tietorakenteet ja algoritmit 1 322 y x x−>p−>p x−>p x D C B A D A B C • tämän jälkeen väritetään x:n isä mustaksi ja isoisä punaiseksi, ja suo- ritetaan kierto oikealle – isoisä on varmasti musta, koska muuten puussa olisi ollut kaksi pu- naista solmua päällekkäin jo en- nen lisäystä Korjauksen jälkeen: • puussa ei enää ole päällekkäisiä punaisia solmuja • korjausoperaatiot yhdessä eivät ri- ko 2. ominaisuutta ⇒puu on ehjä ja korjausalgoritmin suorittaminen voidaan lopettaa COMP.CS.300 Tietorakenteet ja algoritmit 1 323 Poistoalgoritmin yleispiirteet • ensin solmu poistetaan kuten tavallisesta binäärihakupuusta – w osoittaa poistettua solmua • jos w oli punainen tai puu tyhjeni kokonaan, puna-musta-ominaisuudet säilyvät voimassa ⇒ei tarvitse tehdä muuta • muussa tapauksessa korjataan puu RB-DELETE-FIXUPin avulla aloittaen w:n (mahdollisesta) lapsesta x ja sen isästä w→p – TREE-DELETE takaa, että w:llä oli enintään yksi lapsi RB-DELETE(T, z) 1 w := TREE-DELETE(T, z) 2 if w→colour = BLACK and T.root ̸= NIL then 3 if w→left ̸= NIL then 4 x := w→left 5 else 6 x := w→right 7 RB-DELETE-FIXUP(T, x, w→p) 8 return w COMP.CS.300 Tietorakenteet ja algoritmit 1 324 RB-DELETE-FIXUP(T, x, y) 1 while x ̸= T.root and (x = NIL or x→colour = BLACK) do 2 if x = y→left then 3 w := y→right 4 if w→colour = RED then 5 w→colour := BLACK; y→colour := RED 6 LEFT-ROTATE(T, y); w := y→right 7 if (w→left = NIL or w→left→colour = BLACK) and (w→right = NIL or w→right→colour = BLACK) then 8 w→colour := RED; x := y 9 else 10 if w→right = NIL or w→right→colour = BLACK then 11 w→left→colour := BLACK 12 w→colour := RED 13 RIGHT-ROTATE(T, w); w := y→right 14 w→colour := y→colour; y→colour := BLACK 15 w→right→colour := BLACK; LEFT-ROTATE(T, y) 16 x := T.root 17 else . . . ▷sama kuin rivit 3. . . 16 paitsi “left” ja “right” vaihtaneet paikkaa 32 y := y→p 33 x→colour := BLACK COMP.CS.300 Tietorakenteet ja algoritmit 1 325 15.2 AVL-puut ja Splay-puut AVL puu (Adelson-Velsky, Landis mukaan) on binäärihakupuu, jossa jokaisella solmulla on tasapainokerroin: 0, +1, tai -1, kun tasapainossa. • kerroin määräytyy solmun oikean ja vasemman alipuun korkeuksien erotuksesta. Kun uuden solmun lisäys tekee AVL-puusta epätasapainoisen, puu palautetaan tasapainoiseksi tekemällä rotaatioita. COMP.CS.300 Tietorakenteet ja algoritmit 1 326 Mahdollisia rotaatioita on neljä: • Oikealle • Vasemmalle • Kaksois-rotaatio vasen-oikea • Kaksois-rotaatio oikea-vasen 1 2 3 1 2 3 1 2 3 1 2 3 3 1 2 R 1 2 3 1 3 2 1 2 3 L LR RL COMP.CS.300 Tietorakenteet ja algoritmit 1 327 Splay puu on binäärihakupuu, jossa lisäominaisuutena viimeksi haetut alkiot ovat nopeita hakea uudelleen. Splay-operaatio suoritetaan solmulle haun yhteydessä. Tämä ns. splay-askelien sekvenssi siirtää solmun askel askeleelta lähemmäksi juurta ja lopulta juureksi. • Zig-askel: COMP.CS.300 Tietorakenteet ja algoritmit 1 328 • Zig-Zig-askel: • Zig-Zag-askel:"
            },
            {
                "role": "user",
                "content": "Can you summarize the key topics covered in this lecture?"
            },
            {
                "role": "assistant",
                "content": "This lecture covers foundational algorithmic concepts including problem formulation, correctness proof using induction, efficiency analysis using asymptotic notation, data structures (arrays, linked lists, BSTs, heaps), and sorting methods (selection sort, merge sort, counting sort). Additionally, it introduces the Word-RAM computational model, providing a theoretical framework for measuring algorithm efficiency."
            }
        ]
    },
    {
        "system": "Hello! I am Kiran, an AI assistant from Tampere University, dedicated to providing you with accurate and insightful assistance. My goal is to help you understand concepts, summarize lectures, and answer questions effectively.",
        "conversations": [
            {
                "role": "host",
                "content": "Mergesort: a divide and conquer sorting algorithm 1. Introduction 2. Merging 3. Mergesort computation 1. Introduction Q: What does Mergesort do? A: Mergesort will place elements in an array in order (smallest to largest or largest to smallest). Mergesort properties: • requires extra sorting space; to sort array A[1..n] we need Temp[1..n] • uses divide-and-conquer • almost always presented as recursive • uses merging Q: What is merging? A: Combining two sorted arrays into one sorted array. Before merging: two sorted arrays P[1..r] and Q[1..s] After merging: one sorted array that includes all elements from P[1..r] and Q[1..s] Example starting arrays: after merging: 2. Merging Merging sorted arrays P[1..r] and Q[1..s] into single sorted array R[1..(r+s)] uses iterative strategy. Iteration: - add to end of R smallest element from either P or Q that has yet to be added, until either P or Q is empty Final stage: - add to end of R remaining elements of either P or Q, whichever is not empty Pseudocode in MERGE Example starting array: after while- loop iteration arrays A and Temp iL iR iA A = Temp = compute MERGE(A, 1, 4, 8) After handling remainder of subarray A[1..4] in line 30. A = Comments on MERGE: • to handle ’remainder’ of one nonempty subarray for-loop is needed at line 28 and line 30 • ’remainder’ of one nonempty subarray handled via test in while-statement (line 16) and lines 27-30; other approaches are possible • merging can be done without work array Temp, but it is complicated • similar approach can be used for merging arrays that have been rearranged according to other criteria 3. Mergesort computation How is divide and conquer used in merge sort? Divide: split starting array A[1..n] in two equally sized halves: A[1..M] and A[(M+1)..n] Conquer: sort (recursively) both halves A[1..M] and A[(M + 1)..n] Combine: merge sorted halves A[1..M] and A[(M + 1)..n] Example Recursion case: array to be sorted has at least 2 elements Base cases: array to be sorted 1 or less elements Mergesort pseudocode Example starting array: A = compute MERGESORT(A, 1, 8) step code line(s) computation array A recursion level MERGESORT(A, 1, 4) MERGESORT(A, 1, 2) MERGESORT(A, 1, 1) MERGESORT(A, 2, 2) MERGE(A, 1, 1, 2) MERGESORT(A, 3, 4) MERGESORT(A, 3,3) MERGESORT(A,4,4) MERGE(A, 3, 3, 4) Example (contd) Order used in MERGESORT. Comments on MERGESORT: • amount of pseudocode deceptive since MERGE does all the work • efficiency is not sensitive to starting order of array (unlike quicksort) • can be implemented without recursion Tämä teos on lisensoitu Creative Commons Nimeä-EiKaupallinen- EiMuutoksia 4.0 Kansainvälinen -lisenssillä. Tarkastele lisenssiä osoitteessa http://creativecommons.org/licenses/by-nc-nd/4.0/. tekijä: Frank Cameron This work is licensed under the Creative Commons Attribution- NonCommercial-NoDerivatives 4.0 International License. To view a copy of this license, visit http://creativecommons.org/licenses/by-nc-nd/4.0/. made by Frank Cameron"
            },
            {
                "role": "user",
                "content": "Can you summarize the key topics covered in this lecture?"
            },
            {
                "role": "assistant",
                "content": "This lecture covers foundational algorithmic concepts including problem formulation, correctness proof using induction, efficiency analysis using asymptotic notation, data structures (arrays, linked lists, BSTs, heaps), and sorting methods (selection sort, merge sort, counting sort). Additionally, it introduces the Word-RAM computational model, providing a theoretical framework for measuring algorithm efficiency."
            }
        ]
    },
    {
        "system": "Hello! I am Kiran, an AI assistant from Tampere University, dedicated to providing you with accurate and insightful assistance. My goal is to help you understand concepts, summarize lectures, and answer questions effectively.",
        "conversations": [
            {
                "role": "host",
                "content": "Matti Rintala ja Jyke Jokinen Olioiden ohjelmointi C++:lla 13. maaliskuuta 2013 c⃝2012 Matti Rintala ja Jyke Jokinen (Subversion-revisio 1938 ) Tämän teoksen käyttöoikeutta koskee Creative Commons Nimeä-Ei muutoksia-Epäkaupallinen 1.0 Suomi -lisenssi. • Nimeä — Teoksen tekijä on ilmoitettava siten kuin tekijä tai teoksen lisensoija on sen määrännyt (mutta ei siten että ilmoitus viittaisi lisenssinantajan tukevan lisenssinsaajaa tai Teoksen käyttötapaa). • Ei muutettuja teoksia — Teosta ei saa muuttaa, muunnella tai käyttää toisen teoksen pohjana. • Epäkaupallinen — Lisenssi ei salli teoksen käyttöä ansiotarkoituksessa. Lisenssi on nähtävillä kokonaisuudessaan osoitteessa http://creativecommons.org/licenses/by-nd-nc/1.0/fi/ Tämä kirja on taitettu LATEX-ohjelmistolla (http://www.tug.org/). Kaavioiden ja kuvien piirtoon on käytetty XFIG-ohjelmistoa (http://www.xfig.org/) ja ohjelmalistaukset on käsitelty LGrind-muotoili- jalla. Sisa¨lto¨ 3 Sisa¨lto¨ Esipuhe: Sirkkelin ka¨ytto¨ohje . . . . . . . . . . . . . . . . . . . 18 Alkusanat nelja¨nteen, uudistettuun painokseen . . . . . . . . 20 1 Kohti olioita . . . . . . . . . . . . . . . . . . . . . . . . . . . 26 1.1 Ohjelmistojen tekijo¨iden ongelmakentta¨ . . . . . . . . 26 1.2 Laajojen ohjelmistojen teon vaikeus . . . . . . . . . . . 27 1.3 Suurten ohjelmistokokonaisuuksien hallinta . . . . . . 28 1.3.1 Ennen kehittyneita¨ ohjelmointikielia¨ . . . . . . 29 1.3.2 Tietorakenteet . . . . . . . . . . . . . . . . . . . 30 1.3.3 Moduulit . . . . . . . . . . . . . . . . . . . . . . 31 1.3.4 Oliot . . . . . . . . . . . . . . . . . . . . . . . . . 34 1.3.5 Komponentit . . . . . . . . . . . . . . . . . . . . 38 1.4 C++: Moduulit ka¨a¨nno¨syksiko¨illa¨ . . . . . . . . . . . . . 39 1.5 C++: Nimiavaruudet . . . . . . . . . . . . . . . . . . . . . 41 1.5.1 Nimiavaruuksien ma¨a¨ritteleminen . . . . . . . 42 1.5.2 Na¨kyvyystarkenninoperaattori . . . . . . . . . . 42 1.5.3 Nimiavaruuksien hyo¨dyt . . . . . . . . . . . . . 44 1.5.4 std-nimiavaruus . . . . . . . . . . . . . . . . . . 45 1.5.5 Standardin uudet otsikkotiedostot . . . . . . . . 45 1.5.6 Nimiavaruuden synonyymi . . . . . . . . . . . 46 1.5.7 Lyhyiden nimien ka¨ytto¨ (using) . . . . . . . . . 47 1.5.8 Nimea¨ma¨to¨n nimiavaruus . . . . . . . . . . . . 49 1.6 Moduulituki muissa ohjelmointikielissa¨ . . . . . . . . 51 1.6.1 Modula-3 . . . . . . . . . . . . . . . . . . . . . . 51 1.6.2 Java . . . . . . . . . . . . . . . . . . . . . . . . . 51 Sisa¨lto¨ 4 2 Luokat ja oliot . . . . . . . . . . . . . . . . . . . . . . . . . . 54 2.1 Olioiden ominaisuuksia . . . . . . . . . . . . . . . . . . 54 2.1.1 Oliolla on tila . . . . . . . . . . . . . . . . . . . 55 2.1.2 Olio kuuluu luokkaan . . . . . . . . . . . . . . . 56 2.1.3 Oliolla on identiteetti . . . . . . . . . . . . . . . 57 2.2 Luokan dualismi . . . . . . . . . . . . . . . . . . . . . . 58 2.3 C++: Luokat ja oliot . . . . . . . . . . . . . . . . . . . . . 60 2.3.1 Luokan esittely . . . . . . . . . . . . . . . . . . . 60 2.3.2 Ja¨senmuuttujat . . . . . . . . . . . . . . . . . . . 61 2.3.3 Ja¨senfunktiot . . . . . . . . . . . . . . . . . . . . 64 2.3.4 Luokkien ja olioiden ka¨ytto¨ C++:ssa . . . . . . . . 66 3 Olioiden elinkaari . . . . . . . . . . . . . . . . . . . . . . . . 69 3.1 Olion syntyma¨ . . . . . . . . . . . . . . . . . . . . . . . 70 3.2 Olion kuolema . . . . . . . . . . . . . . . . . . . . . . . 71 3.3 Olion elinkaaren ma¨a¨ra¨ytyminen . . . . . . . . . . . . 71 3.3.1 Modula-3 . . . . . . . . . . . . . . . . . . . . . . 72 3.3.2 Smalltalk . . . . . . . . . . . . . . . . . . . . . . . 74 3.3.3 Java . . . . . . . . . . . . . . . . . . . . . . . . . 74 3.3.4 C++ . . . . . . . . . . . . . . . . . . . . . . . . . . 76 3.4 C++: Rakentajat ja purkajat . . . . . . . . . . . . . . . . . 79 3.4.1 Rakentajat . . . . . . . . . . . . . . . . . . . . . 80 3.4.2 Purkajat . . . . . . . . . . . . . . . . . . . . . . . 83 3.5 C++: Dynaaminen luominen ja tuhoaminen . . . . . . . 85 3.5.1 new . . . . . . . . . . . . . . . . . . . . . . . . . . 86 3.5.2 delete . . . . . . . . . . . . . . . . . . . . . . . . 89 3.5.3 Dynaamisesti luodut taulukot . . . . . . . . . . 90 3.5.4 Virheiden va¨ltta¨minen dynaamisessa luomisessa 90 4 Olioiden rajapinnat . . . . . . . . . . . . . . . . . . . . . . . 95 4.1 Rajapinnan suunnittelu . . . . . . . . . . . . . . . . . . 96 4.1.1 Hyva¨n rajapinnan tunnusmerkkeja¨ . . . . . . . 97 4.1.2 Erilaisia rajapintoja ohjelmoinnissa . . . . . . . 98 4.1.3 Rajapintadokumentaation tuottaminen ja ylla¨pito 99 4.2 C++: Na¨kyvyysma¨a¨reet . . . . . . . . . . . . . . . . . . . 101 4.2.1 public . . . . . . . . . . . . . . . . . . . . . . . . 102 4.2.2 private . . . . . . . . . . . . . . . . . . . . . . . 103 4.3 C++: const ja vakio-oliot . . . . . . . . . . . . . . . . . . 105 4.3.1 Perustyyppiset vakiot . . . . . . . . . . . . . . . 105 Sisa¨lto¨ 5 4.3.2 Vakio-oliot . . . . . . . . . . . . . . . . . . . . . 106 4.3.3 Vakioviitteet ja -osoittimet . . . . . . . . . . . . 109 4.4 C++: Luokan ennakkoesittely . . . . . . . . . . . . . . . . 112 4.4.1 Ennakkoesittely kapseloinnissa . . . . . . . . . 113 5 Oliosuunnittelu . . . . . . . . . . . . . . . . . . . . . . . . . 116 5.1 Oliosuunnittelua ohjaavat ominaisuudet . . . . . . . . 117 5.1.1 Mita¨ suunnittelu on? . . . . . . . . . . . . . . . 117 5.1.2 Abstraktio ja tiedon ka¨tkenta¨ . . . . . . . . . . . 118 5.1.3 Osien va¨liset yhteydet ja lokaalisuusperiaate . . 118 5.1.4 Laatu . . . . . . . . . . . . . . . . . . . . . . . . 120 5.2 Oliosuunnittelun aloittaminen . . . . . . . . . . . . . . 121 5.2.1 Luokan vastuualue . . . . . . . . . . . . . . . . 122 5.2.2 Kuinka lo¨yta¨a¨ luokkia? . . . . . . . . . . . . . . 123 5.2.3 CRC-kortti . . . . . . . . . . . . . . . . . . . . . 124 5.2.4 Luokka, attribuutti vai operaatio? . . . . . . . . 126 5.3 Oliosuunnitelman graaﬁnen kuvaus . . . . . . . . . . . 126 5.3.1 UML:n historiaa . . . . . . . . . . . . . . . . . . 127 5.3.2 Luokat, oliot ja rajapinnat . . . . . . . . . . . . . 128 5.3.3 Luokkien va¨liset yhteydet . . . . . . . . . . . . 130 5.3.4 Ajoaikaisen ka¨ytta¨ytymisen kuvaamistapoja . . 138 5.3.5 Ohjelmiston rakennekuvaukset . . . . . . . . . 140 5.4 Saatteeksi suunnitteluun . . . . . . . . . . . . . . . . . 140 6 Periytyminen . . . . . . . . . . . . . . . . . . . . . . . . . . . 142 6.1 Periytyminen, luokkahierarkiat, polymorﬁsmi . . . . . 143 6.2 Periytyminen ja uudelleenka¨ytto¨ . . . . . . . . . . . . . 147 6.3 C++: Periytymisen perusteet . . . . . . . . . . . . . . . . 149 6.3.1 Periytyminen ja na¨kyvyys . . . . . . . . . . . . . 150 6.3.2 Periytyminen ja rakentajat . . . . . . . . . . . . 152 6.3.3 Periytyminen ja purkajat . . . . . . . . . . . . . 154 6.3.4 Aliluokan olion ja kantaluokan suhde . . . . . . 155 6.4 C++: Periytymisen ka¨ytto¨ laajentamiseen . . . . . . . . . 156 6.5 C++: Virtuaalifunktiot ja dynaaminen sitominen . . . . . 159 6.5.1 Virtuaalifunktiot . . . . . . . . . . . . . . . . . . 159 6.5.2 Dynaaminen sitominen . . . . . . . . . . . . . . 160 6.5.3 Olion tyypin ajoaikainen tarkastaminen . . . . 164 6.5.4 Ei-virtuaalifunktiot ja peitta¨minen . . . . . . . . 167 6.5.5 Virtuaalipurkajat . . . . . . . . . . . . . . . . . . 168 Sisa¨lto¨ 6 6.5.6 Virtuaalifunktioiden hinta . . . . . . . . . . . . 169 6.5.7 Virtuaalifunktiot rakentajissa ja purkajissa . . . 170 6.6 Abstraktit kantaluokat . . . . . . . . . . . . . . . . . . . 172 6.7 Moniperiytyminen . . . . . . . . . . . . . . . . . . . . . 175 6.7.1 Moniperiytymisen idea . . . . . . . . . . . . . . 175 6.7.2 Moniperiytyminen eri oliokielissa¨ . . . . . . . . 176 6.7.3 Moniperiytymisen ka¨ytto¨kohteita . . . . . . . . 177 6.7.4 Moniperiytymisen vaaroja . . . . . . . . . . . . 178 6.7.5 Vaihtoehtoja moniperiytymiselle . . . . . . . . . 179 6.8 C++: Moniperiytyminen . . . . . . . . . . . . . . . . . . 180 6.8.1 Moniperiytyminen ja moniselitteisyys . . . . . 181 6.8.2 Toistuva moniperiytyminen . . . . . . . . . . . 186 6.9 Periytyminen ja rajapintaluokat . . . . . . . . . . . . . 190 6.9.1 Rajapintaluokkien ka¨ytto¨ . . . . . . . . . . . . . 191 6.9.2 C++: Rajapintaluokat ja moniperiytyminen . . . . 193 6.9.3 Ongelmia rajapintaluokkien ka¨yto¨ssa¨ . . . . . . 195 6.10 Periytyminen vai kooste? . . . . . . . . . . . . . . . . . 198 6.11 Sovelluskehykset . . . . . . . . . . . . . . . . . . . . . 199 7 Lisa¨a¨ olioiden elinkaaresta . . . . . . . . . . . . . . . . . . 201 7.1 Olioiden kopiointi . . . . . . . . . . . . . . . . . . . . . 202 7.1.1 Erilaiset kopiointitavat . . . . . . . . . . . . . . 203 7.1.2 C++: Kopiorakentaja . . . . . . . . . . . . . . . . . 206 7.1.3 Kopiointi ja viipaloituminen . . . . . . . . . . . 210 7.2 Olioiden sijoittaminen . . . . . . . . . . . . . . . . . . 215 7.2.1 Sijoituksen ja kopioinnin erot . . . . . . . . . . 215 7.2.2 C++: Sijoitusoperaattori . . . . . . . . . . . . . . . 216 7.2.3 Sijoitus ja viipaloituminen . . . . . . . . . . . . 221 7.3 Oliot arvoparametreina ja paluuarvoina . . . . . . . . . 224 7.4 Tyyppimuunnokset . . . . . . . . . . . . . . . . . . . . 227 7.4.1 C++:n tyyppimuunnosoperaattorit . . . . . . . . 228 7.4.2 Ohjelmoijan ma¨a¨rittelema¨t tyyppimuunnokset 232 7.5 Rakentajat ja struct . . . . . . . . . . . . . . . . . . . . 237 8 Lisa¨a¨ rajapinnoista . . . . . . . . . . . . . . . . . . . . . . . 240 8.1 Sopimus rajapinnasta . . . . . . . . . . . . . . . . . . . 240 8.1.1 Palveluiden esi- ja ja¨lkiehdot . . . . . . . . . . . 241 8.1.2 Luokkainvariantti . . . . . . . . . . . . . . . . . 243 8.1.3 Sopimussuunnittelun ka¨ytto¨ . . . . . . . . . . . 244 Sisa¨lto¨ 7 8.1.4 C++: luokan sopimusten tarkastus . . . . . . . . . 244 8.2 Luokan rajapinta ja olion rajapinta . . . . . . . . . . . . 248 8.2.1 Metaluokat ja luokkaoliot . . . . . . . . . . . . . 249 8.2.2 C++: Luokkamuuttujat . . . . . . . . . . . . . . . 250 8.2.3 C++: Luokkafunktiot . . . . . . . . . . . . . . . . 252 8.3 Tyypit osana rajapintaa . . . . . . . . . . . . . . . . . . 253 8.4 Ohjelmakomponentin sisa¨iset rajapinnat . . . . . . . . 258 8.4.1 C++: Ysta¨va¨funktiot . . . . . . . . . . . . . . . . . 259 8.4.2 C++: Ysta¨va¨luokat . . . . . . . . . . . . . . . . . . 260 9 Geneerisyys . . . . . . . . . . . . . . . . . . . . . . . . . . . 263 9.1 Yleiska¨ytto¨isyys, pysyvyys ja vaihtelevuus . . . . . . . 264 9.2 Suunnittelun geneerisyys: suunnittelumallit . . . . . . 267 9.2.1 Suunnittelumallien edut ja haitat . . . . . . . . 267 9.2.2 Suunnittelumallin rakenne . . . . . . . . . . . . 268 9.3 Valikoituja esimerkkeja¨ suunnittelumalleista . . . . . . 271 9.3.1 Kokoelma (Composite) . . . . . . . . . . . . . . . 271 9.3.2 Iteraattori (Iterator) . . . . . . . . . . . . . . . . 273 9.3.3 Silta (Bridge) . . . . . . . . . . . . . . . . . . . . 274 9.3.4 C++: Esimerkki suunnittelumallin toteutuksesta 276 9.4 Geneerisyys ja periytymisen rajoitukset . . . . . . . . . 277 9.5 C++: Toteutuksen geneerisyys: mallit (template) . . . . . 283 9.5.1 Mallit ja tyyppiparametrit . . . . . . . . . . . . 284 9.5.2 Funktiomallit . . . . . . . . . . . . . . . . . . . . 286 9.5.3 Luokkamallit . . . . . . . . . . . . . . . . . . . . 287 9.5.4 Tyyppiparametreille asetetut vaatimukset . . . 290 9.5.5 Erilaiset mallien parametrit . . . . . . . . . . . . 292 9.5.6 Mallien erikoistus . . . . . . . . . . . . . . . . . 295 9.5.7 Mallien ongelmia ja ratkaisuja . . . . . . . . . . 297 10 Geneerinen ohjelmointi: STL ja metaohjelmointi . . . . . . 302 10.1 STL:n perusperiaatteita . . . . . . . . . . . . . . . . . . 303 10.1.1 STL:n rakenne . . . . . . . . . . . . . . . . . . . 304 10.1.2 Algoritmien geneerisyys . . . . . . . . . . . . . 305 10.1.3 Tietorakenteiden jaotteluperusteet . . . . . . . . 306 10.1.4 Tehokkuuskategoriat . . . . . . . . . . . . . . . 308 10.2 STL:n sa¨ilio¨t . . . . . . . . . . . . . . . . . . . . . . . . 310 10.2.1 Sarjat (“pera¨kka¨issa¨ilio¨t”) . . . . . . . . . . . . . 312 10.2.2 Assosiatiiviset sa¨ilio¨t . . . . . . . . . . . . . . . 317 Sisa¨lto¨ 8 10.2.3 Muita sa¨ilio¨ita¨ . . . . . . . . . . . . . . . . . . . 323 10.3 Iteraattorit . . . . . . . . . . . . . . . . . . . . . . . . . 326 10.3.1 Iteraattoreiden ka¨ytto¨kohteet . . . . . . . . . . . 327 10.3.2 Iteraattorikategoriat . . . . . . . . . . . . . . . . 329 10.3.3 Iteraattorit ja sa¨ilio¨t . . . . . . . . . . . . . . . . 332 10.3.4 Iteraattoreiden kelvollisuus . . . . . . . . . . . . 334 10.3.5 Iteraattorisovittimet . . . . . . . . . . . . . . . . 336 10.4 STL:n algoritmit . . . . . . . . . . . . . . . . . . . . . . 337 10.5 Funktio-oliot . . . . . . . . . . . . . . . . . . . . . . . . 340 10.5.1 Toiminnallisuuden va¨litta¨minen algoritmille . . 341 10.5.2 Funktio-olioiden periaate . . . . . . . . . . . . . 343 10.5.3 STL:n valmiit funktio-oliot . . . . . . . . . . . . 347 10.6 C++: Template-metaohjelmointi . . . . . . . . . . . . . . 348 10.6.1 Metaohjelmoinnin ka¨site . . . . . . . . . . . . . 349 10.6.2 Metaohjelmointi ja geneerisyys . . . . . . . . . 351 10.6.3 Metafunktiot . . . . . . . . . . . . . . . . . . . . 352 10.6.4 Esimerkki metafunktioista: numeric_limits . . 357 10.6.5 Esimerkki metaohjelmoinnista: tyypin valinta . 360 10.6.6 Esimerkki metaohjelmoinnista: optimointi . . . 362 11 Virhetilanteet ja poikkeukset . . . . . . . . . . . . . . . . . 366 11.1 Mika¨ virhe on? . . . . . . . . . . . . . . . . . . . . . . . 367 11.2 Mita¨ tehda¨ virhetilanteessa? . . . . . . . . . . . . . . . 369 11.3 Virhehierarkiat . . . . . . . . . . . . . . . . . . . . . . . 371 11.4 Poikkeusten heitta¨minen ja sieppaaminen . . . . . . . 374 11.4.1 Poikkeushierarkian hyva¨ksika¨ytto¨ . . . . . . . . 374 11.4.2 Poikkeukset, joita ei oteta kiinni . . . . . . . . . 377 11.4.3 Sisa¨kka¨iset valvontalohkot . . . . . . . . . . . . 378 11.5 Poikkeukset ja olioiden tuhoaminen . . . . . . . . . . . 380 11.5.1 Poikkeukset ja purkajat . . . . . . . . . . . . . . 380 11.5.2 Poikkeukset ja dynaamisesti luodut oliot . . . . 380 11.6 Poikkeusma¨a¨reet . . . . . . . . . . . . . . . . . . . . . . 382 11.7 Muistivuotojen va¨ltta¨minen: auto_ptr . . . . . . . . . . 383 11.7.1 Automaattiosoittimet ja muistinhallinta . . . . 384 11.7.2 Automaattiosoittimien sijoitus ja kopiointi . . . 385 11.7.3 Automaattiosoittimien ka¨yto¨n rajoitukset . . . . 387 11.8 Olio-ohjelmointi ja poikkeusturvallisuus . . . . . . . . 388 11.8.1 Poikkeustakuut . . . . . . . . . . . . . . . . . . . 389 11.8.2 Poikkeukset ja rakentajat . . . . . . . . . . . . . 393 Sisa¨lto¨ 9 11.8.3 Poikkeukset ja purkajat . . . . . . . . . . . . . . 397 11.9 Esimerkki: poikkeusturvallinen sijoitus . . . . . . . . . 399 11.9.1 Ensimma¨inen versio . . . . . . . . . . . . . . . . 399 11.9.2 Tavoitteena vahva takuu . . . . . . . . . . . . . 401 11.9.3 Lisa¨ta¨a¨n epa¨suoruutta . . . . . . . . . . . . . . . 403 11.9.4 Tilan eriytta¨minen (“pimpl”-idiomi) . . . . . . . 405 11.9.5 Tilan vaihtaminen pa¨ikseen . . . . . . . . . . . 407 Liite A. C++: Ei-olio-ominaisuuksia . . . . . . . . . . . . . . . . 410 A.1 Viitteet . . . . . . . . . . . . . . . . . . . . . . . . . . . 410 A.2 inline . . . . . . . . . . . . . . . . . . . . . . . . . . . . 413 A.3 vector . . . . . . . . . . . . . . . . . . . . . . . . . . . . 414 A.4 string . . . . . . . . . . . . . . . . . . . . . . . . . . . . 416 Liite B. C++-tyyliopas . . . . . . . . . . . . . . . . . . . . . . . . . 420 Kirjallisuutta . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 437 Englanninkieliset termit . . . . . . . . . . . . . . . . . . . . . . 444 LISTAUKSET 10 Listaukset 1.1 Pa¨iva¨ysmoduulin tietorakenne ja osa rajapintaa . . . 34 1.2 Pa¨iva¨ysmoduulin ka¨ytto¨esimerkki . . . . . . . . . . . 35 1.3 Pa¨iva¨ysolioiden ka¨ytto¨ . . . . . . . . . . . . . . . . . . 36 1.4 Pa¨iva¨ysmoduulin esittely C-kielella¨, paivays.h . . . . 39 1.5 Nimiavaruudella toteutettu rajapinta . . . . . . . . . . 43 1.6 Rajapintafunktioiden toteutus erillisessa¨ tiedostossa . 43 1.7 Nimiavaruuden rakenteiden ka¨ytta¨minen . . . . . . . 44 1.8 C-kirjastofunktiot C++:n nimiavaruudessa . . . . . . . . 46 1.9 Nimiavaruuden synonyymi (aliasointi) . . . . . . . . 47 1.10 using-lauseen ka¨ytto¨ . . . . . . . . . . . . . . . . . . . 48 1.11 using-lauseen ka¨ytto¨ eri otsikkotiedostojen kanssa . . 49 1.12 Nimea¨ma¨to¨n nimiavaruus . . . . . . . . . . . . . . . . 50 1.13 Moduulituki Modula-3-kielessa¨ . . . . . . . . . . . . . 52 1.14 Moduulituki Java-kielessa¨ . . . . . . . . . . . . . . . . 52 2.1 Esimerkki luokan esittelysta¨, pienipaivays.hh . . . . 62 2.2 Ja¨senfunktioiden toteutus, pienipaivays.cc . . . . . . 65 2.3 Esimerkki luokan ka¨yto¨sta¨, ppkaytto.cc . . . . . . . . 68 3.1 Esimerkki olion elinkaaresta Modula-3:lla . . . . . . . 73 3.2 Esimerkki olion elinkaaresta Smalltalkilla . . . . . . . 75 3.3 Esimerkki olion elinkaaresta Javalla . . . . . . . . . . 76 3.4 Esimerkki olion elinkaaresta C++:lla . . . . . . . . . . . 78 3.5 Paivays-luokan rakentaja . . . . . . . . . . . . . . . . 80 3.6 Esimerkki rakentajasta olion ollessa ja¨senmuuttujana 82 3.7 Paivays-luokan purkaja . . . . . . . . . . . . . . . . . 84 3.8 Esimerkki olion dynaamisesta luomisesta new’lla¨ . . . 88 3.9 Taulukko, joka omistaa sisa¨lta¨ma¨nsa¨ kokonaisluvut . 92 LISTAUKSET 11 3.10 Taulukko, joka ei omista sisa¨lta¨mia¨a¨n kokonaislukuja 93 4.1 Ja¨senfunktio, joka palauttaa viitteen ja¨senmuuttujaan 104 4.2 Pa¨a¨sy toisen saman luokan olion private-osaan . . . 105 4.3 Pa¨iva¨ysluokka const-sanoineen . . . . . . . . . . . . . 107 4.4 Esimerkki virheesta¨, kun const-sana unohtuu . . . . . 111 4.5 Esimerkki ennakkoesittelysta¨ . . . . . . . . . . . . . . 113 4.6 Ennakkoesittely kapseloinnissa . . . . . . . . . . . . . 114 5.1 Lainausja¨rjestelma¨n esittely, lainausjarjestelma.hh . 134 6.1 Periytymisen syntaksi C++:lla . . . . . . . . . . . . . . . 150 6.2 Periytyminen ja rakentajat . . . . . . . . . . . . . . . . 154 6.3 Kirjan tiedot muistava luokka . . . . . . . . . . . . . . 157 6.4 Kirjaston kirjan palvelut tarjoava aliluokka . . . . . . 158 6.5 Luokan Kirja virtuaalifunktiot . . . . . . . . . . . . . 161 6.6 Luokan KirjastonKirja virtuaalifunktiot . . . . . . . 162 6.7 Dynaaminen sitominen C++:ssa . . . . . . . . . . . . . 163 6.8 Olion tyypin ajoaikainen tarkastaminen . . . . . . . . 165 6.9 Esimerkki typeid-operaattorin ka¨yto¨sta¨ . . . . . . . . 167 6.10 Abstrakteja kantaluokkia ja puhtaita virtuaalifunktioita 173 6.11 Puhdas virtuaalifunktio, jolla on myo¨s toteutus . . . . 174 6.12 Moniselitteisyyden yksi va¨ltta¨mistapa . . . . . . . . . 182 6.13 Ja¨senfunktiokutsun moniselitteisyyden eliminointi . 184 6.14 Moniselitteisyyden eliminointi va¨liluokilla . . . . . . 185 6.15 Erilliset rajapinnat Javassa . . . . . . . . . . . . . . . . 192 6.16 Rajapintaluokkien toteutus moniperiytymisella¨ C++:ssa 193 6.17 Rajapintaluokan purkaja esittelyyn upotettuna . . . . 195 7.1 Esimerkki kopiorakentajasta . . . . . . . . . . . . . . . 207 7.2 Kopiorakentaja aliluokassa . . . . . . . . . . . . . . . 208 7.3 Viipaloitumisen kierta¨minen kloonaa-ja¨senfunktiolla 214 7.4 Esimerkki sijoitusoperaattorista . . . . . . . . . . . . . 217 7.5 Sijoitusoperaattori periytetyssa¨ luokassa . . . . . . . . 220 7.6 Viipaloitumismahdollisuudet sijoituksessa . . . . . . 222 7.7 Viipaloitumisen esta¨minen ajoaikaisella tarkastuksella 223 7.8 Olio arvoparametrina ja paluuarvona . . . . . . . . . 225 7.9 Esimerkki const_cast-muunnoksesta . . . . . . . . . 230 7.10 Tiedon esitystavan muuttaminen ja reinterpret_cast 233 LISTAUKSET 12 7.11 Esimerkki rakentajasta tyyppimuunnoksena . . . . . 234 7.12 Esimerkki muunnosja¨senfunktiosta . . . . . . . . . . 237 7.13 struct-tietorakenne, jossa on olioita . . . . . . . . . . 237 7.14 struct, jolla on rakentaja . . . . . . . . . . . . . . . . 239 8.1 Varmistusrutiinin toteutus . . . . . . . . . . . . . . . . 247 8.2 Esimerkki luokkainvariantin toteutuksesta ja¨senfunk- tiona . . . . . . . . . . . . . . . . . . . . . . . . . . . . 248 8.3 Esimerkki luokkamuuttujien ja -funktioiden ka¨yto¨sta¨ 254 8.4 Tyypillinen pa¨iva¨ysluokan esittely . . . . . . . . . . . 255 8.5 Parannettu pa¨iva¨ysluokan esittely . . . . . . . . . . . 257 8.6 Luokan ma¨a¨rittelema¨ tyyppi paluutyyppina¨ . . . . . . 258 8.7 Laajemman rajapinnan salliminen ysta¨va¨funktioille . 261 8.8 Ysta¨va¨luokat . . . . . . . . . . . . . . . . . . . . . . . 262 9.1 Virhetiedoterajapinnan esittely ja toteutus . . . . . . 278 9.2 Toteutusten kantaluokka, virheikkunatoteutus.hh . . 279 9.3 Toteutus virheikkunoista, virheikkunaversiot.hh . . 279 9.4 Eri virheikkunatoteutusten valinta . . . . . . . . . . . 279 9.5 Parametreista pienemma¨n palauttava funktiomalli . . 286 9.6 Tietotyypin “pari” ma¨a¨ritteleva¨ luokkamalli . . . . . . 288 9.7 Esimerkki luokkamallin Pari ja¨senfunktioista . . . . . 289 9.8 Luokkamallin sisa¨lla¨ oleva ja¨senfunktiomalli . . . . . 290 9.9 Parempi versio listauksen 9.5 funktiomallista . . . . . 292 9.10 Mallin oletusparametrit . . . . . . . . . . . . . . . . . 293 9.11 Malli, jolla on vakioparametri . . . . . . . . . . . . . . 294 9.12 Mallin malliparametri . . . . . . . . . . . . . . . . . . 295 9.13 Luokkamallin Pari erikoistus totuusarvoille . . . . . . 296 9.14 Funktiomallin min erikoistus pa¨iva¨yksille . . . . . . . 297 9.15 Luokkamallin Pari osittaiserikoistus . . . . . . . . . . 297 9.16 Avainsanan export ka¨ytto¨ . . . . . . . . . . . . . . . . 299 9.17 Tyypin ma¨a¨ra¨a¨minen avainsanalla typename . . . . . 300 10.1 Fibonaccin luvut vectorilla . . . . . . . . . . . . . . . 314 10.2 Puskurin toteutus dequella . . . . . . . . . . . . . . . . 316 10.3 Nimien rekistero¨inti setilla¨ . . . . . . . . . . . . . . . 319 10.4 Tehokkaampi versio listauksesta 10.3 . . . . . . . . . 320 10.5 Nimirekistero¨inti multisetilla¨ . . . . . . . . . . . . . . 320 10.6 Nimirekistero¨inti mapilla¨ . . . . . . . . . . . . . . . . . 321 LISTAUKSET 13 10.7 multimapilla¨ toteutettu puhelinluetteloluokka . . . . . 323 10.8 Puhelinluettelon toteutus . . . . . . . . . . . . . . . . 324 10.9 Sa¨ilio¨n la¨pika¨yminen iteraattoreilla . . . . . . . . . . 333 10.10 Esimerkki STL:n algoritmien ka¨yto¨sta¨ . . . . . . . . . 340 10.11 Funktio-osoittimen va¨litta¨minen parametrina . . . . . 342 10.12 Esimerkki funktio-olioluokasta . . . . . . . . . . . . . 344 10.13 Funktio-olion ka¨ytto¨esimerkkeja¨ . . . . . . . . . . . . 345 10.14 Funktio-olion ka¨ytto¨ STL:ssa¨ . . . . . . . . . . . . . . 346 10.15 C++:n funktio-olioiden less ja bind2nd ka¨ytto¨ . . . . . 349 10.16 Esimerkki yksinkertaisesta metaohjelmoinnista . . . . 353 10.17 Yksinkertainen trait-metafunktio . . . . . . . . . . . . 354 10.18 Erikoistamalla tehty template-metafunktio . . . . . . 355 10.19 Osittaiserikoistuksella tehty metafunktio . . . . . . . 356 10.20 Esimerkki numeric_limits-metafunktion ka¨yto¨sta¨ . . 359 10.21 Metafunktion IF toteutus . . . . . . . . . . . . . . . . 361 10.22 Metafunktion IF ka¨ytto¨esimerkki . . . . . . . . . . . . 361 10.23 Esimerkki metaohjelmoinnista optimoinnissa . . . . . 364 11.1 Virhetyypit C++:n luokkina . . . . . . . . . . . . . . . . 373 11.2 Esimerkki omasta virheluokasta . . . . . . . . . . . . 374 11.3 Esimerkki C++:n poikkeuska¨sittelija¨sta¨ . . . . . . . . . 376 11.4 Virhekategorioiden ka¨ytto¨ poikkeuksissa . . . . . . . 377 11.5 Sisa¨kka¨iset valvontalohkot . . . . . . . . . . . . . . . 379 11.6 Esimerkki dynaamisen olion siivoamisesta . . . . . . 381 11.7 Virheisiin varautuminen ja monta dynaamista oliota . 382 11.8 Esimerkki automaattiosoittimen auto_ptr ka¨yto¨sta¨ . . 385 11.9 Automaattiosoitin ja omistuksen siirto . . . . . . . . . 386 11.10 Esimerkki luokasta, jossa on useita osaolioita . . . . . 394 11.11 Olioiden dynaaminen luominen rakentajassa . . . . . 395 11.12 Funktion valvontalohko rakentajassa . . . . . . . . . . 397 11.13 Yksinkertainen luokka, jolla on sijoitusoperaattori . . 399 11.14 Sijoitus, joka pyrkii tarjoamaan vahvan takuun (ei toi- mi) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 402 11.15 Kirjaluokka epa¨suoruuksilla . . . . . . . . . . . . . . . 403 11.16 Uuden kirjaluokan rakentaja, purkaja ja sijoitus . . . 404 11.17 Kirja eriytetylla¨ tilalla ja automaattiosoittimella . . . 406 11.18 Eriytetyn tilan rakentajat, purkaja ja sijoitus . . . . . . 406 11.19 Kirja, jossa on nothrow-vaihto . . . . . . . . . . . . . 408 11.20 Yhdistelma¨ tilan eriytta¨misesta¨ ja vaihdosta . . . . . 409 LISTAUKSET 14 A.1 Osamerkkijonon “ja” etsinta¨ (C++) . . . . . . . . . . . . 417 A.2 Osamerkkijonon “ja” etsinta¨ (C) . . . . . . . . . . . . . 418 KUVAT 15 Kuvat 1.1 Tyypillinen assembly-ohjelman spagettirakenne . . . 30 1.2 Pa¨iva¨ykset tietorakenteina . . . . . . . . . . . . . . . . 31 1.3 Pa¨iva¨yksia¨ ka¨sitteleva¨n moduulin rakenne . . . . . . 32 1.4 Rajapinta osana tietorakennetta (oliot) . . . . . . . . . 35 1.5 Sama otsikkotiedosto voi tulla ka¨ytto¨o¨n useita kertoja 40 1.6 Moduulirakenne C-kielella¨ . . . . . . . . . . . . . . . . 41 2.1 Ongelmasta ohjelmaksi olioilla . . . . . . . . . . . . . 55 2.2 Pa¨iva¨ys-luokka ja siita¨ tehdyt oliot A ja B . . . . . . . 59 4.1 Va¨a¨rin tehty luokkien keskina¨inen esittely (ei toimi) . 112 4.2 Oikein tehty luokkien keskina¨inen esittely . . . . . . 115 5.1 Erilaisia yhteysvaihtoehtoja kuuden moduulin va¨lilla¨ 119 5.2 Moduulien va¨lisia¨ yksisuuntaisia riippuvuuksia . . . 120 5.3 Esimerkki ka¨ytto¨tapauksesta . . . . . . . . . . . . . . 124 5.4 Kuva keskenera¨isen CRC-korttipelin yhdesta¨ kortista . 125 5.5 Luokka, olio ja rajapinta . . . . . . . . . . . . . . . . . 129 5.6 Tarkennetun suunnitelman luokka ja na¨kyvyysma¨a¨reita¨ 130 5.7 UML:n yhteystyyppeja¨ . . . . . . . . . . . . . . . . . . 131 5.8 Luokka “Heippa” ka¨ytta¨a¨ Javan graﬁikkaolioita . . . . 132 5.9 UML-assosiaatioiden lukuma¨a¨ra¨merkinto¨ja¨ . . . . . . 132 5.10 Esimerkki luokkien va¨lisista¨ assosiaatioista . . . . . . 133 5.11 UML:n koostesuhteita . . . . . . . . . . . . . . . . . . 135 5.12 Osaston, sen laitosten ja tyo¨ntekijo¨iden va¨lisia¨ yh- teyksia¨ . . . . . . . . . . . . . . . . . . . . . . . . . . . 136 5.13 Kirjasta periytetty luokka, joka toteuttaa kaksi rajapin- taa . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 137 KUVAT 16 5.14 Palautuspa¨iva¨ma¨a¨ra¨n asettamisen tapahtumasekvenssi 138 5.15 Kirjastokortin tiloja . . . . . . . . . . . . . . . . . . . . 139 6.1 Periytymiseen liittyva¨a¨ terminologiaa . . . . . . . . . 144 6.2 Elio¨ita¨ mallintavan ohjelman periytymishierarkiaa . . 145 6.3 Na¨kyvyytta¨ kuvaava kantaluokka ja periytettyja¨ luokkia 148 6.4 Periytymishierarkia ja oliot [Koskimies, 2000] . . . . . 149 6.5 Periytyminen ja na¨kyvyys . . . . . . . . . . . . . . . . 151 6.6 Moniperiytyminen ja sen vaikutus . . . . . . . . . . . 176 6.7 Toistuva moniperiytyminen . . . . . . . . . . . . . . . 187 6.8 Toistuva moniperiytyminen ja olion rakenne . . . . . 188 6.9 Rakentajat virtuaalisessa moniperiytymisessa¨ . . . . . 189 6.10 Luokat, jotka toteuttavat erilaisia rajapintoja . . . . . 191 6.11 Ongelma yhdistettyjen rajapintojen kanssa . . . . . . 197 6.12 Insino¨o¨ri, viulisti ja isa¨ . . . . . . . . . . . . . . . . . . 199 6.13 Aliohjelmakirjasto ja sovelluskehys . . . . . . . . . . 200 7.1 Viitekopiointi . . . . . . . . . . . . . . . . . . . . . . . 203 7.2 Matalakopiointi . . . . . . . . . . . . . . . . . . . . . . 204 7.3 Syva¨kopiointi . . . . . . . . . . . . . . . . . . . . . . . 205 7.4 Viipaloituminen olion kopioinnissa . . . . . . . . . . 211 7.5 Viipaloituminen olioiden sijoituksessa . . . . . . . . . 223 7.6 Oliot arvoparametreina ja paluuarvoina . . . . . . . . 226 8.1 Luokka- ja metaluokkahierarkiaa Smalltalkissa . . . . . 250 9.1 Pysyvyys- ja vaihtelevuusanalyysi ja yleiska¨ytto¨isyys 266 9.2 Kokoelman toteuttava luokka . . . . . . . . . . . . . . 269 9.3 Taulukko joka sisa¨lta¨a¨ kokoelmia . . . . . . . . . . . . 270 9.4 Suunnittelumallin UML-symboli ja sen ka¨ytto¨ . . . . 271 9.5 Suunnittelumalli Kokoelma . . . . . . . . . . . . . . . 272 9.6 Suunnittelumalli Iteraattori . . . . . . . . . . . . . . . 274 9.7 Suunnittelumalli Silta . . . . . . . . . . . . . . . . . . 276 9.8 Yleiska¨ytto¨isen taulukon toteutus periytta¨ma¨lla¨ . . . 281 10.1 Tietorakenteiden hera¨tta¨mia¨ mielikuvia . . . . . . . . 306 10.2 Erilaisia tehokkuuskategorioita . . . . . . . . . . . . . 310 10.3 Sarjojen operaatioiden tehokkuuksia . . . . . . . . . . 313 10.4 Iteraattorit ja sa¨ilio¨t . . . . . . . . . . . . . . . . . . . . 328 10.5 Iteraattorikategoriat . . . . . . . . . . . . . . . . . . . . 330 KUVAT 17 10.6 Funktio-olioiden idea . . . . . . . . . . . . . . . . . . 345 10.7 Funktio-olio bind2nd(less<int>(), 3) . . . . . . . . . 348 10.8 Metafunktion numeric_limits palvelut . . . . . . . . . 358 11.1 C++-standardin virhekategoriat . . . . . . . . . . . . . . 372 A.1 Esimerkkien viittaukset . . . . . . . . . . . . . . . . . 412 A.2 C-taulukon ja vektorin vertailu . . . . . . . . . . . . . 416 A.3 Merkkijonotaulukon ja C++:n stringin vertailu . . . . 417 18 Esipuhe: Sirkkelin ka¨ytto¨ohje Siirtyminen C-kielesta¨ C++-kieleen ka¨ynnistyi pikkuhiljaa jo 1980-lu- vun loppupuolella. Olin tavattoman innostunut ta¨sta¨, koska na¨in sen mahdollisuutena siirta¨a¨ tutkimuslaboratorioissa syntyneita¨ oliokes- keisiin menetelmiin liittyvia¨ asioita osaksi arkipa¨iva¨ista¨ ohjelmis- totyo¨ta¨. 1990-luvun alkupuoli olikin ohjelmistoteollisuudessa olio- menetelmien ka¨ytto¨o¨noton aikaa, ja hyvin usein ka¨ytto¨o¨notto tapah- tui juuri C++-kieleen siirtymisen kautta. Yleinen ensivaikutelma C++-kielesta¨ oli, etta¨ se on oliopiirteil- la¨ laajennettu C-kieli ja vain hieman C-kielta¨ monimutkaisempi. Jo ensimma¨iset ka¨yta¨nno¨n kokemukset kielesta¨ kuitenkin osoittivat, etta¨ ta¨ma¨ oli na¨ko¨harhaa. Ka¨yta¨nno¨ssa¨ C++-kieleen lisa¨ttyjen olio- ominaisuuksien aiheuttamat implikaatiot ovat paljon laajempia kuin aluksi osattiin ennakoida. Ta¨ta¨ todistavat esimerkiksi yritysten oh- jelmistokehitysta¨ varten laatimat tyylioppaat: C-tyyliopas saattaa olla vain muutamien sivujen mittainen kokoelma sa¨a¨nto¨ja¨, kun pisimma¨t C++-oppaat la¨henteleva¨t sataa sivua. Ka¨yta¨nto¨ on osoittanut, etta¨ C++-kieli sisa¨lta¨a¨ ominaisuudet, joil- la olio-ohjelmoinnin lupaukset muuan muassa uudelleenka¨ytetta¨- vyyden ja ylla¨pidetta¨vyyden osalta pystyta¨a¨n suurelta osin lunasta- maan. Toisaalta olio-ominaisuuksien taitamattomasta ka¨yto¨sta¨ saat- taa aiheutua isoja ongelmia. C++ on kuin tehokas sirkkeli ilman suoja- va¨lineita¨: ammattilaisen ka¨sissa¨ se on tuottava ja varmatoiminen tyo¨- kalu, mutta noviisin ka¨sissa¨ vaarallinen ase. Oman kokemukseni mu- kaan tehosirkkeli-noviisi-yhdistelma¨ ei ole ta¨ma¨n pa¨iva¨n ohjelmisto- tuotannossa harvinainen yhdistelma¨, ja pa¨teva¨lla¨kin ammattilaisella 19 on viela¨ paljon opittavaa. C++-kielta¨ ka¨sittelevia¨ oppikirjoja on kasapa¨in. Useimmat kirjat la¨- hestyva¨t aihetta esittelema¨lla¨ kielen ominaisuuksia, kertomatta mi- ten ja miksi niita¨ ka¨yteta¨a¨n. Ta¨ssa¨ kirjassa keskityta¨a¨n kielen syntak- sin sijasta juuri na¨ihin C++-sirkkelin ka¨ytto¨o¨n liittyviin miten ja mik- si -kysymyksiin. Se on antoisaa luettavaa niin noviisille kuin va¨ha¨n vanhemmallekin konkarille. Harvoinpa kirjalle on olemassa na¨in sel- kea¨ tilaus niin oppilaitoksissa kuin teollisuudessakin. Tampereella 3.9.2000, prof. Ilkka Haikala 20 Alkusanat nelja¨nteen, uudistettuun painokseen Oliokeskeisyyteen liittyva¨ssa¨ markkinahumussa termista¨ oliokeskeinen on muodostunut va¨hitellen termin hyva¨ synonyymi; la¨hes mika¨ hyva¨nsa¨ idea tai tuote voidaan naamioida oliokeskeiseksi myyntitarkoituksissa. Oleellisen erottaminen epa¨oleellisesta tulee ta¨ma¨n myo¨ta¨ yha¨ hanka- lammaksi. – Ilkka Haikala [Haikala ja Ma¨rija¨rvi, 2002] Object-oriented programming is an exceptionally bad idea which could only have originated in California. – Edsger Wybe Dijkstra Olio-ohjelmointi on nopeasti muuttunut “uudesta ja ihmeellises- ta¨” ohjelmointitavasta arkipa¨iva¨iseksi valtavirran ohjelmointimene- telma¨ksi. Samaan aikaan aiheeseen liittyvia¨ kirjoja on julkaistu luke- mattomia ma¨a¨ria¨. Useissa olio-ohjelmointia ka¨sittelevissa¨ kirjoissa on kuitenkin tyy- pillisesti yksi ongelma. Osa kirjoista on kirjoitettu hyvin yleisella¨ ta- solla, ja ne keskittyva¨t oliosuunnitteluun ja olio-ohjelmoinnin teo- riaan paneutumatta siihen, miten na¨ma¨ asiat toteutetaan ka¨yta¨nno¨n olio-ohjelmointia tukevissa kielissa¨. Toiset kirjat ovat puolestaan la¨- hes yksinomaan jonkin oliokielen oppikirjoja, jolloin ne usein keskit- tyva¨t la¨hinna¨ tietyn kielen syntaksin ja erikoisuuksien opettamiseen. 21 Ta¨ma¨ kirja pyrkii osumaan na¨iden kahden a¨a¨ripa¨a¨n puoliva¨liin. Siina¨ pyrita¨a¨n antamaan mahdollisimman monesta olio-ohjelmoin- nin aiheesta seka¨ yleinen (“teoreettinen” olisi ehka¨ liian mahtiponti- nen sana) etta¨ ka¨yta¨nno¨nla¨heinen kuva. Kirjan ka¨yta¨nno¨n puolessa keskityta¨a¨n etupa¨a¨ssa¨ C++-kieleen, koska se on ta¨lla¨ hetkella¨ yleisin olio-ohjelmointia tukevista kielista¨. Myo¨s joidenkin muiden oliokiel- ten ominaisuuksia ka¨yda¨a¨n kuitenkin la¨pi silloin, kun ne oleellisesti eroavat C++:n oliomallista. Kirjan pa¨a¨ma¨a¨ra¨n johdosta ta¨ma¨ teos ei ole ohjelmoinnin alkeis- opas. Se edellytta¨a¨ lukijaltaan “perinteisen” ei-olio-ohjelmoinnin pe- rustaitoja seka¨ C- tai C++-kielen alkeiden tuntemista. Olio-ohjelmoin- nista lukijan ei kuitenkaan tarvitse tieta¨a¨ mita¨a¨n ennen ta¨ma¨ kirjan lukemista. Ta¨ma¨ teos ei myo¨ska¨a¨n pyri olemaan itse C++-kielen oppikirja. Se esittelee suurimman osan C++:n olio-ohjelmointiin liittyvista¨ piirteista¨ ja ka¨sittelee niiden ka¨ytto¨a¨ olio-ohjelmoinnissa, mutta na¨iden piirtei- den kaikkiin yksityiskohtiin ei ole mahdollista paneutua kirjan puit- teissa. Ta¨ma¨n vuoksi C++:aa opettelevan kannattaa hankkia ta¨ma¨n kir- jan rinnalle ka¨sikirjaksi jokin C++-kielen oppikirja, josta voi etsia¨ vas- tauksia itse kielen omituisuuksiin liittyviin kysymyksiin. Kirjaksi kel- paa esim. “The C++ Programming Language” [Stroustrup, 1997] (myo¨s suomennettuna [Stroustrup, 2000]) tai jokin muu monista muista vaihtoehdoista. Kirjan materiaali on saanut alkunsa tekijo¨iden Tampereen teknil- lisella¨ korkeakoululla luennoimasta kurssista Olio-ohjelmointi seka¨ lukuisista yrityksille pidetyista¨ C++-kursseista. Kirjaan liittyvia¨ uuti- sia ja lisa¨tietoja lo¨ytyy WWW-sivulta http://www.cs.tut.fi/~oliot/ kirja/. Sivun kautta voi la¨hetta¨a¨ myo¨s palautetta kirjan tekijo¨ille. Olemme ka¨ytta¨neet kirjaa myo¨s omassa opetuksessamme TTY:lla¨ ja yrityksissa¨. Na¨iden kurssien myo¨ta¨ olemme tehneet kirjaa varten opetuskalvosarjan, jonka toimitamme mielella¨mme halukkaille (yh- teystietomme lo¨ytyva¨t kirjan kotisivulta). Otamme mielella¨mme vastaan kaikki kirjaa koskevat kommentit ja kehitysehdotukset. Kumpikin kirjoittaja mielella¨a¨n syyta¨a¨ toista kai- kista kirjaan viela¨ mahdollisesti ja¨a¨neista¨ virheista¨. 22 Nelja¨s, uudistettu painos Opetus on kehittyva¨ prosessi. Tampereen teknillisella¨ yliopistol- la on ohjelmistotekniikan opetuksessa nykyisin kaksi kurssia olio- ohjelmoinnista (perus- ja jatkokurssi). Erityisesti jatkokurssin tarpei- den mukaisesti olemme laajentaneet edellisia¨ painoksia mm. seu- raavissa aiheissa: C++:n nimiavaruudet, moniperiytyminen, viipa- loituminen kopioinnissa ja sijoituksessa, geneerisyys ja template- metaohjelmointi seka¨ poikkeusturvallisuus. Edelleen painotus on yleisissa¨ olio-ohjelmoinnin periaatteissa ja niiden soveltamisessa C++- ohjelmointikielella¨. Edella¨ mainitut laajemmat lisa¨ykset tehtiin kirjan vuoden 2003 kolmanteen, uudistettuun painokseen. Nyt vuonna 2005 julkaistiin kirjan nelja¨s painos. Siina¨ on kolmanteen painokseen verrattuna teh- ty joitakin korjauksia, lisa¨yksia¨, pa¨ivityksia¨ ja tyylillisia¨ parannuksia. Kirjan rakenne Ta¨ma¨ kirja on tarkoitettu luettavaksi ja¨rjestyksessa¨ alusta loppuun, ja se esittelee olio-ohjelmoinnin ominaisuuksia sellaisessa ja¨rjestykses- sa¨, joka on ainakin tekijo¨iden kursseilla tuntunut toimivalta. Kirjan lukujen rakenne on seuraava: 1. Kohti olioita. Luku toimii johdantona olioajatteluun. Siina¨ ka¨y- da¨a¨n la¨pi ongelmia, jotka ilmeneva¨t suurten ohjelmistojen teke- misessa¨, seka¨ puhutaan modulaarisuudesta olio-ohjelmoinnin “esi-isa¨na¨”. Modulaarisuuden yhteydessa¨ esitella¨a¨n myo¨s C++:n nimiavaruudet (namespace). 2. Luokat ja oliot. Ta¨ma¨ luku ka¨y la¨pi olio-ohjelmoinnin ta¨rkeim- ma¨t ka¨sitteet, luokat ja oliot. Lisa¨ksi luvussa opetetaan C++:n oliomallin perusteet. 3. Olioiden elinkaari. Ta¨ssa¨ luvussa kerrotaan olioiden luomiseen ja tuhoamiseen liittyvista¨ ongelmista ja esitella¨a¨n, miten na¨ma¨ ongelmat on ratkaistu eri oliokielissa¨. 4. Olioiden rajapinnat. Luvussa ka¨sitella¨a¨n rajapintojen suunnit- telun ta¨rkeimpia¨ periaatteita ja kerrotaan, millaisia erilaisia ra- japintoja olio voi C++:ssa tarjota ka¨ytta¨jilleen. 23 5. Oliosuunnittelu. Luku ka¨y la¨pi ta¨rkeimpia¨ oliosuunnittelun menetelmia¨, muiden muassa UML:a¨a¨. 6. Periytyminen. Periytyminen on ehka¨ ta¨rkeimpia¨ olio-ohjelmoin- nin tuomia uusia asioita. Ta¨ma¨ luku ka¨sittelee periytymisen teoriaa yleisesti seka¨ sen toteutusta C++-kielessa¨. Ta¨ssa¨ uudiste- tussa painoksessa luku ka¨sittelee myo¨s moniperiytymista¨ ja sen suhdetta rajapintaluokkiin. 7. Lisa¨a¨ olioiden elinkaaresta. Ta¨ssa¨ luvussa ka¨yda¨a¨n la¨pi lisa¨a¨ olioiden elinkaareen liittyvia¨ asioita, mm. kopioiminen, sijoitta- minen, viipaloituminen (slicing) ja tyyppimuunnokset. 8. Lisa¨a¨ rajapinnoista. Vastaavasti ta¨ma¨ luku sisa¨lta¨a¨ lisa¨tietoa rajapintoihin liittyvista¨ aiheista, kuten sopimussuunnittelusta, luokkatason rajapinnasta, rajapintatyypeista¨ seka¨ komponentin sisa¨isista¨ rajapinnoista. 9. Geneerisyys. Luvussa ka¨sitella¨a¨n yleiska¨ytto¨isyyden ja uudel- leenka¨ytetta¨vyyden periaatteita. Suunnittelun geneerisyydesta¨ esitella¨a¨n suunnittelumallit ja C++:n toteutuksen geneerisyydes- ta¨ mallit (template). Lisa¨ksi luvussa pohditaan periytymisen ja geneerisyyden suhdetta. 10. Geneerinen ohjelmointi — STL ja metaohjelmointi. Ta¨ma¨ luku esittelee C++:n STL-kirjaston esimerkkina¨ geneerisesta¨ ohjelma- kirjastosta. Lisa¨ksi luku antaa katsauksen template-metaohjel- mointiin esimerkkina¨ mukautuvista geneerisista¨ mekanismeis- ta. 11. Virhetilanteet ja poikkeukset. Viimeisessa¨ luvussa ka¨yda¨a¨n viela¨ la¨pi poikkeukset (C++:n tapa virhetilanteiden ka¨sittelyyn) siina¨ laajuudessa, kuin ne liittyva¨t olio-ohjelmointiin. Luku ka¨- sittelee myo¨s olioiden poikkeusturvallisuutta ja sen toteuttamis- ta C++:lla. Lisa¨ksi liitteessa¨ A esitella¨a¨n lyhyesti sellaisia C++:n ei-olio-omi- naisuuksia, joita ei ole ollut C-kielessa¨, mutta joita ka¨yteta¨a¨n ta¨ma¨n kirjan esimerkeissa¨. Liitteessa¨ A ka¨yda¨a¨n la¨pi • viitetyypit 24 • inline-sanan ka¨ytto¨ tehokkuusoptimoinnissa • vector-taulukot C:n taulukkojen korvaajina • string-merkkijonot C:n char*-merkkijonojen korvaajina. Liitteen tarkoituksena on etta¨ lukija, joka ei na¨ita¨ ominaisuuksia tun- ne, saa niista¨ liitteen avulla sellaisen ka¨sityksen, etta¨ kirjan koodie- simerkkien lukeminen onnistuu vaivatta. Kyseisia¨ ominaisuuksia ei kuitenkaan opeteta liitteessa¨ perinpohjaisesti. Liite B sisa¨lta¨a¨ C++-tyylioppaan. Siihen on kera¨tty tekijo¨iden mie- lesta¨ ta¨rkeita¨ ohjelmointityyliin ja C++:n sudenkuoppien va¨ltta¨miseen liittyvia¨ ohjeita selityksineen. Ta¨ma¨ tyyliopas on ka¨yto¨ssa¨ mm. Tam- pereen teknillisen yliopiston Ohjelmistotekniikan laitoksella opetuk- sessa ja ohjelmointiprojekteissa. Kirjan lopussa on myo¨s kirjallisuusluettelo seka¨ aakkosellinen luettelo englanninkielisista¨ olio-ohjelmoinnin termeista¨ ja niiden suomenkielisista¨ vastineista ta¨ssa¨ teoksessa (itse tekstissa¨ jokaises- ta uudesta termista¨ on pyritty kertomaan seka¨ suomenkielinen etta¨ englanninkielinen sana). Lisa¨ksi kirjan loppuun on liitetty normaali aakkosellinen hakemisto. Kiitokset Haluamme kiitta¨a¨ Tampereen teknillisen yliopiston Ohjelmistotek- niikan laitoksen henkilo¨kuntaa ja erityisesti sen professoreita Reino Kurki-Suonio ja Ilkka Haikala, jotka ovat luoneet opetuksellaan, esi- merkilla¨a¨n ja olemuksellaan tyo¨ympa¨risto¨n, jossa on todella ilo tyo¨s- kennella¨. Kirjan ka¨sikirjoituksen kommentoinnista suurkiitokset professo- reille Ilkka Haikala, Hannu-Matti Ja¨rvinen, Kai Koskimies, Reino Kur- ki-Suonio ja Markku Sakkinen seka¨ kollegoillemme Kirsti Ala-Mutka, Joni Helin, Vespe Savikko ja Antti Virtanen. Kiitokset kuuluvat myo¨s olio-ohjelmointi- ja C++-kurssiemme opis- kelijoille, jotka ovat kommenteillaan ja kysymyksilla¨a¨n vaikuttaneet opetuksemme ja materiaalimme sisa¨lto¨o¨n. Lopuksi haluamme viela¨ kiitta¨a¨ kaikkia perheenja¨senia¨mme, su- kulaisiamme, ysta¨via¨mme ja tuttaviamme, joiden ansiosta (tai joista huolimatta) olemme sa¨ilyneet edes jotenkin terveja¨rkisina¨ kirjoitus- urakan aikana. 25 Tampereella 15.4.2005 Matti Rintala Jyke Jokinen 26 Luku 1 Kohti olioita Trurl pa¨a¨tti lopulta vaientaa ha¨net kerta kaikkiaan raken- tamalla koneen, joka osaisi kirjoittaa runoja. Ensin Trurl kera¨si kahdeksansataakaksikymmenta¨ tonnia kybernetiik- kaa ka¨sitteleva¨a¨ kirjallisuutta ja kaksitoistatuhatta tonnia parasta runoutta, istahti aloilleen ja luki kaiken la¨pi. Aina kun ha¨nesta¨ alkoi tuntua, ettei ha¨n pystyisi nielaisemaan ena¨a¨ ainuttakaan kaavakuvaa tai yhta¨lo¨a¨, ha¨n vaihtoi ru- nouteen, ja pa¨invastoin. Jonkin ajan kuluttua ha¨nelle alkoi valjeta, etta¨ koneen rakentaminen olisi lastenleikkia¨ ver- rattuna sen ohjelmoimiseen. Keskivertorunoilijan pa¨a¨ssa¨ olevan ohjelmanhan on kirjoittanut runoilijan oma kult- tuuri, ja ta¨ma¨n kulttuurin puolestaan on ohjelmoinut sita¨ edelta¨nyt kulttuuri ja niin edelleen aina aikojen aamuun asti, jolloin ne tiedonsirut, jotka myo¨hemmin osoittautuvat ta¨rkeiksi tulevaisuuden runoilijalle, pyo¨rteiliva¨t viela¨ kos- moksen syo¨vereiden alkukaaoksessa. Jotta siis voisi onnis- tuneesti ohjelmoida runokoneen, on ensin toistettava koko maailmankaikkeuden kehitys alusta pita¨en — tai ainakin melkoinen osa siita¨. – Trurlin elektrubaduuri [Lem, 1965] 1.1 Ohjelmistojen tekijo¨iden ongelmakentta¨ Tehokkaiden, luotettavien, halpojen, selkeiden, helppoka¨ytto¨isten, 1.2. Laajojen ohjelmistojen teon vaikeus 27 ylla¨pidetta¨vien, siirretta¨vien, pitka¨ika¨isten — sanalla sanoen laaduk- kaiden ohjelmistojen tekija¨t ovat ammattilaisia, jotka joutuvat pa¨i- vitta¨isessa¨ tyo¨ssa¨a¨n toimimaan taiteen ja tieteen rajamailla. Toisaal- ta ohjelmoijien on ymma¨rretta¨va¨ alansa formaalit periaatteet (ohjel- mistojen suunnittelumenetelmista¨ tietorakenteiden ka¨ytto¨o¨n ja oh- jelmointiin liittyva¨a¨n matematiikkaan). Toisaalta ohjelmoijat ovat myo¨s taiteilijoita, jotka muokkaavat olemassa olevista materiaaleis- ta ja valmiista rakennuspalikoista ohjelmiston kokonaisuuden. Ohjel- mien teon luova puoli on varsinkin alan palkkauksessa va¨ha¨lle huo- miolle ja¨tetty puoli. Silti kaikki tuntevat alalla toimivia “taikureita”, jotka hallitsevat bitteja¨ ja niiden kokonaisuuksia muita paremmin. 1.2 Laajojen ohjelmistojen teon vaikeus Jokainen tietokoneohjelmointia opiskellut on kirjoittanut ensimma¨i- sena¨ ohjelmanaan korkeintaan muutaman kymmenen rivin ohjel- man, jonka avulla on saanut tuntuman ohjelmoinnin ka¨yta¨nno¨n puo- leen (ohjelman kirjoittaminen syntaksisesti oikein koneen ymma¨rta¨- ma¨a¨n muotoon, ohjelman suorittaminen tietokoneella ja mahdollises- ti jonkin na¨kyva¨n tuloksen saaminen aikaan, ohjelman toimintalogii- kan muuttaminen ja sovittaminen halutun ongelman ratkaisemisek- si jne.). Koska lyhyet ohjelmanpa¨tka¨t saa useimmiten tekema¨a¨n ha- luamiaan asioita kohtuullisen lyhyella¨ vaivanna¨o¨lla¨ (muutamasta mi- nuutista muutamaan pa¨iva¨a¨n), on yleinen harhaluulo, etta¨ ta¨sta¨ voi- daan yleista¨a¨ suurempien ohjelmistojen valmistamisen olevan vain saman prosessin toistaminen isommalle rivima¨a¨ra¨lle ohjelmakoodia. Ka¨yta¨nno¨ssa¨ jokaiselle ihmiselle tulee jossain vaiheessa vastaan raja tiedon hallinnassa, kun hallittava tietoma¨a¨ra¨ kasvaa liian suu- reksi. Ohjelmien teossa ta¨ma¨ raja tulee na¨kyviin ohjelman rivima¨a¨- ra¨n kasvaessa (muuttujista ei muista ena¨a¨ heti kaikkien ka¨ytto¨tarkoi- tusta, ehto- ja silmukkalauseiden logiikka ha¨ma¨rtyy, tietorakenteiden hallinta sekoaa, osoittimet eiva¨t osoita oikeaan paikkaan jne.). Ohjelmistotekniikan suurimpia ongelmia on suurten ohjelmisto- jen tekemisen vaikeus. Ta¨ma¨ ns. ohjelmistokriisi (“software crisis”) on tavallaan tietotekniikan itsensa¨ aiheuttama ongelma. Tietokone- laitteistojen mm. tallennus- ja prosessointitekniikan ra¨ja¨hdysma¨inen kehitys on mahdollistanut jatkuvasti aikaisempaa laajempien ja mut- kikkaampien ohjelmistojen suorittamisen, mutta na¨iden ohjelmisto- 1.3. Suurten ohjelmistokokonaisuuksien hallinta 28 jen tekemiseen tarvittavat menetelma¨t ja tyo¨kalut eiva¨t ole kehitty- neet la¨heska¨a¨n yhta¨ nopeasti. Ohjelmistotuotannon professori Ilkka Haikala on sanonut aiheesta: “Ohjelmistokriisia¨ ratkomaan on kehitetty mm. uusia tyo¨- kaluja ja tyo¨menetelmia¨. Ta¨sta¨ huolimatta ohjelmistotyo¨n tuottavuuden kasvu on tilastojen mukaan ollut vuosit- tain vain noin nelja¨n prosentin luokkaa. Muutamien vuo- sien va¨lein kaksinkertaistuvaan ohjelmistojen keskima¨a¨- ra¨iseen kokoon verrattuna kasvu on huolestuttavan pieni.” [Haikala ja Ma¨rija¨rvi, 2002] Ongelma ei ole mika¨a¨n uusi ilmio¨ eika¨ ole kyse siita¨, etta¨ se olisi havaittu vasta viime aikoina. Vuonna 1972 vastaanottaessaan Turing Award -palkintoa Edsger W. Dijkstra puhui aiheesta: “As long as there were no machines, programming was no problem at all; when we had a few weak computers, pro- gramming became a mild problem and now that we have gigantic computers, programming has become an equally gigantic problem. In this sense the electronic industry has not solved a single problem, it has only created them — it has created the problem of using its product.” [Dijkstra, 1972] Suurten ohjelmistokokonaisuuksien hallintaan on kehitetty useita erilaisia menetelmia¨, mutta mika¨a¨n niista¨ ei ole osoittautunut muita selva¨sti paremmaksi viisasten kiveksi, joka ratkaisee kaikki ohjelmis- totyo¨n ongelmat. Na¨ista¨ eri menetelmista¨ eniten huomiota ovat viime aikoina saaneet oliokeskeiset menetelma¨t, joissa nimen mukaisesti keskityta¨a¨n olioihin. Mita¨ na¨ma¨ oliot sitten ovat ja miten ne vaikutta- vat ohjelmien suunnitteluun ja toteuttamiseen? Na¨ihin kysymyksiin haemme vastausta seuraavissa luvuissa. 1.3 Suurten ohjelmistokokonaisuuksien hallin- ta Seitsema¨nkymmenta¨luvulla ryhdyttiin kehitta¨ma¨a¨n menetelmia¨ oh- jelmistokriisin ratkaisemiseksi — ta¨ma¨ tyo¨ jatkuu yha¨ edelleen. Yksi 1.3. Suurten ohjelmistokokonaisuuksien hallinta 29 varhaisimmista ratkaisuperiaatteista on tuttu kaikesta inhimillisesta¨ toiminnasta: ongelman jakaminen yhden ihmisen hallittaviin pa- loihin ja yksinkertaistaminen abstrahoimalla. Ta¨ma¨ periaate on na¨hta¨vissa¨, kun tarkastellaan ohjelmoijien ta¨rkeimpien tyo¨kalujen, ohjelmointikielten ja niiden periaatteiden kehitysta¨. Mita¨ abstrahointi tarkoittaa? Voimme aloittaa vaikka sivistyssana- kirjan selvityksella¨: Abstraktio. Ajatustoiminta, jonka avulla jostakin ka¨sit- teesta¨ saadaan yleisempi ka¨site va¨henta¨ma¨lla¨ siita¨ tietty- ja¨ ominaisuuksia. Myo¨s valikointi, jossa jokin ominaisuus tai ominaisuusryhma¨ erotetaan muista yhteyksista¨a¨n tar- kastelun kohteeksi. Abstrahoida. Suorittaa abstraktio, erottaa mielessa¨a¨n olennainen muusta yhteydesta¨. Abstrakti. Abstrahoimalla saatu, puhtaasti ajatuksellinen, ka¨sitteellinen. [Aikio ja Vornanen, 1992] Kera¨ta¨a¨n siis yhteen toisiinsa liittyvia¨ asioita ja nimiteta¨a¨n nii- den kokonaisuutta jollain kuvaavalla “uudella” termilla¨ tai kuvauk- sella. Ohjelmointi on ta¨ynna¨ ta¨llaisia rakenteita. Otetaan esimerkiksi hyvin yleinen operaatio: tiedon tallentaminen massamuistilaitteelle. Sovellusohjelmoijan ei tarvitse tuntea kiintolevyn valmistajan ka¨yt- ta¨ma¨a¨ laitteen ohjauksen protokollaa, koodausta, ajoitusta ja bittien ja¨rjestysta¨, vaan ha¨n voi ka¨ytta¨a¨ erikseen ma¨a¨riteltyja¨ korkeamman (abstraktio)tason operaatioita (esim. open, write ja close). Seuraavissa aliluvuissa na¨emme, miten abstrahoinnilla saadaan selkea¨mma¨ksi tietokoneohjelman rakenteen hallinta. 1.3.1 Ennen kehittyneita¨ ohjelmointikielia¨: ei rakennetta Ohjelmoinnin historian alkuha¨ma¨ra¨ssa¨ ohjelmoinnin tavan ma¨a¨ra¨- si laitteisto. Prosessorien ymma¨rta¨mia¨ ka¨skykoodeja kirjoitettiin suo- raan joko niiden numerokoodeilla tai myo¨hemmin symboleja na¨ik- si koodeiksi muuntavan assembler-ohjelman avulla. Ta¨llaisessa lait- teistonla¨heisessa¨ na¨perryksessa¨ kaikista hiemankin suuremmista oh- jelmista tuli spagettiro¨ykkio¨ita¨, joissa tieto oli yksitta¨isia¨ muistipaik- koja, ja tietoa ka¨sitteleva¨a¨ ohjelmakoodia oli ripoteltuna ympa¨riin- sa¨. Kuvassa 1.1 seuraavalla sivulla on esimerkki ohjelmasta (koodi ja 1.3. Suurten ohjelmistokokonaisuuksien hallinta 30 Data kuukausi_2 päivä_1 vuosi_1 päivä_2 kuukausi_1 Ohjelmakoodi vuosi_2 KUVA 1.1: Tyypillinen assembly-ohjelman spagettirakenne data), joka ka¨sittelee kahta pa¨iva¨ysta¨: ohjelmoija on varannut kolme muistipaikkaa yhdelle pa¨iva¨ykselle ja na¨ita¨ tietoja ka¨sitella¨a¨n ohjel- makoodista suoraan muistipaikkojen osoitteilla. Rakennekuvasta na¨- kee hyvin, etta¨ ta¨llaisesta rakenteesta on esim. ylla¨pitovaiheessa hy- vin vaikea selvitta¨a¨, mitka¨ kaikki ohjelmakoodin kohdat viittaavat jo- honkin ma¨a¨ra¨ttyyn muistipaikkaan (esim. kuukausi 1). 1.3.2 Tietorakenteet: tiedon kera¨a¨minen yhteen Kun ohjelmointikielten kehitys kuusikymmenta¨luvulla pa¨a¨si laajem- massa mitassa alkamaan, ryhdyttiin tekema¨a¨n ohjelmoijien kokemus- ten pohjalta kielia¨, joissa tietoa pystyttiin ka¨sittelema¨a¨n muistipaik- koja korkeammalla abstraktiotasolla — tietorakenteina. Tietorakenteet ovat nimettyja¨ kokonaisuuksia, jotka koostuvat muista tietorakenteista tai ns. kielen perustyypeista¨. Na¨iden hierark- kisten rakenteiden avulla saatiin kera¨ttya¨ yhteen kuuluvat tiedot sa- man nimikkeen alle. Esim. pa¨iva¨ystieto voitaisiin haluta ka¨sitella¨ kol- mena kokonaislukuna, jotka kuvaavat vuotta, kuukautta ja pa¨iva¨a¨. Na¨ista¨ voidaan muodostaa ohjelmointikielen tasolla yksi pa¨iva¨yksek- si nimetty tietorakenne. Vastaavasti ohjelman toiminnallisuutta voi- 1.3. Suurten ohjelmistokokonaisuuksien hallinta 31 Data vuosi päivä kuukausi Päiväys_1 vuosi päivä kuukausi Päiväys_2 Proseduurit KUVA 1.2: Pa¨iva¨ykset tietorakenteina daan jakaa yhden toiminnon toteuttaviin paloihin, joita nimiteta¨a¨n funktioiksi tai proseduureiksi. Kuvassa 1.2 on esimerkki kahdesta pa¨iva¨yksesta¨ ja niiden tieto- rakenteista. Ta¨ssa¨ mallissa pa¨iva¨yksia¨ ka¨sitella¨a¨n ohjelmakoodissa edelleen missa¨ kohtaa tahansa aivan kuten spagettimallissakin. Tie- torakenteita ja funktioita tukevia ohjelmointikielia¨ ovat mm. Pascal ja C. 1.3.3 Moduulit: myo¨s ohjelmakoodi kera¨ta¨a¨n yhteen Vuosi 2000 -ongelma oli tietotekniikassa paljon puhetta ja tyo¨ta¨ syn- nytta¨nyt aihe. Siina¨ ohjelmistoista oli tarkastettava, etta¨ ne eiva¨t oleta kaikkien vuosilukujen olevan muotoa 19xy. Na¨iden kohtien lo¨yta¨mi- nen on edellisten ka¨yta¨nto¨jen mukaan tehdyissa¨ ohjelmistoissa hy- vin vaikeata, silla¨ mika¨ tahansa ohjelmiston osa voi suoraan ka¨sitella¨ pa¨iva¨yksen sita¨ kokonaislukua, joka kuvaa vuosilukua. Jotta ta¨llaiset toiminnalliset viittaukset tietorakenteisiin saataisiin hierarkkiseen hallintaan, pa¨a¨tettiin ma¨a¨ritella¨ joukko ma¨a¨ra¨ttyyn tie- torakenteeseen liittyvia¨ funktioita ja sopia, etta¨ vain na¨illa¨ funktioil- la saa ka¨sitella¨ kyseista¨ tietorakennetta (esim. pa¨iva¨ys, katso ku- va 1.3 seuraavalla sivulla). Ta¨ta¨ funktiokokoelmaa nimiteta¨a¨n raja- pinnaksi (interface) ja sen funktioita rajapintafunktioiksi. 1.3. Suurten ohjelmistokokonaisuuksien hallinta 32 Data päivä kuukausi vuosi Päiväys_1 vuosi päivä kuukausi Päiväys_2 Päiväys−moduuli Proseduurit ja moduulit Rajapinnan toteutus Rajapinta KUVA 1.3: Pa¨iva¨yksia¨ ka¨sitteleva¨n moduulin rakenne Ohjelmointikielten rakenteita, joissa kootaan tietorakenteet ja nii- ta¨ ka¨sitteleva¨ ohjelmakoodi samaan kokonaisuuteen seka¨ erikseen ma¨a¨ritella¨a¨n tietorakenteiden ka¨sittelyyn toiminnallinen rajapinta, nimiteta¨a¨n moduuleiksi (module). Koska moduulit ka¨tkeva¨t niiden rajapinnan toiminnallisuuden toteutuksen, moduuleja tulee tarkas- tella kahdesta na¨ko¨kulmasta: • Moduulin ka¨ytta¨ja¨. Moduulin ka¨ytta¨ja¨ on ohjelmoija, joka tar- vitsee moduulin tarjoamaa palvelua oman koodinsa osana. Ta¨s- sa¨ ulkopuolisessa na¨ko¨kulmassa meita¨ kiinnostaa vain moduu- lin ulkoinen eli julkinen rajapinta: miten sita¨ tulee ka¨ytta¨a¨ ja saammeko sen avulla toteutettua haluamamme toiminnan? (Moduulilla voi olla myo¨s ns. sisa¨inen rajapinta, joka on tarkoi- tettu vain moduulin tekija¨n ka¨ytto¨o¨n.) • Moduulin toteuttaja. Moduulin suunnittelijan ja toteuttajan vastuulla on ma¨a¨ritella¨ ja dokumentoida moduulille rajapinta, joka on yksinkertainen, helppoka¨ytto¨inen, selkea¨ ja ka¨ytto¨kel- poinen. Vain moduulin toteuttajan ta¨ytyy miettia¨ ja toteuttaa rajapinnan takana oleva toiminnallisuus — ta¨ma¨ osa on muil- ta ohjelmoijilta ka¨tkettyna¨ siina¨ mielessa¨, etta¨ heida¨n ei va¨lt- 1.3. Suurten ohjelmistokokonaisuuksien hallinta 33 ta¨ma¨tta¨ tarvitse tuntea toteutuksen yksityiskohtia. Ta¨ta¨ nimite- ta¨a¨n tiedon ka¨tkenna¨ksi. Siina¨ kapseloidaan ka¨yto¨n kannalta turha tieto moduulin sisa¨lle (encapsulation). Kyseessa¨ on ab- strahoinnin ta¨rkeimpia¨ tyo¨kaluja ohjelmoinnissa. Ka¨yta¨nno¨ssa¨ moduuliajattelu voi olla vain sopimus ma¨a¨ra¨tyn ra- japinnan noudattamisesta (esim. Pascal ja C), tai ohjelmointikieli voi jopa kielta¨a¨ (ja tarkastaa) rajapinnan takana olevien tietorakenteiden ka¨sittelyn moduulin ohjelmakoodin ulkopuolelta (Ada, Modula). Rajapinta-ajattelu pakottaa suunnittelemaan tarkemmin ennalta, miten ma¨a¨ra¨ttya¨ tietorakennetta on tarkoitus ka¨ytta¨a¨ eli mita¨ pal- veluita tietorakenteen lisa¨ksi halutaan siihen liittyen tarjota. Tie- don ka¨tkenna¨n ja rajapinta-ajattelun haittapuolina voidaankin pita¨a¨ suunnittelun vaikeutumista. Moduulin tekija¨lle on helppo sanoa pa¨a¨- ma¨a¨ra¨ksi yksinkertainen ja ka¨ytto¨kelpoinen (eli ta¨ydellinen?) rajapin- ta. Na¨iden vaatimusten toteuttamiseen sitten tarvitaankin todellista ohjelmointitaikuria. Ka¨yta¨nno¨n ohjelmistotyo¨ssa¨ rajapintoja joudu- taan tarkastamaan ja tarkentamaan ohjelmiston kokonaissuunnitte- lun edetessa¨ ja jatkossa ohjelmistoa ylla¨pidetta¨essa¨. [Sethi, 1996] Rajapinta edista¨a¨ merkitta¨va¨sti ohjelmistojen ylla¨pidetta¨vyytta¨, koska ne rajapinnan “takana” tehdyt muutokset, jotka eiva¨t vaiku- ta rajapinnalle ma¨a¨riteltyyn toiminnallisuuteen, eiva¨t aiheuta mita¨a¨n muutoksia muuhun ohjelmistoon. Na¨in saadaan abstrahoitua laajan ohjelmiston kokonaisuutta rajapinnoilla toisistaan eroteltuihin mah- dollisimman itsena¨isiin paloihin. Vuosi 2000 -ongelman tapauksessa modulaarisessa ohjelmistossa on ensin tarkastettava, etta¨ rajapinnan kautta ei pa¨a¨sta¨ ka¨sittelema¨a¨n vuosilukuja va¨a¨ra¨lla¨ tavalla. Jos vuosisatojen 1900 ja 2000 va¨liseen ka¨sittelyyn liittyva¨t rakenteet ovat mahdollisia vain moduulin sisa¨lla¨, niin riitta¨a¨, etta¨ tarkastamme kaiken moduulin koodin ja varmistam- me sen toimivan myo¨s vuoden 1999 ja¨lkeen. On ta¨rkea¨ta¨ huomata, etta¨ jos rajapinnan ma¨a¨rittely “paljastaa” vuosiluvun tietorakenteen ulkopuolelle esimerkiksi 0–99 va¨lille ma¨a¨riteltyna¨ kokonaislukuna, niin joudumme edelleen tarkastamaan myo¨s kaiken moduulin ulko- puolella pa¨iva¨yksia¨ ka¨sitteleva¨n ohjelmakoodin. Ta¨llaista rajapintaa voidaan pita¨a¨ huonosti suunniteltuna vuosi 2000 -ongelman kannal- ta. Kyse on kuitenkin ja¨lkiviisaudesta, jos rajapinnan suunnittelun aikaan ohjelmiston arvioitu elinkaari ei ole ylta¨nyt vuoden 1999 ylit- se. Olisiko rajapinnan ma¨a¨rittelija¨n pita¨nyt ottaa huomioon ongelma- 1.3. Suurten ohjelmistokokonaisuuksien hallinta 34 kenta¨n ulkopuolisia asioita? Ja mita¨ kaikkia niista¨? Rajapinta-ajattelu ei ole inhimillisia¨ virheita¨ korjaava tai esta¨va¨ menetelma¨, mutta se on ihmisille luontaista hierarkkista ajattelua tukeva menetelma¨, joka selkeytta¨a¨ suurten ohjelmistojen rakennetta. Modulaarista ohjelmointia tukevia ohjelmointikielia¨ ovat mm. Ada ja Modula. Listauksessa 1.1 on esimerkki pa¨iva¨ys-moduulin tie- torakenteesta ja osasta rajapintaa Modula-3-kielella¨. 1.3.4 Oliot: tietorakenteet, rajapinnat ja toiminnot yhdes- sa¨ Kun tarkastelemme pa¨iva¨ysrajapinnan ka¨ytto¨a¨ (listaus 1.2 seuraaval- la sivulla), niin huomaamme heti, etta¨ rajapintafunktioille on aina va¨- litetta¨va¨ parametrina se tietorakenne, johon operaatio halutaan koh- distaa. Ta¨sta¨ ka¨yta¨nno¨n tarpeesta on la¨hto¨isin ajattelumalli, jossa ei haluta korostaa pelka¨sta¨a¨n moduulin toiminnallista osaa (rajapinta- funktiot) vaan ajatellaan rajapintaa tietorakenteen “osana”. Ta¨ma¨n mallin mukaisia alkioita, jotka yhdista¨va¨t tietorakenteet ja rajapin- nan, nimiteta¨a¨n olioiksi (kuva 1.4 seuraavalla sivulla). Uudessa ajat- telumallissa kohdistetaan rajapintakutsu ma¨a¨ra¨ttyyn olioon, ja ta¨ma¨ na¨kyy myo¨s olio-ohjelmointikielen tasolla (listaus 1.3 sivulla 36). Oliomallin mukaisen ohjelmistojen suunnittelun ja toteutuksen katsotaan sopivan paremmin ihmisten luontaiseen tapaan tarkastel- la ongelmia. Oli toteutettavana ja¨rjestelma¨na¨ lentokoneen toiminnan valvonta tai sˇakkipeli, niin ja¨rjestelma¨n katsotaan koostuvan osista ja na¨iden osien toisiinsa ja ulkomaailmaan kohdistamista toiminnoista. 1 INTERFACE pa¨iva¨ys; 2 3 TYPE 4 PVM : RECORD 5 pa¨iva¨, kuukausi, vuosi : INTEGER; 6 END; 7 8 PROCEDURE Luo( pa¨iva¨, kuukausi, vuosi : INTEGER ) : PVM; 9 PROCEDURE PaljonkoEdella¨( eka, toka : PVM ) : INTEGER; 10 PROCEDURE Tulosta( kohde : PVM ); LISTAUS 1.1: Pa¨iva¨ysmoduulin tietorakenne ja osa rajapintaa 1.3. Suurten ohjelmistokokonaisuuksien hallinta 35 1 IMPORT pa¨iva¨ys; 2 3 VAR vappu : pa¨iva¨ys.PVM; 4 5 BEGIN 6 vappu := pa¨iva¨ys.Luo( 1, 5, 2001 ); 7 pa¨iva¨ys.Tulosta( vappu ); 8 END; LISTAUS 1.2: Pa¨iva¨ysmoduulin ka¨ytto¨esimerkki päivä kuukausi vuosi päivä kuukausi vuosi joulu vappu Proseduurit ja moduulit Oliot KUVA 1.4: Rajapinta osana tietorakennetta (oliot) Ta¨ma¨ uusi ajattelumalli vaikuttaa useisiin ohjelmistojen tekemisen osa-alueisiin: • Ma¨a¨rittely. Ka¨ytetta¨essa¨ olioita on ma¨a¨rittelyssa¨ pidetta¨va¨ mie- lessa¨ ta¨ma¨ pa¨a¨ma¨a¨ra¨ (pa¨a¨dyta¨a¨n oliorakenteeseen). • Suunnittelu. Oliosuunnittelussa on tunnettava olioiden ja luok- kien ominaisuuksia (mm. periytyminen ja geneerisyys), jotta jo suunnitteluvaiheessa pystyta¨a¨n hyo¨dynta¨ma¨a¨n uuden ajattelu- mallin kaikki (tarvittava) teho. 1.3. Suurten ohjelmistokokonaisuuksien hallinta 36 1 PROCEDURE TulostaAikaisempi( pa¨iva¨ A, pa¨iva¨ B : PVM ) 2 BEGIN 3 IF pa¨iva¨ A.PaljonkoEdella¨( pa¨iva¨ B ) > 0 THEN 4 pa¨iva¨ A.Tulosta(); 5 ELSE 6 pa¨iva¨ B.Tulosta(); 7 END; 8 END; 9 10 BEGIN 11 vappu := PVM( 1, 5, 2000 ); 12 joulu := PVM( 24, 12, 1999 ); 13 TulostaAikaisempi( vappu, joulu ); 14 END. LISTAUS 1.3: Pa¨iva¨ysolioiden ka¨ytto¨ • Ohjelmointi. Erityiset olio-ohjelmointikielet helpottavat olioita sisa¨lta¨va¨n suunnitelman toteuttamista huomattavasti. Tyo¨kalui- na na¨ma¨ kielet ovat aikaisempia mutkikkaampia, koska ne tuo- vat lisa¨a¨ uusia laajoja ominaisuuksia aikaisempiin (proseduraa- lisiin) kieliin verrattuna. Perinteisessa¨ ohjelmistosuunnittelussa on otettu la¨hto¨kohdaksi osat (data, tietorakenteet) tai toiminnot (funktiot ja rajapinnat) ja py- ritty rakentamaan koko ohjelmisto ta¨sta¨ yhdesta¨ jaottelusta la¨htien. Olio-ohjelmoinnin yksi ta¨rkeimmista¨ suunnitteluperiaatteista on jat- kuvasti yritta¨a¨ pita¨a¨ na¨ita¨ molempia na¨ko¨kantoja mukana suunnitte- luprosessissa: ja¨rjestelma¨sta¨ tunnistetaan seka¨ olioita etta¨ niiden va¨- lisia¨ toiminnallisuuksia. Ongelmala¨hto¨inen ja laitteistola¨heinen na¨ko¨kulma olioihin Olion voidaan katsoa olevan osa sen ongelman ratkaisua, johon oh- jelmistolla pyrita¨a¨n. Ta¨ma¨ on ulkoinen tai ongelmala¨hto¨inen na¨ko¨- kulma oliohin. Ohjelmiston suunnittelija pyrkii tunnistamaan esim. sˇakkipelista¨ pelin mallintamiseen tarvittavat oliot: erilaiset nappulat, lauta, peliajan mittaava kello jne. Olioista koostuvan ohjelmiston toiminnallisuus on olioiden va¨lis- ta¨ kommunikointia. Oliot kutsuvat toistensa rajapintojen kautta tar- vitsemiaan toimintoja. Esimerkiksi sˇakkinappulaolio voi pyyta¨a¨ sˇak- 1.3. Suurten ohjelmistokokonaisuuksien hallinta 37 kilautaa mallintavalta oliolta tietoa siita¨, voiko se siirtya¨ ma¨a¨ra¨ttyyn ruutuun. Kyselyn seurauksena lautaolio voi taas vuorostaan kysya¨ muilta nappuloilta niiden nykyisia¨ paikkoja jne. Ohjelmiston ylim- ma¨n tason “logiikka” voidaan na¨in ma¨a¨ritella¨ olioiden rajapintojen ja olioiden va¨lisen kommunikaation avulla — ta¨ssa¨ suunnitteluvai- heessa ei tarvitse ottaa kantaa siihen, miten na¨ma¨ toiminnot tullaan yksityiskohtaisesti toteuttamaan. Toisaalta jokaisen olion voi ajatella edustavan erillista¨ tietokonet- ta, jolla on oma ohjelmakoodi (rajapinnan takainen toteutus) ja muis- ti (olion tietorakenteen arvot eli olion tila). Na¨ma¨ “minikoneet” kom- munikoivat la¨hetta¨ma¨lla¨ toisilleen viesteja¨, joilla pyydeta¨a¨n jonkin toiminnon suorittamista toisessa koneessa eli oliossa. Ta¨ma¨ ajattelu- malli on sisa¨inen tai laitteistonla¨heinen na¨ko¨kulma olioihin. Malli voi olla hyo¨dyllinen esimerkiksi hajautetuissa ja¨rjestelmissa¨, joissa osa ohjelmiston olioista tulee lopullisessa toteutuksessa todellisuu- dessakin sijaitsemaan eri fyysisissa¨ tietokoneissa. [Sethi, 1996] Ohjelmiston staattiset ja dynaamiset osat Oliomallia ei ole tarkoitettu syrja¨ytta¨ma¨a¨n modulaarisuutta vaan ta¨y- denta¨ma¨a¨n sita¨. Moduulien avulla ohjelmistoon voidaan tehda¨ yleen- sa¨ ylimmilla¨ tasoilla olevaa jakoa osiin ja niiden va¨lisiin rajapintoi- hin: ka¨ytto¨liittyma¨n na¨kyma¨t, na¨ytto¨laitteiden rajapinnat, tietokanta jne. Koska ta¨ma¨ jaottelu osiin on ohjelmassa pysyva¨, sita¨ nimiteta¨a¨n staattiseksi jaotteluksi. Dynaamiset osat tarkoittavat tietorakenteiden ja rajapintojen ko- koelmia, joissa yhden mallin mukaisia olioita voi esiintya¨ ohjelman ajoaikana useita. Esim. pa¨iva¨ysoliolla on aina sama toiminnallinen rajapinta ja sama tietorakenne (kolme kokonaislukua). Eri pa¨iva¨yk- silla¨ voi olla erilaiset arvot tietorakenteessa. Saman mallin (pa¨iva¨ys) eri ilmentymilla¨ (oliot) sanotaan olevan erilainen tila (tietorakenteen arvot). Olioiden dynaamisuutta on myo¨s se, etta¨ niita¨ voi syntya¨ ja tuhoutua ohjelman suorituksen aikana — kutakin moduulia taas on olemassa yksi kappale koko ohjelmiston ajossaolon ajan. Olio on itsena¨inen kokonaisuus Oliokeskeisessa¨ ajattelumallissa olioiden katsotaan olevan itsena¨isia¨ kokonaisuuksia, joille on ma¨a¨ritelty tarkka vastuualue (responsibil- 1.3. Suurten ohjelmistokokonaisuuksien hallinta 38 ity) ohjelmistossa [Budd, 2002]. Vastuualue on nimitys kaikelle sille, mita¨ olion on ma¨a¨ra¨tty toteuttavan. Esim. pa¨iva¨ysoliolla on ma¨a¨ritel- tyna¨ vastuu tiedon taltioinnista (olio kuvaa yhta¨ pa¨iva¨ma¨a¨ra¨a¨) ja jul- kinen rajapinta (tarjotut palvelut). Vastuualue kattaa muutakin kuin ohjelmointikielen muuttujia ja funktioita. Se voi esimerkiksi ma¨a¨ri- tella¨ etta¨ jokin olio on vastuussa ma¨a¨ra¨ttyjen muiden olioiden luomi- sesta ja tuhoamisesta eli niiden elinkaaresta. 1.3.5 Komponentit: moduulit ja oliot yhdessa¨ On ta¨rkea¨ta¨ huomata, etta¨ moduulit ja oliot voivat muodostaa hierar- kioita ja kokonaisuuksia yhdessa¨. Ohjelmistossa voidaan esim. ylim- ma¨lla¨ tasolla ma¨a¨ritella¨ ajanlaskusta vastuussa oleva moduuli, jonka tarjoamista palveluista yksi on pa¨iva¨yksia¨ kuvaavat oliot. Hyvin suunniteltu rajapintojen, olioiden ja toteutusten (samalla rajapinnalla voi olla esimerkiksi vaihtoehtoisia toteutuksia) kokoel- maa on viime aikoina ryhdytty nimitta¨ma¨a¨n komponentiksi (compo- nent). Ideana olisi tarjota ohjelmistojen valmistajille hieman saman- lainen tilanne kuin ta¨lla¨ hetkella¨ on mm. elektroniikkasuunnittelijoil- la — he voivat suunnitella ja valmistaa tuotteitaan hyvin laajan val- miin komponenttivalikoiman avulla. Tuote luodaan yhdistelema¨lla¨ elektroniikkakomponentit uusiksi mutkikkaammiksi tuotteiksi. Vas- taavalla tavalla uudelleenka¨ytto¨a¨ varten suunniteltuja ohjelmakom- ponentteja voitaisiin pita¨a¨ uusien ohjelmistojen pohjana. Ohjelmistokomponentti on itsena¨inen kokonaisuus, jota sen tar- joamien julkisten rajapintojen avulla voidaan hyo¨dynta¨a¨ osana suu- rempaa kokonaisuutta. Komponentti kera¨a¨ yhteen ma¨a¨ra¨tyn vastuu- alueen toteuttamiseen tarvittavat ohjelmiston moduulit ja oliot (olio- ohjelmana toteutus on usein kokoelma luokkia). Komponenttimarkki- noita on ta¨lla¨ hetkella¨ olemassa erityisesti graaﬁsten ka¨ytto¨liittymien alueella, mutta vasta tulevaisuus tulee na¨ytta¨ma¨a¨n, onko kyseessa¨ oh- jelmistojen tekemista¨ suuremmassa mittakaavassa muokkaava ajatte- lumalli. Aivan viime vuosien ohjelmistokomponentit sisa¨lta¨va¨t usein ra- japintadokumentaation ja la¨hdekoodin lisa¨ksi myo¨s valmiin bina¨a¨- rimuotoisen toteutuksen. Ta¨llo¨in komponentti otetaan sellaisenaan (valmiiksi konekoodiksi ka¨a¨nnettyna¨) ka¨ytto¨o¨n omaan ohjelmistoon. Yksi ta¨llainen komponenttiarkkitehtuuri on JavaBeans [JavaBeans, 2001]. 1.4. C++: Moduulit ka¨a¨nno¨syksiko¨illa¨ 39 1.4 C++: Moduulit ka¨a¨nno¨syksiko¨illa¨ Seuraavassa luvussa esitelta¨va¨ C++:n luokkarakenne sisa¨lta¨a¨ olio-omi- naisuuksien lisa¨ksi modulaarisuuden ta¨rkeimma¨t piirteet: julkisen rajapinnan ja toteutuksen ka¨tkenna¨n. Ta¨ssa¨ ja seuraavassa aliluvussa esitella¨a¨n kaksi tapaa tehda¨ staattisia moduulirakenteita C++:lla. Staattisella moduulilla tarkoitetaan ka¨a¨nno¨saikana luotua ja ko- ko ohjelman suoritusajan samana pysyva¨a¨ kokonaisuutta, jonka myo¨s ka¨a¨nta¨ja¨ ymma¨rta¨a¨ erilliseksi yksiko¨ksi. C ja C++ -ka¨a¨nta¨ja¨t ovat aina ka¨sitelleet yhta¨ la¨hdekooditiedostoa kerrallaan yhtena¨ kokonaisuute- na, jonka ka¨a¨nno¨ksessa¨ pita¨a¨ olla na¨kyvilla¨ kaikkien ka¨ytettyjen ra- kenteiden esittelyt. [Kerninghan ja Ritchie, 1988] Ohjelmoitavan moduulin julkisen rajapinnan esittely voidaan laittaa ns. otsikkotiedostoon (header, listaus 1.4), joka taas voidaan #include-esika¨a¨nta¨ja¨komennolla ottaa na¨kyville kaikkiin moduulia ka¨ytta¨viin tiedostoihin. Suurissa ohjelmissa #include-rakenteet tulevat mutkikkaiksi ja monitasoisiksi. Ka¨a¨nta¨ja¨ voi ta¨llo¨in virheellisesti saada ka¨sittelyyn saman otsikkotiedoston useaan kertaan saman ka¨a¨nno¨ksen aikana, mika¨ on virhetilanne koska kieli sallii esittelyiden esiintyva¨n vain kerran samassa ka¨a¨nno¨ksessa¨. Esimerkiksi pa¨iva¨ysmoduulin esitte- lya¨ tarvitaan useassa korkeamman tason moduulin otsikkotiedostos- sa, jotka ka¨a¨nno¨syksikko¨ ottaa na¨kyville #include-ka¨skylla¨, kuten ku- va 1.5 seuraavalla sivulla na¨ytta¨a¨. Esimerkkilistauksen 1.4 alussa na¨- kyva¨lla¨ esika¨a¨nta¨ja¨n ehdollisella ka¨a¨nta¨misella¨ (#ifndef X #define X 1 #ifndef PAIVAYS H 2 #define PAIVAYS H 3 4 typedef struct paivays data { 5 int p , k , v ; 6 } paivays PVM; 7 8 paivays PVM paivays luo( int paiva, int kuukausi, int vuosi ); 9 void paivays tulosta( paivays PVM kohde ); ... 10 #endif /* PAIVAYS H */ LISTAUS 1.4: Pa¨iva¨ysmoduulin esittely C-kielella¨, paivays.h 1.4. C++: Moduulit ka¨a¨nno¨syksiko¨illa¨ 40 . . . #endif) saadaan varmistettua, etta¨ esittely na¨kyy ka¨a¨nta¨ja¨lle vain yhden kerran. Eri puolella ohjelmistoa (kooditiedostoja) esitellyt rakenteet toteu- tetaan yhdessa¨ ka¨a¨nno¨syksiko¨ssa¨, joka lopuksi linkiteta¨a¨n mukaan valmiiseen ohjelmaan. Ka¨a¨nno¨syksikko¨ on C-kielessa¨ yksi tiedosto (joka mahdollisesti on ottanut ka¨a¨nno¨kseen mukaan toisia tiedosto- ja #include-rakenteella), josta ka¨a¨nta¨ja¨ tekee objektitiedoston. Linki- tysvaihe pita¨a¨ huolen siita¨, etta¨ esittelyosan na¨hneiden ka¨a¨nno¨syksi- ko¨iden kutsut moduulin funktioihin meneva¨t oikeaan osaa ohjelmaa lopullisessa suoritettavassa ohjelmabina¨a¨rissa¨ (kuva 1.6 seuraavalla sivulla). Ta¨ssa¨ ratkaisussa on kaksi ongelmaa: 1. Kaikki eri moduulien ka¨ytta¨ma¨t nimet (funktioiden ja muuttu- jien nimet) ovat oletuksena ka¨ytetta¨vissa¨ kaikkialla ohjelmassa (kunhan ne esitella¨a¨n ka¨a¨nno¨syksiko¨ssa¨). Ka¨a¨nno¨syksiko¨n pai- kalliseen ka¨ytto¨o¨n tarkoitetut nimet on ohjelmoijan itse merkit- ta¨va¨ ma¨a¨reella¨ static. 2. On olemassa vain yksi ylimma¨n tason (globaali) na¨kyvyysalue, jossa kaikki moduulien julkisten rajapintojen symbolit ovat na¨- kyvissa¨. Jos esim. usea moduuli esittelee funktion Tulosta, niin /* pääohjelma */ #include \"tietokanta.h\" #include \"loki.h\" /* ... */ #include \"paivays.h\" /* tietokanta.h */ /* loki.h */ #include \"paivays.h\" paivays.h KUVA 1.5: Sama otsikkotiedosto voi tulla ka¨ytto¨o¨n useita kertoja 1.5. C++: Nimiavaruudet 41 #include \"paivays.h\" void paivays_tulosta( paivays_PVM kohde ) { /* ... */ } Päiväysmoduulin toteutus #include \"paivays.h\" ... paivays_tulosta(vappu); call _pv_tul proc: _pv_tul #include #include void paivays_tulosta( paivays_PVM kohde ); /* paivays.h */ call _pv_tul proc: _pv_tul linkitys linkitys käännös Päiväyksen käyttö käännös Lopullinen ohjelmabinääri Lähdekooditiedostot Tuotettu konekoodi call _label_xyz call _label_xyz call _label_xyz KUVA 1.6: Moduulirakenne C-kielella¨ ohjelma ei ka¨a¨nny, koska sama symboli on lopullisessa bina¨a¨ris- sa¨ ma¨a¨riteltyna¨ useita kertoja. Ta¨ma¨n nimiavaruuden “roskaan- tumisen” takia on moduulin toteuttajan pyritta¨va¨ nimea¨misella¨ va¨ltta¨ma¨a¨n nimikonﬂikteja. Esimerkkissa¨ ta¨ma¨ on tehty liitta¨- ma¨lla¨ moduulin nimiin etuliite “paivays ”. 1.5 C++: Nimiavaruudet Ratkaisuksi suurten ohjelmistojen rajapintojen nimikonﬂikteihin ja modulaarisen rakenteen esitta¨miseen ISOC++ -standardi [ISO, 1998] tarjoaa nimiavaruudet (namespace). Nimiavaruuksien tarkoituksena 1.5. C++: Nimiavaruudet 42 on tarjota kielen syntaksin tasolla oleva hierarkkinen nimea¨miska¨y- ta¨nto¨. Hierarkia auttaa jakamaan ohjelmistoa osiin seka¨ ka¨yta¨nno¨n ohjelmoinnissa esta¨a¨ nimikonﬂikteja ohjelmiston eri moduulien va¨- lilla¨. Kuvaavat ja tunnetut nimet voivat esiintya¨ suuressa ohjelmistos- sa useassa paikassa ja ne ta¨ytyy pystya¨ erottamaan toisistaan. Esimer- kiksi aliohjelmat Pa¨iva¨ys::tulosta() ja Kirjastonkirja::tulosta() suorittavat saman semanttisen operaation (tulostamisen), mutta etu- liite kertoo, minka¨ moduulin ma¨a¨rittelema¨sta¨ tulostusoperaatiosta on kyse. 1.5.1 Nimiavaruuksien ma¨a¨ritteleminen Ka¨yta¨nno¨ssa¨ C++-ohjelmoija voi uudella avainsanalla namespace koo- ta yhteen toisiinsa liittyvia¨ ohjelmakokonaisuuksia, jotka voivat olla mita¨ tahansa C++-ohjelmakoodia. Kaikki esimerkkimme pa¨iva¨ma¨a¨ria¨ kuvaavat ohjelmiston osat (tietorakenteet, tietotyypit, vakiot, oliot ja funktiot) voidaan kera¨ta¨ yhteen nimikkeen namespace Paivays\u0017 alle, kuten listauksessa 1.5 seuraavalla sivulla on tehty. Nimiavaruuden voi toisessa ka¨a¨nno¨syksiko¨ssa¨ (eli la¨hdekooditie- dostossa) “avata” uudelleen laajentamista varten, jolloin moduulin esittelema¨t rajapintafunktiot voidaan toteuttaa toisessa ohjelmatie- dostossa (listauksessa 1.6 seuraavalla sivulla). Ta¨lla¨ tavoin moduu- lin esittely ja toteutus saadaan eroteltua toisistaan. Tiedon yksinker- taistamisen (abstrahointi) mukaisesti moduulin ka¨ytta¨ja¨n pita¨isi pys- tya¨ hyo¨dynta¨ma¨a¨n moduulia ainoastaan sen esittelya¨ (hh-tiedosto) ja dokumentointia tutkimalla. “Tylsa¨t” ja epa¨oleelliset toteutusyksityis- kohdat on piilotettu erilliseen ka¨a¨nno¨syksikko¨o¨n, joka kuitenkin on osa samaa C++-nimiavaruutta. 1.5.2 Na¨kyvyystarkenninoperaattori Kaikki nimiavaruuden sisa¨lla¨ olevat tunnistenimet ovat oletukse- na na¨kyvissa¨ eli ka¨ytetta¨vissa¨ vain kyseisen nimiavaruuden sisa¨l- la¨ (paivays.hh-tiedostossa esitelty tietue PVM on ka¨yto¨ssa¨ listaukses- sa 1.6 seuraavalla sivulla). Ulkopuolelta nimiavaruuden sisa¨lla¨ ole- . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . \u0017ISOC++ -standardi sallisi tunnisteiden nimissa¨ mm. skandinaavisia symboleja, jolloin esi- merkkimme nimiavaruuden nimi olisi Pa¨iva¨ys. Koska ka¨yta¨nno¨ssa¨ ka¨a¨nta¨ja¨t eiva¨t ta¨llaista ominaisuutta tue, ohjelmaesimerkkiemme nimet ovat ilman suomen kielen kirjaimia ˚a, a¨ ja o¨. 1.5. C++: Nimiavaruudet 43 1 // Pa¨iva¨ys-moduulin rajapinnan esittely (tiedosto: paivays.hh) 2 #ifndef PAIVAYS HH 3 #define PAIVAYS HH 4 5 namespace Paivays { 6 7 // Pa¨iva¨yksien tietorakenne: 8 struct Pvm { int p , k , v ; }; 9 10 // Julkisen rajapinnan funktiot: 11 12 Pvm luo( int paiva, int kuukausi, int vuosi ); 13 // Palauttaa uuden alustetun pa¨iva¨yksen. 14 15 void tulosta( Pvm kohde ); 16 // Tulostetaan pa¨iva¨ys ja¨rjestelma¨n oletuslaitteelle. ... 17 } 18 #endif LISTAUS 1.5: Nimiavaruudella toteutettu rajapinta 1 // Pa¨iva¨ys-moduulin toteutus (tiedosto: paivays.cc) 2 #include \"paivays.hh\" /* rajapinnan esittely */ 3 4 namespace Paivays { 5 6 Pvm luo( int paiva, int kuukausi, int vuosi ) 7 { 8 Pvm paluuarvo; ... 9 return paluuarvo; 10 } 11 12 void tulosta( Pvm p ) 13 { ... 14 } ... 15 } LISTAUS 1.6: Rajapintafunktioiden toteutus erillisessa¨ tiedostossa 1.5. C++: Nimiavaruudet 44 viin rakenteisiin voidaan viitata na¨kyvyystarkenninoperaattorin (::) avulla (listaus 1.7). Tarkoituksena on, etta¨ ohjelmoija kertoo ka¨ytto¨paikassa mita¨ ko- konaisuutta ja mita¨ alkiota sen sisa¨lla¨ ha¨n kulloinkin tarkoittaa (on- ko ka¨yto¨ssa¨ funktio Paivays::tulosta() vai Kirja::tulosta()). Ta- paa voidaan kritisoida ylima¨a¨ra¨isella¨ kirjoitusvaivalla, mutta ta¨ssa¨ kannattaa huomioida myo¨s mukaan tuleva dokumentaatio. Moduu- linimet kertovat heti, mita¨ rakenteita ka¨yteta¨a¨n, eika¨ niita¨ luultavasti tarvitse ena¨a¨ erikseen kommentoida ohjelmaan. Nimiavaruuksia voi olla myo¨s sisa¨kka¨in, jolloin na¨ky- vyystarkentimia ketjutetaan oikean alkion osoittamiseksi (Tietokanta::Yhteys::SQL). C++ ei aseta mita¨a¨n erityisia¨ rajoja ta¨ma¨n ketjun pituudelle, mutta eiva¨t pitka¨t ketjut ena¨a¨ noudata nimiavaruuksien alkupera¨ista¨ ajatusta selkeydesta¨ hierarkian avulla. 1.5.3 Nimiavaruuksien hyo¨dyt Toteuttamalla moduulit nimiavaruuksien avulla saadaan C-kielen malliin verrattuna seuraavat parannukset: • Nimikonﬂiktien vaara va¨henee merkitta¨va¨sti, koska jokaisen moduulin rajapintanimet ovat omassa nimetyssa¨ na¨kyvyysalu- eessaan (tietysti todella laajoissa ohjelmistoissa on pidetta¨va¨ huoli siita¨, etteiva¨t nimiavaruuksien nimet taas vuorostaan ole samoja eri moduuleilla). • Moduulin ma¨a¨rittelemien rakenteiden ka¨ytto¨ on kielen syntak- sin tasolla na¨kyva¨n rakenteen (na¨kyvyystarkennin ::) vuoksi 1 #include \"paivays.hh\" 2 3 int main() { 4 Paivays::Pvm vappu; 5 vappu = Paivays::luo( 1,5,2001 ); 6 Paivays::tulosta( vappu ); ... 7 LISTAUS 1.7: Nimiavaruuden rakenteiden ka¨ytta¨minen 1.5. C++: Nimiavaruudet 45 selkea¨mpa¨a¨ (vertaa esim. Modula-3:n moduuliproseduurien kut- sutapa pisteoperaattorilla listauksessa 1.2 sivulla 35). • Hierarkkisuudesta huolimatta moduulin sisa¨lla¨ on ka¨ytetta¨vis- sa¨ lyhyet nimet. Koodista na¨hda¨a¨n syntaksin tasolla esimerkik- si, mitka¨ funktiokutsut kohdistuvat saman moduulin sisa¨lle ja mitka¨ muualle ohjelmistoon. Nimiavaruudet ovat kehittyneet C++:aan va¨hitellen korjaamaan ha- vaittuja puutteita. Ne ovat melko uusi ominaisuus, joten on olemassa paljon C++ ohjelmakoodia, jossa niita¨ ei ka¨yteta¨, mutta nimiavaruuk- sien etujen takia niiden ka¨ytto¨a¨ suositellaan yleisesti. 1.5.4 std-nimiavaruus ISOC++ -standardi ma¨a¨rittelee omaan ka¨ytto¨o¨nsa¨ std-nimisen nimia- varuuden. Ta¨ma¨n nimen alle on kera¨tty kaikki kielen ma¨a¨rittelyn esittelemien rakenteiden nimet (muutamaa poikkeusta, kuten funk- tiota exit, lukuun ottamatta). Esimerkiksi C:n tulostusrutiini printf ja C++:n tulostusolio cout ovat ISOC++:n mukaisesti toteutetussa ka¨a¨n- ta¨ja¨ssa¨ nimilla¨ std::printf ja std::cout, jotta ne eiva¨t aiheuttaisi ongelmia ohjelman muiden nimien kanssa. Koska kirjastoon kuuluu myo¨s C-kielesta¨ tulleita funktioita, std-nimiavaruus sisa¨lta¨a¨ satoja ni- mia¨. std-nimiavaruus on olemassa kaikissa nykyaikaisissa C++-ka¨a¨nta¨- jissa¨, ja ainoastaan sen rakenteiden ka¨ytta¨minen on sallittua — ta¨ha¨n nimiavaruuteen ei saa itse lisa¨ta¨ uusia ohjelmarakenteita. 1.5.5 Standardin uudet otsikkotiedostot Samalla kun kielen ma¨a¨rittelema¨t rakenteet on siirretty std-nimia- varuuden sisa¨a¨n, ovat myo¨s otsikkotiedostojen nimet vaihtuneet. Ai- kana ennen nimiavaruuksia C++:ssa otettiin tulostusoperaatiot ka¨yt- to¨o¨n esiprosessorin ka¨skylla¨ #include <iostream.h>. Nyt oikea (std- nimiavaruudessa oleva) tulostusoperaatioiden esittely saadaan ka¨s- kylla¨ #include <iostream>. Ta¨ma¨ tiedostotyyppiin viittava .h-liit- teen puuttuminen on ominaisuus kaikissa standardin ma¨a¨rittelemis- sa¨ otsikko-“tiedostoissa”. Sana “Tiedostot” on lainausmerkeissa¨, kos- ka standardi ei ma¨a¨ra¨a¨ rakenteiden olevan missa¨a¨n tiedostossa, vaan em. rivi ainoastaan kertoo ka¨a¨nta¨ja¨lle, etta¨ kyseiset esittelyt otetaan 1.5. C++: Nimiavaruudet 46 ka¨ytto¨o¨n — ka¨a¨nta¨ja¨ saa toteuttaa ominaisuuden vapaasti vaikka tie- tokantahakuna. C-kielen kautta standardiin tulleiden otsikkotiedostojen nimista¨ on myo¨s poistunut loppuliite “.h” ja lisa¨ksi niiden eteen on lisa¨t- ty kirjain “c” korostamaan C-kielesta¨ pera¨isin olevia rakenteita. Esi- merkiksi merkkitaulukoiden ka¨sittelyyn tarkoitetut funktiot (strcpy yms.) saadaan ka¨ytto¨o¨n ka¨skylla¨ #include <cstring>. Na¨iden “van- hojen” funktioiden ka¨yto¨sta¨ yhdessa¨ C++-tulostusoperaatioiden kanssa na¨kyy esimerkki listauksessa 1.8. 1.5.6 Nimiavaruuden synonyymi Nimiavaruuden nimi voi myo¨s olla synonyymi (alias) toiselle jo ole- massa olevalle nimiavaruudelle. Ta¨llo¨in kaikki alias-nimeen tehdyt viittaukset ka¨ytta¨ytyva¨t alkupera¨isen nimen tavoin. Ta¨ta¨ ominaisuut- ta voidaan hyo¨dynta¨a¨ esimerkiksi silloin, jos samalle moduulille on olemassa useita vaihtoehtoisia toteutuksia. Moduulia ka¨ytta¨va¨ ohjel- makoodi kirjoitetaan ka¨ytta¨ma¨lla¨ alias-nimea¨ ja todellinen ka¨ytto¨o¨n otettava moduuli valitaan synonyymin ma¨a¨rittelyn yhteydessa¨ (katso listaus 1.9 seuraavalla sivulla). Kun ollaan nimea¨ma¨ssa¨ yleiska¨ytto¨isia¨ ohjelmakoodikirjastoja, on ta¨rkea¨ta¨ valita myo¨s kokonaisuuden nimiavaruudelle nimi, joka ei helposti aiheuta nimikonﬂiktia muun ohjelmiston kanssa. Esimer- 1 #include <cstdlib> // pa¨a¨ohjelman paluuarvo EXIT SUCCESS 2 #include <iostream> // C++:n tulostus 3 #include <cstring> // C:n merkkitaulukkofunktiot 4 5 int main() 6 { 7 char const* const p = \"Jyrki Jokinen\"; 8 char puskuri[ 42 ]; 9 std::strcpy( puskuri, \"Jyke \" ); 10 std::strcat( puskuri, std::strstr(p, \"Jokinen\") ); 11 12 std::cout << puskuri << std::endl; 13 return EXIT SUCCESS; 14 } LISTAUS 1.8: C-kirjastofunktiot C++:n nimiavaruudessa 1.5. C++: Nimiavaruudet 47 1 #include \"prjlib/string.hh\" 2 #include <string> 3 4 int main() { 5 #ifdef PRJLIB OPTIMOINNIT KAYTOSSA 6 namespace Str = ComAcmeFastPrjlib; 7 #else 8 namespace Str = std; 9 #endif 10 11 Str::string esimerkkijono; ... 12 } LISTAUS 1.9: Nimiavaruuden synonyymi (aliasointi) kiksi String lienee huono kirjaston nimi, silla¨ usealla valmistajalla on varmasti kiinnostusta tehda¨ samanlainen rakenne. Virallisen ni- men kannattaa olla pitka¨ (OhjTutFiOpetusMerkkijono), ja ohjelmoijat voivat omassa koodissaan ka¨ytta¨a¨ moduulia lyhyemma¨lla¨ etuliitteel- la¨ synonyymin avulla. 1.5.7 Lyhyiden nimien ka¨ytto¨ (using) Etuliitteen “std::” toistaminen jatkuvasti esimerkiksi cout-tulostuk- sessa on varmasti raskaalta ja turhalta tuntuva rakenne.] Jos samas- sa ohjelmalohkossa ka¨yteta¨a¨n useita kertoja samaa nimiavaruuden si- sa¨lla¨ olevaa nimea¨, ohjelmoijien kirjoitusvaivaa helpottamaan on ole- massa using-lause, joka “nostaa” nimiavaruuden sisa¨lla¨ olevan nimen ka¨ytetta¨va¨ksi ohjelman nykyisen na¨kyvyysalueen sisa¨lle. Peruska¨y- to¨ssa¨ kerrotaan yksitta¨inen rakenne, jota halutaan ka¨ytta¨a¨. Esimerkik- si using std::cout mahdollistaa tulostusolion nimen ka¨yto¨n ilman etuliitetta¨ (katso listaus 1.10 seuraavalla sivulla). Jotta nimiavaruuk- sien alkupera¨inen ka¨ytto¨tarkoitus sa¨ilyisi, using-lauseet tulisi laittaa aina mahdollisimman la¨helle niiden tarvittua ka¨ytto¨kohtaa ohjelma- tiedostossa (funktion tai koodilohkon alkuun, jossa ne samalla toimi- vat dokumenttia ka¨ytetyista¨ ulkopuolisista rakenteista). . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ]Kaikki C++-ka¨a¨nta¨ja¨t eiva¨t vaadi std::-etuliitteen ka¨ytta¨mista¨, mutta kyseessa¨ on kielen standardin vaatimuksen vastainen toiminnallisuus. 1.5. C++: Nimiavaruudet 48 1 #include \"paivays.hh\" 2 #include <iostream> 3 4 void kerroPaivays( Paivays::Pvm p ) 5 { 6 using std::cout; // ka¨yteta¨a¨n ma¨a¨ra¨ttya¨ nimea¨ 7 using std::endl; 8 using namespace Paivays; // ka¨yteta¨a¨n kaikkia nimiavaruuden nimia¨ 9 cout << \"Ta¨na¨a¨n on: \"; 10 tulosta( p ); // Kutsuu rutiinia Paivays::Tulosta 11 cout << endl; 12 } LISTAUS 1.10: using-lauseen ka¨ytto¨ Turvallisuussyista¨ using tulisi laittaa ohjelmakooditiedoston al- kuun vasta kaikkien #include-ka¨skyjen ja¨lkeen, jottei se vahingos- sa sotke otsikkotiedostojen toimintaa va¨a¨ra¨a¨n paikkaan nostetulla ni- mella¨. Sama sotkemisen va¨ltta¨minen tarkoittaa myo¨s sita¨, ettei using- lauseita kannata laittaa otsikkotiedostojen sisa¨lle (etuka¨teen ei tiede- ta¨ missa¨ yhteydessa¨ tai ja¨rjestyksessa¨ tiedoston sisa¨lta¨mia¨ esittelyita¨ ka¨yteta¨a¨n). [Sutter, 2000] Hyva¨ perusperiaate on ka¨ytta¨a¨ using-lausetta mahdollisimman la¨- hella¨ sita¨ aluetta, jossa sen on tarkoitus olla voimassa (yleensa¨ koo- dilohko tai funktio). Toisaalta paljon ka¨ytetyissa¨ rakenteissa on usein dokumentaation kannalta selkea¨mpa¨a¨ kirjoittaa using heti siihen liit- tyva¨n otsikkotiedoston #include-ka¨skyn ja¨lkeen. Ta¨ma¨ suositus taas on heti ristiriidassa edellisen kappaleen “sotkemissa¨a¨nno¨n” kanssa. Nimiavaruudet ovat C++:ssa sen verran uusi asia, etta¨ parasta mahdol- lista suositusta selkeyden ja turvallisuuden kannalta ta¨ssa¨ asiassa on vaikea antaa. Hyva¨ kompromissi on luottaa ka¨a¨nta¨ja¨n std-kirjastojen olevan oikein toimivia, jolloin niiden esittelyiden va¨liin voi kirjoittaa huoletta using-lauseita, mutta omissa osissa ja ostettujen kirjastojen kanssa kirjoittaa using-lauseet vasta kaikkien esittelyiden (otsikkotie- dostojen) ja¨lkeen. Listaus 1.11 seuraavalla sivulla on esimerkki ta¨sta¨ “yhdistelma¨sa¨a¨nno¨sta¨”. Kun normaali using-lause nostaa na¨kyville yhden nimen, sen eri- koistapaus using namespace nostaa nimiavaruuden sisa¨lta¨ kaikki ni- met nykyiselle tasolle ja siten ka¨yta¨nno¨ssa¨ poistaa nimiavaruuden ka¨yto¨sta¨ kokonaan. Erityisesti lausetta using namespace std tulisi ai- 1.5. C++: Nimiavaruudet 49 1 // Omat rakenteet esitella¨a¨n std-kirjastoja ennen 2 // (ta¨ma¨ siksi etta¨ saamme tarkastettua niiden sisa¨lta¨va¨n kaikki 3 // tarvittavat #include-ka¨skyt ts. ne ovat itsena¨isesti ka¨a¨ntyvia¨ yksikko¨ja¨) 4 #include \"paivays.hh\" 5 #include \"swbus.hh\" 6 #include \"tietokanta.hh\" 7 #include \"loki.hh\" 8 // Kaikista yleisimmin ta¨ssa¨ tiedostossa ka¨ytetyt std-rakenteet esitella¨a¨n 9 // heti niihin liittyva¨n otsikkotiedoston ja¨lkeen 10 #include <iostream> 11 using std::cout; 12 using std::endl; 13 #include <vector> 14 using std::vector; 15 #include <string> 16 using std::string; 17 // lopuksi omiin moduuleihin liittyva¨t using-lauseet 18 using Paivays::PVM; 19 using Loki::varoitus; 20 using Loki::virhe; LISTAUS 1.11: using-lauseen ka¨ytto¨ eri otsikkotiedostojen kanssa na va¨ltta¨a¨ C++-ohjelmissa. Ta¨ta¨ rakennetta ka¨yteta¨a¨n kylla¨ usein ope- tuksessa ja lyhyissa¨ esimerkkiohjelmissa selkeyden saavuttamiseksi, mutta suurissa C++-ohjelmistoissa emme sita¨ suosittele. 1.5.8 Nimea¨ma¨to¨n nimiavaruus Nimiavaruuksiin on ma¨a¨ritelty erikoistapaus nimea¨ma¨to¨n nimiava- ruus (unnamed namespace), jolla rajoitetaan funktioiden ja muuttu- jien nimien na¨kyvyys yhteen ka¨a¨nno¨syksikko¨o¨n (ta¨ma¨ tehtiin aikai- semmin C- ja C++-kielissa¨ static-avainsanan avulla). Nimea¨ma¨tto¨ma¨l- la¨ nimiavaruudella voidaan dokumentoida ne osat moduulin ohjel- makoodia, jotka ovat olemassa ainoastaan sen sisa¨ista¨ toteutusta var- ten. Samalla ka¨a¨nta¨ja¨ myo¨s pita¨a¨ huolen, ettei niita¨ vahingossa pys- tyta¨ ka¨sittelema¨a¨n moduulin ulkopuolelta. Listauksessa 1.12 seuraavalla sivulla ma¨a¨ritella¨a¨n yksi muuttu- ja ja funktio ka¨a¨nno¨syksiko¨n paikallisiksi nimea¨ma¨tto¨ma¨lla¨ nimiava- ruudella. Ta¨ma¨n nimiavaruuden sisa¨lla¨ olevat rakenteet ovat samas- sa ka¨a¨nno¨syksiko¨ssa¨ (tiedostossa) suoraan ka¨ytetta¨vissa¨ ilman na¨- kyvyystarkenninta (jota nimea¨ma¨tto¨ma¨lla¨ nimiavaruudella ei tietysti 1.5. C++: Nimiavaruudet 50 edes ole), mutta ne eiva¨t ole ka¨ytetta¨vissa¨ ka¨a¨nno¨syksiko¨n ulkopuo- lella. Nimea¨ma¨to¨n nimiavaruus suojaa sen sisa¨lla¨ olevat nimet tavalli- sen nimiavaruuden tapaan. Vaikka useassa ka¨a¨nno¨syksiko¨ssa¨ on sa- moilla nimilla¨ olevia rakenteita, niin ne eiva¨t sotke toisiaan, kunhan kaikki ovat nimea¨ma¨tto¨ma¨n nimiavaruuden sisa¨lla¨. Ta¨ma¨n voi ajatel- la tapahtuvan siten, etta¨ ka¨a¨nta¨ja¨ tuottaa kulissien takana uniikin ni- men jokaiselle nimea¨ma¨tto¨ma¨lle nimiavaruudelle. Jos nimea¨ma¨to¨n- ta¨ nimiavaruutta ka¨ytta¨a¨ tavallisen nimiavaruuden sisa¨lla¨, on nimea¨- ma¨tto¨ma¨n nimiavaruuden sisa¨lto¨ ka¨ytetta¨vissa¨ ainoastaan ta¨ma¨n ta- vallisen nimiavaruuden sisa¨lla¨ (esim. moduulin sisa¨inen funktio tai tietorakenne). 1 static unsigned long int laskuri; // Vanha tapa 2 3 // ISOC++:n mukainen tapa 4 // nimea¨ma¨to¨n nimiavaruus: 5 namespace { 6 unsigned long int viiteLaskuri; 7 void lisaaViiteLaskuria() { 8 ++viiteLaskuri; ... 9 } 10 } 11 12 // Julkinen operaatio: 13 Pvm luoPaivaysOlio() { 14 lisaaViiteLaskuria(); 15 // ylla¨oleva rutiinin kutsu toimii samassa tiedostossa ilman 16 // using-lausetta tai na¨kyvyystarkenninta. 17 // rutiinia ei pysty kutsumaan tiedoston ulkopuolisesta koodista. ... 18 } 19 LISTAUS 1.12: Nimea¨ma¨to¨n nimiavaruus 1.6. Moduulituki muissa ohjelmointikielissa¨ 51 1.6 Moduulituki muissa ohjelmointikielissa¨ Modulaarisuus on jo vanha ja hyva¨ksi havaittu ohjelmointikielten ominaisuus, joten ei ole ylla¨tta¨va¨a¨, etta¨ sita¨ tuetaan tavalla tai toisella monissa nykyisissa¨ ohjelmointikielissa¨. Seuraavassa esittelemme ly- hyesti Modula-3- ja Java-kielten tukea ohjelman jakamisessa moduu- leihin. 1.6.1 Modula-3 [Bo¨szo¨rme´nyi ja Weich, 1996] Modula-3-kielen yksi ta¨rkeimmista¨ suunnitteluperusteista on ollut modulaarisen ohjelmoinnin tukeminen. Moduulien julkinen rajapin- ta kirjoitetaan erilliseen tiedostoon, jossa on na¨kyvissa¨ ainoastaan moduulin tarjoamien palveluiden ka¨ytta¨miseen tarvittavat tiedot. Ta¨- ma¨ na¨kyy kuvasta 1.13 seuraavalla sivulla. Moduulin ka¨ytta¨ja¨lle on rajapinnan esittelyssa¨ tarpeeksi informaatiota Pa¨iva¨ys-moduulin tar- joaman tietotyypin (T) ka¨ytta¨miseen. Moduulin tekija¨ on ainoa, jon- ka kirjoittamassa toteutuksessa “paljastetaan” pa¨iva¨ystietorakenteen koko muoto. Modula-3:n tapa esitta¨a¨ “vajaita” tyyppima¨a¨rittelyja¨ on hyvin pit- ka¨lle vietya¨ moduulin ka¨ytta¨ja¨n kannalta turhan tiedon ka¨tkemista¨. Ohjelmamoduuleja ka¨sitteleva¨n ka¨a¨nta¨ja¨ohjelman on tietysti tunnet- tava tietotyypin Pa¨iva¨ys.T rakenne ja sen viema¨ muistinvaraus, mut- ta ta¨ma¨ ka¨a¨nno¨stekninen yksityiskohta on piilotettu ohjelmoijalta. 1.6.2 Java [Arnold ja Gosling, 1996] Javassa ohjelman nimet jaetaan kokonaisuuksiin tavalla, joka on hy- vin la¨hella¨ C++:n nimiavaruuksia. Ta¨sta¨ on esimerkki listauksessa 1.14 seuraavalla sivulla. Java-kooditiedoston alussa voidaan avainsanalla package kertoa, minka¨ nimiseen pakkaukseen tiedostossa esitelta¨va¨t nimet kuuluvat. Javassa pakkausten nimet muodostavat hierarkian pisteella¨ erotelluilla osanimilla¨. Nimet otetaan ka¨ytto¨o¨n omassa oh- jelmakoodissa import-lauseella. Oletuksena ka¨ytto¨o¨n tulee vain yksi nimi, mutta jos haluaan ka¨ytta¨a¨ kaikkia pakkauksen julkisia nimia¨, ka¨yteta¨a¨n merkinta¨a¨ ta¨hti (.*). Javassa on myo¨s rakenne interface, mutta se ei ole moduulien rajapintaesittely. Kyseisella¨ rakenteella esitella¨a¨n lupaus ma¨a¨ra¨tyn- 1.6. Moduulituki muissa ohjelmointikielissa¨ 52 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . esittely . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1 INTERFACE Pa¨iva¨ys; 2 3 TYPE T <: REFANY; (* Kerrotaan vain, etta¨ T on viite dataan, jonka 4 rakennetta moduulin ka¨ytta¨ja¨n ei tarvitse tieta¨a¨ *) 5 6 PROCEDURE Luo( paiva¨, kuukausi, vuosi : INTEGER ) : T; 7 PROCEDURE Tulosta( kohde : T ); 8 . . . 9 END Pa¨iva¨ys. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ka¨ytto¨ . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1 MODULE Ka¨ytto¨; 2 3 IMPORT Pa¨iva¨ys; 4 5 VAR vappu : Pa¨iva¨ys.T; 6 BEGIN 7 vappu := Pa¨iva¨ys.Luo( 1, 5, 2001 ); 8 Pa¨iva¨ys.Tulosta( vappu ); . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ma¨a¨rittely . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1 MODULE Pa¨iva¨ys; 2 3 REVEAL T = BRANDED REF RECORD 4 pa¨iva¨, kuukausi, vuosi : INTEGER; 5 END; 6 . . . LISTAUS 1.13: Moduulituki Modula-3-kielessa¨ 1 package com.jyke.oliokirja.heippa; 2 3 import java.applet.Applet; // ka¨yteta¨a¨n luokkaa Applet 4 import java.awt.*; // awt-moduulista ka¨ytto¨o¨n kaikki 5 6 public class Heippa extends Applet { 7 public void paint(Graphics g) // java.awt.Graphics 8 { 9 g.drawString(\"Heippa!\", 0, 0); 10 } 11 } LISTAUS 1.14: Moduulituki Java-kielessa¨ 1.6. Moduulituki muissa ohjelmointikielissa¨ 53 laisesta rajapinnasta, jonka jokin luokka voi toteuttaa (katso aliluku 6.9). Moduulit ja niiden rajapinnat ovat ta¨ta¨ laajempi ka¨site. 54 Luku 2 Luokat ja oliot Lucius: “Ay me! This object kills me.” – The Tragedy of Titus Andronicus, Act 3, Scene 1 [Shakespeare, 1593] Ohjelmointikielten olioka¨site on pera¨isin Simula-67 -ohjelmointikie- lesta¨, joka kehitettiin erilaisten simulointimallien toteutusta varten [Sethi, 1996]. Kun ongelmana on esimerkiksi simuloida pankissa asioivien asiakkaiden keskima¨a¨ra¨ista¨ odotusaikaa, Simula-ohjelmas- sa kuvataan simuloitavan ongelman kannalta oleellisia rakenteita oh- jelmointikielen rakenteina, joita nimitettiin olioiksi: asiakas, jono asiakkaita, toimihenkilo¨. Simulointiohjelma “pyo¨ritteli” na¨ita¨ olioita muistissaan: asiakasolio tulee pankkiin, menee jonoon, odottaa vuo- roaan, menee palveltavaksi, kertoo asiansa A, jonka palveluun menee aikaa T, kiitta¨a¨ ja poistuu simulaatiosta. Vaikka todelliset asiakkaat ovat yksilo¨ita¨, ohjelman mallintamat abstraktit asiakasoliot sisa¨lta¨- va¨t paljon yhteisia¨ piirteita¨. Kuvaamalla na¨ma¨ yhteiset piirteet ohjel- massa vain kerran saadaan sa¨a¨stetyksi aikaa ja vaivaa. Samat piirteet omaavat oliot kuuluvat samaan luokkaan ja niilla¨ kaikilla on luokan ma¨a¨rittelema¨t ominaisuudet ja toiminnallisuus. 2.1 Olioiden ominaisuuksia Olioita voidaan tarkastella usealta na¨ko¨kannalta. Kuten moduuleja voidaan olioitakin tarkastella ulkoapa¨in (olion ka¨ytta¨ja¨) tai sisa¨lta¨ ka¨- 2.1. Olioiden ominaisuuksia 55 sin (olion suunnittelija ja toteuttaja). Olion julkinen rajapinta kertoo ka¨ytta¨ja¨lle, mita¨ tehta¨via¨ se lupaa toteuttaa vastuualueellaan. Olioiden ka¨ytto¨tarkoitus on kaksijakoinen: niilla¨ pyrita¨a¨n mallin- tamaan ongelmaa (ma¨a¨rittelyssa¨), ja toisaalta ne pyrkiva¨t tarjoamaan ka¨yta¨nno¨llisen tavan ohjelmiston toteuttamiseen (suunnittelu ja to- teutus). Na¨iden “todellisten” ja ohjelmallisten olioiden ero on na¨ky- vissa¨ kuvassa 2.1. Olio-ohjelmien tekeminen ei kuitenkaan ole mis- sa¨a¨n nimessa¨ ta¨ma¨n kuvan nuolten mukainen suoraviivainen proses- si, jossa toteutetaan vain todelliset oliot jollain olio-ohjelmointikielel- la¨. Aina kaikki ma¨a¨rittelyssa¨ lo¨ydetyt “todelliset” oliot eiva¨t ole olioi- ta toteutetussa ohjelmassa, ja ohjelmiston toteutuksessa usein tarvi- taan olioita, joita ma¨a¨rittelyssa¨ ei ole olemassa (toteutusta tukevat oliot). 2.1.1 Oliolla on tila Jokaisella oliolla on aina olemassa tila, jonka sisa¨lta¨ma¨ informaatio on oleellinen ohjelmiston toiminnan kannalta. Kirjastonkirja-olio voi esimerkiksi sisa¨lta¨a¨ tiedon kirjan nimesta¨, ISBN-numerosta ja han- Ongelma Todelliset oliot metodi() Virkailija jonon_perään() Jono pois_jonosta() metodi() Matti metodi() Jyke Ohjelmiston oliot Asiakas palvelu() Asia Keskimääräinen jonotusaika Nimi palvelu() Asia Keskimääräinen jonotusaika Nimi Satunnainen Asiakas Vakituinen Asiakas palvelu() Asia Keskimääräinen jonotusaika Nimi Luokat ja niiden hierarkiat Määrittely Suunnittelu ja toteutus KUVA 2.1: Ongelmasta ohjelmaksi olioilla 2.1. Olioiden ominaisuuksia 56 kintapa¨iva¨ma¨a¨ra¨n. Suunnittelussa olion tila koostuu attribuuteista, joilla kerrotaan mita¨ informaatiota olion vastuualueen toteutuksessa tarvitaan. Toteutuksessa na¨ma¨ attribuutit toteutetaan ohjelmointikie- len tarjoamilla tavoilla, joita voivat olla muuttujat, tietueet tai oliot — olion sisa¨isessa¨ toteutuksessa on hyvin tavallista ka¨ytta¨a¨ toisia (mah- dollisesti muualta hankittuja kirjastoituja) olioita hyva¨ksi. Kirjaston- kirja-olio voi toteutuksessa siis sisa¨lta¨a¨ merkkijonon (kirjan nimi), numerotietueen (ISBN) ja pa¨iva¨ma¨a¨ra¨-olion. Olion tila muuttuu ohjelman suorituksen aikana ja usein ta¨ma¨ tila kapseloidaan piiloon olion sisa¨lle siten, etta¨ sita¨ voidaan tarkastella ja muuttaa ainoastaan olion julkisen rajapinnan tarjoamien palveluiden kautta. Yleensa¨ pideta¨a¨n suunnittelu- tai ohjelmointivirheena¨ tilan- netta, jossa ohjelmassa olevilla olioilla ei ole ma¨a¨riteltya¨ tilaa jonain suoritusajankohtana. 2.1.2 Olio kuuluu luokkaan Kirjastonkirja-olioiden tilaan kuuluu nimi (merkkijono, max 256 merkkia¨), ISBN (ma¨a¨ra¨muotoinen tietue numeroita) ja hankintapa¨i- va¨ (pa¨iva¨ys-olio). Jokaisella kirjalla on tietysti omat tietonsa, mutta attribuuttien tietotyypit ovat kaikilla Kirjastonkirjoilla samat ja na¨ma¨ voidaan ma¨a¨ritella¨ keskitetysti yhdessa¨ paikassa (luokka). Vastaavasti palvelut (julkinen rajapinta) on jokaisella Kirjastonkirjalla sama ja ne- kin kannattaa ma¨a¨ritella¨ vain kerran. Luokka voidaan ajatella raken- teeksi, joka edustaa kaikkia samalla tavoin rakentuneita (attribuutit, palvelut ja ka¨ytta¨ytyminen) olioita. Luokka myo¨s ma¨a¨ra¨a¨ miten ky- seisen tyyppisia¨ olioita luodaan ja tuhotaan ohjelmassa. Jos kerran olio on mallinnuksen ja toteutuksen perusyksikko¨, niin miksi puhua mista¨a¨n luokista? Ta¨sma¨lleen samasta syysta¨ kuin aikai- semmin modulaarisuuden puolustuspuheessa perusideana on asioi- den yksinkertaistaminen — abstrahointi. Ohjelmointikielen luokka kertoo “yleisen rakenteen” yksitta¨isen olion tilalle ja ma¨a¨rittelee raja- pinnat olion ka¨ytta¨miselle. Useampaa oliota edustava luokka on olio- ohjelmien suunnittelussa ehka¨ eniten ka¨ytetty rakenne. Ohjelmisto- jen rakennetta kuvaavat piirrokset ovat usein luokkakaavioita ja pu- huvat luokkien ominaisuuksista, eiva¨t yksitta¨isista¨ olioista. Kun ohjelmassa luodaan uusi olio, puhutaan olion instantioinnis- ta, jolloin olion luokka ma¨a¨ra¨a¨ muodostuvan uuden olion rakenteen (olion kuluttaman muistin ma¨a¨ra¨n seka¨ olion tilan sisa¨isen raken- 2.1. Olioiden ominaisuuksia 57 teen) ja uuden olion alkutilan alustustoimien avulla (joita ka¨sitella¨a¨n luvussa 3). Olioita voidaan verrata ohjelmointikielen muuttujiin, joille yhtei- set asiat kertoo muuttujan tietotyyppi. Olioilla tyyppia¨ vastaa olion luokka. Luokka on olio-ohjelmien perusyksikko¨, jonka varaan laajem- mat ja vahvemmat ominaisuudet rakentuvat. Olio-ohjelmien uudelleenka¨ytetta¨vyytta¨ lisa¨a¨ ominaisuus, jossa olemassa olevaan luokkaan voidaan lisa¨ta¨ tai muuttaa ominaisuuk- sia (periytyminen, luku 6). Luokat voivat muodostaa keskina¨isia¨ hie- rarkioita, joissa samaan “kokoelmaan” kuuluvien luokkien oliot voi- vat toimia yhtena¨isesti ja tarvittaessa hieman toisistaan poikkeaval- la tavalla (periytyminen ja polymorﬁsmi, aliluku 6.1). Luokista it- sesta¨a¨n voidaan tehda¨ yleiska¨ytto¨isia¨ malleja (“metaluokkia”), jois- ta muodostetaan malliin sopivia luokkainstansseja (geneerisyys, lu- ku 9). Useimmat olio-ohjelmointikielet tuottavat ohjelmia, joissa jo- kainen olio tieta¨a¨ mihin luokkaan se kuuluu. La¨hes kaikki olio-ohjel- mointikielet tarjoavat myo¨s mekanismin, jolla ajoaikana voidaan tar- kastella, kuuluuko olio ma¨a¨ra¨ttyyn luokkaan (C++:n RTTI on ka¨sitelty aliluvussa 6.5.3). Luokka on ka¨site, joka esiintyy ainoastaan ohjelman suunnitte- lu- ja toteutusvaiheessa. Ohjelmiston suorituksen aikana on olemas- sa vain olioita.\u0017 Ta¨ma¨ na¨ko¨kulma ha¨ma¨rtyy helposti, koska varsin usein olio-ohjelman suunnitteluvaiheessa on na¨kyvissa¨ vain ohjel- man luokkarakenne. Luokka on kuvaustapa, jolla voidaan puhua sa- maan “ryhma¨a¨n” eli luokkaan kuuluvien olioiden yhteisista¨ ominai- suuksista, ja lopullisessa ohjelmistossa on aina toimimassa luokan mallin toteuttavia olioita. 2.1.3 Oliolla on identiteetti Koska olion sisa¨inen tila on muista olioista riippumaton, on ta¨ysin mahdollista, etta¨ jonain ajanhetkena¨ kahdella tai useammalla saman luokan oliolla on ta¨sma¨lleen sama tila. Ajatellaan luokka, joka kuvaa pa¨iva¨ma¨a¨ria¨. Voimme luoda ta¨ma¨n mallin mukaisesti oliot A ja B, jotka molemmat kuvaavat samaa pa¨iva¨ma¨a¨ra¨a¨ (vaikkapa 22.4.2069). Kuinka voimme erottaa na¨ma¨ oliot toisistaan, kun molempien pa¨i- va¨ma¨a¨ra¨attribuutilla on sama arvo? Kysymys voi kuulostaa oudolta, . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . \u0017Poikkeuksena ovat erityistapaukset, joissa halutaan kera¨ta¨ oliojoukon ominaisuuksia yh- teen “luokkaolioksi” (aliluku 8.2.1) 2.2. Luokan dualismi 58 koska teksti itsessa¨a¨n sisa¨lta¨a¨ tiedon erottelusta (“oliot A ja B”). Oleel- lista on huomata, etta¨ ta¨ssa¨ on jokin olioiden ulkopuolinen tapa (ni- mea¨minen), jolla ne erotetaan toisistaan. Ohjelmiston toteuttamiseksi on va¨ltta¨ma¨to¨nta¨, etta¨ jokainen ole- massa oleva olio voidaan yksika¨sitteisesti tunnistaa ja ka¨sitella¨. Ta¨ta¨ tarkoitusta varten olio-ohjelmointikielet tarjoavat identiteetin (iden- tity) ka¨sitteen. Yksinkertaisimmillaan identiteetti voi olla muuttujanimi, joka kertoo mista¨ oliosta puhutaan (“muuttujassa A tai B”). Identiteetti voi olla myo¨s esim. muistiosoite, joka kertoo missa¨ olio sijaitsee muistis- sa. Oikein toimiva ja¨rjestelma¨ ei talleta olioita muistiin pa¨a¨llekka¨in, joten muistiosoite erottelee kaikki oliot toisistaan eli antaa niille yk- silo¨llisen identiteetin. Mutkikkaammissa nykyaikaisissa ja¨rjestelmissa¨ voi olioita olla taltioituna tietokannoissa tai niita¨ voidaan ka¨sitella¨ hajautetusti tie- toverkossa, jolloin yksika¨sitteinen identiteetti (ja¨rjestelma¨n- tai jopa maailmanlaajuisena) on va¨ltta¨ma¨to¨n olioiden ka¨yto¨ssa¨. Identitettia¨ voidaan ka¨ytta¨a¨ tarkistamaan tarvittaessa, onko ky- seessa¨ sama olio ilman etta¨ olion tila vaikuttaa tulokseen (tai etta¨ si- ta¨ edes tarvitsee tarkistaa). Esimerkissa¨ voidaan sanoa olioiden A ja B olevan yhta¨ suuria (edustavan samaa pa¨iva¨ma¨a¨ra¨a¨) mutta ne eiva¨t ole sama olio, koska identiteetit ovat erilaiset (“A ei ole B”). Olioiden eri ominaisuuksia (tila, identiteetti ja luokka) on havainnollistettu ku- vassa 2.2 seuraavalla sivulla. 2.2 Luokan dualismi Olio-ohjelmointikielen luokalla voidaan katsoa olevan kaksi esiin- tymismuotoa tai katselukulmaa: moduuli ja tyyppi. Luokka sisa¨lta¨a¨ niiden molempien ominaisuuksia ja pystyy esiintyma¨a¨n ohjelmassa kumman tahansa normaalissa ka¨ytto¨tarkoituksessa. • Luokka moduulina. Luokka toteuttaa rajapintojen ja tiedon ka¨t- kenna¨n periaatteet: luokan suunnittelija ma¨a¨rittelee julkiseen rajapintaan ne operaatiot, joilla luokasta tehtya¨ oliota voidaan ka¨sitella¨. Kaikki pelka¨sta¨a¨n luokan toteutukseen liittyva¨t omi- naisuudet (tiladata ja sisa¨iset funktiot) pystyta¨a¨n ka¨tkema¨a¨n luokan ka¨ytta¨ja¨lta¨. 2.2. Luokan dualismi 59 Paivays paiva kuukausi vuosi (a) Luokka 22 2069 4 22 2069 4 A B (b) oliot KUVA 2.2: Pa¨iva¨ys-luokka ja siita¨ tehdyt oliot A ja B • Luokka tietotyyppina¨. Luokka esiintyy olio-kielessa¨ “ykko¨sluo- kan kansalaisena” kielen perustietotyyppien (esim. liukuluvut) kanssa. Kaikki muuttujille tutut operaatiot (esim. laskutoimi- tukset, sijoitukset ja arvojen va¨litys parametreina) voidaan ma¨a¨- ritella¨ myo¨s luokasta tehtyjen olioiden yhteydessa¨. Ta¨ma¨ kahtiajako na¨hda¨a¨n myo¨s silloin, jos ajattelemme jo ennen olioka¨sitetta¨ olleita tietotyyppeja¨ “olioina”. Esimerkiksi useimmissa ohjelmointikielissa¨ ka¨ytetta¨vissa¨ oleva liukulukutyyppi sisa¨lta¨a¨ na¨ma¨ kaksi na¨ko¨kulmaa: • Liukuluku moduulina. Automaattisesti oletamme, etta¨ liukulu- vuilla on olemassa laskuoperaatioita (ainakin yhteen-, va¨hen- nys-, kerto- ja jakolasku), lukuja voidaan alustaa ma¨a¨ra¨ttyyn ar- voon ja vertaamalla liukulukuja saadaan niiden va¨lille suuruus- ja¨rjestys. Oliomaailmassa na¨ma¨ operaatiot ovat liukulukuluok- kaan kuuluville olioille ma¨a¨ritellyn julkisen rajapinnan palve- luita. Liukulukua ka¨ytta¨va¨n ohjelmoijan ei tarvitse tuntea ka¨y- tetyn laiteympa¨risto¨n liukuluvun esitysmuotoa konekoodin ta- solla (esim. IEEE-754). Ta¨ma¨ toteutusyksityiskohta on ka¨tketty liukulukumoduulin sisa¨lle. 2.3. C++: Luokat ja oliot 60 • Liukuluku tietotyyppina¨. Voimme esimerkiksi tehda¨ liukutyyp- pia¨ olevia muuttujia, sijoittaa niita¨ toisiin muuttujiin ja va¨litta¨a¨ muuttujien arvoja parametreina. Ta¨ma¨ olioiden ka¨ytto¨ muuttu- jina on ta¨rkein ero staattista rakennetta edustavien moduulien ja luokista tehtyjen olioiden va¨lilla¨. 2.3 C++: Luokat ja oliot Jo aiemmin on mainittu, etta¨ yksi tapa ajatella olioita ja luokkia on jakaa niiden sisa¨lto¨ kahteen osaan — toimintoihin ja tietorakenteisiin — ja ajatella olioita ika¨a¨n kuin tietorakenteina, joihin on “liitetty” niihin kohdistuvat toiminnot. Ta¨ma¨ ajattelutapa on ilmeisesti ollut la¨hto¨kohtana, kun C++-kiel- ta¨ on kehitetty C-kielesta¨ [Stroustrup, 1994]. C++:n luokat muistuttavat joiltain osin eritta¨in paljon C:n struct-tietotyyppeja¨ (jopa niin paljon, etta¨ C++:ssa itse kielen kannalta avainsanat struct ja class ovat la¨- hestulkoon vaihtokelpoisia). Aivan kuten struct-tietotyypitkin, myo¨s luokat ta¨ytyy C++:ssa esitella¨ jokaisessa ka¨a¨nno¨syksiko¨ssa¨ ennen kuin niita¨ voi ka¨ytta¨a¨. Tietotyypeista¨ poiketen luokilla on kuitenkin myo¨s sisa¨inen toteutuksensa — rajapinnan palvelut toteuttava ohjelmakoo- di tai toisin sanottuna rajapintafunktioiden toteutus —, joka kirjoite- taan tyypillisesti erikseen omaan tiedostoonsa (tai tiedostoihinsa). 2.3.1 Luokan esittely Luokan ka¨ytta¨ja¨lle riitta¨a¨ yleensa¨ luokan esittely. Luokan esittely ker- too luokasta kaiken luokan ka¨ytto¨o¨n tarvittavan. Ohjelmoijalle se ker- too luokan ka¨ytto¨o¨n tarvittavan rajapinnan. Ka¨a¨nta¨ja¨a¨ varten luokan esittely sisa¨lta¨a¨ tietoa luokan periytymissuhteista seka¨ tietoa luokan sisa¨llo¨sta¨ muistinvarausta yms. varten. Kuten esittelyt yleensa¨, myo¨s luokan esittelyt kirjoitetaan tyypillisesti otsikkotiedostoon, josta luo- kan ka¨ytta¨ja¨ sitten lukee ne #include-komennolla omaan kooditiedos- toonsa. Luokan esittely alkaa avainsanalla class, jonka ja¨lkeen tulee esi- telta¨va¨n luokan nimi. Ta¨ma¨n ja¨lkeen kerrotaan aaltosuluissa, mita¨ luokka sisa¨lta¨a¨. Luokan sisa¨lto¨ voi koostua seuraavista asioista: 2.3. C++: Luokat ja oliot 61 • Ja¨senmuuttujat ovat luokan olion sisa¨lla¨ olevia muuttujia. Ja¨- senmuuttujia ka¨yteta¨a¨n olion sisa¨isen tilan muistamiseen. Ja¨- senmuuttujista kerrotaan aliluvussa 2.3.2. • Ja¨senfunktiot ovat funktioita, joihin kirjoitetaan luokan toimin- nallisuus. Luokan rajapinta koostuu ja¨senfunktioista. Rajapin- nan lisa¨ksi luokalla voi olla myo¨s ja¨senfunktioita, jotka eiva¨t na¨y ulospa¨in. Ja¨senfunktioita ka¨sitella¨a¨n aliluvussa 2.3.3. • Sisa¨iset tyypit ovat luokan rajapintaan tai sisa¨iseen toteutuk- seen liittyvia¨ tyyppima¨a¨rittelyja¨. Niista¨ kerrotaan tarkemmin aliluvussa 8.3. • Luokkamuuttujat ovat luokan kaikille olioille yhteisia¨ muuttu- jia. Niita¨ voidaan ka¨ytta¨a¨ sellaisen tiedon tallettamiseen, joka ei ole oliokohtaista, vaan koskee kaikkia luokan olioita yhdessa¨. Luokkamuuttujat ka¨sitella¨a¨n tarkemmin aliluvussa 8.2.2. • Luokkafunktiot ovat luokkamuuttujien vastine funktioiden puolella. Ne ovat funktioita, joiden toiminnallisuus ei koske yksitta¨ista¨ oliota, vaan koko luokkaa kokonaisuutena. Yleen- sa¨ luokkafunktioita ka¨yteta¨a¨n luokkamuuttujien ka¨sittelyyn. Luokkafunktioista kerrotaan aliluvussa 8.2.3. Listaus 2.1 seuraavalla sivulla sisa¨lta¨a¨ esimerkkina¨ yksinkertaisen luokan PieniPaivays esittelyn, joka on kirjoitettu tiedostoon pieni- paivays.hh. Huomaa, etta¨ ta¨ma¨ luokkaesimerkki on tarkoituksella yk- sinkertaistettu, ja siita¨ puuttuu useita C++:n luokille tarpeellisia asioita (kertaustehta¨va¨: etsi luokkaesimerkin puutteet ,). On syyta¨ huomata, etta¨ ta¨ssa¨ teoksessa ka¨ytetty termi “luokan esittely” ei kirjaimellisesti vastaa englanninkielista¨ C++-terminologi- aa, jossa samasta asiasta ka¨yteta¨a¨n nimitysta¨ “class deﬁnition” (kirjai- mellisesti “luokan ma¨a¨rittely”). Sana “esittely” kuvaa tilannetta kui- tenkin ehka¨ paremmin, koska esittelyn yhteydessa¨ ei kerrota luokan rajapintafunktioiden toteutusta. 2.3.2 Ja¨senmuuttujat Ja¨senmuuttujista (data member) ka¨yteta¨a¨n myo¨s nimityksia¨ “attri- buutti”, “kentta¨”, “instanssimuuttuja” ja “tietoja¨sen”. Ne ovat olioon kiintea¨sti liittyvia¨ muuttujia, jotka muodostavat olion sisa¨isen tilan. 2.3. C++: Luokat ja oliot 62 1 #ifndef PIENIPAIVAYS HH 2 #define PIENIPAIVAYS HH 3 4 class PieniPaivays 5 { 6 public: 7 void asetaPaivays(unsigned int p, unsigned int k, unsigned int v); 8 void sijoitaPaivays(PieniPaivays& p); 9 void asetaPaiva(unsigned int paiva); 10 void asetaKk(unsigned int kuukausi); 11 void asetaVuosi(unsigned int vuosi); 12 13 unsigned int annaPaiva(); 14 unsigned int annaKk(); 15 unsigned int annaVuosi(); 16 17 void etene(int n); 18 int paljonkoEdella(PieniPaivays& p); 19 20 private: 21 unsigned int paiva ; 22 unsigned int kuukausi ; 23 unsigned int vuosi ; 24 }; 25 26 #endif LISTAUS 2.1: Esimerkki luokan esittelysta¨, pienipaivays.hh Tyypilta¨a¨n ja¨senmuuttujat voivat olla mita¨ tahansa — kokonaisluku- ja, osoittimia, viitteita¨ tai vaikkapa toisia olioita. Ja¨senmuuttujat ta¨ytyy C++:ssa esitella¨ luokan esittelyn sisa¨lla¨, ja niiden esittely muistuttaa suuresti tavallisten muuttujien esittelya¨. Listauksen 2.1 riveilla¨ 21–23 on esitelty pa¨iva¨ysluokan tarvitsemat ja¨- senmuuttujat, joihin pa¨iva¨ysolion sisa¨lta¨ma¨ pa¨iva¨ystieto talletetaan. Ja¨senmuuttujien nimea¨misessa¨ ei C++:n kannalta ole mita¨a¨n erityisia¨ sa¨a¨nto¨ja¨, mutta useissa ohjelmointityyleissa¨ (katso liite B) ja¨senmuut- tujat nimeta¨a¨n niin, etta¨ ne on koodissa helppo erottaa esimerkiksi ja¨senfunktioiden parametreista. Ta¨ssa¨ teoksessa ka¨yteta¨a¨n C++:ssa kohtalaisen yleista¨ nimea¨mista- paa, jossa ja¨senmuuttujien nimien pera¨a¨n lisa¨ta¨a¨n alaviiva. Vastaava englanninkielisessa¨ ohjelmoinnissa yleinen ka¨yta¨nto¨ on lisa¨ta¨ ja¨sen- muuttujien eteen sana “my”, siis myDay, myMonth, myYear ja niin edel- 2.3. C++: Luokat ja oliot 63 leen. Sen sijaan joissain teoksissa na¨kyva¨ tapa lisa¨ta¨ ja¨senmuuttujien nimien eteen alaviiva ( paiva) ei ole suotava, koska C++-standardi va- raa tietyt alaviivoilla alkavat nimet ka¨a¨nta¨ja¨n sisa¨iseen ka¨ytto¨o¨n. Olio-ohjelmoinnissa on varsin tavallista, etta¨ olion ja¨senmuuttu- jat ovat tyypilta¨a¨n toisia olioita. Pa¨iva¨ystyyppista¨ oliota voisi esimer- kiksi ka¨ytta¨a¨ ja¨senmuuttujana luokassa, joka kuvaa kirjaston tietoja¨r- jestelma¨ssa¨ yhden kirjan tietoja, joihin kuuluu myo¨s palautuspa¨iva¨. Osa ta¨llaisen luokan esittelysta¨ voisi olla seuraavanlainen: class Kirja { ... private: string nimi ; // C++:n merkkijonoluokka, ks. liitteen A aliluku A.4 string tekijannimi ; PieniPaivays palautuspvm ; ... }; Ta¨llainen hierarkkinen rakenne, jossa olio sisa¨lta¨a¨ ja¨senmuuttuja- naan olion, jolla taas on sisa¨lla¨a¨n omat ja¨senmuuttujansa, on hyvin tyypillinen olio-ohjelmoinnissa. Sen avulla olion sisa¨lta¨ma¨ data voi- daan abstrahoida ja kapseloida selkeiksi kokonaisuuksiksi, ja olioi- den rajapintojen avulla myo¨s tiedon ka¨sittely voidaan jakaa selkeisiin osiin luokkien avulla. Koska ja¨senmuuttujat liittyva¨t kiintea¨sti olioon, niiden elinkaari on ta¨sma¨lleen sama kuin olionkin. Kun olio syntyy, syntyva¨t myo¨s kaikki sen ja¨senmuuttujat. Samoin ja¨senmuuttujat tuhoutuvat aina samalla kuin itse oliokin. Kaikki olion sisa¨inen tieto ei kuitenkaan ole luonteeltaan sellaista, etta¨ se olisi saatavilla heti olion syntyma¨n yhteydessa¨ tai etta¨ se kesta¨isi olion tuhoutumiseen saakka. Esimer- kiksi listaoliolla olisi ja¨rkeva¨a¨ olla ja¨senmuuttujissaan tallessa listan sisa¨lta¨ma¨t alkiot. Ta¨ma¨ ei kuitenkaan ole suoraan mahdollista, kos- ka listan alkiot eiva¨t ole tiedossa listaolion syntyessa¨ ja vastaavasti niita¨ voidaan poistaa ja lisa¨ta¨ listaolioon koska tahansa. Ta¨llaisissa tapauksissa on C++:ssa tapana laittaa olioon ja¨senmuuttujaksi osoitin tai osoittimia, joiden pa¨a¨ha¨n voi sitten myo¨hemmin sijoittaa tietoa, joka ei ole olion “omistuksessa” koko olion elinaikaa. Yleensa¨ ta¨llai- sissa tapauksissa tarvitaan myo¨s olioiden tai datan dynaamista luo- 2.3. C++: Luokat ja oliot 64 mista, josta kerrotaan enemma¨n aliluvussa 3.3.4. Olioiden elinkaarta ka¨sitella¨a¨n tarkemmin luvuissa 3 ja 7. 2.3.3 Ja¨senfunktiot Ja¨senfunktioita (member function) kutsutaan myo¨s monilla muilla ni- milla¨ olio-ohjelmoinnissa. Yleisimpia¨ ovat “metodi”, “rutiini”, “pal- velu” tai “operaatio”. Kaikki na¨ma¨ nimitykset kuvaavat saman asian eri puolia. Ja¨senfunktiot ovat funktioita, jotka toteuttavat olion tar- joamat palvelut ja joiden avulla eri oliot kommunikoivat keskena¨a¨n. Kaikki luokan toiminnallisuus kirjoitetaan ja¨senfunktioihin (ja luok- kafunktioihin, joista enemma¨n aliluvussa 8.2.3). Kuten tavalliset funktiotkin, myo¨s ja¨senfunktiot ta¨ytyy esitella¨ en- nen kuin niita¨ voi ka¨ytta¨a¨. Rivit 7–18 listauksessa 2.1 sivulla 62 sisa¨l- ta¨va¨t luokan PieniPaivays ja¨senfunktioiden esittelyt luokan esittelyn sisa¨ssa¨. Ja¨senfunktioiden esittelyssa¨ syntaksi on la¨hes ta¨sma¨lleen sa- ma kuin tavallistenkin funktioiden esittelyssa¨. Ja¨senfunktion esittelyssa¨ kerrotaan vain, milta¨ ja¨senfunktio na¨yt- ta¨a¨ ulospa¨in eli miten sita¨ voi kutsua. Tavallisten funktioiden tapaan ja¨senfunktioiden varsinainen toteutus (koodi) kirjoitetaan erilliseen kooditiedostoon. Yleinen ka¨yta¨nto¨ on, etta¨ jokaista luokkaa kohti kir- joitetaan yksi ja¨senfunktioiden toteutukset sisa¨lta¨va¨ kooditiedosto, mutta C++ ei mitenka¨a¨n rajoita ta¨ta¨. Joskus saattaa olla tarpeen hajot- taa luokan toteutus useisiin tiedostoihin, joskus taas on ja¨rkeva¨a¨ kir- joittaa muutaman toisiinsa tiiviisti liittyva¨n luokan toteutus samaan kooditiedostoon — ta¨llo¨in usein myo¨s luokkien esittelyt kirjoitetaan samaan otsikkotiedostoon. Listaus 2.2 seuraavalla sivulla sisa¨lta¨a¨ osan PieniPaivays-luo- kan ja¨senfunktioiden toteutuksista, jotka ovat tiedostossa pienipai- vays.cc. Tiedoston alussa luetaan ensin sisa¨a¨n luokkaesittely tiedos- tosta pienipaivays.hh. Ta¨ma¨ on aina tehta¨va¨ kooditiedostoissa en- nen ja¨senfunktioiden ma¨a¨rittelyja¨, jotta ka¨a¨nta¨ja¨ saa luettua esittelys- ta¨ tarvitsemansa tiedot luokasta. Ja¨senfunktioiden ma¨a¨rittely on syntaksiltaan hyvin samanlainen kuin tavallisen C++:n funktion ma¨a¨rittely. Na¨kyvin ero on, etta¨ ma¨a¨rit- telyn alussa ka¨yteta¨a¨n na¨kyvyystarkenninta ::, eli ja¨senfunktion ni- mi esiintyy aina muodossa Luokannimi::jfunktionnimi. Ta¨ma¨ on tar- peen, jotta ka¨a¨nta¨ja¨ tieta¨a¨, minka¨ luokan ja¨senfunktiota ollaan ma¨a¨- 2.3. C++: Luokat ja oliot 65 1 #include \"pienipaivays.hh\" 2 3 void PieniPaivays::asetaPaivays(unsigned int p, unsigned int k, 4 unsigned int v) 5 { 6 asetaPaiva(p); 7 asetaKk(k); 8 asetaVuosi(v); 9 } 10 11 void PieniPaivays::asetaPaiva(unsigned int paiva) 12 { 13 paiva = paiva; 14 } 15 16 unsigned int PieniPaivays::annaPaiva() 17 { 18 return paiva ; 19 } ... LISTAUS 2.2: Ja¨senfunktioiden toteutus, pienipaivays.cc rittelema¨ssa¨ — useissa luokissa kun voi olla samannimisia¨ ja¨senfunk- tioita. Toinen ero tavallisiin funktioihin on, etta¨ ja¨senfunktion koodis- sa voi viitata “oman olion” ja¨senmuuttujiin ja toisiin ja¨senfunktioi- hin suoraan. Ta¨ma¨ on mahdollista, koska ja¨senfunktioita kutsutaan aina jonkin olion kautta, joten ja¨senfunktio “tieta¨a¨” kutsun yhteydes- sa¨, minka¨ olion palvelua ollaan suorittamassa. Niinpa¨ listauksen 2.2 rivilla¨ 6 kutsutaan oman olion asetaPaiva-ja¨senfunktiota, joka puo- lestaan sijoittaa parametrina tulleen pa¨iva¨yksen talteen oman olion ja¨senmuuttujaan paivays rivilla¨ 13. Vastaavasti rivilla¨ 18 ja¨senfunk- tio annaPaiva palauttaa paluuarvonaan oman olion ja¨senmuuttujan paiva arvon. Suora pa¨a¨sy olion ja¨senmuuttujiin ja ja¨senfunktioi- hin on luontevaa, onhan ja¨senfunktion toteutuksen tarkoitus nime- nomaan tarjota olion ka¨ytta¨ja¨lle jokin palvelu operoimalla olion sisa¨l- ta¨ma¨lla¨ tiedolla ja mahdollisesti ka¨ytta¨ma¨lla¨ hyva¨kseen muita olion tarjoamia palveluita. Joskus ja¨senfunktioiden koodissa tulee tarve erikseen viitata olioon itseensa¨. Tyypillinen esimerkki ta¨sta¨ on tilanne, jossa ja¨sen- 2.3. C++: Luokat ja oliot 66 funktio joutuu antamaan olion itsensa¨ parametrina jollekin toiselle funktiolle. Ta¨llaisia tilanteita varten ja¨senfunktioiden koodissa voi ka¨yta¨a¨ erityismerkinta¨a¨ this. Luokan X ja¨senfunktion koodissa thisin tyyppi on “osoitin X-olioon”, eli se ka¨ytta¨ytyy aivan kuin se olisi esi- telty lauseella “X* this;”. Osoittimen arvo on automaattisesti sellai- nen, etta¨ se osoittaa olioon, jonka ja¨senfunktion koodia ollaan suorit- tamassa, siis “olioon itseensa¨”. Jos esimerkiksi ohjelmassa on funktio rekisteroiPvm, joka ot- taa parametrikseen osoittimen pa¨iva¨ysolioon, voi pa¨iva¨ysolion jo- kin ja¨senfunktio rekistero¨ida¨ olion itsensa¨ tietokantaan lauseel- la rekisteroiPvm(this);. Jos funktio ottaisikin osoittimen sijaan parametrikseen viitteen] pa¨iva¨ysolioon, olisi syntaksi vastaavasti rekisteroiPvm(*this);. Joissain oliokielissa¨ ja¨senfunktioiden koodissa ei voi lainkaan ka¨ytta¨a¨ ja¨senmuuttujia tai ja¨senfunktioita suoraan, vaan niihin ta¨y- tyy aina viitata erikseen olion itsensa¨ kautta, esimerkiksi syntaksilla this->jmuuttuja tai self.muuttuja. Vaikka C++:ssakin ta¨ma¨ olisi mah- dollista this-merkinna¨n avulla, ei se ole tapana. 2.3.4 Luokkien ja olioiden ka¨ytto¨ C++:ssa Kun kooditiedostossa luokan esittely on luettu sisa¨a¨n, voidaan luo- kasta luoda koodissa olioita samalla tavalla kuin normaaleja muut- tujia luodaan C++:ssa. Muutenkin luokka ka¨ytta¨ytyy kuten mika¨ ta- hansa muukin ka¨ytta¨ja¨n itsensa¨ ma¨a¨rittelema¨ tyyppi, kuten esim. struct-tietorakenne. Ta¨ssa¨ suhteessa C++ heijastaa suoraan aliluvus- sa 2.2 mainittua “luokka tietotyyppina¨” -periaatetta. Listauksessa 2.3 sivulla 68 on esimerkki yksinkertaisesta pa¨a¨- ohjelmasta, jossa ka¨yteta¨a¨n aiemmin ma¨a¨riteltya¨ PieniPaivays-luok- kaa. Ja¨lleen tiedoston alussa luetaan sisa¨a¨n luokan esittely otsik- kotiedostosta. Rivilla¨ 14 luodaan luokasta PieniPaivays pa¨iva¨ys- olio aivan kuten seuraavalla rivilla¨ luodaan tyypista¨ int kokonais- lukumuuttuja. Ta¨ma¨n ja¨lkeen ja¨senfunktioita kutsutaan syntaksilla olionNimi.jfunktio(parametrit), kuten tapahtuu rivilla¨ 17. Jos olion sijaan ka¨yteta¨a¨n osoitinta olioon, ja¨senfunktioita kutsutaan syntaksil- la osoitin->jfunktio(parametrit). Ta¨ma¨ na¨kyy rivilla¨ 30. Jos taas ka¨yteta¨a¨n viitetta¨ olioon, na¨ytta¨a¨ ja¨senfunktion kutsu samalta kuin . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ]C++:n viitetyypin ka¨ytto¨ esitella¨a¨n lyhyesti liitteen A aliluvussa A.1. 2.3. C++: Luokat ja oliot 67 suoraan oliota ka¨ytetta¨essa¨. Ta¨sta¨ on esimerkkina¨ viitteen pvm kautta tapahtuva kutsu rivilla¨ 40. Olioita voi myo¨s va¨litta¨a¨ parametreina toisiin funktioihin (ja ja¨- senfunktioihin). Listauksen rivi 27 na¨ytta¨a¨ funktion, joka ottaa para- metrinaan osoittimen olioon. Vastaavasti rivilla¨ 37 on funktio, joka ottaa parametrina viitteen olioon. Olioiden va¨litta¨minen “normaalei- na” arvoparametreina on myo¨s mahdollista, mutta se vaatii kopiora- kentajan ka¨sitteen tuntemista, jota ka¨sitella¨a¨n myo¨hemmin aliluvus- sa 7.1.2. Huomaa, etta¨ kun funktioon va¨litta¨a¨ osoittimen tai viitteen olioon, niin funktiossa osoittimen tai viitteen la¨pi tehta¨va¨t ja¨senfunk- tiokutsut kohdistuvat alkupera¨iseen olioon, aivan kuten normaalisti- kin osoitin- ja viiteparametreja ka¨ytetta¨essa¨. 2.3. C++: Luokat ja oliot 68 1 #include \"pienipaivays.hh\" 2 3 #include <iostream> 4 using std::cout; 5 using std::endl; 6 7 // Funktio, joka tarkistaa, onko vuoden sisa¨lla¨ karkauspa¨iva¨a¨ 8 bool onkoKarkauspaivaa(PieniPaivays* pvm p); 9 // Funktio, joka kertoo montako pa¨iva¨a¨ on saman vuoden jouluun 10 int kauankoJouluun(PieniPaivays& pvm); 11 12 int main() 13 { 14 PieniPaivays kesapaiva; 15 int paiviaJouluun = 0; 16 17 kesapaiva.asetaPaivays(21,7,1999); 18 if (onkoKarkauspaivaa(&kesapaiva)) 19 { 20 cout << \"Karkauspa¨iva¨ on alle vuoden pa¨a¨ssa¨.\" << endl; 21 } 22 paiviaJouluun = kauankoJouluun(kesapaiva); 23 cout << \"Jouluun on \" << paiviaJouluun << \" pa¨iva¨a¨.\" << endl; 24 } 25 26 // Karkauspa¨iva¨funktion (typera¨) toteutus 27 bool onkoKarkauspaivaa(PieniPaivays* pvm p) 28 { 29 unsigned int vanhaPaiva = pvm p->annaPaiva(); // Pa¨iva¨ talteen 30 pvm p->etene(365); // Siirry 365 pa¨iva¨a¨ eteenpa¨in 31 // Karkauspa¨iva¨ on tullut vastaan, jos 365 pa¨iva¨a¨ ei ollut koko vuosi 32 bool oliKarkauspaiva = (pvm p->annaPaiva() != vanhaPaiva); 33 pvm p->etene(-365); // Palauta vanha pa¨iva¨ menema¨lla¨ takaisin 34 return oliKarkauspaiva; 35 } 36 // Jouluunlaskun toteutus 37 int kauankoJouluun(PieniPaivays& pvm) 38 { 39 PieniPaivays joulu; 40 joulu.asetaPaivays(24, 12, pvm.annaVuosi()); // Saman vuoden joulu 41 return joulu.paljonkoEdella(pvm); // Paljonko joulu on edella¨? 42 } LISTAUS 2.3: Esimerkki luokan ka¨yto¨sta¨, ppkaytto.cc 69 Luku 3 Olioiden elinkaari Bernie: “But I did okay, didn’t I? — I mean I got, what, ﬁfteen thousand years. That’s pretty good. Isn’t it? I lived a pretty long time.” Death %: “You lived what anybody gets, Bernie. — You got a lifetime. — No more. — No less. — You got a lifetime.” – Brief Lives [Gaiman ja muut, 1994] “Normaalissa” ei-olio-ohjelmoinnissa muuttujien “elinkaaresta” ei yleensa¨ ole tarpeen erikseen puhua — muuttujia luodaan kun niita¨ tarvitaan, ja ka¨yteta¨a¨n niin kauan kuin ka¨yteta¨a¨n. Ainoat muistetta- vat asiat ovat muuttujien alustus tarvittaessa seka¨ dynaamisesti vara- tun muistin vapauttaminen. Paikallisten muuttujien ka¨ytto¨ on viela¨ helpompaa, koska niita¨ ei tarvitse koskaan muistaa vapauttaa. Olio-ohjelmoinnissa olioiden ka¨ytta¨minen sen sijaan vaatii enem- ma¨n harkintaa. Oliot voivat olla monimutkaisiakin kokonaisuuksia ja niiden “syntyma¨ ja kuolema” saattavat vaatia kaikenlaisia toimen- piteita¨. Olio-ohjelmoinnin vastuualueajattelun mukaista olisi se, etta¨ olio itse vastaisi ta¨llaisista toimenpiteista¨, jotta ne eiva¨t ja¨isi ka¨ytta¨ja¨n harteille. Ta¨ssa¨ luvussa perehdyta¨a¨n tarkemmin olioiden elinkaareen liitty- viin kysymyksiin ja katsotaan, miten siihen liittyva¨t ongelmat on rat- kaistu eri oliokielissa¨. Erityisesti tutkitaan, millaista tukea C++ antaa olioiden elinkaaren hallintaan. 3.1. Olion syntyma¨ 70 3.1 Olion syntyma¨ Olioiden luominen saattaa vaatia monimutkaisiakin toimenpiteita¨, ennen kuin olio on ka¨ytto¨valmis. Tyypillisia¨ ta¨llaisia toimenpiteita¨ ovat • muistin varaaminen (merkkijono-olio varaa syntyessa¨a¨n tietyn minimima¨a¨ra¨n muistia) • toisten olioiden luominen (kirjasto-olio luo syntyessa¨a¨n tarvit- tavat kortisto-oliot) • erina¨iset rekistero¨itymiset (palvelinolio rekistero¨i syntyessa¨a¨n palvelunsa ja¨rjestelma¨nlaajuiseen tietokantaan) • resurssien ka¨ytto¨o¨notto (tietokantaolio avaa syntyessa¨a¨n tieto- kannan sisa¨lta¨va¨n tiedoston ja lukee sen muistiin) • muut toimenpiteet (ikkunaolio piirta¨a¨ syntyessa¨a¨n ikkunan ruudulle). Na¨ma¨ alkutoimenpiteet vastaavat muuttujien alustusta, mutta saattavat oliosta riippuen olla luonteeltaan paljon monimutkaisem- pia. Tietysti ei-olio-ohjelmoinnissakin on ta¨ytynyt tehda¨ vastaavia operaatioita (esim. tietorakenteen alustus), mutta niiden suorittami- nen on yleensa¨ ja¨a¨nyt enemma¨n tai va¨hemma¨n ohjelmoijan itsensa¨ vastuulle, ja yksi tyypillinen ohjelmointivirhe on ollut alustusfunk- tiokutsun unohtaminen. Olio-ohjelmoinnissa olisi luontevaa sysa¨ta¨ alustustoimenpiteet olion itsensa¨ vastuulle, ja ta¨ha¨n eri oliokielet tar- joavat tukea vaihtelevassa ma¨a¨rin. Tyypillisesti olion luomiseen liittyva¨t tehta¨va¨t jakautuvat oliokie- lissa¨ kahteen vaiheeseen: 1. olion “datan” (siis ja¨senmuuttujien) luominen. Jos ja¨senmuut- tujat ovat itse olioita, niiden luominen on puolestaan ja¨lleen kaksivaiheinen prosessi. . . 2. muut alustustoimenpiteet. Olion “datan” luominen muistuttaa niin paljon tavallisten muut- tujien luomista, etta¨ ka¨yta¨nno¨ssa¨ kaikki oliokielet tekeva¨t sen au- tomaattisesti olion luomisen yhteydessa¨. Ta¨ha¨n vaiheeseen kuuluu 3.2. Olion kuolema 71 muistin varaaminen olion ja¨senmuuttujille ja mahdollisesti ja¨sen- muuttujien alustus. Muiden alustustoimenpiteiden “automatisoin- nissa” oliokielissa¨ on suuriakin eroja. 3.2 Olion kuolema Kun oliota on ka¨ytetty ja siita¨ halutaan eroon, ta¨ytyy olion suorit- taa yleensa¨ luomiselleen vastakkaiset “siivoustoimenpiteet”. Edelli- sen aliluvun listaa mukaillen tyypillisia¨ siivoustoimenpiteita¨ ovat muistinvapautus, toisten olioiden tuhoaminen, rekistereista¨ poistu- minen, resurssien vapauttaminen ja vaikkapa ikkunan poistaminen ruudulta. Ei-olio-ohjelmoinnissa ta¨llaiset siivoustoimenpiteet ovat olleet muuttujan tai tietorakenteen ka¨ytta¨ja¨n vastuulla (ohjelmoijan on pi- ta¨nyt muistaa kutsua siivous- tai vapautusfunktiota ennen muuttujan tuhoutumista). Samoin kuin olion luomisessakin, olion tuhoamiseen liittyva¨t toimenpiteet olisi ka¨teva¨a¨ saada olion itsensa¨ vastuulle. Eri oliokielten tuki ta¨ha¨n vaihtelee aivan kuten olion luomisessakin. 3.3 Olion elinkaaren ma¨a¨ra¨ytyminen Olion luomisen ja tuhoutumisen yhteydessa¨ suoritettavien toimenpi- teiden lisa¨ksi oliokielissa¨ on eroja sen suhteen, miten olion syntyma¨- ja varsinkin tuhoutumishetki ja na¨ihin liittyva¨t toimenpiteeet ma¨a¨- ra¨ytyva¨t. Ta¨ssa¨ oliokielet voi jakaa karkeasti kolmeen ryhma¨a¨n: 1. Kieli tarjoaa vain muistin varaamiseen ja vapauttamiseen tar- vittavat operaatiot, ja olion alustaminen ja siivoaminen ja¨a¨va¨t ohjelmoijan itsensa¨ vastuulle. Muistin vapauttaminen tapahtuu jossain kielissa¨ automaattisesti. Usein na¨issa¨ kielissa¨ on ros- kienkeruu (garbage collection): ohjelmassa on mukana ka¨a¨nta¨- ja¨n tuottama koodi, joka osaa etsia¨ turhaksi ja¨a¨neet muistialueet ja vapauttaa ne. 2. Kieli hoitaa olion tuhoamiseen liittyva¨t toimenpiteet automaat- tisesti, mutta olion tuhoutumishetki ei ole ma¨a¨ra¨tty, eli se ei ole ohjelmoijan kontrolloitavissa. Olion luomisessa ei yleensa¨ ole ta¨ta¨ vaihtoehtoa, koska useimmissa kielissa¨ olion luominen on (ainakin la¨hes) aina ohjelmoijan itsensa¨ pa¨a¨tetta¨vissa¨. 3.3. Olion elinkaaren ma¨a¨ra¨ytyminen 72 3. Kieli pita¨a¨ huolen alustustoimenpiteista¨ oliota luotaessa ja sii- voustoimenpiteista¨ oliota tuhottaessa. Lisa¨ksi olion luomisen ja tuhoamisen tapahtumishetki on tarkkaan ma¨a¨ra¨tty. Oliokielet sisa¨lta¨va¨t em. ominaisuuksia vaihtelevissa ma¨a¨rin ja eri tavoin sekoitettuna. Seuraavassa on esitelty joidenkin yleisimpien oliokielten tapoja toteuttaa asiat. 3.3.1 Modula-3 Modula-3 on oliokieli, joka kuuluu miltei puhtaasti kategoriaan 1. Kie- li antaa mahdollisuuden varata oliolle muistia ja ta¨ssa¨ yhteydessa¨ an- taa halutuille ja¨senmuuttujille alkuarvot. Ja¨senmuuttujien alustuksen lisa¨ksi kieli ei kuitenkaan tue automaattisesti mita¨a¨n muita alustus- toimenpiteita¨, vaan ohjelmoija voi halutessaan itse ma¨a¨ritella¨ alus- tuksen suorittavan ja¨senfunktion ja kutsua sita¨ omassa koodissaan heti olion luomisen ja¨lkeen. Samoin olion siivoustoimenpiteet ja¨a¨va¨t ohjelmoijan itsensa¨ vas- tuulle. Modula-3:ssa on roskienkeruu, joten olion varaaman muis- tin vapauttaminen tapahtuu automaattisesti. Roskienkeruu ei kuiten- kaan suorita mita¨a¨n oliokohtaisia siivoustoimenpiteita¨, eli ta¨ssa¨ mie- lessa¨ Modula-3 vapauttaa automaattisesti vain olion alla olevan muis- tin, kun taas itse olio vain “unohdetaan”. Ta¨ma¨n vuoksi ohjelmoijan ta¨ytyy itse halutessaan ma¨a¨ritella¨ siivousja¨senfunktio ja kutsua sita¨ sopivassa kohdassa ohjelmaa, kun oliota ei ena¨a¨ tarvita. Modula-3:n ka¨ytta¨ma¨ elinkaarimalli tuo mukanaan paljon vaaroja. Mika¨li olio ei vaadi mita¨a¨n erityisia¨ alustus- ja siivoustoimenpitei- ta¨, riitta¨a¨ Modula-3:n tarjoama tuki elinkaarelle ta¨ysin. Mika¨li ta¨llai- sia toimenpiteita¨ kuitenkin tarvitaan, ne ja¨a¨va¨t kokonaan ohjelmoi- jan vastuulle, jolloin niiden unohtumisen vaara on suuri. Ta¨llo¨in Modula-3:n roskienkeruun tuoma hyo¨ty eliminoituu kokonaan, koska ohjelmoijan ta¨ytyy itse pita¨a¨ kirjaa siita¨, milloin oliota ei ena¨a¨ tarvita, ja kutsua kirjoittamaansa siivousja¨senfunktiota. Listaus 3.1 seuraavalla sivulla na¨ytta¨a¨ Modula-3:lla kirjoitetun funktion naytaViesti, jossa luodaan ja tuhotaan olio. Ta¨ssa¨ olion elinkaari kesta¨a¨ siis funktion suorituksen ajan. 3.3. Olion elinkaaren ma¨a¨ra¨ytyminen 73 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . OkDialogi.i3 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1 INTERFACE OkDialogi; 2 TYPE 3 T <: Public; 4 Public = OBJECT 5 METHODS (* Esitella¨a¨n olion julkinen rajapinta *) 6 alusta( viesti : TEXT ); 7 siivoa(); 8 odotaOKta(); 9 END; (* Public *) 10 (* Moduulin proseduuri, joka tulostaa viestin ja odottaa OK-nappia *) 11 PROCEDURE naytaViesti(); 12 END OkDialogi. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . OkDialogi.m3 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1 MODULE OkDialogi; 2 REVEAL 3 T = Public BRANDED OBJECT 4 (* Ta¨ha¨n paikalliset muuttujat *) 5 OVERRIDES (* Ma¨a¨ritella¨a¨n olion tarjoamat palvelut *) 6 alusta := Alustus; 7 siivoa := Siivous; 8 odotaOKta := OKodotus; 9 END; (* T *) 10 11 PROCEDURE Alustus( self : T; viesti : TEXT ) = 12 BEGIN (* Ta¨nne ikkunan avaaminen, viestin kirjoittaminen siihen 13 ja muut luomistoimenpiteet *) 14 END Alustus; 15 16 PROCEDURE Siivous( self : T ) = 17 BEGIN 18 (* Ta¨nne ikkunan sulkeminen ja muut siivoustoimenpiteet *) 19 END Siivous; 20 21 PROCEDURE OKodotus( self : T ) = 22 BEGIN 23 (* Ta¨nne nappulan painamisen odottamisen koodi *) 24 END OKodotus; 25 26 PROCEDURE naytaViesti() = 27 VAR dialogi := NEW(T); (* Varataan oliolle muistialue *) 28 BEGIN 29 dialogi.alusta(\"Virhe!\"); (* Alustetaan olio *) 30 dialogi.odotaOKta(); (* Odota, etta¨ ka¨ytta¨ja¨ kuittaa viestin *) 31 dialogi.siivoa(); (* Olion siivoustoimenpiteet *) 32 (* proseduurista palattaessa roskienkeruu pita¨a¨ huolen olion muistista *) 33 END naytaViesti; 34 35 BEGIN 36 (* Moduulin alustuskoodi *) 37 END OkDialogi. LISTAUS 3.1: Esimerkki olion elinkaaresta Modula-3:lla 3.3. Olion elinkaaren ma¨a¨ra¨ytyminen 74 3.3.2 Smalltalk Smalltalk-kielessa¨ olioiden elinkaari on olioiden tuhoutumisen kan- nalta samanlainen kuin Modula-3:ssa eli olioille ei suoriteta auto- maattisesti mita¨a¨n siivoustoimenpiteita¨ ja roskienkeruu pita¨a¨ huolen olion varaaman muistin vapauttamisesta. Sen sijaan olioiden luomi- sen yhteydessa¨ Smalltalk antaa mahdollisuuden jonkinasteiseen auto- matisointiin. Smalltalkissa olioita luodaan antamalla luokalle ka¨sky new. Ta¨ma¨ puolestaan luo uuden olion ja palauttaa sen paluuarvonaan. Luomis- ka¨skylle voi antaa parametreja ja sen sisa¨lta¨ma¨ koodi voi suorittaa olion alustustoimenpiteita¨, joten ohjelmoijan ei tarvitse itse muistaa kutsua alustusja¨senfunktiota luomisen yhteydessa¨. Listaus 3.2 seuraavalla sivulla sisa¨lta¨a¨ esimerkin olion elinkaares- ta Smalltalkilla kirjoitettuna (Smalltalkissa ei oikeastaan ole “ohjelma- listauksen” ka¨sitetta¨, listauksessa on vain kirjoitettu tarvittavat koo- dilohkot pera¨kka¨in). 3.3.3 Java Java-kielessa¨ olioiden elinkaari on hieman kaksipiippuinen. Olioita luotaessa Java tarjoaa mahdollisuuden alustustoimenpiteisiin raken- tajaksi kutsutun ja¨senfunktion avulla. Ta¨ta¨ ja¨senfunktiota kutsutaan automaattisesti oliota luotaessa. Sen sijaan olion siivoustoimenpiteet ovat ongelmallisempia. Myo¨s Javassa on roskienkeruu, eli ka¨yto¨sta¨ poistuneiden olioiden viema¨ muistitila ei tuota ongelmia. Javassa ohjelmoijalla ei ole mita¨a¨n mahdollisuutta ilmoittaa, milloin olion elinkaari loppuu, vaan ros- kienkeruualgoritmi pa¨a¨ttelee itse, milloin olio on tarpeeton eli mil- loin siihen ei ole ulkopuolisia viitteita¨. Kieli ei kuitenkaan ma¨a¨rittele, kuinka pian olion tarpeettomaksi tulon ja¨lkeen roskienkeruu suorite- taan. Periaatteessa Java antaa ohjelmoijalle mahdollisuuden kirjoittaa ns. ﬁnalize-ja¨senfunktioita, joita roskienkeruu kutsuu ennen kuin olio siivotaan muistista. Ta¨ma¨ mekanismi ei kuitenkaan ole aina ka¨yt- to¨kelpoinen, koska ohjelmoija ei voi tieta¨a¨, milloin roskienkeruu sat- tuu ﬁnalize-ja¨senfunktiota kutsumaan. On myo¨s mahdollista, etta¨ oh- jelman suoritus loppuu ennen kuin roskienkeruu siivoaa olion muis- tista, jolloin ﬁnalizea ei kutsuta ollenkaan. 3.3. Olion elinkaaren ma¨a¨ra¨ytyminen 75 1 Object subclass: #OkDialogi 2 instanceVariableNames: ’viesti ’ 3 classVariableNames: ’’ 4 poolDictionaries: ’’ 5 category: ’Oliokirja’ 6 7 alusta: aViesti 8 “OkDialogi-olion luomistoimenpiteet“ 9 viesti := aViesti. 10 “Ta¨nne ikkunan avaaminen, viestin kirjoittaminen siihen ja muut 11 luomistoimenpiteet” 12 ^ self 13 14 new: aViesti 15 “OkDialogi-olion alusta-ja¨senfunktion kutsumiseen tarvittava kikka“ 16 ^ super new alusta: aViesti 17 18 odotaOKta 19 “Ta¨a¨lla¨ odotetaan OK-napin painamista” 20 21 siivoa 22 “OkDialogi-olion siivoustoimenpiteet“ 23 “Ta¨nne ikkunan sulkeminen ja muut siivoustoimenpiteet” 24 25 naytaViesti 26 “Funktio joka tulostaa viestin ja odottaa OK-nappia” 27 | dialogi | “Paikallinen muuttuja” 28 dialogi := OkDialogi new: ’Virhe!!’. “Olio syntyy, luomistoimenpiteet” 29 dialogi odotaOKta. “Odota, etta¨ ka¨ytta¨ja¨ kuittaa viestin” 30 dialogi siivoa “Suorita siivoustoimenpiteet” 31 “Funktiosta palattaessa roskienkeruu pita¨a¨ huolen olion muistista” LISTAUS 3.2: Esimerkki olion elinkaaresta Smalltalkilla Na¨in ollen Javassa olion alustustoimenpiteet suoritetaan auto- maattisesti, mutta jos siivoustoimenpiteet vaativat muistin vapautta- mista monimutkaisempia asioita, ohjelmoija joutuu ka¨yta¨nno¨ssa¨ koo- daamaan oman siivousja¨senfunktionsa ja kutsumaan sita¨ itse sellai- sessa ohjelman kohdassa, jossa tieta¨a¨ olion tulleen tarpeettomaksi. Listaus 3.3 seuraavalla sivulla sisa¨lta¨a¨ esimerkin olion elinkaaresta Javalla kirjoitettuna. 3.3. Olion elinkaaren ma¨a¨ra¨ytyminen 76 1 public class OkDialogi // OkDialogi-luokan esittely 2 { 3 public OkDialogi(String viesti) 4 { // Ta¨nne ikkunan avaaminen, viestin kirjoittaminen siihen 5 // ja muut luomistoimenpiteet 6 } 7 8 public void siivoa() 9 { // Ta¨nne ikkunan sulkeminen ja muut siivoustoimenpiteet 10 } 11 12 public void odotaOKta() 13 { // Ta¨ha¨n nappulan painamisen odottamisen koodi 14 } 15 16 // Ta¨ha¨n luokan loput ja¨senfunktiot ja sisa¨inen toteutus 17 18 // Luokkafunktio joka tulostaa viestin ja odottaa OK-nappia 19 public static void naytaViesti() 20 { 21 // Olio luodaan, luomistoimenpiteet 22 OkDialogi dialogi = new OkDialogi(\"Virhe!\"); 23 dialogi.odotaOKta(); // Odota, etta¨ ka¨ytta¨ja¨ kuittaa viestin 24 dialogi.siivoa(); // Olion siivoustoimenpiteet 25 // Funktiosta palattaessa roskienkeruu pita¨a¨ huolen olion muistista 26 } 27 } LISTAUS 3.3: Esimerkki olion elinkaaresta Javalla 3.3.4 C++ C++ sisa¨lta¨a¨ vaihtelevantasoisen tuen olioiden elinkaaren hallintaan. Tietyissa¨ tapauksissa oliot tuhoutuvat automaattisesti, toisissa tu- hoaminen taas ja¨teta¨a¨n ohjelmoijan vastuulle. Kielessa¨ voi Javan ta- paan ma¨a¨ritella¨ olion alustustoimenpiteet suorittavan rakentajaja¨sen- funktion, jota kutsutaan automaattisesti oliota luotaessa. Vastaavasti C++:ssa on mahdollista kirjoittaa purkaja, joka on ja¨senfunktio, joka sisa¨lta¨a¨ kaikki siivoustoimenpiteet ja jota C++ kutsuu automaattisesti oliota tuhottaessa. C++:n rakentajista ja purkajista kerrotaan enemma¨n aliluvussa 3.4. 3.3. Olion elinkaaren ma¨a¨ra¨ytyminen 77 Staattinen elinkaari C++:ssa on kahdentyyppisia¨ olioiden elinkaaria. Olioilla, jotka luodaan tavallisen muuttujan tapaan esimerkiksi funktion paikallisiksi muut- tujiksi, globaaleiksi muuttujiksi\u0017 tai olion ja¨senmuuttujiksi, on staat- tinen elinkaari. Ta¨ma¨ tarkoittaa, etta¨ olion syntyma¨- ja tuhoutumis- hetki on ma¨a¨ra¨tty jo ka¨a¨nno¨saikana ja ka¨a¨nta¨ja¨ osaa automaattisesti suorittaa tarvittavat luomistoimenpiteet olion syntyessa¨ ja vastaavas- ti siivoustoimenpiteet heti olion tuhoutuessa. Listaus 3.4 seuraavalla sivulla sisa¨lta¨a¨ esimerkin olion elinkaa- resta C++:lla kirjoitettuna. Riveilla¨ 20–26 on esimerkki staattisen elin- kaaren ka¨ytta¨misesta¨. OkDialogi-olio dialogi luodaan aivan kuten ta- vallinen paikallinen muuttuja, ja sen rakentajalle annetaan luomisen yhteydessa¨ tarvittavat parametrit, ta¨ssa¨ tapauksessa dialogin teksti. Koska dialogi on funktion paikallinen muuttuja, sen elinkaari ra- jautuu funktion sisa¨lle. Funktion lopussa rivilla¨ 26 olio tuhotaan au- tomaattisesti ja sen purkajaa kutsutaan siivoustoimenpiteita¨ varten. Jos ta¨llaisia paikallisia olioita on useita, ne tuhotaan ka¨a¨nteisessa¨ ja¨r- jestyksessa¨ niiden luomisja¨rjestykseen na¨hden. Staattisen elinkaaren ka¨ytta¨minen on C++:lla ohjelmoitaessa suosi- teltavaa aina, kun se on mahdollista, koska ta¨llo¨in ohjelma pita¨a¨ auto- maattisesti huolen olioiden tuhoamisesta elinkaaren lopussa. Ta¨llo¨in ei ole vaaraa muistivuodoista eika¨ siivoustoimenpiteiden unohtumi- sesta. Paikallisten muuttujien, globaalien muuttujien ja olioiden ja¨sen- muuttujien lisa¨ksi funktioiden parametreilla, luokkien luokkamuut- tujilla ja ka¨a¨nta¨ja¨n luomilla va¨liaikaisolioilla on C++:ssa staattinen elinkaari. Ohjelmoijan ei itse tarvitse — eika¨ ha¨n voikaan — huoleh- tia ta¨llaisten olioiden tuhoamisesta, vaan ka¨a¨nta¨ja¨ itse tuhoaa oliot niiden elinkaaren pa¨a¨ttyessa¨. Staattinen elinkari helpottaa ohjelmoijan tyo¨ta¨, kun ka¨a¨nta¨ja¨ pita¨a¨ huolen olioiden tuhoamisesta. Ta¨sta¨ huolimatta staattisenkaan elin- kaaren ka¨ytta¨minen ei saisi tuudittaa va¨a¨ra¨a¨n turvallisuuden tuntee- seen. C++:ssa on mahdollista tehda¨ vakavia ohjelmointivirheita¨ staat- tisen elinkaarenkin avulla. Jos funktio esimerkiksi palauttaa paluuar- vonaan osoittimen (tai viitteen) paikalliseen muuttujaan, ehtii ta¨ma¨ muuttuja tuhoutua ennen kuin kutsuja pystyy ka¨ytta¨ma¨a¨n osoitin- ta. Tuloksena on viallinen osoitin, jonka la¨pi viittaamisen vaikutuk- . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . \u0017Etha¨n ka¨yta¨ globaaleja muuttujia, etha¨n?/ 3.3. Olion elinkaaren ma¨a¨ra¨ytyminen 78 1 // OkDialogi-luokan esittely 2 class OkDialogi 3 { 4 public: 5 OkDialogi(string const& viesti); 6 ~OkDialogi(); 7 void odotaOKta(); 8 // Ta¨ha¨n luokan loput ja¨senfunktiot ja sisa¨inen toteutus 9 }; 10 // OkDialogi-olion luomistoimenpiteet 11 OkDialogi::OkDialogi(string const& viesti) 12 { // Ta¨nne ikkunan avaaminen, viestin kirjoittaminen siihen 13 // ja muut luomistoimenpiteet 14 } 15 // OkDialogi-olion siivoustoimenpiteet 16 OkDialogi::~OkDialogi() 17 { // Ta¨nne ikkunan sulkeminen ja muut siivoustoimenpiteet 18 } 19 // Funktio joka tulostaa viestin ja odottaa OK-nappia 20 void naytaViesti1() // Staattisella dialogin elinkaarella 21 { 22 OkDialogi dialogi(\"Virhe!\"); // Olio syntyy, luomistoimenpiteet 23 dialogi.odotaOKta(); // Odota, etta¨ ka¨ytta¨ja¨ kuittaa viestin 24 // Funktion loppuessa dialogin elinkaari pa¨a¨ttyy, siivoustoimenpiteet 25 // suoritetaan ja olio tuhoutuu 26 } 27 // Dynaamisella dialogin elinkaarella (typera¨a¨) 28 void naytaViesti2() 29 { 30 OkDialogi* dialogip = new OkDialogi(\"Virhe!\"); // Luomistoimenpiteet 31 dialogip->odotaOKta(); // Odota, etta¨ ka¨ytta¨ja¨ kuittaa viestin 32 delete dialogip; dialogip = 0; // Tuhoaminen ja siivoustoimenpiteet 33 } LISTAUS 3.4: Esimerkki olion elinkaaresta C++:lla 3.4. C++: Rakentajat ja purkajat 79 set ovat ta¨ysin ma¨a¨rittelema¨tto¨ma¨t. Sen vuoksi staattisen elinkaaren ka¨yto¨ssa¨ onkin ta¨rkea¨a¨, etta¨ ohjelmoija todella tiedostaa sen, etta¨ olio tuhoutuu automaattisesti tietyssa¨ kohtaa ohjelmaa. Dynaaminen elinkaari Toinen mahdollisuus on ja¨tta¨a¨ olion elinkaaresta huolehtiminen oh- jelmoijan itsensa¨ vastuulle. Ta¨llo¨in olio ei automaattisesti tuhoudu ollenkaan, vaan se ta¨ytyy erikseen tuhota. Ta¨llainen on usein tar- peen, jos olio luodaan yhdessa¨ funktiossa ja tuhotaan toisessa. Myo¨s dynaaminen sitominen ja polymorﬁsmi (aliluku 6.5) vaativat usein dynaamisen elinkaaren ka¨ytto¨a¨. Listauksen 3.4 riveilla¨ 27–33 oleva funktio ka¨ytta¨a¨ dialogissa dy- naamista elinkaarta. Olion elinkaari saadaan dynaamiseksi luomalla olio new-operaattorilla. Parametreikseen new ottaa luotavan olion luo- kan seka¨ rakentajan parametrit. Ta¨ma¨n ja¨lkeen se luo uuden olion, suorittaa alustustoimenpiteet ja palauttaa osoittimen olioon. Oliota ka¨yteta¨a¨n ta¨ma¨n ja¨lkeen ko. osoittimen la¨pi. Kun oliosta halutaan pa¨a¨sta¨ eroon, ta¨ytyy se erikseen tuhota osoittimen pa¨a¨sta¨ operaatto- rilla delete, joka suorittaa olion siivoustoimenpiteet ja tuhoaa olion. Na¨ita¨ operaattoreita ka¨sitella¨a¨n tarkemmin aliluvussa 3.5. Dynaamisen elinkaaren ka¨ytto¨ on “vaarallisempaa” kuin staatti- sen, koska ohjelmoijan ta¨ytyy itse pita¨a¨ huoli siita¨, etta¨ kaikki oliot tuhotaan, kun niita¨ ei ena¨a¨ tarvita. Erityisen vaikeaa ta¨ma¨ on virheti- lanteiden sattuessa, jolloin on vaikeaa muistaa, mitka¨ oliot on jo luotu ja ta¨ytyy na¨in ollen tuhota. Varsinkin poikkeusmekanismia (luku 11) ka¨ytetta¨essa¨ ta¨ma¨ on vaikeaa. C++-standardi tarjoaa auto ptr-tyypin, joka helpottaa jonkin verran dynaamisen elinkaaren olioiden hallin- taa. Siita¨ kerrotaan aliluvussa 11.7. 3.4 C++: Rakentajat ja purkajat Kuten edella¨ on ka¨ynyt ilmi, olioiden luominen ja tuhoaminen eroaa tavallisten muuttujien luomisesta ja tuhoamisesta siina¨, etta¨ olioiden luominen ja tuhoaminen saattaa vaatia alustus- ja siivoustoimenpitei- ta¨ luokasta riippuen. C++:ssa ta¨ma¨ on toteutettu niin, etta¨ joka luokalla on kaksi erityisja¨senfunktiota, rakentaja ja purkaja, joita kutsutaan automaattisesti aina olion luomisen ja tuhoamisen yhteydessa¨. 3.4. C++: Rakentajat ja purkajat 80 3.4.1 Rakentajat Rakentaja (constructor) on ja¨senfunktio, jonka tehta¨va¨na¨ on hoitaa kaikki uuden olion alustamiseen liittyva¨t toimenpiteet. Siita¨ ka¨yte- ta¨a¨n joskus myo¨s nimitysta¨ “muodostin”. Rakentajan suorittamia toi- menpiteita¨ ovat ainakin ja¨senmuuttujien alustaminen, olioon kuulu- vien olion ulkopuolisten tietorakenteiden ja olioiden luominen seka¨ mahdollisesti olion rekistero¨iminen jonnekin yms. Ka¨a¨nta¨ja¨ tunnistaa rakentajan sen nimesta¨: rakentajan nimi on ai- na sama kuin luokan nimi. Rakentaja voi saada valinnaisen ma¨a¨ra¨n parametreja, mika¨li niita¨ tarvitaan olion alustamiseen. Listaus 3.5 na¨ytta¨a¨ Paivays-luokan rakentajan, joka saa parametreinaan uuden pa¨iva¨yksen pa¨iva¨n, kuukauden ja vuoden. Rakentaja ei koskaan pa- lauta mita¨a¨n paluuarvoa, ja ta¨sta¨ johtuen rakentajan esittelyssa¨ ja ma¨a¨rittelyssa¨ ei paluutyyppia¨ merkita¨ lainkaan — ei edes avainsa- nalla void. Rakentajan esittely luokan esittelyn yhteydessa¨ ei eroa tavalli- sen ja¨senfunktion esittelysta¨ muuten kuin paluutyypin puuttumisen osalta. Sen sijaan sen ma¨a¨rittely on hieman tavallisuudesta poikkea- . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . paivays.hh . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1 class Paivays 2 { 3 public: 4 Paivays(unsigned int p, unsigned int k, unsigned int v); ... 22 }; . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . paivays.cc . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6 Paivays::Paivays(unsigned int p, unsigned int k, unsigned int v) 7 : paiva (p), kuukausi (k), vuosi (v) 8 { // Ei mita¨a¨n tehta¨va¨a¨ ta¨a¨lla¨ 9 } LISTAUS 3.5: Paivays-luokan rakentaja 3.4. C++: Rakentajat ja purkajat 81 va. Rakentajan ma¨a¨rittely on muotoa Luokannimi::Luokannimi(Parametrilista) : jmuutt1(alkuarvo1), jmuutt2(alkuarvo2), . . . // Alustuslista { // Muut alustustoimenpiteet } Alustuslista (initialization list) sisa¨lta¨a¨ luettelon luokan ja¨sen- muuttujista ja jokaisen ja¨senmuuttujan pera¨ssa¨ suluissa ja¨senmuut- tujan alustamiseen tarvittavat alkuarvot. Joissain C++-oppikirjoissa alustuslistaa ei ka¨yteta¨ lainkaan, vaan ja¨senmuuttujiin sijoitetaan ar- vot rakentajan koodilohkossa. Ta¨ma¨ ei ole hyva¨a¨ tyylia¨ (sijoituksen ja alustuksessa ka¨ytetta¨va¨n kopioinnin eroja ka¨sitella¨a¨n myo¨hemmin aliluvussa 7.2.1). Rakentajan runkoon (siis aaltosulkujen sisa¨lle) voi kirjoittaa muu- ta olion alustamiseen tarvittavaa koodia. Varsin usein ka¨y kuitenkin niin, etta¨ runko ja¨a¨ tyhja¨ksi, koska luokan oliot eiva¨t tarvitse muita alustustoimenpiteita¨ kuin ja¨senmuuttujien alustamisen. Luokalla voi olla useita vaihtoehtoisia rakentajia, kunhan ne vain saavat eri ma¨a¨ra¨n tai eri tyyppisia¨ parametreja niin, etta¨ olion luo- misen yhteydessa¨ ka¨a¨nta¨ja¨ voi parametrien perusteella pa¨a¨tella¨, mita¨ rakentajaa tulee kutsua.] Ja¨senmuuttujien alustaminen Mika¨li ja¨senmuuttuja on jotakin perustyyppia¨, kuten int, annetaan sille alustuslistassa yksinkertaisesti alkuarvo kuten listauksen 3.5 esi- merkissa¨. Jos taas ja¨senmuuttuja on olio, annetaan alustuslistassa ja¨- senmuuttujan nimen pera¨a¨n sen rakentajan tarvitsemat parametrit — aivan kuten normaalissa olion ma¨a¨rittelyssa¨ olion nimen pera¨a¨n tulevat suluissa rakentajan parametrit. Listaus 3.6 seuraavalla sivul- la na¨ytta¨a¨ yksinkertaisen Henkilo-luokan rakentajan, jossa alustetaan Paivays-tyyppinen ja¨senmuuttujaolio syntymapvm . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ]Ta¨ta¨ kielen ominaisuutta, jossa ohjelmassa voi olla useita samannimisia¨ funktioita, jotka eroavat toisistaan parametrien ma¨a¨ra¨n tai tyyppien perusteella, kutsutaan funktioiden kuor- mittamiseksi (overloading). C++:ssa la¨hes mita¨ tahansa funktioita voi kuormittaa, kunhan vain funktiokutsun yhteydessa¨ ka¨a¨nta¨ja¨ pystyy parametreista pa¨a¨ttelema¨a¨n oikean version funk- tiosta. 3.4. C++: Rakentajat ja purkajat 82 1 class Henkilo 2 { 3 public: 4 Henkilo(string const& nimi, unsigned int syntymavuosi, 5 unsigned int syntymakk, unsigned int syntymapv); ... 6 private: 7 string nimi ; 8 Paivays syntymapvm ; ... 9 }; ... 10 Henkilo::Henkilo(string const& nimi, unsigned int syntymavuosi, 11 unsigned int syntymakk, unsigned int syntymapv) 12 : nimi (nimi), syntymapvm (syntymapv, syntymakk, syntymavuosi) 13 { 14 // Ta¨nne loput henkilo¨n alustuksesta 15 } LISTAUS 3.6: Esimerkki rakentajasta olion ollessa ja¨senmuuttujana Oletusrakentaja Oletusrakentajaksi (default constructor) kutsutaan rakentajaa, joka ei saa yhta¨a¨n parametria. Sen ta¨ytyy siis pystya¨ alustamaan olio il- man ulkopuolista tietoa, joten on ka¨teva¨a¨ ajatella, etta¨ sen avulla luo- daan “oletusarvoinen” olio. Oletusrakentajaa kutsutaan seuraavissa tapauksissa: • Jos olio luodaan ilman etta¨ sen rakentajalle annetaan paramet- reja. Huomaa, etta¨ ta¨llo¨in myo¨s parametrien ympa¨rilla¨ normaa- listi olevat sulut ja¨a¨va¨t pois: Lista l; // Luodaan tyhja¨ lista, kutsutaan oletusrakentajaa • Jos ja¨senmuuttujana olevan olion alustus unohtuu “isa¨nta¨olion” rakentajan alustuslistasta, alustetaan ja¨senmuuttujaolio ka¨yt- ta¨en oletusrakentajaa, mika¨li sellainen on olemassa (muuten annetaan virheilmoitus). Se, etta¨ alustuksen unohtamisesta ei tule virheilmoitusta, on yksi syy va¨ltta¨a¨ oletusrakentajia. 3.4. C++: Rakentajat ja purkajat 83 Perustyyppia¨ olevilla muuttujilla ei ole oletusrakentajaa, joten tyyliin int i; esitellyt muuttujat ja¨a¨va¨t alustamatta, kuten C-kieles- sa¨kin. Samoin ka¨y perustyyppisille ja¨senmuuttujille, joita ei aluste- ta rakentajassa. Ta¨ma¨n vuoksi on eritta¨in ta¨rkea¨a¨, etta¨ kaikki perus- tyyppia¨ olevat (ja¨sen)muuttujat alustetaan aina johonkin ja¨rkeva¨a¨n alkuarvoon! Oletusrakentajan toinen erityisominaisuus on, etta¨ jos luokalle ei ole kirjoitettu yhta¨ka¨a¨n rakentajaa, tekee ka¨a¨nta¨ja¨ sille automaatti- sesti tyhja¨n oletusrakentajan, joka ei tee mita¨a¨n muuta kuin alustaa kaikki ja¨senmuuttujat niiden oletusrakentajilla (jos ta¨ma¨ ei onnistu, annetaan virheilmoitus). Koska ka¨a¨nta¨ja¨n itse tekema¨t rakentajat har- voin tekeva¨t sita¨ mita¨ halutaan, kannattaa jokaiseen luokkaan kirjoit- taa aina oma rakentaja. Oletusrakentajaa ka¨yteta¨a¨n myo¨s silloin, kun luodaan C++:n perus- taulukko, joka sisa¨lta¨a¨ olioita: Lista listaTaulukko[10]; // Luodaan 10 tyhja¨n listan taulukko Ta¨llo¨in jokainen taulukon alkio alustetaan ka¨ytta¨en oletusrakentajaa. Mika¨li luokalla ei ole oletusrakentajaa, ei siita¨ voi myo¨ska¨a¨n teh- da¨ ta¨llaisia taulukoita. Ta¨ma¨ ongelma ratkeaa helpoiten ka¨ytta¨ma¨l- la¨ STL:n vector-luokkaa, johon oikein alustetut oliot voi lisa¨ta¨ yksi kerrallaan vaikkapa silmukassa (vector-luokasta on lyhyt esittely liit- teen A aliluvussa A.3). Kopiorakentaja Kopiorakentaja (copy constructor) on rakentaja, joka saa parametri- naan viitteen jo olemassa olevaan saman luokan olioon. Sen tehta¨va¨- na¨ on luoda identtinen kopio parametrina saadusta oliosta, ja ka¨a¨nta¨- ja¨ kutsuu sita¨ automaattisesti tietyissa¨ tilanteissa, joissa kopion luo- minen on tarpeen. Kopiorakentajaa ka¨sitella¨a¨n tarkemmin aliluvus- sa 7.1.2. 3.4.2 Purkajat Purkajaksi (destructor) kutsutun ja¨senfunktion tehta¨va¨na¨ on suorit- taa tarvittavat siivoustoimenpiteet olion tuhoutuessa. Ta¨llaisia ovat 3.4. C++: Rakentajat ja purkajat 84 muiden muassa olioon kuuluvien olion ulkopuolisten tietorakentei- den ja olioiden tuhoaminen seka¨ tiedostojen sulkeminen. Purkajasta ka¨yteta¨a¨n myo¨s nimityksia¨ “hajotin” ja “ha¨vitin”. Kuten rakentajankin tapauksessa, ka¨a¨nta¨ja¨ tunnistaa purkajan sen nimesta¨. Purkajan nimi alkaa matomerkilla¨ ‘~’, jonka pera¨a¨n tulee heti luokan nimi.^ Purkaja ei saa parametreja, eika¨ sen esittelyyn eika¨ ma¨a¨rittelyyn merkita¨ paluuarvoa kuten ei rakentajiinkaan. Listaus 3.7 na¨ytta¨a¨ pa¨iva¨ysluokan purkajan esimerkkina¨ purkajan syntaksista. Kun olion tuhoutumisen aika tulee, ka¨a¨nta¨ja¨ pita¨a¨ itse automaatti- sesti huolen olion ja¨senmuuttujien tuhoamisesta. Na¨in ohjelmoijan ei tarvitse purkajaa kirjoittaessaan huolehtia ta¨sta¨. Olion ja¨senmuuttu- jat tuhotaan vasta purkajan varsinaisen koodin suorituksen ja¨lkeen, joten itse purkajan koodissa ja¨senmuuttujiin voi viitata normaalis- ti. On kuitenkin huomattava, etta¨ automaattinen tuhoaminen koskee vain olion omia ja¨senmuuttujia, ei esimerkiksi dynaamisesti osoitti- mien pa¨a¨ha¨n luotuja olioita. Purkajien toiminta on muutenkin varsin automaattista. Ohjelmoi- jan ei koskaan tarvitse itse kutsua luokan purkajaa, vaan ka¨a¨nta¨ja¨ pi- ta¨a¨ huolen purkajan kutsumisesta, kun olio tuhotaan. Ta¨ma¨ tapahtuu mm. seuraavissa tapauksissa: . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ^Lausekkeessa esiintyessa¨a¨nha¨n matomerkki ‘~’ on C++:ssa¨ “bitti-not”-operaattori, jolla ka¨a¨nneta¨a¨n luvun bitit pa¨invastaisiksi. Niinpa¨ purkajan nimi ika¨a¨n kuin kuvaa sita¨, etta¨ se on rakentajalle vastakkainen operaatio. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . paivays.hh . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1 class Paivays 2 { 3 public: ... 5 ~Paivays(); ... 22 }; . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . paivays.cc . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 11 Paivays::~Paivays() 12 { // Ei siivottavaa 13 } LISTAUS 3.7: Paivays-luokan purkaja 3.5. C++: Dynaaminen luominen ja tuhoaminen 85 • Paikallisten olioiden purkajia kutsutaan automaattisesti juuri ennen kuin niiden elinkaari (eli na¨kyvyysalue) loppuu koodi- lohkon loppuaaltosulun kohdalla. Huomaa, etta¨ ta¨ma¨ tarkoittaa sita¨, etta¨ paikalliset oliot tuhotaan vasta return-lauseen ja¨lkeen. • Globaalien olioiden purkajia kutsutaan main-funktion loputtua. • Funktion parametrioliot tuhoutuvat aivan kuten funktion pai- kalliset oliot. • Ja¨senmuuttujina olevien olioiden purkajia kutsutaan, kun nii- den “isa¨nta¨olio” tuhoutuu. • Dynaamisesti luotujen olioiden purkajia (katso seuraava alilu- ku) kutsutaan, kun oliota tuhotaan deletella¨. • Taulukossa olevien olioiden purkajia kutsutaan, kun taulukko tuhoutuu. Mika¨li olion tuhoutumisessa ei tarvita mita¨a¨n erityisia¨ siivous- toimenpiteita¨, luokalle ei periaatteessa ole pakko kirjoittaa purka- jaa. Ta¨llaisessakin tapauksessa kannattaa kuitenkin kirjoittaa luokalle tyhja¨ purkaja, koska muutoin koodin lukijalla hera¨a¨ helposti epa¨ilys, etta¨ luokan kirjoittaja on vain unohtanut kirjoittaa purkajan. 3.5 C++: Dynaaminen luominen ja tuhoaminen Kuten aiemmin ta¨ssa¨ luvussa on jo todettu, staattisen elinkaaren ka¨yt- to¨ C++-ohjelmoinnissa on suotavaa, koska ta¨llo¨in ka¨a¨nta¨ja¨ pita¨a¨ huolen siita¨, etta¨ kaikki luodut oliot aikanaan myo¨s tuhotaan. Varsin usein tulee kuitenkin tarve luoda olioita, joiden elinkaarta ei voida ka¨a¨n- no¨saikana rajata tiettyyn osaan koodia. Ta¨llaiset oliot ta¨ytyy luoda dynaamisella elinkaarella, jolloin kaikki vastuu olioiden tuhoamises- ta siirtyy ohjelmoijalle. Ta¨ssa¨ luvussa “olion luomista dynaamisella elinkaarella” kutsutaan yksinkertaisesti “olion dynaamiseksi luomi- seksi”. Muistivuodot ovat jo pitka¨a¨n olleet tyypillisia¨ ohjelmissa, joissa ohjelmoija on unohtanut vapauttaa dynaamisesti varaamansa muis- tin. Usein muistivuotoihin kuitenkin suhtaudutaan va¨ha¨tteleva¨sti, koska “ka¨ytto¨ja¨rjestelma¨ kuitenkin vapauttaa muistin ohjelman lop- puessa”. Olio-ohjelmoinnissa tilanne on kuitenkin vakavampi, koska 3.5. C++: Dynaaminen luominen ja tuhoaminen 86 oliolla on yleensa¨ purkaja, jonka koodi suoritetaan olion tuhoamisen yhteydessa¨. Mika¨li nyt ohjelmoija unohtaa tuhota dynaamisesti vara- tun olion, ei purkajaa koskaan suoriteta! Nyt siis pelka¨n muistivuo- don lisa¨ksi myo¨s osa ohjelmakoodista ja¨a¨ suorittamatta — mahdolli- sesti vakavin seurauksin. Esimerkiksi kelpaa mainiosti tietokantaolio, joka sa¨ilytta¨a¨ tieto- kantaa tiedostossa. On hyvin mahdollista, etta¨ olion koodi on kirjoi- tettu optimoidusti niin, etta¨ olio puskuroi tietoa muistiin ja pa¨ivitta¨a¨ tiedostossa olevaa tietokantaa vain tietyin va¨liajoin. Jos nyt ta¨llainen tietokantaolio on luotu dynaamisesti ja se unohdetaan tuhota, sen purkaja ja¨a¨ suorittamatta ja muistiin puskuroitu tieto pa¨ivitta¨ma¨tta¨ tiedostoon. Olioiden dynaamiseen luomiseen liittyy muitakin ongelmia. Niis- ta¨ yleisimpia¨ ovat • olion tuhoamisen unohtaminen • muistin loppuminen (olion luomisen epa¨onnistuminen) • olion tuhoaminen kahteen kertaan • jo tuhotun olion ka¨ytta¨minen (ta¨ma¨ vaara on muuallakin kuin dynaamisen elinkaaren yhteydessa¨) • sekoilut C++:n taulukkojen tuhoamisessa (aliluku 3.5.3). Kaikista na¨ista¨ vaaroista huolimatta olioiden dynaaminen luomi- nen on varsin usein tarpeellista C++:lla ohjelmoitaessa. Sita¨ ka¨ytetta¨es- sa¨ on vain oltava eritta¨in huolellinen. 3.5.1 new Olion luominen dynaamisesti tapahtuu syntaksilla new Tyyppinimi(alustusparametrit) Operaattori new luo uuden (nimetto¨ma¨n) olion ja palauttaa osoitti- men siihen. Esimerkiksi uuden pa¨iva¨ysolion ja uuden kokonaisluvun voi luoda seuraavasti: Paivays* pvm p = new Paivays(1,1,1000); int* luku p = new int(5); // Uusi int, alkuarvona 5 3.5. C++: Dynaaminen luominen ja tuhoaminen 87 Mika¨li uudelle oliolle ei saada varatuksi riitta¨va¨sti muistia, heitta¨a¨ new poikkeuksen, jonka tyyppi on std::bad alloc. Ta¨ma¨n poikkeuk- sen sieppaaminen vaatii otsikkotiedoston <new> lukemista. . . . . . . . . Ekskursio: Poikkeukset (lisa¨a¨ luvussa 11) . . . . . . . . Poikkeukset (exception) ovat C++:ssa uusi tapa hoitaa ohjelman virhetilanteita. Kun ohjelmassa havaitaan virhe, virheen ha- vainnut koodinkohta heitta¨a¨ (throw) “ilmaan” poikkeusolion, jonka ylemmilla¨ ohjelman kutsutasoilla olevat virheka¨sittelija¨t voivat siepata (catch). Yksinkertaisimmillaan poikkeusmekanismi na¨ytta¨a¨ seuraavan- laiselta: 1 try 2 { 3 // Ta¨nne koodi, jossa syntyvia¨ virhetilanteita tarkkaillaan 4 } 5 catch (VirheenTyyppi& vo) 6 { // Virheolio vastaanotetaan ‘‘parametrina’’ 7 // Ta¨nne virheka¨sittelykoodi, joka voi ka¨ytta¨a¨ virheoliota vo 8 } 9 // Ta¨a¨lta¨ jatketaan Kun ta¨llainen rakenne tulee ohjelmassa vastaan, ohjelman suo- ritus siirtyy suoraan try-lohkon sisa¨lle, jossa jatketaan normaa- listi. Mika¨li virhetta¨ ei lohkon sisa¨lla¨ tapahdu, hypa¨ta¨a¨n catch- lohkon yli ja jatketaan ohjelman suoritusta koko try-catch-ra- kenteen ja¨lkeen. Jos try-lohkon sisa¨lla¨ heiteta¨a¨n poikkeus, tarkastetaan kelpaa- ko se tyyppinsa¨ perusteella catch-lohkon parametriksi. Jos se kelpaa, siirtyy ohjelman suoritus catch-lohkon sisa¨a¨n. Kun ta¨- ma¨n lohkon koodi on suoritettu, ohjelman suoritus jatkuu try- catch-rakenteen ja¨lkeisesta¨ ohjelmakoodista. Ta¨ma¨ tarkoittaa sita¨, etta¨ try-lohkoon ei ena¨a¨ palata virheka¨sittelyn ja¨lkeen. Virheka¨sittelija¨ voi myo¨s lopuksi heitta¨a¨ virheen uudelleen il- maan komennolla throw;, jos virheesta¨ ei voida toipua koko- naan eli virhe halutaan va¨litta¨a¨ viela¨ ylemma¨s ohjelmassa. Jos poikkeusta ei oteta kiinni missa¨a¨n, keskeyteta¨a¨n ohjelman suo- ritus virheilmoitukseen. Ylla¨ oleva poikkeuska¨sittelyn kuvaus on pahasti puutteellinen mutta riitta¨a¨ ta¨ssa¨ vaiheessa. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3.5. C++: Dynaaminen luominen ja tuhoaminen 88 Muistin loppumiseen tulisi aina varautua. Tyypillinen esimerk- ki dynaamisesta olion luomisesta ja muistin loppumisen hallinnasta lo¨ytyy riveilta¨ 12–21 listauksesta 3.8. Joskus poikkeukset ovat turhan raskas tapa varautua muistin lop- 1 #include \"paivays.hh\" 2 3 #include <new> 4 #include <cstdlib> 5 #include <iostream> 6 using std::cout; 7 using std::cerr; 8 using std::endl; 9 10 int main() 11 { 12 Paivays* joulu p = 0; // Nollataan kaiken varalta 13 try 14 { 15 joulu p = new Paivays(24,12,1999); // Poikkeus, jos muisti loppuu 16 } 17 catch (std::bad alloc&) 18 { // Virheka¨sittely, jos muisti loppui 19 cerr << \"Muisti loppui, lopetan!\" << endl; 20 return EXIT FAILURE; 21 } 22 23 Paivays* vappu p = new(std::nothrow) Paivays(1,5,2000); 24 if (vappu p == 0) 25 { 26 // Virheka¨sittely, jos muisti loppui 27 cerr << \"Muisti loppui, lopetan!\" << endl; 28 delete joulu p; joulu p = 0; // Tuhotaan jo varattu olio! 29 return EXIT FAILURE; 30 } 31 32 // Ta¨a¨lta¨ jatketaan, jos virhetta¨ ei sattunut 33 cout << \"Joulun ja vapun va¨lissa¨ pa¨ivia¨ on \" 34 << joulu p->paljonkoEdella(*vappu p) << endl; 35 36 delete vappu p; vappu p = 0; // Dynaamisesti luodut oliot tuhottava 37 delete joulu p; joulu p = 0; 38 } LISTAUS 3.8: Esimerkki olion dynaamisesta luomisesta new’lla¨ 3.5. C++: Dynaaminen luominen ja tuhoaminen 89 pumiseen. Ta¨llaisia tilanteita varten new-operaattorista on myo¨s ole- massa versio new(std::nothrow), joka muistin loppuessa ei heita¨ poikkeusta vaan palauttaa nollaosoittimen. Ko. operaattorin ka¨ytto¨ vaatii myo¨s otsikkotiedoston <new> lukemista. Tietysti muistin lop- pumiseen ta¨ytyy varautua ta¨ssa¨kin tapauksessa. Listauksen 3.8 ri- veilla¨ 23–30 on esimerkki ta¨sta¨. Vanhoissa C++-kielen versioissa nolla- osoittimen palauttaminen oli new’n oletustoiminto, koska poikkeuksia ei viela¨ ollut. Yleensa¨ olioiden dynaamiseen luomiseen kannattaa ka¨ytta¨a¨ “nor- maalia” poikkeuksen heitta¨va¨a¨ new’ta¨, koska ta¨llo¨in ohjelman suoritus ainakin keskeytyy, jos muistin loppumiseen ei varauduta. Nollaosoit- timen palauttava nothrow-versio sen sijaan jatkaa ohjelman suoritus- ta, kunnes ohjelma jossain vaiheessa todenna¨ko¨isesti sekoaa, kun se yritta¨a¨ ka¨ytta¨a¨ olematonta oliota. Ta¨ssa¨ vaiheessa saattaa kuitenkin olla todella vaikea paikallistaa, mista¨ virhe oikeastaan on johtunut. 3.5.2 delete Ohjelmoijan ta¨ytyy itse muistaa tuhota new’lla¨ luomansa oliot. Ta¨- ma¨ tapahtuu operaattorilla delete, jolle annetaan operandina osoitin new’lla¨ varattuun olioon. Operaattori delete tuhoaa kyseisen olion ja vapauttaa sen ka¨ytta¨ma¨n muistitilan. Tuhoamisen yhteydessa¨ olion purkajaa kutsutaan normaalisti (periytymista¨ ka¨ytetta¨essa¨ luokan purkajan tulee olla virtuaalinen, katso aliluku 6.5.5). Jos deletelle an- taa olioon osoittavan osoittimen sijaan nollaosoittimen, ei kyseessa¨ ole virhe. Ta¨llo¨in delete ei vain tee mita¨a¨n. Koska olion tuhoamisen ja¨lkeen deletelle annettu osoitin ei ena¨a¨ osoita mihinka¨a¨n ja¨rkeva¨a¨n, kannattaa sen arvoksi asettaa nolla. Ta¨- ma¨ ei ole pakollista, mutta kylla¨kin hyva¨n ohjelmointityylin mukais- ta. Poikkeuksiin ei deleten tapauksessa tarvitse varautua, koska olion tuhoamisessa ei muistin loppumista tai muita virheita¨ pita¨isi tapahtua. Poikkeuksista kertovassa luvussa 11 ka¨sitella¨a¨n mahdolli- sia olion tuhoamisen aikaisia virhetilanteita tarkemmin. Esimerkkilistauksen 3.8 riveilla¨ 36–37 tuhotaan dynaamisesti va- ratut oliot. Huomaa myo¨s rivi 28, jossa virheenka¨sittelyn yhteydes- sa¨ ta¨ytyy muistaa tuhota dynaamisesti luotu olio joulu p. Ta¨llaiset virhetilanteissa tapahtuvat tuhoamiset unohtuvat eritta¨in helposti ja 3.5. C++: Dynaaminen luominen ja tuhoaminen 90 ovat yksi lisa¨syy valita staattinen luominen dynaamisen luomisen si- jaan aina, kun se on mahdollista. 3.5.3 Dynaamisesti luodut taulukot Joskus tulee tarve luoda dynaamisesti myo¨s taulukoita. Ta¨llo¨in kan- nattaa pyrkia¨ mahdollisuuksien mukaan ka¨ytta¨ma¨a¨n STL:n tauluk- koa vector, jonka ka¨ytto¨ on paljon mukavampaa ja turvallisempaa kuin C++:n C-kielesta¨ periytyva¨n perustaulukkotyypin. Na¨iden vector- taulukoiden (tai muiden STL:n tietotyyppien) dynaaminen luominen tapahtuu normaalisti new’lla¨ ja tuhoaminen deletella¨. Niihin ei liity mita¨a¨n erityisia¨ virhemahdollisuuksia. Mika¨li perustaulukoita ta¨ytyy kuitenkin luoda dynaamisesti, se tehda¨a¨n operaattorilla new[ ] (“tau- lukko-new”) seuraavasti: Lista* lista p = new Lista; // Yksitta¨inen olio Lista* listaTaulukko p = new Lista[10]; // Taulukollinen olioita Suurin virhela¨hde dynaamisesti luotujen taulukkojen kanssa on, etta¨ ne ta¨ytyy tuhota operaattorilla delete[ ] (“taulukko-delete”): delete[ ] listaTaulukko p; listaTaulukko p = 0; Mika¨li taulukon yritta¨a¨ tuhota tavallisella deletella¨, on ohjelman toi- minta ma¨a¨rittelema¨to¨n, mutta tuskin toivotun mukainen. Ongelmal- liseksi tilanteen tekee se, etta¨ itse osoittimen tyypista¨ ka¨a¨nta¨ja¨ ei voi mitenka¨a¨n na¨hda¨, onko osoittimen pa¨a¨ssa¨ taulukko vai yksitta¨inen olio. Ta¨ma¨n vuoksi ka¨a¨nta¨ja¨ ei pysty varoittamaan va¨a¨ra¨sta¨ deletesta¨. Myo¨s pa¨invastainen kielto pa¨tee, eli yksitta¨isia¨ olioita ei saa tuhota taulukko-deletella¨, vaikka ka¨a¨nta¨ja¨ ei ta¨sta¨ varoitakaan. 3.5.4 Virheiden va¨ltta¨minen dynaamisessa luomisessa Dynaamisen muistinhallinnan virheet ovat yleisimpia¨ virhetyyppe- ja¨ ohjelmoinnissa. Valmiista ohjelmasta virheen lo¨yta¨minen on usein todella vaivalloista, koska dynaamisen muistinhallinnan virheet il- meneva¨t yleensa¨ aivan eri paikassa kuin missa¨ itse virhe on. Niinpa¨ paras tapa virheiden korjaamiseen on niiden syntymisen esta¨minen jo ohjelmaa suunniteltaessa. 3.5. C++: Dynaaminen luominen ja tuhoaminen 91 Virheiden syyt ovat usein samoja ohjelmasta toiseen, joten ohjel- mointiyhteiso¨n keskuudessa on kehittynyt erilaisia muistisa¨a¨nto¨ja¨ ja “kansanperinnetta¨”, jotka auttavat virheiden ehka¨isyssa¨. Alla esite- ta¨a¨n joitain tyypillisimpia¨ virhetilanteita ja yksinkertaisia sa¨a¨nto¨ja¨, joilla virheiden syntymista¨ voi pyrkia¨ va¨ltta¨ma¨a¨n. Muistivuodot Ehka¨ yleisin muistivuotojen syy on, etta¨ ohjelmaa suunniteltaessa ei ole suunniteltu sita¨, minka¨ ohjelman osan vastuulla dynaamisesti luodun olion tuhoaminen on. Vastuualueiden suunnittelu on muu- tenkin oliosuunnittelun ta¨rkeimpia¨ asioita, joten kunnollisella olio- suunnittelulla saadaan eliminoitua myo¨s suuri osa dynaamisen muis- tinhallinnan ongelmista. Tuhoamisvastuun voi ajatella myo¨s niin, et- ta¨ jokaisen dynaamisesti (ja muutenkin) luodun olion tulisi olla aina jonkin ohjelmanosan tai olion omistuksessa ja omistajan vastuulla on myo¨s tuhota dynaamisesti luotu olio. Dynaamisesti luotujen olioiden hallinnassa pa¨teva¨t samat sa¨a¨nno¨t kuin perinteisessa¨ ei-olio-ohjelmoinnissa dynaamisen muistin hal- linnassa. Olioajattelu voi kylla¨kin auttaa paljon sa¨a¨nto¨jen noudatta- misessa. Ta¨rkeimpia¨ ta¨llaisia sa¨a¨nto¨ja¨ on, etta¨ dynaamisesti luodun olion omistuksen tulisi mielella¨a¨n sa¨ilya¨ samana olion elinajan. Ta¨ma¨ tarkoittaa sita¨, etta¨ mieluiten saman ohjelman osan tai olion, joka on luonut toisen olion dynaamisesti, pita¨isi myo¨s vastata olion tuhoamisesta. Ta¨ma¨n ohjeen noudattaminen helpottaa suunnit- telua, koska suunnittelijan ei tarvitse ta¨llo¨in pita¨a¨ mielessa¨a¨n, minka¨ ohjelman osan vastuulla kunkin olion tuhoamisvastuu kulloinkin on. Jos esimerkiksi olion rakentajassa luodaan dynaamisesti uusi olio, on luonnollista, etta¨ se tuhotaan saman olion purkajassa. Vastaavasti jos jostain ohjelmamoduulista lo¨ytyy funktio, joka palauttaa paluuar- vonaan osoittimen dynaamisesti luotuun olioon, pita¨isi samasta oh- jelmamoduulista lo¨ytya¨ myo¨s funktio, jonka avulla saatu olio voidaan “palauttaa” moduuliin (toisin sanoen funktio, joka huolehtii olion tu- hoamisesta). Esimerkkina¨ ta¨sta¨ on vaikkapa STL:n taulukkoluokka vector. Mi- ka¨li luodaan taulukko vector<int>, joka sisa¨lta¨a¨ kokonaislukuja, ei taulukon ka¨ytta¨ja¨n tarvitse miettia¨ ollenkaan dynaamista muistin- hallintaa, kuten listaus 3.9 seuraavalla sivulla osoittaa. Taulukkoluo- kan toteutus pita¨a¨ huolen kaikesta muistinhallinnasta ja piilottaa sen 3.5. C++: Dynaaminen luominen ja tuhoaminen 92 ka¨ytta¨ja¨lta¨. Jos sen sijaan taulukkoon vector<int*> talletetaan dy- naamisesti luotuja kokonaislukuja kuten listauksessa 3.10 seuraaval- la sivulla), na¨ma¨ kokonaisluvut ovat ka¨ytta¨ja¨n luomia ja niinpa¨ nii- den tuhoamisvastuu on taulukon ka¨ytta¨ja¨lla¨, ei itse taulukkoluokalla. Mika¨li olion omistus (vastuu tuhota dynaamisesti luodut oliot) halutaan siirta¨a¨ ohjelman osalta toiselle esimerkiksi (ja¨- sen)funktiokutsun yhteydessa¨, ta¨ma¨ tulee dokumentoida na¨kyva¨sti funktion rajapintadokumenttiin. Ta¨llo¨in seka¨ funktion ka¨ytta¨ja¨ etta¨ toteuttaja ovat ainakin periaatteessa tietoisia siita¨, kummalla olion tu- hoamisvastuu funktiokutsun ja¨lkeen on. Uudessa C++:n kirjastossa on ta¨llaisia tilanteita varten auto ptr-malli, jota voi ka¨ytta¨a¨ eksplisiitti- sesti kertomaan, etta¨ olion omistus siirreta¨a¨n toiseen paikkaan. Ta¨- ma¨n automaattiosoittimen ka¨yto¨sta¨ kerrotaan lisa¨a¨ aliluvussa 11.7. Kahteen kertaan tuhoaminen ja ka¨ytto¨ tuhoamisen ja¨lkeen Toinen yleinen virhe on, etta¨ muistivuotojen pelossa liioitellaan tu- hoamista ja joko tuhotaan olio kahteen kertaan (yleensa¨ eri osoitti- mien la¨pi) tai sitten olio tuhotaan liian aikaisin ja sita¨ yriteta¨a¨n ka¨yt- 1 void esim() 2 { 3 vector<int> taul; 4 5 // Luodaan luvut 6 for (int i=0; i<10; ++i) 7 { 8 taul.push back(i); 9 } 10 11 // Ka¨yteta¨a¨n lukuja 12 for (int i=0; i<10; ++i) 13 { 14 cout << taul[i] << endl; 15 } 16 17 // Lukujen tuhoaminen vectorin vastuulla, vektori tuhoutuu itsesta¨a¨n 18 } LISTAUS 3.9: Taulukko, joka omistaa sisa¨lta¨ma¨nsa¨ kokonaisluvut 3.5. C++: Dynaaminen luominen ja tuhoaminen 93 1 void esim2() 2 { 3 vector<int*> taul; 4 5 // Luodaan luvut 6 for (int i=0; i<10; ++i) 7 { 8 int* lukup = 0; 9 try 10 { 11 lukup = new int(i); // Luodaan uusi kokonaisluku 12 taul.push back(lukup); 13 } 14 catch (std::bad alloc&) 15 { 16 // Muisti lopussa, ta¨ytyy siivota ja¨ljet eli jo luodut luvut 17 delete lukup; lukup = 0; 18 for (int j=0; j<i; ++j) { delete taul[j]; taul[j] = 0; } 19 throw; // Heiteta¨a¨n virhe edelleen 20 } 21 } 22 // Ka¨yteta¨a¨n lukuja 23 for (int i=0; i<10; ++i) { cout << *(taul[i]) << endl; } 24 // Lopuksi tuhotaan luvut 25 for (int i=0; i<10; ++i) { delete taul[i]; taul[i] = 0; } 26 // Itse taulukko tuhoutuu automaattisesti 27 } LISTAUS 3.10: Taulukko, joka ei omista sisa¨lta¨mia¨a¨n kokonaislukuja ta¨a¨ viela¨ tuhoamisen ja¨lkeen. Ta¨sta¨ aiheutuvat virheet ovat yleensa¨ yhta¨ vaikeita ja¨ljitta¨a¨ kuin itse muistivuodotkin, ja virheiden vaiku- tukset saattavat olla paljon mystisempia¨. Samat suunnitteluperiaatteet, jotka esta¨va¨t muistivuotoja, aut- tavat yleensa¨ ehka¨isema¨a¨n ennalta na¨ma¨kin virheet. Kun ohjelma suunnitellaan niin, etta¨ jokaisen dynaamisesti luodun olion omistaa kerrallaan vain yksi ohjelmanosa, ei pelkoa kahteen kertaan tuhoami- sesta ole. Toinen hyva¨ peukalosa¨a¨nto¨ on nollata osoittimet aina sen ja¨lkeen, kun niiden pa¨a¨sta¨ tuhotaan dynaamisesti varattu olio: Paivays* p = new Paivays; // Ka¨yteta¨a¨n pa¨iva¨ysta¨ delete p; p = 0; 3.5. C++: Dynaaminen luominen ja tuhoaminen 94 Mika¨li nyt jo tuhottua oliota yriteta¨a¨n ka¨ytta¨a¨ osoittimen p kautta, on ohjelmalla hyva¨t mahdollisuudet kaatua tyhja¨n osoittimen la¨pi ta- pahtuvaan muistiviittaukseen — ainakin UNIX-pohjaisissa ka¨ytto¨ja¨r- jestelmissa¨. Jos p ja¨tetta¨isiin vanhaan arvoonsa, osoittaisi se suurella todenna¨ko¨isyydella¨ siihen osaan muistia, jossa olio on ollut. Ta¨llo¨in osoittimen la¨pi tapahtuvat viittaukset saattaisivat jopa toimia jonkin aikaa “zombie-oliota” ka¨ytta¨en, kunnes kyseinen muistialue otetaan uudelleen ka¨ytto¨o¨n ja siihen luodaan uusi olio, joka kirjoittaa datan- sa vanhan datan pa¨a¨lle. Muistivuodot virhetilanteissa Vaikka muistivuodot saisikin muuten kuriin, tulee ongelma paljon monimutkaisemmaksi, kun ohjelman pita¨isi pyrkia¨ toipumaan vir- hetilanteista, vaikkapa muistin loppumisesta tai tiedostovirheesta¨. Yleensa¨ virheen sattuessa ohjelma siirtyy tavalla tai toisella virheen- ka¨sittelykoodiin, ja ta¨ta¨ koodia kirjoitettaessa helposti unohtuu tuho- ta ennen virhetta¨ dynaamisesti luodut oliot, joita ei ena¨a¨ virheesta¨ toipumisen ja¨lkeen tarvita. Listauksen 3.10 riveilla¨ 14–20 oleva vir- heka¨sittelija¨ antaa ehka¨ jonkinlaisen kuvan siita¨, mita¨ kaikkea muis- tivuotojen esta¨minen vaatii._ Virhetilanteista toipumista C++:ssa ka¨sitella¨a¨n tarkemmin luvus- sa 11. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . _Mainittakoon, etta¨ esimerkkia¨ kirjoitettaessa ohjelmaan tahtoi aina ja¨a¨da¨ jokin virhetilan- ne, jota ei otettu huomioon. Listauksessa oleva kolmas (!) versio tuntuu toimivalta, mutta jos lo¨yda¨t viela¨ siita¨kin muistivuodon, ota yhteytta¨., 95 Luku 4 Olioiden rajapinnat None of us is ever exclusively a student or a teacher. We are always both at the same time, and must always think of ourselves as such. A teacher can get you started on com- puters, but no one can teach you everything you need to know. At a certain point, you must teach yourself through trial and error, looking over other people’s shoulders, futz- ing around, ﬁguring out what methods work best for you. The same is true in life. A teacher can guide you, but in the end, you have to make your own way. As you ﬁgure things out on your own, you share what you’ve learned with others. This is the Japanese idea of the sensei. We often see the word translated as “teacher” in English, but literally it means “one who has gone before”. In that regard, we are all senseis to somebody. Zen masters say, “When the student is ready, the teacher will appear.” This means two things: When you’re ready to learn, all things appear before you as teachers. And when you’re ready to teach, the teacher within you appears. The more you recognize the help you’ve received from those who gave ahead of you, the easier it becomes to give of yourself to those coming up behind. – Philip Toshio Sudo: Zen Computer [Sudo, 1999] 4.1. Rajapinnan suunnittelu 96 Kun edella¨ on ka¨sitelty modulaarisuutta ja olioita, on ka¨site “rajapin- ta” tullut esille moneen kertaan. Rajapinta on esitetty sina¨ olion osa- na, jonka olion ka¨ytta¨ja¨ “na¨kee” ja jota siis pa¨a¨see kutsumaan olion ul- kopuolelta. Rajapinta on komponentin se osa, jonka avulla ohjelmoi- jan tulisi pystya¨ ka¨ytta¨ma¨a¨n komponenttia ilman etta¨ tieta¨a¨ mita¨a¨n sen sisa¨isesta¨ toteutuksesta. Rajapinnasta ta¨ytyisi siis lo¨ytya¨ kaikki ka¨ytta¨miseen tarvittava informaatio. Ta¨ma¨ on jotain enemma¨n kuin kokoelma ja¨senfunktioita — rajapinnasta pita¨isi olla myo¨s ka¨ytto¨oh- je, joka kertoo miten sita¨ on tarkoitus ka¨ytta¨a¨ ja miten se ka¨ytta¨ytyy erilaisissa (myo¨s poikkeuksillisissa) tilanteissa. 4.1 Rajapinnan suunnittelu Rajapinta on muille ohjelmoijille kirjoitettu ka¨ytto¨ohje komponentis- tamme. Se vastaan kysymykseen “Miten ta¨ta¨ ka¨yteta¨a¨n?”. Rajapinnan taakse ka¨tketty toteutus ei ole rasittamassa ta¨ta¨ ka¨ytto¨ohjetta tai ha¨- ma¨a¨ma¨ssa¨ sen lukijaa. Rajapintojen ka¨ytto¨ ja toteutuksen ka¨tkenta¨ on yksi ehka¨ ta¨rkeim- mista¨ ohjelmistotuotannon perusperiaatteista, mutta silti sen ta¨r- keyden perustelu uraansa aloittelevalle ohjelmistoammattilaiselle on vaikeaa. Merkityksen tajuaa yleensa¨ itsesta¨a¨nselvyytena¨ sen ja¨lkeen, kun on osallistunut tekema¨a¨n niin isoa ohjelmistoa, ettei sen sisa¨ista¨ toteutusta pysty kerralla hallitsemaan ja ymma¨rta¨ma¨a¨n yksi ihminen. Na¨issa¨ tilanteissa komponenttijako helpottaa ratkaisevasti, kun yksit- ta¨isen ohjelmoijan ei tarvitse jatkuvasti miettia¨ kokonaisuutta, vaan ainoastaan omaa koodiaan ja niita¨ rajapintoja ulkopuolelle, joita ha¨n silla¨ hetkella¨ tarvitsee. David Parnas on muotoillut ta¨sta¨ komponenttien suunnittelusta kaksi sa¨a¨nto¨a¨, jotka nykya¨a¨n yleisesti tunnetaan Parnasin periaatteina (Parnas’s Principles) [Budd, 2002]: • Ohjelmakomponentin suunnittelijan tulee antaa komponentin ka¨ytta¨ja¨lle kaikki tarvittava tieto, jotta komponenttia pystyy ka¨ytta¨ma¨a¨n tehokkaasti hyva¨kseen, mutta ei mita¨a¨n muuta tie- toa (komponentista). • Ohjelmakomponentin suunnittelijalla tulee olla ka¨yto¨ssa¨a¨n kaikki tarvittava informaatio komponentille ma¨a¨ra¨ttyjen vastui- 4.1. Rajapinnan suunnittelu 97 den toteuttamiseksi, mutta ei mita¨a¨n muuta (ylima¨a¨ra¨ista¨) tie- toa. 4.1.1 Hyva¨n rajapinnan tunnusmerkkeja¨ Rajapintojen suunnittelusta on hyvin vaikeata antaa tarkkoja ohjei- ta, ja edella¨ mainittujen Parnasin periaatteiden noudattaminen johtaa melko nopeasti ristiriitatilanteisiin. Kokemus ja tieto lisa¨a¨va¨t kuiten- kin jokaisella ohjelmoijalla ka¨sitysta¨ siita¨, minka¨ tyyppisia¨ rajapin- toja on helppo, jopa mukava, ka¨ytta¨a¨, ja mitka¨ ovat hankalia. Na¨is- ta¨ kokemuksista kannattaa ottaa oppia ja pyrkia¨ itse aina tuottamaan rajapintoja, jotka ovat silla¨ mukavalla puolella. Ehka¨pa¨ osaksi ohjel- moinnin opetusta tulisi ottaa “hyvien ohjelmien ja rajapintojen kirjal- lisuus”, jossa tutustutaan yleisesti hyviksi todettuihin ohjelmistoihin ja komponenttikirjastoihin. Seuraava lista ei pyri olemaan ta¨ydellinen, vaan on tarkoitettu he- ra¨tta¨ma¨a¨n ajattelemaan niita¨ asioita, joita rajapintasuunnittelussa tu- lisi osata huomioida: • Noudata hyva¨ksi havaittuja toimintamalleja. Esimerkiksi oheis- laitteita kuvaavat rakenteet sisa¨lta¨va¨t usein seuraavat ka¨ytto¨vai- heet: avaaminen, ka¨ytto¨ ja sulkeminen — ta¨ma¨ on useille oh- jelmoijille tuttu ja luonteva toimintaketju. Vastaavasti jos kom- ponentti sisa¨lta¨a¨ paljon erilaisia tietoalkioita tai olioita, niiden luettelointi ja la¨pika¨ynti kannattaa toteuttaa kaikissa rajapin- noissa yhtena¨isella¨ tavalla. • Mieti komponentin ka¨ytto¨ alusta loppuun ulkopuolisen ka¨yt- ta¨ja¨n kannalta. Miten komponentti lo¨ytyy ja¨rjestelma¨ssa¨? Mis- sa¨ ja¨rjestyksessa¨ sen tarjoamia palveluita on tarkoitus ka¨ytta¨a¨? Ovatko jotkin palvelut ka¨ytetta¨vissa¨ vain osan aikaa? Dokumen- taatiossa oleva ka¨ytto¨tapaus “normaalika¨ytto¨ja¨rjestyksesta¨” voi helpottaa rajapinnan ymma¨rta¨mista¨. • Dokumentoi komponentin riippuvuudet. Ka¨ytta¨ja¨lle ei saa tulla ylla¨tyksena¨, etta¨ komponentti vaatii muita komponentteja toi- miakseen, tai etta¨ rajapinnan ka¨sittelemilla¨ olioilla pita¨a¨ olla jo- kin ominaisuus (esimerkiksi jokainen olio osaa tehda¨ itsesta¨a¨n kopion). 4.1. Rajapinnan suunnittelu 98 • Dokumentoi rajapintaan liittyva¨t elinkaaret ja omistusvastuut. Jos komponentin sisa¨lle annetaan rajapinnan avulla olio, tuhou- tuuko se komponentin toimesta (eli omistusvastuu siirtyy) vai ta¨ytyyko¨ rajapinnan ka¨ytta¨ja¨n edelleen huolehtia olion tuhoa- misesta? • Ma¨a¨rittele ja dokumentoi virhetilanteiden ka¨sittely. Pyrkiiko¨ komponentti ka¨sittelema¨a¨n huomaamansa virhetilanteet itse, vai va¨litta¨a¨ko¨ se tiedon niista¨ edelleen? Milla¨ mekanismilla vir- heista¨ ilmoitetaan? • Mieti rajapinnan operaatioiden nimea¨minen tarkkaan. Va¨lta¨ sa- noja, joilla komponentin ka¨ytto¨alueella voi olla useita merkityk- sia¨. Yleensa¨ a¨a¨neen lausuttavissa olevat nimet ovat helpoimmin ymma¨rretta¨via¨. Listalla esiintyy useaan kertaan “dokumentoi”. Ta¨ma¨ korostaa si- ta¨, etta¨ a¨a¨rimma¨isen harvoin rajapinnan luettelemat operaatiot (ja¨sen- funktiot) kertovat riitta¨va¨sti komponentista. Rajapintaoperaatioiden yhteyteen on liitetta¨va¨ kuvausta, joka kertoo tarkemmin rajapinnan ka¨yto¨sta¨ ja ka¨ytta¨ytymisesta¨ kokonaisuutena. 4.1.2 Erilaisia rajapintoja ohjelmoinnissa Olion rajapinnan ma¨a¨ra¨ytyminen ohjelmointikielen tasolla vaihte- lee ohjelmointikielesta¨ toiseen. Joissain oliokielissa¨ esimerkiksi ja¨- senmuuttujat eiva¨t koskaan na¨y kuin oliolle itselleen, mutta kaikki ja¨senfunktiot na¨kyva¨t ulospa¨in. Toisissa kielissa¨ (kuten C++:ssa) taas ohjelmoija pa¨a¨tta¨a¨ na¨kyvyydesta¨. Ohjelmassa saattaa tulla myo¨s tarve siihen, etta¨ yhdella¨ oliolla oli- si erilainen rajapinta riippuen siita¨, mika¨ ohjelman osa oliota ka¨ytta¨a¨. Erilaisia rajapintatarpeita voivat olla ainakin • luokan “tavallisen” ka¨ytta¨ja¨n na¨kema¨ rajapinta (yleisin, julki- nen rajapinta) • luokan sisa¨iseen toteutukseen tarvittava rajapinta • rajapinta, jonka olio na¨kee toisesta saman luokan oliosta • rajapinta, jonka kantaluokka tarjoaa aliluokalle (aliluku 6.3.1) 4.1. Rajapinnan suunnittelu 99 • toisiinsa kiintea¨sti liittyvien luokkien (esim. komponentin ja moduulin) toisilleen tarjoamat rajapinnat (aliluku 8.4) • rajapinta, jonka la¨pi ei voi muuttaa olion sisa¨lto¨a¨ (aliluku 4.3) • “rajapinta”, jonka la¨pi olioon voi vain viitata, mutta ei ka¨ytta¨a¨ (aliluku 4.4) • rajapintaluokat tai muut erilliset rajapinnat, jotka luokka lupaa toteuttaa (aliluku 6.9) • aikariippuva rajapinta, jossa vain osa rajapinnasta on ka¨ytetta¨- vissa¨ eri ajanhetkilla¨ (esimerkiksi suurin osa tiedostorajapinnan operaatioista on ka¨ytetta¨vissa¨ vasta sitten kuin ka¨sitelta¨va¨ tie- dosto on avattu) • toteutuksen riippuvuudet ma¨a¨ritteleva¨ rajapinta. Ta¨ma¨ taval- laan ka¨a¨nteinen rajapinta luettelee esimerkiksi ne palvelut, joita toteutus tarvitsee ka¨ytto¨ja¨rjestelma¨lta¨ ja kirjastoilta. Suurin osa oliokielista¨ tarjoaa mahdollisuuden ma¨a¨ra¨ta¨ vain osan ylla¨mainituista rajapinnoista, ja tavat joilla “erityisrajapintoja” on mahdollista ma¨a¨ritella¨, vaihtelevat suuresti kielesta¨ toiseen ja ovat enemma¨n tai va¨hemma¨n teenna¨isia¨. Ta¨ssa¨ teoksessa ka¨sitella¨a¨n la¨- hinna¨ C++-kielen tarjoamia mahdollisuuksia. Jotkin C++:n rajapintao- minaisuuksista ovat parempia kuin monissa muissa kielissa¨, toiset taas selva¨sti huonompia. 4.1.3 Rajapintadokumentaation tuottaminen ja ylla¨pito Rajapintojen dokumentaatio on yksi ta¨rkeimmista¨ ohjelmoijan tyo¨va¨- lineista¨, koska moduulien, kirjastojen ja luokkien oikeaa ka¨ytto¨a¨ ei yleensa¨ pysty pa¨a¨ttelema¨a¨n pelka¨sta¨ ohjelmakoodista. Dokumentaa- tiosta tulisi selkea¨sti ka¨yda¨ ilmi, miten rajapintaa on tarkoitus ka¨ytta¨a¨ ja mita¨ ehtoja sen ka¨yto¨lle asetetaan (na¨ita¨ ehtoja ka¨yda¨a¨n tarkemmin la¨pi aliluvussa 8.1, jossa puhutaan sopimussuunnittelusta, design by contract). Koska luokan tai moduulin ulkopuoliset ka¨ytta¨ja¨t nojautuvat etu- pa¨a¨ssa¨ rajapinnan dokumentaatioon, on eritta¨in ta¨rkea¨a¨ etta¨ ta¨ma¨ do- kumentaatio pa¨iviteta¨a¨n aina rajapinnan muuttuessa. Rajapintadoku- mentaatiota tarvitaan usein myo¨s suunnitteluvaiheen lisa¨ksi varsi- 4.1. Rajapinnan suunnittelu 100 naisessa koodausvaiheessa, kun rajapinnasta ta¨ytyy tarkastaa rajapin- nan ka¨yto¨n yksityiskohtia kuten parametrien tyyppeja¨ ja ja¨rjestysta¨. Ta¨llo¨in olisi ka¨teva¨a¨ etta¨ dokumentaatio na¨ytta¨isi rajapinnan myo¨s ohjelmointikielen tasolla. Na¨ista¨ tarpeista on syntynyt idea kirjoittaa ainakin osa rajapin- nan dokumentaatiosta ohjelman kooditiedostojen sisa¨a¨n komment- tien muodossa. Ma¨a¨ra¨muotoisista kommenteista voidaan sitten sopi- valla tyo¨kalulla tuottaa automaattisesti ihmiselle helppolukuinen ra- japintadokumentaatio. Ta¨ma¨ tietysti helpottaa dokumentaation ajan tasalla pita¨mista¨ suuresti, koska dokumentaatio voidaan helposti tuottaa uudelleen rajapinnan muuttuessa. Lisa¨ksi rajapintadokumen- taatiosta voidaan tuottaa nettiselaimella ka¨ytetta¨va¨ versio, jolloin sen linkkeja¨ seuraamalla pystyy helposti navigoimaan dokumentaation sisa¨lla¨. Java-kielessa¨ tyo¨kalu rajapintadokumentaation tuottamiseen on nimelta¨a¨n Javadoc [Sun Microsystems, 2005], ja se on integroitu osaksi Javan normaalia kehitysympa¨risto¨a¨. Muun muassa Javan omien kirjastojen rajapintadokumentaatiot on yleensa¨ tuotettu Javadocin avulla. Toinen laajalti ka¨ytetty rajapintojen dokumentaatiotyo¨kalu on ni- melta¨a¨n Doxygen [Doxygen, 2005]. Se on ilmainen open source -ohjelma, jonka tukema kielivalikoima on varsin laaja: C++, C, Java, Objective-C, IDL, seka¨ rajoitetusti PHP, C# ja D (tilanne keva¨a¨lla¨ 2005). Doxygen tekee myo¨s la¨hdekoodista rajapintadokumentaation tueksi osittaisia luokkakaavioita, riippuvuusgraafeja, ohjelmalistauk- sia. Kaikki na¨ma¨ voidaan tuottaa seka¨ perinteisina¨ dokumentteina et- ta¨ selaimella navigoitavassa muodossa. Vaikka rajapintadokumentaation ylla¨pita¨miseen ka¨ytetta¨isiinkin automaattisia tyo¨kaluja, eiva¨t tyo¨kalut kuitenkaan vapauta ohjelmoi- jaa rajapinnan suunnittelusta ja dokumentoinnista. Kaikki aliluvus- sa 4.1.1 mainitut suunnittelusa¨a¨nno¨t pa¨teva¨t riippumatta siita¨, miten rajapintadokumentaatio tuotetaan. Sen sijaan pelka¨sta¨ dokumentoi- mattomasta ohjelmakoodista kiireessa¨ tyo¨kalulla tuotettu “rajapinta- dokumentti” saattaa jopa antaa valheellisen kuvan siita¨, etta¨ rajapin- ta olisi kunnolla dokumentoitu, vaikka todellisuudessa tuotettu raja- pintadokumentaatio toistaakin vain pelka¨n ohjelmakoodirajapinnan hieman koreammassa muodossa. 4.2. C++: Na¨kyvyysma¨a¨reet 101 4.2 C++: Na¨kyvyysma¨a¨reet C++:ssa kieli itse ei pakota olion ja¨senten na¨kyvyytta¨ tiettyyn muot- tiin. Na¨kyvyyden sa¨a¨ta¨mista¨ varten kielessa¨ on avainsanat public, protected ja private. Luokan esittelyssa¨ na¨ma¨ avainsanat toimivat “otsikkoina”, jotka ma¨a¨ra¨a¨va¨t niiden ja¨lkeen tulevien esittelyjen na¨- kyvyyden. Perinteisesti na¨ma¨ ma¨a¨reet esiintyva¨t luokan esittelyssa¨ edella¨ mainitussa ja¨rjestyksessa¨, mutta ta¨ma¨ on vain tyyliseikka, ei kielen ma¨a¨ra¨ma¨ ja¨rjestys. Luokan esittely on tyypillisesti muotoa class Luokannimi { public: // Ta¨nne tulevat asiat na¨kyva¨t luokasta ulos protected: // Ta¨nne tulevat asiat na¨kyva¨t vain aliluokille private: // Ta¨nne tulevat asiat eiva¨t na¨y ulospa¨in }; Jos luokkaesittelyn alussa ei anneta minka¨a¨nlaista na¨kyvyysma¨a¨- retta¨, on oletuksena C++:ssa private, koska se on “tiukin” na¨kyvyys- ma¨a¨reista¨. Vaikka monet oppikirjat (esim. [Stroustrup, 1997]) ka¨ytta¨- va¨tkin ta¨ta¨ hyva¨kseen ja esitteleva¨t luokan sisa¨iset ja¨senmuuttujat en- simma¨isena¨ ilman mita¨a¨n na¨kyvyysma¨a¨retta¨, ei niiden antamaa esi- merkkia¨ kannata seurata. Luokan esittelyn tarkoituksena on kertoa luokan ka¨ytta¨ja¨lle, miten luokan olioita ka¨yteta¨a¨n, ja ta¨ta¨ varten ka¨yt- ta¨ja¨ tarvitsee luokan julkisen rajapinnan eli public-osan. Selkeyden vuoksi on siis syyta¨ kirjoittaa public-osa ensimma¨isena¨, jotta sita¨ ei tarvitse etsia¨ luokan sisa¨isen toteutuksen pera¨sta¨. Edellisen koodiesimerkin kommenteissa olevat “selitykset” eri na¨- kyvyysma¨a¨reiden merkityksesta¨ ovat vain ylimalkaisia. Alla seloste- taan na¨kyvyysma¨a¨reiden public ja private tarkka merkitys ja yri- teta¨a¨n antaa jonkinlainen kuva siita¨, miten niita¨ on tarkoitus ka¨yt- ta¨a¨. Ma¨a¨re protected liittyy olennaisesti periytymiseen ja ka¨sitella¨a¨n myo¨hemmin aliluvussa 6.3.1. Ilman periytymista¨ protected ka¨ytta¨y- tyy olennaisilta osin samoin kuin private. 4.2. C++: Na¨kyvyysma¨a¨reet 102 4.2.1 public Na¨kyvyysma¨a¨re public on ma¨a¨reista¨ yksinkertaisin siina¨ mielessa¨, ettei se rajoita ja¨senfunktioiden (ja -muuttujien) na¨kyvyytta¨ milla¨a¨n lailla, ja niita¨ voi ka¨ytta¨a¨ missa¨ tahansa ohjelman osassa. Na¨in luokan public-osa ma¨a¨ra¨a¨ luokan julkisen rajapinnan, jonka kautta luokan normaali ka¨ytto¨ on tarkoitettu tapahtuvaksi. Luokan julkisen rajapinnan suunnitteleminen on vaativa tehta¨va¨. Mika¨li rajapinnasta ja¨a¨ pois jotain oleellista — esimerkiksi ja¨senfunk- tio jonkin olennaisen asian tekemiseksi —, ei luokan ka¨ytta¨ja¨lla¨ ole mita¨a¨n mahdollisuutta korjata puutetta, koska luokan sisa¨iseen toteu- tukseen ei pa¨a¨se ka¨siksi. Toisaalta julkiseen rajapintaan ei kannata “varmuuden vuoksi” laittaa mita¨a¨n ylima¨a¨ra¨ista¨. Luokan suunnitte- lijan kannalta julkinen rajapinta on lupaus luokan tarjoamista pal- veluista, joten julkiseen rajapintaan laitettujen asioiden pita¨isi pysya¨ muuttumattomina — luokan ka¨ytta¨ja¨n koodihan riippuu julkisesta rajapinnasta. Na¨in luokan ylla¨pidon vuoksi rajapinta pita¨isi pyrkia¨ sa¨ilytta¨ma¨a¨n mahdollisimman yksinkertaisena. Ta¨ma¨ pyrkimys “mi- nimaaliseen mutta ta¨ydelliseen” rajapintaan onkin usein mainittu hy- va¨n rajapinnan tunnusmerkkina¨ [Meyers, 1998, Item 18]. Ja¨senmuuttujat on syyta¨ pita¨a¨ visusti poissa julkisesta rajapinnas- ta useistakin syista¨. Ensinna¨kin koko olioajattelun peruskivi on sisa¨i- sen toteutuksen ka¨tkeminen. Vaikka ja¨senmuuttuja olisikin luonteel- taan sellainen, etta¨ olion ka¨ytta¨ja¨n pita¨isi pa¨a¨sta¨ ka¨siksi siihen, ei sita¨ silti kannata laittaa julkiseen rajapintaan. Ta¨ha¨n on etupa¨a¨ssa¨ kaksi syyta¨: • Joskus myo¨hemmin luokkaa ylla¨pidetta¨essa¨ saattaa tulla tarve siirta¨a¨ kyseinen ja¨senmuuttuja jonnekin muualle, esimerkiksi osoittimen pa¨a¨ha¨n olion ulkopuolelle tai kenties korvata koko ja¨senmuuttuja jollain toisella rakenteella. Mika¨li ja¨senmuuttuja on julkisessa rajapinnassa, luokan ka¨ytta¨jien koodi riippuu siita¨ eika¨ sita¨ voi poistaa. • Olio itse ei saa mita¨a¨n tietoa siita¨, milloin julkisessa rajapinnas- sa olevasta ja¨senmuuttujasta luetaan tietoa tai milloin siihen si- joitetaan uusi arvo. Ta¨llo¨in olio ei voi mitenka¨a¨n reagoida esi- merkiksi ja¨senmuuttujan arvon vaihdoksiin tai siihen, etta¨ ja¨- senmuuttujan arvoa ylipa¨a¨ta¨a¨n on kysytty. 4.2. C++: Na¨kyvyysma¨a¨reet 103 Mika¨li jotakin ja¨senmuuttujaa tunnutaan tarvitsevan luokan jul- kisessa rajapinnassa, kannattaa ensin miettia¨ onko tarve todellinen. Yleensa¨ ja¨senmuuttujiin liittyva¨t palvelut tulisi toteuttaa itse luokan ja¨senfunktioissa, joten tarve julkiseen ja¨senmuuttujaan saattaa olla merkki siita¨, etta¨ luokan vastuualueeseen kuuluvia palveluita yrite- ta¨a¨n toteuttaa luokan ulkopuolella. Jos ja¨senmuuttujan arvoa todel- la kuitenkin tarvitaan, kannattaa lisa¨ta¨ julkiseen rajapintaan sopivat “aseta”- ja “anna”-ja¨senfunktiot (setter ja getter), joiden koodissa ja¨- senmuuttujaan sijoitetaan tai vastaavasti sen arvo luetaan. Na¨in on tehty esim. PieniPaivays-luokassa, josta lo¨ytyva¨t asetus- ja lukufunk- tiot pa¨iva¨lle, kuukaudelle ja vuodelle (listaus 2.1 sivulla 62). Jos julkiseen rajapintaan pyrkiva¨ ja¨senmuuttuja on olio, eiva¨t ase- tus- ja lukufunktiot yleensa¨ riita¨ kattamaan ja¨senmuuttujaolion ka¨yt- to¨tarvetta. Ta¨llo¨in on joskus ka¨teva¨a¨ kirjoittaa “anna”-ja¨senfunktio, joka palauttaa viitteen ja¨senmuuttujaan. Funktion paluuarvon avul- la pa¨a¨see ka¨siksi ja¨senmuuttujaolioon, mutta ja¨senmuuttujaa ei sil- ti tarvitse siirta¨a¨ public-puolelle. Listauksessa 4.1 seuraavalla sivul- la on esimerkki luokan Kirja ja¨senfunktiosta annaPalautusPvm, jonka kautta kirjan palautuspa¨iva¨ma¨a¨ra¨a¨n pa¨a¨see ka¨siksi. Ta¨llaisten funk- tioiden kautta olion ka¨ytta¨ja¨ pa¨a¨see edelleen muuttamaan ja¨senmuut- tujan arvoa olion huomaamatta. Ta¨ma¨ voidaan esta¨a¨ palauttamalla ja¨senfunktiosta vakioviite, jolloin ja¨senmuuttujaoliota ei voi viitteen la¨pi muuttaa (vakioviitteet ka¨sitella¨a¨n aliluvussa 4.3.3). 4.2.2 private Na¨kyvyysma¨a¨re private on C++:n ma¨a¨reista¨ kaikkein rajoittavin. Ja¨- senmuuttujiin ja ja¨senfunktioihin, jotka on esitelty private-osassa, pa¨a¨see ka¨siksi vain saman luokan ja¨senfunktioiden koodissa (ja ysta¨- va¨funktioissa ja -luokkissa, joista kerrotaan enemma¨n aliluvussa 8.4). Koska koko olioajattelun yksi la¨hto¨kohdista on ollut olion sisa¨isen to- teutuksen ka¨tkeminen, tulisi luokan ja¨senmuuttujien aina olla kap- seloituna luokan private-osaan. Luokan ja¨senfunktioiden toteutuksessa tulee usein tarve kirjoit- taa “apufunktioita”, joita kutsutaan useista eri ja¨senfunktioista. Na¨- ma¨ on ka¨teva¨a¨ kirjoittaa luokan private-puolelle “yksityisiksi” ja¨sen- funktioiksi. Ta¨llo¨in luokan omat ja¨senfunktiot pa¨a¨seva¨t kutsumaan niita¨, mutta apufunktiot eiva¨t silti kuulu luokan julkiseen rajapin- taan, joten niihin ei pa¨a¨se ka¨siksi luokan ulkopuolelta. 4.2. C++: Na¨kyvyysma¨a¨reet 104 1 class Kirja 2 { 3 public: ... 4 PieniPaivays& annaPalautusPvm(); 5 private: ... 6 PieniPaivays palautuspvm ; 7 }; ... 8 PieniPaivays& Kirja::annaPalautusPvm() 9 { 10 return palautuspvm ; 11 } ... 12 int aikaaJaljella(Kirja& kirja) 13 { 14 return kirja.annaPalautusPvm().paljonkoEdella(tanaan); 15 } LISTAUS 4.1: Ja¨senfunktio, joka palauttaa viitteen ja¨senmuuttujaan C++:ssa (ja Javassa) luokan private-osaan pa¨a¨see ka¨siksi olion omien ja¨senfunktioiden koodi. Lisa¨ksi myo¨s toiset saman luokan oliot voivat ka¨sitella¨ toistensa private-osia. Ka¨yta¨nno¨ssa¨ ta¨ma¨ tar- koittaa sita¨, etta¨ jos olion ja¨senfunktio saa ka¨ytto¨o¨nsa¨ toisen saman luokan olion esimerkiksi parametrina, se pa¨a¨see ka¨siksi myo¨s ta¨- ma¨n toisen olion private-osaan ja siis myo¨s ja¨senmuuttujiin. Luokan PieniPaivays ja¨senfunktio sijoitaPaivays on esimerkkina¨ ta¨sta¨ lis- tauksessa 4.2 seuraavalla sivulla. Siina¨ pa¨iva¨yksen ja¨senmuuttujien arvot sijoitetaan toisesta samantyyppisesta¨ oliosta. Pa¨a¨sy toisen olion private-osaan on tietylla¨ tavalla luontevaa, koska olion sisa¨isen toteutuksen piilottaminen toiselta saman luokan oliolta on sina¨nsa¨ turhaa. Joissain oliokielissa¨ kuten Smalltalkissa on kuitenkin otettu viela¨ tiukempi kanta, eika¨ toinen saman luokan olio pa¨a¨se ka¨siksi toisen olion ja¨senmuuttujiin. 4.3. C++: const ja vakio-oliot 105 1 void PieniPaivays::sijoitaPaivays(PieniPaivays& p) 2 { 3 paiva = p.paiva ; 4 kuukausi = p.kuukausi ; 5 vuosi = p.vuosi ; 6 } LISTAUS 4.2: Pa¨a¨sy toisen saman luokan olion private-osaan 4.3 C++: const ja vakio-oliot Monissa ohjelmointikielissa¨ on jo ammoisista ajoista la¨htien ollut mahdollisuus ma¨a¨ritella¨ muuttujien lisa¨ksi vakioita, jotka eroavat muuttujista siina¨, etta¨ niiden arvoa ei voi muuttaa. C-kieleen ta¨ma¨ mahdollisuus tuli ANSI-standardin mukana 80-luvulla, kun kieleen lisa¨ttiin avainsana const. C++:ssa const on otettu mukaan myo¨s olio- ominaisuuksiin, jossa sen ka¨ytto¨ on osoittautunut eritta¨in hyo¨dylli- seksi. 4.3.1 Perustyyppiset vakiot C-kielessa¨ perustyyppiset vakiot — la¨hinna¨ kokonaisluku- ja liukulu- kuvakiot — ma¨a¨riteltiin perinteisesti #define-esika¨a¨nta¨ja¨komennol- la. Syyna¨ ta¨ha¨n oli, etta¨ vaikka ANSI-standardi toikin kieleen const- ma¨a¨reen, sen toiminta oli tehotonta eika¨ sita¨ voinut ka¨ytta¨a¨ kaikissa tilanteissa. C++:ssa tilanne on korjattu, eika¨ #define-vakioita ole ena¨a¨ syyta¨ ka¨ytta¨a¨. Perustyyppinen vakio ma¨a¨ritella¨a¨n aivan kuten muuttuja, mutta tyypin yhteyteen lisa¨ta¨a¨n ma¨a¨re const: 1 int const MAX MJONON KOKO = 30000; 2 double const PI = 3.14159265; 3 int const RAJA = annaRaja(); 4 char mjono[MAX MJONON KOKO]; const-vakiot ka¨ytta¨ytyva¨t muuten kuin muuttujat, mutta niihin ei voi sijoittaa. Ta¨ma¨n lisa¨ksi kokonaislukuvakiota voi ka¨ytta¨a¨ myo¨s paikoissa, joissa ka¨a¨nta¨ja¨ vaatii ka¨a¨nno¨saikaisia vakioita, kuten esi- merkiksi taulukkojen ko’oissa (katso rivi 4 edellisessa¨ esimerkissa¨). 4.3. C++: const ja vakio-oliot 106 C:sta¨ poiketen vakioiden alustusarvon ei tarvitse olla ka¨a¨nno¨saikai- nen vaan se voi olla esim. funktion paluuarvo kuten rivilla¨ 3 — ta¨l- laista ei-ka¨a¨nno¨saikaista vakiota ei kylla¨ka¨a¨n sitten voi ka¨ytta¨a¨ esi- merkiksi taulukon kokoa ma¨a¨ra¨a¨ma¨a¨n. Kielen kannalta on aivan sama, onko sana const ennen tyypin nimea¨ vai sen ja¨lkeen. Aiemmin ka¨ytettiin yksinomaan tapaa, jossa const tulee ennen tyyppia¨ (const int i), ja ta¨ta¨ tapaa na¨kee edelleen- kin valtaosassa koodia. Tapa laittaa const-sana vasta tyypin nimen ja¨lkeen on kuitenkin C++:n kannalta loogisempi, ja sita¨ na¨kee ka¨ytet- ta¨va¨n yha¨ enemma¨n uudessa C++-koodissa. (Vastaavat muutkin tyypin ma¨a¨reet kuten osoitin-* ja viite-& tulevat vasta tyypin ja¨lkeen. Ta¨l- la¨ on merkitysta¨ kun ma¨a¨reita¨ on monta pera¨kka¨in.) Ta¨ssa¨ teoksessa on siirrytty ka¨ytta¨ma¨a¨n uutta ka¨yta¨nto¨a¨ vuoden 2005 painoksesta al- kaen. const-vakiot ovat normaalisti paikallisia siina¨ ka¨a¨nno¨syksiko¨ssa¨, jossa ne ma¨a¨ritella¨a¨n. Ka¨yta¨nno¨ssa¨ ta¨ma¨ tarkoittaa sita¨, etta¨ const- vakiot voi sijoittaa otsikkotiedostoihin, vaikka sinne ei normaalisti muuttujia laitetakaan. Usein olio-ohjelmoinnissa vakiot liittyva¨t kiintea¨sti jonkin tietyn luokan ka¨ytto¨o¨n. Ta¨llo¨in vakiot kannattaa kapseloida luokan sisa¨a¨n luokkavakioiksi, joista kerrotaan tarkemmin aliluvussa 8.2.2. 4.3.2 Vakio-oliot Olioista voi tehda¨ “vakio-olioita” aivan samaan tapaan kuin perus- tyypeista¨ luodaan vakioita — lisa¨ta¨a¨n olion ma¨a¨rittelyn ja¨lkeen sana const: Paivays const joulu(24,12,1999); Olioiden tapauksessa const-sanan vaikutus on kuitenkin moni- mutkaisempi asia. Perustyypeista¨ puhuttaessa const-sanan vaikutus on helppo selitta¨a¨ — muuttujaan ei yksinkertaisesti saa sijoittaa. Olioiden tapauksessa tilanne on paljon hankalampi. Sijoittaminen ei ole va¨ltta¨ma¨tta¨ mieleka¨s toimenpide kaikille olioille (olioiden sijoitta- mista ka¨sitella¨a¨n aliluvussa 7.2). Jos halutaan puhua “vakio-olioista”, onkin oleellista se, ettei ta¨llaisen vakio-olion tilaa pystyta¨ muutta- maan. Koska olion tila on kapseloitu olion sisa¨a¨n, voi olion tila muut- tua vain ja¨senfunktiokutsun seurauksena. 4.3. C++: const ja vakio-oliot 107 C++:ssa vakio-olioiden ka¨site on toteutettu jakamalla ja¨senfunktiot kahteen ryhma¨a¨n — niihin, jotka voivat muuttaa olion tilaa ja niihin, jotka eiva¨t. Ta¨ma¨n ja¨lkeen on ma¨a¨ra¨tty, etta¨ vakio-olioille saa kutsua vain niita¨ ja¨senfunktioita, jotka eiva¨t voi muuttaa olion tilaa. Saman asian voi myo¨s ajatella niin, etta¨ C++:ssa vakio-olioilla on erilainen rajapinta, joka on vain osajoukko luokan koko rajapinnasta. Ja¨senfunktiot, joiden ei haluta muuttavan olion tilaa, merkita¨a¨n ma¨a¨reella¨ const, joka tulee ja¨senfunktion parametrilistan pera¨a¨n seka¨ luokan esittelyssa¨ etta¨ ja¨senfunktion ma¨a¨rittelyssa¨. Listauksessa 4.3 on luokan Paivays esittely, jossa pa¨iva¨ysta¨ muuttamattomat ja¨sen- funktiot on merkitty vakioiksi. Listauksessa on myo¨s esimerkkina¨ yh- den ja¨senfunktion ma¨a¨rittely. Ka¨a¨nta¨ja¨ pita¨a¨ huolen siita¨, etta¨ const-sanan mukanaan tuomaa 1 class Paivays 2 { 3 public: 4 Paivays(unsigned int p, unsigned int k, unsigned int v); 5 ~Paivays(); 6 7 void asetaPaiva(unsigned int paiva); 8 void asetaKk(unsigned int kuukausi); 9 void asetaVuosi(unsigned int vuosi); 10 11 unsigned int annaPaiva() const; 12 unsigned int annaKk() const; 13 unsigned int annaVuosi() const; 14 15 void etene(int n); 16 int paljonkoEdella(Paivays const& p) const; 17 18 private: 19 unsigned int paiva ; 20 unsigned int kuukausi ; 21 unsigned int vuosi ; 22 }; . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ma¨a¨rittely . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1 unsigned int Paivays::annaPaiva() const 2 { 3 return paiva ; 4 } LISTAUS 4.3: Pa¨iva¨ysluokka const-sanoineen 4.3. C++: const ja vakio-oliot 108 “vakioisuutta” noudatetaan. Jo aiemmin on mainittu, etta¨ vakio- olioille voi kutsua vain vakioja¨senfunktioita. Ta¨ma¨n lisa¨ksi ka¨a¨nta¨- ja¨ pyrkii valvomaan, etta¨ vakioja¨senfunktioiden koodissa ei muuteta olion tilaa. Ta¨ma¨n se tekee asettamalla seuraavat rajoitukset vakioja¨- senfunktion koodille: • Vakioja¨senfunktion koodissa ei voi muuttaa ja¨senmuuttujien ar- voja (toisin sanoen ja¨senmuuttujat ka¨ytta¨ytyva¨t ika¨a¨n kuin ne olisi ma¨a¨ritelty const-ma¨a¨reella¨). • Vakioja¨senfunktion koodissa voi kutsua omalle oliolle vain toi- sia vakioja¨senfunktioita. Ta¨ma¨ rajoitus koskee siis vain ja¨sen- funktion omaan olioon kohdistuvia kutsuja. • Vakioja¨senfunktion koodissa osoitin this on tyyppia¨ “osoitin vakio-olioon” (katso seuraava aliluku). Edella¨ mainitut rajoitukset eiva¨t kuitenkaan riita¨ varmistamaan ta¨ydellisesti, ettei vakioja¨senfunktiossa olion tila muutu. Osa olion tilaan kuuluvasta tiedostahan saattaa nimitta¨in olla olion ulkopuolel- la esim. osoittimien pa¨a¨ssa¨. Ta¨llaiseen dataan eiva¨t ka¨a¨nta¨ja¨n tarkas- tukset ulotu, vaan ohjelmoijan on itsensa¨ pidetta¨va¨ huoli siita¨, ettei vakioja¨senfunktiossa muuteta mita¨a¨n sellaista, jonka katsotaan kuu- luvan olion tilaan. Joskus harvoin saattaa olla aihetta ma¨a¨ritella¨ ja¨senmuuttuja, jota voisi muuttaa myo¨s vakioja¨senfunktioissa. Ta¨llainen on perusteltua vain, jos ja¨senmuuttujan muuttaminen ei vaikuta olion “todelliseen” tilaan. Ta¨llaisia ja¨senmuuttujia on mahdollista saada aikaan lisa¨a¨ma¨l- la¨ niiden esittelyn eteen avainsana mutable. C++ antaa myo¨s mahdollisuuden siihen, etta¨ luokka tarjoaa kak- si samannimista¨ ja¨senfunktiota samoilla parametreilla, jos toinen on vakioja¨senfunktio ja toinen ei. Ta¨llaisessa tapauksessa ja¨senfunk- tion kutsuminen toimii niin, etta¨ vakio-olioille ja vakio-osoittimien ja -viitteiden la¨pi kutsutaan vakioja¨senfunktiota, muuten tavallista. Ta¨sta¨ erottelusta on joskus hyo¨tya¨, koska vakioja¨senfunktio voi esi- merkiksi palauttaa vakio-osoittimen olion dataan, tavallinen ja¨sen- funktio taas normaalin osoittimen. 4.3. C++: const ja vakio-oliot 109 4.3.3 Vakioviitteet ja -osoittimet Varsinaisia vakio-olioita tarvitaan olio-ohjelmoinnissa a¨a¨rimma¨isen harvoin, koska olio-ohjelmoinnin yksi perusajatuksista on, etta¨ olioi- den tila muuttuu niihin kohdistettujen toimintojen tuloksena. Vakio- olion ka¨site muuttuu kuitenkin eritta¨in ka¨ytto¨kelpoiseksi, kun ote- taan ka¨ytto¨o¨n vakioviitteet ja -osoittimet. Vakioviitteella¨ tarkoitetaan ta¨ssa¨ kirjassa viitetta¨, jonka la¨pi asiat na¨ytta¨va¨t vakioilta. Vastaavasti vakio-osoitin on osoitin, jota ka¨ytet- ta¨essa¨ sen pa¨a¨ssa¨ oleva asia vaikuttaa vakiolta. Termia¨ “vakio-osoi- tin” ei tule sekoittaa termiin “osoitinvakio”, jolla tarkoitetaan osoitin- ta, joka itse on vakio, ts. osoitinta ei voi muuttaa osoittamaan toiseen paikkaan. Huomaa, etta¨ kaikki alla esitetyt asiat pa¨teva¨t niin vakio- osoittimille kuin vakioviitteille, vaikka tekstissa¨ mainittaisiinkin vain toinen. Osoittimista ja viitteista¨ saadaan vakio-osoittimia ja -viitteita¨ li- sa¨a¨ma¨lla¨ niiden esittelyyn ma¨a¨re const: char const* mjono; Paivays const& p; Vakio-osoittimia ka¨ytetta¨essa¨ osoittimen pa¨a¨ssa¨ oleva olio tai data ka¨ytta¨ytyy ika¨a¨n kuin se olisi vakio — riippumatta siita¨, onko olio tai data alunperin ma¨a¨ritelty vakioksi. Ta¨ma¨ siis tarkoittaa sita¨, etta¨ vakio-osoittimen la¨pi sijoittaminen ja muiden kuin vakioja¨senfunk- tioiden kutsuminen on mahdotonta. Vakioviitteiden hyo¨ty tulee siita¨, etta¨ niiden avulla voidaan olios- ta na¨kyva¨ rajapinta rajata sellaiseksi, etta¨ olion muuttaminen vakio- viitteen kautta tulee mahdottomaksi. Jos esimerkiksi funktio ottaa pa- rametrikseen vakioviitteen pa¨iva¨ysolioon, voi funktion ka¨ytta¨ja¨ luot- taa siihen, etta¨ funktiolle va¨litetyn pa¨iva¨ysolion sisa¨lta¨ma¨ pa¨iva¨ys on varmasti sama myo¨s funktiokutsun ja¨lkeen. Vakioviitteiden ka¨ytto¨ on myo¨s tehokas “dokumentointikeino”, jolla voi kertoa, ettei tiettya¨ oliota tai dataa ole tarkoitus muuttaa. Erityisen tehokkaaksi ta¨ma¨n dokumentointikeinon tekee se, etta¨ ka¨a¨nta¨ja¨ takaa sen noudattami- sen. Esimerkiksi seuraava koodi ei mene ka¨a¨nta¨ja¨sta¨ la¨pi: void muutanKuitenkin(Paivays const& pvm) { pvm.asetaPaiva(1); // KA¨A¨NNO¨ SVIRHE: asetaPaiva ei vakiojf. } 4.3. C++: const ja vakio-oliot 110 C++ sisa¨lta¨a¨ automaattiset tyyppimuunnokset ei-vakio-osoittimista vakio-osoittimiksi (ja sama viitteille), mutta ei toiseen suuntaan: 1 char* mjono = \"Ka¨yta¨ string-luokkaa char*:n sijaan\"; 2 char const* vakiomjono = mjono; // Ok: ei-vakio ⇒vakio 3 char* mjono2 = vakiomjono; // KA¨A¨NNO¨ SVIRHE: vakio ⇒ei-vakio Na¨ma¨ kielen sa¨a¨nno¨t tarkoittavat ka¨yta¨nno¨ssa¨, etta¨ ei-vakio-olioita- kin voi ka¨sitella¨ ika¨a¨n kuin ne olisivat vakioita, mutta vakio-olioista ei milla¨a¨n saa tavallisia. Ta¨ma¨ on ja¨rkeva¨a¨, kun muistetaan, etta¨ va- kio-olion rajapinta on vain osajoukko normaaliolion rajapinnasta. Vakioviitteiden yleisin ka¨ytto¨kohde on epa¨ilema¨tta¨ funktioiden ja ja¨senfunktioiden parametrit. Mika¨li funktio ottaa parametrinaan viit- teen olioon, jota sen ei ole tarve muuttaa, tulisi parametriviitteen olla aina vakioviite. Sama pa¨tee tietysti myo¨s osoittimille. Osoittimia ka¨y- teta¨a¨n yleisesti myo¨s olioiden ja¨senmuuttujina yms. Ta¨llo¨in kannat- taa miettia¨, onko osoittimen la¨pi tarpeen muuttaa oliota. Jos vastaus on ei, kannattaa ka¨ytta¨a¨ vakio-osoitinta. Edella¨ on jo mainittu kaksi vakioviitteiden ja -osoittimien ta¨rke- a¨a¨ ka¨ytto¨syyta¨: ka¨ytto¨ dokumentointimielessa¨ rajapintaa rajaamaan ja ka¨a¨nta¨ja¨n tekema¨t tarkastukset siita¨, etta¨ vakiorajapintaa todella noudatetaan. Kolmas syy ka¨ytta¨a¨ vakioviitteita¨ esimerkiksi funktioi- den parametrina on niin yksinkertainen, etta¨ se unohtuu helposti: vakio-olion voi laittaa ainoastaan vakioviitteen tai vakio-osoittimen pa¨a¨ha¨n. Jos funktio ottaa parametrinaan tavallisen viitteen olioon, ei funk- tiolle voi antaa parametrina vakio-oliota, koska tavallisen viitteen kautta tulisi olion muuttaminen mahdolliseksi. Listauksessa 4.4 seu- raavalla sivulla on esimerkki tyypillisesta¨ tilanteesta, joka syntyy kun yhdesta¨ funktiosta unohtuu const-sana viitteen edesta¨ pois. Funktio saakoHameenKuukaudessa\u0017 on kirjoitettu oikeaoppisesti niin, etta¨ se ot- taa parametrinaan vakioviitteen pa¨iva¨ykseen — eiha¨n funktion ole tarkoitus muuttaa parametrina tullutta pa¨iva¨ysta¨. Funktion toteutuk- sessa kuitenkin kutsutaan toista funktiota onkoKarkauspaivaa, jonka kirjoittaja ei ole ka¨ytta¨nyt vakioviitetta¨ parametrina, vaikka karkaus- pa¨iva¨n testaamisen ei varmaankaan ole tarkoitus muuttaa testattavaa . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . \u0017Funktio testaa, onko annetusta pa¨iva¨sta¨ kuukauden sisa¨lla¨ mahdollisuus saada hamekan- gasta. (Karkauspa¨iva¨na¨ kosimisesta kielta¨ytyva¨n ta¨ytyy ostaa kosijalle hamekangas. Epa¨reilua niita¨ miehia¨ kohtaan, jotka eiva¨t ka¨yta¨ hametta.) 4.3. C++: const ja vakio-oliot 111 pa¨iva¨a¨. Ta¨ma¨ aiheuttaa ka¨a¨nno¨svirheen, kun vakioviitetta¨ yriteta¨a¨n antaa parametrina funktiolle, jonka ottama viite ei ole vakio. Toinen tyypillinen vakio-olioihin liittyva¨ virhe on, etta¨ luokkaa suunniteltaessa unohdetaan varustaa olion tilaa muuttamattomat ja¨- senfunktiot const-ma¨a¨reella¨. Ta¨llo¨in ka¨y niin, etta¨ vakioviitteiden kautta oliolle ei voi kutsua ainuttakaan ja¨senfunktiota! Aiemmin ta¨s- sa¨ luvussa ollut PieniPaivays-luokka (listaus 2.1 sivulla 62) on esi- merkki ta¨llaisesta virheellisesta¨ luokasta, jossa luokan suunnitteli- jan hutilointi esta¨a¨ luokan ka¨ytta¨jia¨ hyo¨dynta¨ma¨sta¨ kielen turvaomi- naisuuksia. Ta¨llaisten virhetilanteiden varalle C++-kielessa¨ on tyyppi- muunnos const cast, josta kerrotaan aliluvussa 7.4.1. Joskus hyvin harvoin on tarve saada itse osoittimesta vakio sen osoittaman olion sijaan — toisin sanoen halutaan, etta¨ osoitinta ei voi muuttaa osoittamaan toiseen paikkaan. Silloin puhutaan osoitin- vakiosta. C++:n syntaksi seuraa ta¨ssa¨ kohtaa ta¨ma¨n kirjan ka¨yta¨nto¨a¨ laittaa const aina tyypin ja¨lkeen. Osoittimen saa vakioksi lisa¨a¨ma¨lla¨ const-sanan osoitintyypin ta¨hden ja¨lkeen: char* const osoitinvakio = \"Loogista, eiko¨ totta\"; Vastaavasti osoittimen, jota ei saa muuttaa ja jonka osoittamaa dataa ei myo¨ska¨a¨n saa muuttaa, tyyppi on char const* const. 1 #include \"paivays.hh\" 2 3 bool onkoKarkauspaivaa(Paivays* pvm p); 4 5 bool saakoHameenKuukaudessa(Paivays const& nyt) 6 { // Karkauspa¨iva¨ on kuukauden sisa¨lla¨, jos on helmikuu ja 7 // vuoden sisa¨lle osuu karkauspa¨iva¨ 8 if (nyt.annaKk() != 2) 9 { 10 return false; // Ei helmikuu 11 } 12 else 13 { 14 return onkoKarkauspaivaa(&nyt); // KA¨A¨NNO¨ SVIRHE 15 } 16 } LISTAUS 4.4: Esimerkki virheesta¨, kun const-sana unohtuu 4.4. C++: Luokan ennakkoesittely 112 4.4 C++: Luokan ennakkoesittely Huolellisesta ohjelmiston rakenteen suunnittelusta huolimatta tulee vastaan tilanteita, joissa luokkien va¨linen assosiaatio on kaksisuun- tainen, eli kaksi luokkaa tarvitsee tietoa toisistaan vastuualueensa to- teuttamisessa. Esimerkiksi kirjastoja¨rjestelma¨ssa¨ lainaustapahtumaa mallintava luokka tarvitsee tiedon kohteena olevasta kirjasta ja kirja voi sisa¨lta¨a¨ tiedon mihin lainaukseen se liittyy. C++-ohjelmoinnin kannalta ta¨ma¨ tarkoittaa tilannetta, jossa mo- lempien luokkien tulisi tieta¨a¨ toistensa esittely ennen omaa toteu- tustaan (kuva 4.1). Ta¨llainen tilanne on mahdoton, koska siina¨ en- nen luokan Laina esittelya¨ (laina.hh) yriteta¨a¨n lukea sisa¨a¨n luokan KirjastonKirja esittely, jonka alussa luetaan luokan Laina esittely, jonka alussa. . . Ta¨llainen syklinen rakenne johtaa va¨ista¨ma¨tta¨ siihen, etta¨ ka¨a¨nta¨ja¨ antaa otsikkotiedostoja lukiessaan virheilmoituksen. Ratkaisu ta¨ha¨n “kumpi esitella¨a¨n ensin: muna vai kana?” -ongel- maan on esitella¨ toisesta luokasta vain tieto sen olemassaolosta (ei ko- konaista rakennetta). Ta¨llo¨in luokan koko esittelya¨ ei C++:ssa tarvitse lukea sisa¨a¨n ja edella¨ mainittu ongelma poistuu. Pelka¨n luokan ole- massaolon esittely tehda¨a¨n C++:ssa¨ ennakkoesittelylla¨ (forward decla- ration), jossa kerrotaan luokasta vain sen nimi: class Laina; Koska ennakkoesitellyn luokan sisa¨inen rakenne (muistinkulutus), rajapintafunktiot ja periytymissuhteet eiva¨t ole tiedossa, C++ sallii ka¨ytta¨a¨ ennakkoesiteltya¨ luokkaa vain sellaisissa paikoissa, joissa na¨i- ta¨ ominaisuuksia ei tarvita. Ka¨yta¨nno¨ssa¨ ennakkoesitellysta¨ luokasta VÄÄRIN! laina.hh kkirja.hh // tarvitaan \"KirjastonKirja\" #include \"kkirja.hh\" class Laina { // ... private: KirjastonKirja& kohde_; }; class KirjastonKirja { // ... private: Laina* kenella_; }; // käytetään luokkaa \"Laina\" #include \"laina.hh\" KUVA 4.1: Va¨a¨rin tehty luokkien keskina¨inen esittely (ei toimi) 4.4. C++: Luokan ennakkoesittely 113 voidaan tehda¨ ta¨ma¨n luokan olioihin osoittavia viitteita¨ ja osoittimia, mutta na¨iden la¨pi ei voi tehda¨ rajapintakutsuja. Samoin luokkaa voi ka¨ytta¨a¨ funktioiden ja ja¨senfunktioiden esittelyissa¨ parametrina ja pa- luuarvona. Luokan KirjastonKirja sisa¨lta¨va¨ssa¨ otsikkotiedostossa voidaan nyt vain ennakkoesitella¨ luokka Laina, jolloin sen koko esittelya¨ ei tarvitse lukea sisa¨a¨n eika¨ syklisyysongelmaa tule, kuten listaukses- ta 4.5 na¨hda¨a¨n. Luokan Laina otsikko- ja toteutustiedostossa taas voidaan ka¨yt- ta¨a¨ luokan KirjastonKirja esittelya¨ ilman ongelmia (kuva 4.2 sivul- la 115). Kuvaan on myo¨s merkitty numeroin ka¨a¨nno¨ksen eteneminen tiedostoa kkirja.cc ka¨a¨nnetta¨essa¨. 4.4.1 Ennakkoesittely kapseloinnissa Ennakkoesittely ei ole ainoastaan suunnittelun silmukoiden “oikomi- seen” tarkoitettu menetelma¨. Sen avulla voi halutessaan ja¨tta¨a¨ ker- tomatta rajapinnan ka¨ytta¨ja¨lle yksityiskohtia tietorakenteiden sisa¨l- lo¨ista¨. Jos rajapinta julkistaa luokasta tai struct-tietorakenteesta vain ennakkoesittelyn, rajapintaa ka¨ytta¨va¨ ohjelmoija pakotetaan ka¨sitte- lema¨a¨n esiteltya¨ rakennetta vain rajapintafunktioiden avulla. Vertaa oheista moduuliesimerkkia¨ (listaus 4.6 seuraavalla sivulla) vastaa- vaan esimerkkiin Modula-3:lla kirjoitettuna (aliluku 1.6.1 sivulla 51). Ennakkoesittelylla¨ voidaan myo¨s va¨henta¨a¨ #include-ka¨skyn kaut- ta syntyvia¨ tiedostojen va¨lisia¨ riippuvuuksia ka¨ytta¨ma¨lla¨ ennakko- esittelya¨ siella¨, missa¨ ta¨ydellisen luokkaesittelyn ka¨ytto¨ ei ole aivan va¨ltta¨ma¨to¨nta¨. Ta¨sta¨ ominaisuudesta on esimerkki aliluvussa 9.3.4. 1 // kkirja.hh 2 class Laina; // ennakkoesittely 3 4 class KirjastonKirja { ... 5 private: 6 Laina* kenella ; 7 }; LISTAUS 4.5: Esimerkki ennakkoesittelysta¨ 4.4. C++: Luokan ennakkoesittely 114 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ennakko-paivays.hh . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1 namespace Paivays { 2 // Ennakkoesittely pa¨iva¨yksia¨ kuvaavasta tietorakenteesta: 3 struct PVM; 4 // Palauttaa uuden pa¨iva¨yksen. HUOM! Tuhottava rutiinilla ‘‘Tuhoa()’’ 5 PVM* Luo( int paiva, int kuukausi, int vuosi ); 6 // Poistaa ka¨yto¨sta¨ rutiinilla ‘‘Luo()’’ ka¨ytto¨o¨notetun pa¨iva¨yksen 7 void Tuhoa( PVM* p ); 8 } . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ennakko-paivays.cc . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1 #include \"ennakko-paivays.hh\" 2 namespace Paivays { 3 // Pa¨iva¨ysten tietorakenne: 4 struct PVM { 5 inline PVM( int p, int k, int v ); 6 int p , k , v ; 7 }; 8 inline PVM::PVM( int p, int k, int v ) : p (p), k (k), v (v) {} 9 10 PVM* Luo( int paiva, int kuukausi, int vuosi ) 11 { 12 return new PVM( paiva, kuukausi, vuosi ); 13 } 14 15 void Tuhoa( PVM* p ) 16 { 17 delete p; 18 } 19 } . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ennakko-kaytto.cc . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1 #include \"ennakko-paivays.hh\" 2 3 void kaytto() { 4 Paivays::PVM* vappu = 0; 5 vappu = Paivays::Luo( 1, 5, 2001 ); ... 6 Paivays::Tuhoa( vappu ); 7 } LISTAUS 4.6: Ennakkoesittely kapseloinnissa 4.4. C++: Luokan ennakkoesittely 115 #include \"laina.hh\" // jäsenfunktioiden toteutus 2 3 4 käännös alkaa 1 käännös päättyy #include \"laina.hh\" #include \"kkirja.hh\" // jäsenfunktioiden toteutus kkirja.cc laina.cc kkirja.hh class KirjastonKirja { // ... }; #include \"kkirja.hh\" class Laina { // ... }; laina.hh // viitataan luokkaan \"Laina\" class Laina; 6 5 KUVA 4.2: Oikein tehty luokkien keskina¨inen esittely 116 Luku 5 Oliosuunnittelu Koska era¨a¨t suunnittelun piirteet eiva¨t ole analyyttisia¨ tai tieteellisia¨, suunnittelun opetus ei yleensa¨ sovi hyvin ar- vovaltaisiin teknisiin korkeakouluihin, joiden tyo¨ntekija¨t tuntevat olonsa kotoisammaksi opettaessaan matematiik- kaa ja tieteita¨ kuin kertoessaan, miten yksilo¨n fysikaali- sen maailman ilmio¨ita¨ koskevaa arvostelukykya¨, estetiik- kaa, luovuutta ja herkkyytta¨ kehiteta¨a¨n. Monet tekniikan professorit toivovat, etta¨ suunnittelu olisi tieteellisempa¨a¨. Itse asiassa tekniikan piirissa¨ on ajoittain liikkeita¨, joiden tavoitteena on tehda¨ suunnittelusta analyyttisempa¨a¨. Ne epa¨onnistuvat suunnittelun pehmea¨mpien osien suhteen. Ka¨yta¨nno¨ssa¨ suunnittelu on huomattavasti kriittisempa¨a¨ ja arvostetumpaa toimintaa kuin sen asema tekniikan kou- lutuksessa antaa ymma¨rta¨a¨. – Insino¨o¨rin maailma [Adams, 1991] Ohjelmistoma¨a¨rittely ja -suunnittelu on hyvin laaja-alainen alue, jo- hon on olemassa erilaisia teorioita, menetelmia¨ ja kansanperinnet- ta¨. Kokonaiskuvan alueesta saa esim. teoksesta “Ohjelmistotuotanto” [Haikala ja Ma¨rija¨rvi, 2002]. Kannattaa myo¨s pita¨a¨ aina mielessa¨, etta¨ ohjelmistojen valmistaminen ei ole ainoastaan tietojenka¨sittelya¨ — ohjelmiston “sieluna” olevan toimintojen ﬁlosoﬁan ma¨a¨rittelee aina ihminen ja la¨hes aina ihmisia¨ varten [Roszak, 1992]. Ta¨ssa¨ luvussa esitella¨a¨n muutamia oliosuunnitteluun sopivia yleisia¨ periaatteita ja kuvaustapoja. 5.1. Oliosuunnittelua ohjaavat ominaisuudet 117 5.1 Oliosuunnittelua ohjaavat ominaisuudet Ohjelmiston jako moduuleihin ja luokkiin on ratkaisu, jota ohjaavat monet tekija¨t: ongelman ratkaisu, ka¨ytetyt tyo¨kalut, menetelma¨t, ko- kemus, “talon perinteet”, reunaehdot, jatkokehityksen suunnitelmat jne. Luokkahuone-esimerkkeja¨ laajemmissa ohjelmistoissa ei koskaan ole olemassa vain yhta¨ ainoata oikeata tapaa tehda¨ moduuli- ja luok- kajakoa. Seuraavat aliluvut esitteleva¨t niita¨ asioita, joita oliosuunnit- telussa tulisi osata ottaa huomioon [Booch, 1987]. 5.1.1 Mita¨ suunnittelu on? Kaikessa insino¨o¨rityo¨ssa¨ suunnittelulla pyrita¨a¨n lo¨yta¨ma¨a¨n ratkai- su johonkin ongelmaan. Ratkaisun “rakennepiirustusten” avulla voi- daan valmistaa haluttu tuote. Suunnittelu on reitti ongelman kuvauk- sen ja ma¨a¨rittelyn seka¨ lopullisen tuotteen va¨lilla¨. Suunnittelun tarkoituksena on saada aikaan ja¨rjestelma¨, joka • toteuttaa ongelman (toiminnallisen) kuvauksen • voidaan toteuttaa ka¨yto¨ssa¨ olevilla raaka-aineilla ja resursseilla • sopii implisiittisiin ja eksplisiittisiin resurssirajoihin [Booch, 1991, s. 20] • erityisesti ohjelmistojen tapauksessa lopputuloksen tulisi olla varautunut jatkokehitykseen ja ylla¨pitoon (esimerkiksi laaduk- kaan dokumentaation avulla). Varsinkin ja¨rjestelma¨n “piilo-oletusten” kaivaminen esille osaksi suunnitelmaa on yksi vaikeimmista suunnittelutyo¨n osista. Ohjelmistosuunnittelu on termi, joka voidaan ma¨a¨ritella¨ tarkoit- tamaan ohjelmiston vaatimusten ja toiminnallisuuden suunnittelua menetelma¨lla¨, jossa lopputuloksena saadaan mahdollisimman hel- posti toteutettava (ohjelmoitava) suunnitelma (esimerkiksi suunnit- teludokumentti). Oliosuunnittelussa ta¨ma¨ tarkoittaa sita¨, etta¨ jo suun- nitteluvaiheessa on otettava huomioon valitun menetelma¨n (olioiden) ominaisuudet ja pyritta¨va¨ tekema¨a¨n suunnitelma, joka tukee olioiden ka¨ytto¨a¨ toteutusvaiheessa. Ohjelmistotuotteiden tapauksessa lopputulos on a¨a¨rimma¨isen harvoin kerralla valmis (ja heti pera¨a¨n unohdettu) kokonaisuus vaan 5.1. Oliosuunnittelua ohjaavat ominaisuudet 118 tuotteen julkistuksen ja¨lkeen sita¨ kehiteta¨a¨n eteenpa¨in lisa¨a¨ma¨lla¨ ominaisuuksia ja (valitettavan usein) korjaamalla aikaisempiin ver- sioihin ja¨a¨neita¨ virheita¨. 5.1.2 Abstraktio ja tiedon ka¨tkenta¨ Oliosuunnittelun ta¨rkeimpia¨ tyo¨kaluja on jo aikaisemmin esitelty moduulien esittelyn yhteydessa¨ (katso luku 1.3.3). Modulaarisuus on ohjelmiston jakoa paremmin hallittaviin kokonaisuuksiin abstrahoin- nin ja tiedon ka¨tkenna¨n avulla. Suunnitteluvaiheessa ta¨ma¨ tarkoittaa ohjelmiston jakamista paloihin joko osittavalla tai kokoavalla jaotte- lulla. • Osittava (top-down). Haetaan ja¨rjestelma¨n suurimmat kokonai- suudet, jotka usein ovat toiminnallisia, kuten ka¨ytto¨liittyma¨, tietokanta, syo¨tto¨ ja tulostus, tietoliikenne ja rajapinnat muihin ohjelmiin. Na¨ita¨ osakokonaisuuksia jaetaan taas vuorostaan pie- nempiin paloihin, joista lopulta muodostuu moduuleja ja luok- kia. • Kokoava (bottom-up). Jos suunnittelun alussa tunnetaan par- haiten joidenkin osaja¨rjestelmien toiminta, niin moduulisuun- nittelu voidaan aloittaa niista¨ ja myo¨hemmin kera¨ta¨ na¨ita¨ osia suuremmiksi kokonaisuuksiksi. Ka¨yta¨nno¨n suunnittelutyo¨ ei tietenka¨a¨n seuraa pelka¨sta¨a¨n jompaa kumpaa na¨ista¨ tavoista vaan on niiden yhdistelma¨. Jaottelun yhtey- dessa¨ tunnistetaan ohjelmiston staattista rakennetta, josta tulee mo- duulijako ja dynaamisia rakenteita (olioita), jotka kuvataan luokki- na. Koska luokka pystyy ilmaisemaan kaikki moduulin ta¨rkeimma¨t ominaisuudet, useat oliosuunnittelumenetelma¨t ka¨ytta¨va¨t suunnitte- luvaiheessa vain luokkia. 5.1.3 Osien va¨liset yhteydet ja lokaalisuusperiaate Jotta ohjelmiston moduulien va¨linen kommunikaatio ei muistuttaisi spagettikoodin aikaisia sotkuja, suunnitteluvaiheessa on pyritta¨va¨ pi- ta¨ma¨a¨n moduulien va¨liset viittaukset mahdollisimman pienina¨. Mo- duulin A katsotaan viittaavan moduuliin B, kun A tarvitsee jotain 5.1. Oliosuunnittelua ohjaavat ominaisuudet 119 palvelua B:n julkisesta rajapinnasta. Kokoavassa suunnittelussa pyri- ta¨a¨n kera¨a¨ma¨a¨n kaikki la¨heisesti toisiinsa kuuluvat moduulit samaan ylemma¨n tason kokonaisuuteen (esimerkiksi kaikki ajanlaskuun ja kalenteriin liittyva¨t moduulit). Ta¨ma¨ vahvasti kytkeytyneiden mo- duulien paketoiminen uuden pelkistetymma¨n rajapinnan taakse on esimerkki lokaalisuuden sa¨ilytta¨misesta¨ suunnittelussa (lokaalisuus- periaate). Kuvassa 5.1 on erilaisia riippuvuusvaihtoehtoja moduulien va¨lil- la¨. Jos alija¨rjestelma¨ssa¨ on n moduulia, niiden va¨lilla¨ on va¨hinta¨a¨n n −1 riippuvuutta (jokainen osa tieta¨a¨ kokonaisuuden julkisen ra- japinnan, joka on yksi osa). Maksimissaan kaikki moduulit tieta¨va¨t kaikkien muiden olemassaolosta, jolloin riippuvuuksien lukuma¨a¨ra¨ on n(n−1) 2 [Meyer, 1997]. Lokaalisuusperiaate pyrkii minimoimaan ohjelmakomponenttien va¨lisia¨ yhteyksia¨ ja na¨in pita¨ma¨a¨n kokonaisuuden kompleksisuutta paremmin hallinnassa. Osien va¨listen riippuvuuksien lukuma¨a¨ra¨n va¨henemisen lisa¨ksi ohjelmiston rakenne yksinkertaistuu, jos riip- puvuudet pideta¨a¨n aina mahdollisuuksien mukaan yksisuuntaisina. Pa¨iva¨ysmoduulin osat eiva¨t tieda¨ (eiva¨tka¨ va¨lita¨ siita¨), etta¨ niita¨ ka¨y- teta¨a¨n suuremman kokonaisuuden osina. Jos riippuvuudet muodos- tavat syklisia¨ silmukkaviittauksia, mutkistuu rakenteiden toteuttami- nen: jos A tarvitsee B:ta¨ ja B tarvitsee A:ta, niin kuinka A voidaan toteuttaa ennen kuin on esitelty millainen on B, jota taas ei voida to- teuttaa ennen kuin A:n toteutus on valmis jne. (C++:n ratkaisu ta¨ha¨n “muna-kana” -ongelmaan on esitetty aliluvussa 4.4.) Kuvassa 5.2 seuraavalla sivulla on esimerkkeja¨ erilaisista yksi- suuntaisista riippuvuuksista, joista yhdessa¨ riippuvuudet muodosta- vat silmukan. KUVA 5.1: Erilaisia yhteysvaihtoehtoja kuuden moduulin va¨lilla¨ 5.1. Oliosuunnittelua ohjaavat ominaisuudet 120 Syklinen KUVA 5.2: Moduulien va¨lisia¨ yksisuuntaisia riippuvuuksia 5.1.4 Laatu Laadukkaisiin suunnitelmiin, moduuleihin ja luokkiin kuuluvia omi- naisuuksia on helppo luetella ([Booch, 1987], [Liberty, 1998], [Meyer, 1997]), mutta ka¨yta¨nno¨ssa¨ vaikea toteuttaa kaikilta osiltaan: • Oikeellisuus. Ohjelmiston komponenttien (moduuli, luokka tai koko ohjelmisto) tulee toteuttaa kaikki toiminnot suunnitelman ma¨a¨rittelema¨lla¨ tavalla. • Virheita¨ sieta¨va¨. Komponenttien tulee varautua virhetilantei- siin (va¨a¨ra¨ syo¨te, muistin loppuminen jne.) ja osata toimia vir- hetilanteen havaitessaan etuka¨teen suunnitellulla tavalla (ro- bustness). • Selkeys. Ohjelmiston toiminnallisten yksiko¨iden rajapintojen dokumentointi ja toteutus on tehta¨va¨ yhtena¨isella¨ ja selkea¨lla¨ menetelma¨lla¨, jotta niiden muuttaminen ja uudelleenka¨ytto¨ oli- si helppoa. • Etuka¨teissuunnittelu. Moduulien ja luokkien suunnittelussa tulee pyrkia¨ ottamaan huomioon kohdeprojektin tarpeiden li- sa¨ksi yleisempia¨ na¨ko¨kohtia, jotta syntyva¨ komponentti olisi suuremmalla todenna¨ko¨isyydella¨ myo¨s ka¨ytto¨kelpoinen osa tu- levia projekteja. • Tehokkuus. Komponenttien tulee ka¨ytta¨a¨ tuhlailematta hyva¨k- seen ja¨rjestelma¨n resursseja (prosessointiaika ja muisti). 5.2. Oliosuunnittelun aloittaminen 121 • Siirretta¨vyys. Komponenttien suunnittelussa ja toteutuksessa tulisi ottaa huomioon, etta¨ osia mahdollisesti ka¨yteta¨a¨n tulevai- suudessa niiden kehitysympa¨risto¨sta¨ poikkeavassa ympa¨risto¨s- sa¨. • Elinkaaren huomioiva. Laadukkaan ohjelmiston rakenteen ja dokumentaation tulisi tukea jatkokehitysta¨, jota kaikille “todel- lisille” ohjelmistoille tullaan jossain vaiheessa tekema¨a¨n. “Rational design process and how to fake it” Ta¨ma¨ David Parnasin [Clements ja Parnas, 1986] artikkelin otsikko kuvaa ka¨yta¨nno¨n ohjelmistotyo¨n ristiriitaa usein hyvinkin ylevien suunnitteluperiaatteiden kanssa. Na¨ita¨ ristiriitoja on useita. Mika¨ on oikein toimiva ohjelma? (Asiakkaan ja ma¨a¨rittelija¨n kanta ei va¨ltta¨ma¨tta¨ ole aina sama.) Mi- hin kaikkiin mahdollisiin (ja mahdottomalta tuntuviin) virhetilantei- siin ohjelmiston tulisi varautua — ja miten? Kun aikataulut, kiire ja stressi painavat pa¨a¨lle, niin kuka jaksaa ajatella ohjelmiston tulevia ylla¨pita¨jia¨ ja koodin selkeytta¨? Vaikka ka¨yta¨nno¨n tyo¨ aina va¨lilla¨ onkin kaukana akatemian yle- vista¨ pa¨a¨ma¨a¨rista¨, niin harva silti va¨itta¨a¨, etta¨ huolellisesta ohjelmis- tojen suunnittelusta tulisi luopua kokonaan. Vaikka joissain osissa joudutaankin joustamaan, niin jo pita¨ma¨lla¨ aina mielessa¨a¨n yleva¨m- pia¨ pa¨a¨ma¨a¨ria¨ pystyy varmasti pita¨ma¨a¨n laatunsa “aloittelijan spaget- tiha¨kkyra¨a¨” parempana.\u0017 5.2 Oliosuunnittelun aloittaminen Alku aina hankalaa. Ensimma¨isena¨ ta¨ytyy pita¨a¨ mielessa¨ kaikki ko- konaisuuteen liittyva¨t asiat. Oliosuunnittelun pa¨a¨vaiheet ovat (luet- telossa tarkoitetaan komponentilla ohjelmiston moduuleja ja luokkia) [Booch, 1987] 1. komponenttien tunnistaminen 2. komponenttien vastuualueiden ma¨a¨rittely . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . \u0017Kun joku keksii ideointiin, suunnitteluun, ohjelmointiin ja ihmisten johtamiseen menetel- ma¨n, jolla saa ylivoimaista laatua ta¨ysin aikataulujen ja budjetin mukaisesti, niin pyyda¨mme no¨yra¨sti kertomaan siita¨ meille ja muullekin maailmalle. 5.2. Oliosuunnittelun aloittaminen 122 3. komponenttien va¨listen suhteiden ma¨a¨rittely (keskina¨inen na¨- kyvyys) 4. komponenttien rajapintojen ma¨a¨rittely (esimerkiksi formaalis- ti matemaattisella notaatiolla, mahdollisimman yksika¨sitteises- ti ohjelmointikielen rakenteilla tai sanallisella kuvauksella) 5. viimeisena¨ vaiheena edella¨ ma¨a¨riteltyjen luokkien ja moduu- lien seka¨ niiden muodostaman kokonaisuuden (=ohjelmisto) toteutus. Luettelo on hyva¨ esimerkki sa¨a¨nno¨ista¨, joissa ka¨yta¨nno¨n suunnit- telutyo¨ssa¨ ei edeta¨ yksi kohta kerrallaan — “Ta¨ma¨ sa¨a¨nto¨ mietita¨a¨n nyt loppuun ennen kuin jatketaan seuraavaan vaiheeseen”. Ta¨rkea¨ osa suunnittelua on ideointi, jota ei kannata rajoittaa vain yhteen osa- alueeseen kerrallaan. Kun johonkin kohtaan sopiva idea tulee mie- leen, niin se kannattaa kirjoittaa muistiin ja luottaa siihen, etta¨ myo¨- hemmin suunnittelun viimeistelyssa¨ mahdolliset turhat tai mahdot- tomat ideat karsiutuvat pois. 5.2.1 Luokan vastuualue Jokaisesta olio-ohjelmassa olevasta luokasta pita¨isi olla olemassa sel- kea¨ ja kattava kuvaus. Yhdella¨ sanalla ilmaistuna on ma¨a¨ritelta¨va¨ luo- kan vastuualue [Budd, 2002]. Vastuualue ma¨a¨rittelee sen, mita¨ ky- seisen luokan olioiden on tarkoitus mallintaa ohjelmistossa. Vastuu- alueeseen kuuluvat luokan tarjoamat palvelut (jotka ovat ka¨ytetta¨vis- sa¨ luokan julkisen rajapinnan kautta) seka¨ luokkaan kuuluvan olion toiminnan ymma¨rta¨misen kannalta oleellinen tilatieto eli attribuutit. Ka¨yta¨nno¨n luokissa tilatietoon kuuluu usein muutakin kuin yksitta¨i- sia¨ ohjelmointikielen muuttujia. Luokka voi muun ohella omistaa toi- sia olioita (esim. Pa¨iva¨ys voi sisa¨ltya¨ jonkin toisen luokan tilaan). Erityisesti kannattaa ottaa huomioon, etta¨ suunnitteluvaiheessa on tarkoitus kirjata vain “julkisen” toiminnallisuuden ymma¨rta¨misen kannalta oleellista informaatiota. Esimerkiksi na¨ytto¨laitteella olevaa graﬁikkapistetta¨ kuvaava luokka sisa¨lta¨a¨ suunnitteluvaiheessa attri- buutin “sijainti”, ja ta¨ta¨ tietoa voidaan ka¨sitella¨ rajapinnan tarjoamien palveluiden avulla. Suunnitteluvaiheessa ei ole oleellista ma¨a¨ritella¨ sita¨, onko ja¨rjestelma¨ssa¨ sijainti-informaation toteutus x- ja y-koordi- naatit kokonaislukuina vaiko esimerkiksi polaarikoordinaatiston kul- 5.2. Oliosuunnittelun aloittaminen 123 ma- ja sa¨detiedot. Ta¨ma¨ sijaintiattribuutin toteutus on luokan sisa¨i- nen asia, ja siihen liittyva¨t pa¨a¨to¨kset tulisi tehda¨ luokan toteutusvai- heessa. (Edelleen ka¨yta¨nno¨n tyo¨ssa¨ on usein hyo¨dyllista¨ miettia¨ myo¨s toteutukseen liittyvia¨ asioita suunnittelun aikana, mutta niiden ei tu- lisi pa¨a¨sta¨ hallitsemaan ja sotkemaan ylemma¨n tason suunnittelua.) 5.2.2 Kuinka lo¨yta¨a¨ luokkia? Helpommin sanottu kuin tehty. Useat oliomenetelma¨t la¨hteva¨t ongel- man kuvauksesta. Ohjelmistosuunnittelun ka¨ynnistyessa¨ pita¨isi ol- la olemassa va¨hinta¨a¨n toiminnallinen kuvaus siita¨, mita¨ ollaan teke- ma¨ssa¨ (sama pa¨tee kaikkeen insino¨o¨ritoimintaan [Adams, 1991]). Ta¨- ma¨n niin sanotun ma¨a¨rittelyvaiheen dokumentista etsita¨a¨n (vaikka- pa alleviivaamalla) substantiivit, joista sitten kenties tehda¨a¨n olioita. Yksi mahdollinen la¨hto¨kohta ta¨lle “oliometsa¨stykselle” on ka¨ytto¨ta- paukset (use case), [Jacobson ja muut, 1994], joissa on pyritty kuvaa- maan yksitta¨inen ohjelmiston osa ka¨ytta¨ja¨roolin (actor) na¨ko¨kulmas- ta. Ka¨ytto¨tapaus pyrkii kuvaamaan lyhyesti, mutta mahdollisimman kattavasti, yksitta¨iset ohjelmiston toimintaan liittyva¨t osat. (Na¨ytta¨- va¨t keskena¨a¨n ristiriitaisilta vaatimuksilta — ja ovatkin sita¨!) Na¨ista¨ “na¨ytellyista¨” ohjelman ta¨rkeimmista¨ toiminnoista muodostuu ma¨a¨- rittely koko ohjelmiston toiminnallisista vaatimuksista. Kuvassa 5.3 seuraavalla sivulla on esimerkki kirjaston tietoja¨rjestelma¨n yhdesta¨ ka¨ytto¨tapauksesta. Substantiivien hakumenetelma¨n hyva¨na¨ puolena on, etta¨ mallin- nettavan ongelman alueelta otetaan sopivan kokoisia kokonaisuuk- sia ohjelman rakenteeseen. Toisaalta ohjelma tarvitsee luultavasti toimiakseen muitakin osia kuin ne, jotka lo¨ytyva¨t suoraan ongel- man kuvauksesta. Emme voi siis olla varmoja, etta¨ saamme laaduk- kaan oliosuunnitelman vain pelka¨n ma¨a¨rittelyn substantiivien avul- la. Esimerkkina¨ ka¨ytta¨ma¨mme kirjaston toiminnallisuuden kuvauk- sessa esiintyy varmasti tietoa lainausajoista ja muista vastaavista ajankohdista ja ajanjaksoista. Pelka¨lla¨ substantiivihaulla emme sil- ti va¨ltta¨ma¨tta¨ keksi, etta¨ saatamme tarvita pa¨iva¨yspalveluita varten oman olion tai moduulin ohjelmistoomme. 5.2. Oliosuunnittelun aloittaminen 124 Nimi: Kirjan lainaaminen, versio 1.0 / Jyke Suorittajat: Asiakas itsepalvelupa¨a¨tteella¨ tai virkailija Esiehdot: Lainattavan kirjan ja lainaajan tiedot ovat ja¨r- jestelma¨ssa¨, lainauksen suorittaja on kerrottu ja¨rjestelma¨lle. Kuvaus: Asiakas tai virkailija syo¨tta¨a¨ kirjan tiedot ja¨r- jestelma¨a¨n joko viivakoodin lukijalla tai na¨p- pa¨insyo¨tteella¨ (ISBN-tunniste tai muu yksi- selitteinen tieto). Ja¨rjestelma¨ kirjaa lainaus- tapahtuman asiakkaan lainaustietoihin. Lai- naustapahtuman pa¨a¨ttyminen kerrotaan lai- nauksen suorittajalle na¨ytto¨laitteen avulla. Poikkeukset: Esiehdot eiva¨t ole kunnossa, lainaaja on lai- nauskiellossa tai kirja on merkitty varatuksi muualle. Lopputulos: Kirja on merkitty lainatuksi asiakkaalle ma¨a¨- ra¨ajaksi. Muut vaatimukset: Pa¨iva¨ssa¨ pystytta¨va¨ ka¨sittelema¨a¨n 50 000 lai- naustapahtumaa, onnistuneen lainauksen on kirjauduttava alle kahdessa sekunnissa, vir- hetilanteissa ka¨ytta¨ja¨lle on annettava selva¨t toimintaohjeet jatkotoimenpiteista¨. KUVA 5.3: Esimerkki ka¨ytto¨tapauksesta 5.2.3 CRC-kortti CRC-korttimenetelma¨ on “korttipeli”, jossa yhdisteta¨a¨n komponent- tien vastuualueiden ja niiden va¨listen suhteiden miettiminen. Jokai- sesta potentiaalisesta komponentista kirjoitetaan kortti, jossa on mer- kittyna¨ seuraavat asiat: komponentin (olio tai moduuli) nimi (Com- ponent), kuvaus komponentin vastuualueesta (Responsibility) ja lis- ta niista¨ komponenteista (julkisista rajapinnoista), joita ta¨ma¨ kompo- nentti toimiakseen tarvitsee (Collaborators). Kun komponenttia mietitta¨essa¨ tulee mieleen uusi mahdollinen komponentti, tehda¨a¨n siita¨ heti kortti. Kun kortin jo omaavasta kom- ponentista tulee mieleen sille kuuluva toiminnallisuus, kirjataan ta¨- 5.2. Oliosuunnittelun aloittaminen 125 ma¨ vastuu kortille. Jos vastuun toteuttamiseen tarvitaan selva¨sti toi- sen komponentin apua, niin ta¨ma¨n yhteistyo¨kumppanin nimi kirja- taan myo¨s muistiin. Ta¨ta¨ kuka-mita¨ -syklia¨ toistamalla saadaan kir- jatuksi hyvin luontevasti eri ja¨rjestyksessa¨ tulleita ideoita heti muis- tiin. Kuvassa 5.4 on esimerkki CRC-kortista. [Beck ja Cunningham, 1989] CRC-kortteja voidaan hyo¨dynta¨a¨ myo¨s ka¨ytto¨tapausten kanssa: ka¨ytto¨tapaus ka¨yda¨a¨n la¨vitse korttien avulla ja tarkastetaan, onko ole- massa tarvittavat kortit ja niiden yhteistyo¨kumppanit, joilla ka¨ytto¨ta- pauksen toiminnallisuus saadaan aikaan (ja jos kaikkia ei ole, niin samalla saadaan mietityksi uusia kortteja) [Wilkinson, 1995]. Vastaa- vasti ta¨sta¨ korttien “pyo¨ritta¨misesta¨” voidaan saada aikaan hyo¨dylli- sia¨ aikaisemmin huomaamatta ja¨a¨neita¨ ka¨ytto¨tapauksia. Kun kompo- nentteja on kera¨tty tarpeeksi, niiden rakennetta ja keskina¨isia¨ suh- teita voidaan kuvata tarkemmin seuraavassa aliluvussa esitelta¨va¨lla¨ graaﬁsella kuvaustavalla. KUVA 5.4: Kuva keskenera¨isen CRC-korttipelin yhdesta¨ kortista 5.3. Oliosuunnitelman graaﬁnen kuvaus 126 CRC-korttien haittapuolia ovat huono skaalautuvuus ja ylla¨pidon vaikeus. Suurissa ohjelmistoissa tulisi “korttipeli”-ideoinnin tasosta riippuen satoja tai jopa tuhansia kortteja, joiden hallinta menee jos- sain vaiheessa varmasti mahdottomaksi. Ka¨yta¨nno¨n suunnittelun ai- kana tulee usein myo¨s tarve palata tarkentamaan ja korjaamaan suun- nittelun aikaisempaa vaihetta, jolloin CRC-korttien ylla¨pidosta tulee ongelma. Na¨iden ongelmien hallitsemiseksi on kehitetty tietokoneel- la ka¨sitelta¨via¨ oliosuunnitelman graaﬁsia kuvausmenetelmia¨, joista tutustumme tarkemmin UML:a¨a¨n aliluvussa 5.3. 5.2.4 Luokka, attribuutti vai operaatio? Oliosuunnittelussa tulee (varsinkin aloittelevalle suunnittelijalle) usein vastaan tilanne, jossa ei ole aivan varma siita¨, onko ka¨sitelta¨va¨ asia luokka vaiko “vain” jonkin luokan attribuuttiominaisuus. Ta¨ha¨n- ka¨a¨n suunnittelun osaan ei ole olemassa viisastenkivea¨, mutta muu- tamia ohjenuoria kylla¨kin. Toiminnalliset osat, kuten “siirta¨minen”, “kasvattaminen” ja “mo- nistaminen” on melko helppo ymma¨rta¨a¨ olioiden toimintoina eli nii- den luokkien operaatioina. Ominaisuudet kuvaavat jotain jo olemas- sa olevaa, joten ne sopisivat luokkien attribuuteiksi: “va¨ri”, “sijain- ti”, “koko”, “ika¨” jne. Kaikki ne osat, joilla on ominaisuuksia ja joi- ta ka¨yteta¨a¨n operaatioiden avulla, ovat olioita (suunnittelun luokkia). [Koskimies, 2000] 5.3 Oliosuunnitelman graaﬁnen kuvaus Uniﬁed Modelling Language (UML) [OMG, 2002b] on nykyisin eniten huomiota saanut oliosuunnitelmien graaﬁnen kuvausnotaatio. UML on Object Management Groupin vuonna 1997 hyva¨ksyma¨ standardi, jonka viimeisin versio (8.6.1999) on 1.3 [OMG, 2002a]. Seuraavien alilukujen on tarkoitus antaa UML:sta sellainen yleis- kuvaus, etta¨ myo¨hemmissa¨ luvuissa sita¨ hyo¨dynta¨en piirretyt kaa- viot olisivat ymma¨rretta¨via¨. Kattava kuvaus UML:sta lo¨ytyy teoksista “The Uniﬁed Modeling Language Reference Manual” [Rumbaugh ja muut, 1999] ja “The Uniﬁed Modeling Language User Guide” [Booch ja muut, 1999]. 5.3. Oliosuunnitelman graaﬁnen kuvaus 127 On ta¨rkea¨ta¨ huomata, etta¨ UML on vain kuvaustapa. Se ei ole me- netelma¨, joka ottaisi kantaa siihen, miten luokkia ja niiden va¨lisia¨ suhteita tulisi suunnitella, mika¨ olisi hyva¨ rajapinta tai miten ylipa¨a¨- ta¨a¨n ohjelmisto pita¨isi jakaa osiin. UML sijoittuu na¨in ohjelmointi- kielten ja suunnittelumenetelmien va¨limaastoon. 5.3.1 UML:n historiaa UML on kehittynyt useista 90-luvulla syntyneista¨ olioiden kuvaus- ja suunnittelumenetelmista¨ yhdista¨en niiden ominaisuuksia. UML:n ta¨rkeimma¨t edelta¨ja¨t ovat • James Rumbaughin Object Modeling Technique, OMT [Rumbaugh ja muut, 1991] • Grady Boochin ka¨ytta¨ma¨ esitystapa, joka tunnetaan tekija¨n mu- kaan Boochin notaationa [Booch, 1991] • Ivar Jacobsonin ka¨ytto¨tapaukset [Jacobson ja muut, 1994]. UML:n suunnittelijat ovat asettaneet sille muun muassa seuraavia pa¨a¨ma¨a¨ria¨ [OMG, 2002b]: • UML tarjoaa valmiiksi ma¨a¨ritellyt visuaaliset rakenteet (ulkoa- su ja ka¨ytto¨tarkoitus), joilla suunnittelija voi yhtena¨ista¨a¨ ohjel- mistosuunnitelmien “rakennepiirustuksia”. • UML sisa¨lta¨a¨ laajennus- ja erikoistamismekanismit, joilla perus- notaatiota pystyta¨a¨n soveltamaan myo¨s erityistilanteissa. • UML on riippumaton ohjelmointikielista¨ ja ohjelmiston tuotan- toprosessista. • UML pyrkii tukemaan graaﬁsen suunnitelman automaattista (koneellista) ka¨sittelya¨. UML:n avulla tehdyt ohjelmiston kuvaukset jakautuvat kahteen pa¨a¨osaan: pysyva¨a¨ (staattista) perusrakennetta kuvaaviin luokkakaa- vioihin (ohjelmiston luokat, rajapinnat ja oliot seka¨ niiden va¨liset suhteet) ja ajoaikaista ka¨ytta¨ytymista¨ kuvaaviin (dynaamisiin) kaa- vioihin. Seuraavissa aliluvuissa esitella¨a¨n UML:n perusnotaatioita esimerkeilla¨. 5.3. Oliosuunnitelman graaﬁnen kuvaus 128 5.3.2 Luokat, oliot ja rajapinnat UML:ssa¨ luokka kuvataan laatikkona, jossa luetellaan luokan ominai- suudet, joista ta¨rkeimma¨t ovat luokan nimi, attribuutit, julkisen ra- japinnan palvelut ja kuvaus luokan vastuualueesta. Kukin na¨ista¨ on luokkalaatikossa omassa lokerossaan. Yleensa¨ na¨kyville piirreta¨a¨n ai- nakin kolme lokeroa (nimi, attribuutit ja palvelut). Jos jokin na¨ista¨ kolmesta osasta on kuvatun asian kannalta turha, niin lokero ja¨teta¨a¨n tyhja¨ksi. Luokasta tehty olio on kuvaustavaltaan muuten samanlainen, mutta laatikon yla¨reunassa on alleviivattuna olion nimi (identiteet- ti) ja tyyppi eli olion luokan nimi. Myo¨s oliolla olevien attribuuttien arvot voidaan merkita¨ na¨kyviin. Usein yksi toiminnallinen rajapinta halutaan kuvata omana yk- sikko¨na¨a¨n. Ta¨llainen julkisen rajapinnan esittelyn sisa¨lta¨va¨ laatikko merkita¨a¨n UML:ssa¨ sanalla interface. Useampi kuin yksi luokka pys- tyy lupaamaan, etta¨ se toteuttaa jonkin rajapinnan ma¨a¨rittelema¨n toi- minnallisuuden (ja yksi luokka voi toteuttaa useita rajapintoja). Kuvassa 5.5 seuraavalla sivulla on esimerkki luokasta, oliosta ja rajapinnasta. Luokka kuvaa yksinkertaista graﬁikkapistetta¨, ja siita¨ on tehty olio, jolle on annettu nimi p1. Luokassa on ma¨a¨ra¨tty pisteen vas- tuulle tieto siita¨, missa¨ kohdassa na¨ytto¨laitetta se sijaitsee, attribuutil- la “sijainti”. Oliosta na¨hda¨a¨n, etta¨ ta¨ma¨ attribuutti on toteutettu olion sisa¨lla¨ olevilla koordinaattimuuttujilla x ja y. “Na¨kyvyys” on rajapin- ta, joka ma¨a¨rittelee mita¨ operaatioita na¨kyvyytta¨ tukeville olioille voi suorittaa. UML ma¨a¨rittelee luokan alkioille myo¨s erilaisia na¨kyvyysma¨a¨rei- ta¨ ja muita niiden kaltaisia ohjelmointikielten rakenteita, kuten ja¨sen- funktioiden paluuarvoja ja parametreja. Na¨ma¨ ominaisuudet kertovat jo melko tarkalla tasolla miten luokka halutaan toteuttaa valitulla oh- jelmointikielella¨. Na¨ita¨ lisa¨ma¨a¨reita¨ ka¨yteta¨a¨n myo¨s hyva¨ksi tietoko- neavusteiseen ohjelmakoodin tuottamiseen UML-suunnitelmista. Ohjelmiston rakentamisen alkuvaiheessa (ma¨a¨rittely, ylimma¨n ta- son suunnitelmat ja analyysit) luokkien pita¨isi sisa¨lta¨a¨ ainoastaan luokan vastuualueen ymma¨rta¨misen kannalta oleelliset asiat (att- ribuutit, palvelut ja vastuualueen kuvaus). Toteutukseen liittyvien asioiden ei pita¨isi olla sotkemassa ylimma¨n tason suunnitteludoku- mentteja ja luokkakaavioita. 5.3. Oliosuunnitelman graaﬁnen kuvaus 129 sijainti lue_sijainti() siirrä( uusi_sijainti ) Piste sijainti Edustaa yhtä grafiikkanäytöllä sijaitsevaa pistettä. (a) Luokka p1 : Piste pair<int,int> lue_sijainti() siirrä( pair<int,int> ) sijainti = ( 7, 9 ) (b) Olio piirrä() piilota() Näkyvyys «interface» (c) Rajapinta KUVA 5.5: Luokka, olio ja rajapinta Isoissa ohjelmistoissa ylimma¨n tason abstrakteja suunnitteluku- vauksia usein tarkennetaan la¨hemma¨ksi toteutusta ja ta¨llo¨in myo¨s luokkien ja rajapintojen tarkempi kuvaus on usein ka¨yto¨ssa¨. Kuvas- sa 5.6 seuraavalla sivulla on esimerkki aikaisemmasta pisteluokas- ta, jolle on ma¨a¨ritelty kaksi ja¨senmuuttujaa (private-na¨kyvyydella¨) ja julkisen rajapinnan (public) ja¨senfunktioita. Lisa¨ksi yksi ja¨sen- funktioista on ja¨tetty ainoastaan luokasta periytettyjen osien ka¨ytto¨o¨n (protected). Kuvassa on myo¨s taulukko erilaisista UML:n ma¨a¨rittelemista¨ na¨- kyvyystasoista. Koska na¨kyvyydet liittyva¨t la¨heisesti ohjelmointikie- liin, niita¨ ei kaikissa toteutuskielissa¨ ole suoraan ka¨ytetta¨vissa¨. Esi- merkiksi pakkauksen na¨kyvyystaso on Java:ssa helposti suoraan ka¨y- tetta¨vissa¨ oleva ominaisuus, mutta C++:ssa se toteutetaan ysta¨va¨-omi- naisuuden avulla (aliluku 8.4.1 sivulla 259). UML luokkasuunnittelussa pideta¨a¨n yleisena¨ rajanvetona, etta¨ kaaviot sisa¨lta¨va¨t ne asiat, jotka kertovat luokalta vaaditut asiat (vas- tuualue). Koska toteutus kuitenkin tehda¨a¨n erikseen ohjelmointikie- lella¨, niin on melko turhaa laittaa suunnittelukaavioon asioita jotka ovat vain ohjelmointia tukevia tai ovat ainoastaan toteutusvaihee- seen liittyvia¨ asioita. Yksi esimerkki ta¨llaisesta UML -kaavion “tur- hasta” tavarasta (scaffolding code) ovat ja¨senmuuttujille halutut aseta ja anna -ja¨senfunktiot (aliluku 4.2 sivulla 101). Toteutuksen kannal- 5.3. Oliosuunnitelman graaﬁnen kuvaus 130 + bool siirrä( int dx, int dy ) + pair<int,int> lue_sijainti() Piste − x : int − y : int # aseta_xy( pair<int,int> ) (a) Toteutusluokka Na¨kyvyys Merkinta¨ Ka¨ytetta¨vissa¨ public + kaikki ohjelmiston oliot voivat ka¨ytta¨a¨ protected # luokan oliot ja periytettyjen luokkien oliot private − vain samaa luokkaa olevat oliot package ∼ samassa pakkauksessa olevien luok- kien oliot (b) Na¨kyvyysma¨a¨reita¨ KUVA 5.6: Tarkennetun suunnitelman luokka ja na¨kyvyysma¨a¨reita¨ ta kyseiset ja¨senfunktiot voivat olla oleellisia, mutta niiden merkinta¨ luokkakaavioon ei tuo mita¨a¨n lisa¨informaatiota siita¨ mika¨ on luokan vastuualue. 5.3.3 Luokkien va¨liset yhteydet Suunniteltaessa ohjelmiston luokkia on ta¨rkea¨ta¨ pystya¨ merkitse- ma¨a¨n myo¨s luokkien keskina¨isia¨ yhteyksia¨, riippuvuuksia ja niiden kokonaisuuksien toimintoja. Koska ohjelmisto on ma¨a¨ra¨ koota kes- kena¨a¨n kommunikoivista olioista, tarvitsemme tavan kuvata na¨iden olioiden ja niiden luokkien va¨lisia¨ yhteyksia¨. UML:n erilaisia yhteys- tapoja on koottuna kuvassa 5.7 seuraavalla sivulla. 5.3. Oliosuunnitelman graaﬁnen kuvaus 131 Yhteys Tarkenne Selitys Symboli Riippuvuus Luokka tai moduuli viittaa kohteeseen. Assosiaatio Luokat tai oliot tarvitsevat toisiaan vastuualueensa toteutuksessa. Yksisuuntainen Vain toinen assosiaation pa¨ista¨ tieta¨a¨ assosiaatiosta (kuka ⇒keta¨ -suhde kerrotaan nuolella). Muodostuminen Olioiden elinkaaret on tiukasti sidottu toisiinsa. Koostuminen Olioiden elinkaaret liittyva¨t toisiinsa. Periytyminen Laajennussuhde. Toteuttaminen Luokka toteuttaa erikseen ma¨a¨ritellyn rajapinnan. Rajapintanimi KUVA 5.7: UML:n yhteystyyppeja¨ Riippuvuus Heikoin yhteys on tieto siita¨, etta¨ luokka A tarvitsee luokan B olioi- ta (esimerkiksi rajapintafunktioidensa parametrina). Ta¨ma¨ ilmaistaan katkoviivalla varustetulla nuolella, joka osoittaa luokasta A luokkaan B. Luokan A sanotaan riippuvan (depend) luokasta B. Kuvassa 5.8 seuraavalla sivulla luokka Heippa vaatii (riippuvuudella ilmaistuna), etta¨ luokka java.awt.Graphics on sen ka¨ytetta¨vissa¨ (vertaa Java-esi- merkkiin aliluvussa 1.6.2). 5.3. Oliosuunnitelman graaﬁnen kuvaus 132 Heippa java.awt.Graphics KUVA 5.8: Luokka “Heippa” ka¨ytta¨a¨ Javan graﬁikkaolioita Assosiaatiot UML:n assosiaatio (association) on kahden luokan va¨lille piirretty viiva, jonka avulla kerrotaan luokkien liittyva¨n toisiinsa ohjelmis- ton rakenteen kannalta. Assosiaatio on riippuvuutta vahvempi omi- naisuus (luokat ka¨ytta¨va¨t toistensa rajapintapalveluita osana oman vastuualueensa toteuttamista). Assosiaatioon merkita¨a¨n usein tar- kennuksia kuvaamaan sen ominaisuuksia. Ta¨rkeimma¨t tarkennuk- set ovat assosiaation nimi, lukuma¨a¨ra¨suhteet (multiplicity) ja luok- kien “roolinimet” (role). Yleisimmin ka¨ytettyja¨ lukuma¨a¨ra¨n ilmaisuja UML:n yhteyksissa¨ on esitetty kuvassa 5.9. Kuvassa 5.10 seuraavalla sivulla on kolme esimerkkia¨ assosiaa- tioista. Graﬁikkapisteita¨ ja na¨ytto¨laitetta mallintavien luokkien va¨lilla¨ on merkitty tarkennuksena lukuma¨a¨ra¨suhteet. Kunkin luokan olioita koskeva lukuma¨a¨ra¨tieto luetaan assosiaatioviivan toisesta pa¨a¨sta¨, eli kuvassa on seuraavat lukuma¨a¨ra¨suhteet: • “Na¨ytto¨laitteeseen liittyy yksi tai useampi piste” (siis na¨yto¨lla¨ on aina va¨hinta¨a¨n yksi piste). Merkinta¨ Selitys Jos lukuma¨a¨ra¨a¨ ei ilmoiteta, se voi olla mika¨ tahansa n Tasan n kappaletta ∗ Useita (nolla tai useampia) 0..1 Vaihtoehto (nolla tai yksi kappale) m..n Lukuma¨a¨ra¨ (m ≤olioiden lukuma¨a¨ra¨ ≤n) 1..2, 7, 8..10 Useita lukuma¨a¨ra¨va¨leja¨ KUVA 5.9: UML-assosiaatioiden lukuma¨a¨ra¨merkinto¨ja¨ 5.3. Oliosuunnitelman graaﬁnen kuvaus 133 • “Jokaiseen pisteeseen liittyy aina yksi na¨ytto¨laite” (siis jokainen pisteolio tieta¨a¨ milla¨ na¨yto¨lla¨ se sijaitsee). Luokkien Yritys ja Henkilo¨ va¨lilla¨ on “palkkaussuhteeksi” nimetty assosiaatio. Ta¨ssa¨ tapauksessa Yritys on tyo¨nantajan roolissa ja Hen- kilo¨ tyo¨ntekija¨na¨. Usein luokkien va¨linen yhteys on olemassa vain toiseen suun- taan (lokaalisuusperiaate) eli vain toisen luokan tarvitsee tieta¨a¨ toisen olemassaolosta. Oletuksena UML-assosiaation katsotaan olevan kak- sisuuntainen (kumpikin luokka tieta¨a¨ yhteyden olemassaolosta). Jos yhteys on yksisuuntainen, se ilmoitetaan piirta¨ma¨lla¨ nuoli kohti sita¨ luokkaa, joka ei tieda¨ tai va¨lita¨ assosiaation olemassaolosta (esimerk- kikuvassa lainausja¨rjestelma¨ ka¨sittelee kirjaston kirjoja, mutta niita¨ mallintavat oliot eiva¨t tieda¨ Lainausja¨rjestelma¨-luokan olemassaolos- ta). C++: Yksisuuntaiset suhteet pystyta¨a¨n toteuttamaan C++:lla hyvin suoraviivaisesti: toista luokkaa tarvitseva osa ottaa na¨kyville kohde- luokan esittelyn #include-komennolla kohdeluokan otsikkotiedostos- ta ja viittaus ka¨ytetta¨viin olioihin talletetaan osoitin- tai viiteja¨sen- muuttujaan (listaus 5.1 seuraavalla sivulla). Esimerkissa¨ on valittu talletustietorakenteeksi vektori, koska ma¨a¨rittelyn (kuva 5.10) mu- kaisesti emme etuka¨teen tieda¨ lukuma¨a¨ra¨n yla¨rajaa (∗). Vaihtoehdon (0..1) yleisin toteutus on osoitinja¨senmuuttuja kohdeolioon, joka voi olla arvossa nolla, kun kohdetta ei ole olemassa. Kun lukuma¨a¨ra¨ on tasan n kappaletta, lukuma¨a¨ra¨tieto kannattaa kertoa vektorin raken- tajan kutsun yhteydessa¨: 1..* 1 Näyttölaite Piste Lainausjärjestelmä Kirjastonkirja * Yritys Henkilö Työnantaja Palkkaa Työntekijä KUVA 5.10: Esimerkki luokkien va¨lisista¨ assosiaatioista 5.3. Oliosuunnitelman graaﬁnen kuvaus 134 1 #ifndef LAINAUSJARJESTELMA HH 2 #define LAINAUSJARJESTELMA HH 3 4 // Lainauksissa ka¨sitella¨a¨n kirjastonkirjoja 5 #include \"kirjastonkirja.hh\" 6 class LainausJarjestelma 7 { ... 8 private: 9 vector<KirjastonKirja*> PikkuKirjastonTietokanta ; 10 }; 11 #endif LISTAUS 5.1: Lainausja¨rjestelma¨n esittely, lainausjarjestelma.hh 1 LainausJarjestelma::LainausJarjestelma() : 2 PikkuKirjastonTietokanta ( n ) 3 { ... Kun luokkien va¨linen assosiaatio on kaksisuuntainen, joudutaan ka¨ytta¨ma¨a¨n aliluvussa 4.4 selostettua ennakkoesittelymekanismia. Koosteet Kooste (aggregate) on assosiaation erikoistapaus, jonka avulla ma¨a¨ri- tella¨a¨n luokkien va¨lisen yhteyden lisa¨ksi niiden va¨lille omistussuh- de. UML ma¨a¨rittelee kaksi koostetyyppia¨: kokoonpanollinen kooste (muodostuminen, composite aggregate) ja jaettu kooste (koostuminen, shared aggregate). Koosteen symboli on UML:ssa¨ vinonelio¨ (“salmiakki”). Salmiakki on assosiaatioviivan pa¨a¨ssa¨ kiinni siina¨ luokassa, jonka osana toisen luokan olio on. Kuvassa 5.11 seuraavalla sivulla on esimerkki kooste- suhteista. Umpinaisella mustalla salmiakilla on merkitty, etta¨ kirjas- ton kirja muodostuu yhdesta¨ pa¨iva¨ysoliosta (kokoonpanollinen koos- tuminen). Kun ta¨ma¨ pa¨iva¨ysolio on liitetty isa¨nta¨luokkaansa, vastuu sen elinkaaresta on sidottu isa¨nta¨luokkaan. Kun kirjastonkirjaolio tu- houtuu, samalla tuhoutuu myo¨s sen omistama pa¨iva¨ysolio. Ta¨ma¨n 5.3. Oliosuunnitelman graaﬁnen kuvaus 135 elinkaarisuhteen vuoksi ma¨a¨ritella¨a¨n myo¨s, etta¨ yksi olio saa olla muodostumissuhteessa vain yhteen paikkaan. Jaetussa koosteessa (koostuminen) olio voi kuulua useaan isa¨nta¨- luokkaan ja olion elinkaari ei va¨ltta¨ma¨tta¨ pa¨a¨ty yhta¨aikaa emoluokan kanssa. Esimerkkikuvassa jokainen kirjaston kirja koostuu kustanta- jan tiedot sisa¨lta¨va¨sta¨ erillisesta¨ oliosta. Sama kustantajaolio voi olla osana useamman kirjan tietoja. C++: Koosteet toteutetaan C++:ssa ja¨senmuuttujien avulla. Jos koos- tumissuhteen kohde voi muuttua olion elinkaaren aikana (jaettu kooste), toteutuksessa ka¨yteta¨a¨n osoitinja¨senmuuttujaa. Kun koos- teoliolle halutaan ta¨sma¨lleen sama elinkaari kuin emolle, kannattaa C++:ssa ka¨ytta¨a¨ olioja¨senmuuttujaa. Milloin ka¨ytta¨a¨ koostetta? UML ei tee ohjelmistosuunnittelijan tehta¨va¨a¨ helpoksi ma¨a¨rittelema¨l- la¨ tarkkoja sa¨a¨nto¨ja¨ siita¨, milloin mita¨kin yhteytta¨ tulisi ka¨ytta¨a¨ (ta¨ma¨ on tietoinen valinta, silla¨ UML ei ota kantaa ka¨ytettyyn suunnittelu- menetelma¨a¨n). Mielesta¨mme useimmissa tapauksissa ohje yhteyden “lajin” va- lintaan on selkea¨: Jos luokka A tarvitsee tiukasti koko elinkaaren- sa ajan luokkaa B, kyseessa¨ on kooste, muutoin assosiaatio. Miten sitten ma¨a¨ritella¨a¨n “tiukasti”? Kun koosteen kohteella ei ole ena¨a¨ ole- massa yhta¨ka¨a¨n “emo-oliota”, se on ohjelmiston kannalta turha. As- sosiaatiossa olioilla on oma oikeutuksensa olemassaoloon myo¨s yk- sina¨a¨n. Kuva 5.12 seuraavalla sivulla yritta¨a¨ valottaa tilannetta: jos kuvan mukaisessa mallinnuksessa osasto lakkaa olemasta, niin sen Päiväys KirjastonKirja Kustantaja tiedot N 1 1 KUVA 5.11: UML:n koostesuhteita 5.3. Oliosuunnitelman graaﬁnen kuvaus 136 koosteosat eli laitokset lakkaavat myo¨s olemasta (ovat turhia); tyo¨nte- kija¨oliot jatkavat kuitenkin olemassaoloaan vaikka niiden tyo¨nantaja- assosiaatio poistuisikin. Koostetyypin (koostuminen tai muodostuminen) valitsemiseen vaikuttaa useimmiten myo¨s kohdeluokan olion vastuualue koko oh- jelmistossa. Jos samaa kohdeluokan oliota ka¨ytta¨a¨ yksikin toinen olio tai kohdeoliota ei haluta (esim. operaation raskauden takia) luoda ja tuhota useita kertoja, kyseessa¨ on koostuminen (jaettu kooste). Yleisimmin ka¨ytetty yhteys on assosiaatio. Jos halutaan korostaa yhteyden “heikkoutta” tai lyhytkestoisuutta (esimerkiksi kohdeoliota tarvitaan vain parametreina, tai kohteena on tietty ja¨rjestelma¨n ik- kunointikirjasto, jonka on pakko olla olemassa toiminnan kannalta), ka¨yteta¨a¨n riippuvuutta. Periytyminen ja toteuttaminen UML:ssa¨ periytyminen (inheritance, generalization) merkita¨a¨n nuo- liviivalla, joka osoittaa aliluokasta kohti kantaluokkaa. Kuvassa 5.13 seuraavalla sivulla Kirja on kantaluokka, josta on periytetty uusi luokka KirjastonKirja. . . . . . . . . Ekskursio: Periytyminen (lisa¨a¨ luvussa 6) . . . . . . . . Periytyminen on olio-ohjelmoinnin ominaisuus, jossa tehda¨a¨n uusi luokka olemassa olevan mallin (luokan) pohjalta. Periy- tetty luokka eli aliluokka sisa¨lta¨a¨ (perii) kaikki sen mallin eli kantaluokan ominaisuudet (attribuutit ja rajapinnan). Aliluok- Laitos Osasto Työntekijä 6 työnantaja * KUVA 5.12: Osaston, sen laitosten ja tyo¨ntekijo¨iden va¨lisia¨ yhteyksia¨ 5.3. Oliosuunnitelman graaﬁnen kuvaus 137 Kirja KirjastonKirja UDKhyllyluokka() UDKmerkkijono() UDKkoodi() UDKkoodaus «interface» SQLtalletus KUVA 5.13: Kirjasta periytetty luokka, joka toteuttaa kaksi rajapintaa kaan voidaan lisa¨ta¨ uusia ominaisuuksia, ja kantaluokan omi- naisuuksia voidaan tarvittaessa muuttaa. Koska periytymisella¨ luodun uuden luokan rajapinta on ole- tuksena sama kuin kantaluokalla, uuden luokan sanotaan ole- van ka¨ytta¨ytymiselta¨a¨n myo¨s kantaluokan olio (laajennettu versio siita¨). Ta¨ma¨ niin sanottu “is-a”-suhde on yksi olio-oh- jelmoinnin ja oliosuunnittelun ta¨rkeimmista¨ ominaisuuksista. Olio-ohjelmointikielten periytymisominaisuuksista ja niiden vaikutuksista suunnittelupa¨a¨to¨ksiin kerrotaan myo¨hemmin tarkemmin. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . UML:ssa¨ luokka voi ilmaista kahdella tavalla toteuttavansa (real- ization, reﬁnement) tietyn rajapinnan: luokan kylkeen piirreta¨a¨n vii- van pa¨a¨ssa¨ oleva rajapinnan nimella¨ varustettu pallo (era¨a¨nlainen tikkunekku) tai periytymista¨ muistuttavalla katkoviivalla. Esimerkki- kuvassa KirjastonKirja lupaa toteuttaa UDKkoodaus- ja SQLtalletus- nimisten rajapintojen ma¨a¨rittelema¨n toiminnallisuuden. Molemmat kuvaustavat tarkoittavat samaa asiaa, ja yleensa¨ kuvaan valitaan sii- hen parhaiten sopiva tapa (tikkunekku on yleisesti ka¨yto¨ssa¨ silloin, kun rajapinta on esitelty kuvan ulkopuolella tai kaukana sita¨ ka¨ytta¨- va¨sta¨ luokasta). 5.3. Oliosuunnitelman graaﬁnen kuvaus 138 5.3.4 Ajoaikaisen ka¨ytta¨ytymisen kuvaamistapoja Edella¨ esitellyilla¨ luokkakaavioilla voidaan kuvata ohjelmiston tarvit- semien luokkien rakenne, niiden va¨liset yhteydet ja lukuma¨a¨ra¨t. Na¨- ma¨ ovat staattisia rakennekuvauksia. Kyseessa¨ on kuitenkin vain puo- let kattavasta ohjelmiston kuvauksesta. Yhta¨ ta¨rkea¨a¨ on kuvata, miten oliot toimivat sisa¨isesti ja keskena¨a¨n ohjelman suorituksen aikana. Myo¨s ta¨ma¨n dynaamisen ka¨ytta¨ytymisen kuvaamiseen UML tarjoaa useita kuvaustapoja. Tapahtumasekvenssit Lopullisessa ohjelmistossa luokista tehda¨a¨n olioita, jotka kutsuvat toistensa palveluita jossain ja¨rjestyksessa¨. Na¨ita¨ ajoaikaisia “suori- tusja¨lkia¨” voidaan kuvata UML:lla graaﬁsesti tapahtumasekvensseil- la¨ (sequence diagram). Esimerkki tapahtumasekvenssista¨ on kuvas- sa 5.14. Tapahtumasekvenssikuvan perusideana on piirta¨a¨ rinnakkain pystyviivoja, jotka kuvaavat aina yhden olion rajapintaa. Kuvassa kul- kee aika ylha¨a¨lta¨ alas, ja ajan funktiona piirreta¨a¨n rajapintojen va¨- lilla¨ tapahtuvia funktiokutsuja. Esimerkissa¨ tarkasteltavalle funktiol- : KirjaTietokanta a1 : KirjastonKirja <<destroy>> <<create>> return a1 asetaPalPVM(pvm) LainaaKirja() haeKirja( ISBN ) päivitä( a1 ) vapauta( a1 ) KUVA 5.14: Palautuspa¨iva¨ma¨a¨ra¨n asettamisen tapahtumasekvenssi 5.3. Oliosuunnitelman graaﬁnen kuvaus 139 le (kuvan vasemmassa reunassa oleva pystyviiva) on annettu teh- ta¨va¨ksi pa¨ivitta¨a¨ lainattavaan kirjaston kirjaan tieto siita¨, koska lai- na on palautettava. Rutiini pyyta¨a¨ ensin tietokannasta kyseista¨ kir- jaa mallintavan olion, jonka rajapintaan kohdistetaan pa¨ivityskutsu (AsetaPalPVM). Lopuksi tietokanta pa¨iviteta¨a¨n uuteen tietoon ja ope- raation suorittamista varten luotu (va¨liaikainen) olio a1 tuhotaan. Olion tuhoutuminen eli sen elinkaaren pa¨a¨ttyminen ilmaistaan ta- pahtumasekvenssissa¨ pystyviivan alareunaan piirretylla¨ ruksilla. Tilakoneet Kun olion ka¨ytta¨ytyminen halutaan ma¨a¨ritella¨ tarkemmin kuin sanal- lisesti ja rajapintafunktioiden kutsujen avulla (tapahtumasekvenssi), voidaan ka¨ytta¨a¨ hyva¨ksi tilakonetta (state machine). Tilakoneessa (katso kuva 5.15) on piirretyna¨ toiminnallisia tilo- ja ja niiden va¨lisia¨ siirtymia¨. Olion (tai yksitta¨isen rajapintafunktion) kokonaistoiminta kerrotaan kuvaamalla kaikki mahdolliset tilat, jois- sa se voi olla, ja tapahtumat, joilla siirryta¨a¨n tilasta toiseen. Olio on aina jossain tilassa, jota nimiteta¨a¨n nykyiseksi tilaksi. Tilasiirtyma¨ ta- pahtuu, kun siirtyma¨a¨ kuvaavaan nuoleen liittyva¨ ehto toteutuu. Ti- lasiirtyma¨ssa¨ voidaan kuvata ehdon lisa¨ksi toiminta (esimerkiksi ra- japintafunktion kutsu), joka suoritetaan samalla kun olio siirreta¨a¨n siirtyma¨n ma¨a¨ra¨a¨ma¨a¨n uuteen tilaan. Poistettu Käytettävissä Kateissa Lainauskielto Voimassa pois käytöstä pois arkistosta Anottu hylkäys hyväksytty anomus KUVA 5.15: Kirjastokortin tiloja 5.4. Saatteeksi suunnitteluun 140 Tilakoneen mallintama suoritus alkaa aina alkutilasta (musta ym- pyra¨), ja jos nykyiseksi tilaksi tulee tilasiirtymien seurauksena erityi- nen lopetustila (musta ympyra¨ valkoisella reunuksella), tilakoneen suoritus pa¨a¨ttyy (tilakoneen kuvaaman olion tai funktiokutsun toi- minnallisuus pa¨a¨ttyy). Esimerkkikuvassa on mallinnettu kirjastokortin mahdollisia tiloja sen “elinaikana”. Kun hakemus kirjastokortista on hyva¨ksytty, kortti aktivoidaan ja suoritus toimii kokoomatilan (“Voimassa”) sisa¨lla¨. Kun kortti poistetaan ka¨yto¨sta¨, siirryta¨a¨n tilaan “Poistettu” riippumatta sii- ta¨, missa¨ “Voimassa”-tilan alitilassa tapahtumahetkella¨ on oltu. 5.3.5 Ohjelmiston rakennekuvaukset Suurissa ohjelmistoissa luokkiin keskittyva¨t luokkakaaviot ja tapah- tumasekvenssit eiva¨t riita¨ kuvaamaan ohjelmiston kokonaisuutta tar- peeksi abstraktilla tasolla. Ta¨ta¨ ongelmaa varten UML ma¨a¨rittelee eri- laisia arkkitehtuurikuvauksia, joilla on tarkoitus kuvata ohjelmiston rakennetta sen ylimma¨lla¨ tasolla. (Ka¨ytto¨tapaukset kirjoitetaan usein ta¨ma¨n ylimma¨n tason terminologiaa ja rakennetta ka¨ytta¨en.) Looginen na¨kyma¨ (Logical View) ohjelmistoon on kuva kaikista ta¨rkeimmista¨ komponenteista ja niiden keskina¨isista¨ suhteista (esi- merkiksi ka¨ytto¨liittyma¨, ulkopuoliset ka¨ytto¨liittyma¨kirjastot, bisnes- logiikka, tietokanta ja tietoliikenne). Na¨iden komponenttien sisa¨lla¨ on luokkakaaviot kunkin komponentin tarkemmasta suunnittelusta. Jos ohjelmistossa on useita suoritussa¨ikeita¨ tai prosesseja, Proses- sina¨kyma¨ (Process View) kuvaa na¨iden sa¨ikeiden va¨lista¨ yhteistoi- mintaa ja keskina¨isen kommunikaation protokollaa. Sijoitteluna¨kyma¨ (Deployment View) na¨ytta¨a¨ hajautetussa ohjel- mistossa sen, missa¨ tietoverkon tietokoneessa (tai vaikkapa UNIXin prosessissa) mikin ohjelmiston osa suoritetaan. 5.4 Saatteeksi suunnitteluun Suunnittelu on pa¨a¨ttyma¨to¨n matka ja tehta¨va¨a¨ suunnitelmaa pys- tyy “viilaamaan” loputtomiin. Vaikka kuinka haluaisi jatkaa ka¨ytto¨ta- pausten kirjoittamista aina levyaseman kirjoituspa¨a¨n ka¨ytta¨ja¨rooliin asti, tulisi ka¨yta¨nno¨n tyo¨ssa¨ osata lopettaa ma¨a¨rittely ja suunnittelu 5.4. Saatteeksi suunnitteluun 141 joskus. Liiallisuuksiin mennytta¨ ma¨a¨rittelya¨ kutsutaan termilla¨ anal- ysis paralysis [Liberty, 1998]. Koska sitten on suunniteltu tarpeeksi? Koska perustelimme suunnittelun ta¨rkeytta¨ silla¨, etta¨ mutkikas oh- jelmisto saadaan jaetuksi yhden ihmisen ymma¨rta¨miin ja toteutetta- vissa oleviin osiin, niin voimme ma¨a¨ritella¨ lopetusehdoksi tilanteen, jossa kaikki projektin moduulien toteuttajat ovat ymma¨rta¨neet omal- ta osaltaan vaadittavat vastuut ja osaavat ka¨ytta¨a¨ muiden moduulien julkisia rajapintoja. Kuinka sitten varmistutaan ta¨sta¨ taitaa olla toisen tarinan paikka. . . 142 Luku 6 Periytyminen For animals, the entire universe has been neatly divided into things to (a) mate with, (b) eat, (c) run away from, and (d) rocks. – Equal Rites [Pratchett, 1987] Yksi ihmismielelle ominainen piirre on pyrkimys kategorisoida — ryhmitella¨ asioita ja ka¨sitteita¨ eri luokkiin niista¨ lo¨ytyvien yhta¨la¨i- syyksien perusteella. Ta¨ta¨ kategorisointia tekeva¨t jo aivan pienet lap- setkin, ja ihmisten ka¨ytta¨ma¨ kielikin perustuu suurelta osalta sanoi- hin, jotka eiva¨t tarkoita yksitta¨ista¨ asiaa vaan koko joukkoa keskena¨a¨n jollain lailla samankaltaisia asioita. Kategorisointia ka¨yteta¨a¨n yleisesti myo¨s tieteissa¨. Biologiassa Carl von Linne´ ka¨ytti ta¨ta¨ periaatetta 1700-luvulla jaotellessaan kasve- ja ja sienia¨ kategorioihin teoksessaan “Systema Naturae” [Linnaeus, 1748]. On kuitenkin huomattava, etta¨ ta¨ma¨ pyrkimys jaotteluun on nimenomaan ihmisen ajattelusta la¨hto¨isin ja etta¨ todellinen maailma ei va¨ltta¨ma¨tta¨ aina taivu kovin hyvin ta¨llaisiin malleihin. Koska jaottelu aliryhmiin erilaisten yhteisten ominaisuuksien pe- rusteella on niin luontevaa ihmisille, se on pyritty ottamaan ka¨yt- to¨o¨n myo¨s ohjelmoinnissa. Olio-ohjelmoinnin painopiste on kahdes- sa asiassa: olioiden ulkoisessa ka¨ytta¨ytymisessa¨ ja niiden sisa¨isessa¨ toteutuksessa. Niinpa¨ onkin luonnollista keskittya¨ kategorisoinnissa na¨ihin aihealueisiin. 6.1. Periytyminen, luokkahierarkiat, polymorﬁsmi 143 Ulkoisen ka¨ytta¨ytymisen jaottelulla saadaan ryhmiteltya¨ luokkia joukoiksi, jotka joiltain osin ka¨ytta¨ytyva¨t yhteneva¨isella¨ tavalla. Ta¨s- ta¨ on hyo¨tya¨, silla¨ ta¨llo¨in sama ohjelmakoodi voi ka¨sitella¨ kaikkien na¨iden luokkien olioita, koska olioiden ka¨ytta¨ytyminen on samanta- paista. Sisa¨isen toteutuksen jaottelussa taas on pyrkimys saada “tislatuk- si” eri luokkien yhteisia¨ osia yhteen paikkaan, jotta samaa ohjelma- koodia ei tarvitsisi kirjoittaa moneen kertaan. Ta¨sta¨ on tietysti selva¨a¨ hyo¨tya¨ ylla¨pidossa ja virheiden korjaamisessa, ja ohjelman kokokin voi pienentya¨. Lisa¨ksi koodiin lisa¨tyt uudet luokat voivat ottaa suo- raan ka¨ytto¨o¨nsa¨ olemassa olevien luokkien ominaisuuksia, eli koodia voidaan ka¨ytta¨a¨ uudelleen. Ta¨lle luokkien jaottelulle ja yhteisista¨ ominaisuuksista muodos- tuville “sukulaisuussuhteille” on annettu olio-ohjelmoinnissa nimi periytyminen (inheritance), ja monet pita¨va¨t sita¨ kapseloinnin ohel- la olio-ohjelmoinnin ta¨rkeimpa¨na¨ uutena ominaisuutena perinteisiin ohjelmointikieliin verrattuna. Periytymisen tarkasta merkityksesta¨ ja ka¨ytto¨tavoista keskustellaan ja va¨itella¨a¨n olioteoreetikkojen kesken kuitenkin edelleen (matematiikkaa pelka¨a¨ma¨tto¨ma¨t voivat tutustua vaikka Martı´n Abadin ja Luca Cardellin kirjaan “Theory of Objects” [Abadi ja Cardelli, 1996]). Periytymisessa¨ on kyse suhteesta, jossa yksi luokka pohjautuu toi- seen luokkaan ja perii sen ominaisuudet. Monissa “puhtaissa” olio- kielissa¨ jokainen ka¨ytta¨ja¨n ma¨a¨rittelema¨ luokka on aina periytetty jostain toisesta luokasta, ainakin kieleen erikseen ma¨a¨ritellysta¨ “kaik- kien luokkien a¨idista¨” — luokasta Object tai vastaavasta. Toisissa kie- lissa¨, kuten C++:ssa, ta¨llaista periytymispakkoa ei kuitenkaan ole vaan oletusarvoisesti uusi luokka ei liity mitenka¨a¨n jo olemassa oleviin luokkiin. Kuvassa 6.1 seuraavalla sivulla on lueteltu ta¨ssa¨ teoksessa ka¨ytettyja¨ periytymiseen liittyvia¨ termeja¨ ja selitetty ne lyhyesti. 6.1 Periytyminen, luokkahierarkiat, polymorﬁs- mi Varsinkin eurooppalaiset olio-ohjelmoinnin asiantuntijat korostavat periytymisessa¨ olioiden ulkoisen ka¨ytta¨ytymisen — rajapintojen — kategorista jaottelua [Koskimies, 2000]. Kuvassa 6.2 sivulla 145 on 6.1. Periytyminen, luokkahierarkiat, polymorﬁsmi 144 Termi Selitys Periytyminen (inheritance), (johtaminen) Uuden luokan muodostuminen olemassa ole- van luokan pohjalta niin, etta¨ uusi luokka si- sa¨lta¨a¨ kaikki toisen luokan ominaisuudet. Kantaluokka (base class), (yliluokka (superclass, parent class)) Alkupera¨inen luokka, jonka ominaisuudet pe- riytyva¨t uuteen luokkaan. Aliluokka (subclass), (periytetty/johdettu luokka (derived class)) Uusi luokka, joka perii kantaluokan ominai- suudet ja voi ta¨ma¨n lisa¨ksi sisa¨lta¨a¨ uusia omi- naisuuksia. Periytymishierarkia (inheritance hierarchy), (luokkahierarkia, (class hierarchy)) Kantaluokista ja niiden aliluokista muodos- tuva puumainen rakenne, jossa puussa alem- pana olevat luokat periva¨t ylempa¨na¨ olevien ominaisuudet. Katso kuva 6.4 sivulla 149. Esi-isa¨ (ancestor) Periytymishierarkiassa luokan kantaluokka, kantaluokan kantaluokka, ta¨ma¨n kantaluokka jne. ovat luokan esi-isia¨. Ja¨lkela¨inen (descendant) Kaikki kantaluokasta periytetyt luokat, niista¨ periytetyt luokat jne. ovat luokan ja¨lkela¨isia¨. KUVA 6.1: Periytymiseen liittyva¨a¨ terminologiaa esitetty elio¨iden toimintaa mallintavan ohjelman periytymishierar- kiaa (inheritance hierarchy), tosin varsin rajoitetusti. Ta¨ssa¨ esimerkissa¨ vain luokat Homesieni, Koira, Ihminen, Kana ja Satakieli ovat varsinaisia “todellisia elio¨ita¨ mallintavia” luokkia, joista ohjelmassa tehda¨a¨n olioita. Luokat Elio¨, Sieni, Ela¨in, Nisa¨ka¨s ja Lintu puolestaan vain kuvaavat todellisten luokkien “yla¨ka¨sittei- ta¨”. Niita¨ ka¨yteta¨a¨n ryhmittelema¨a¨n todellisia luokkia kategorioihin, joihin kuuluvien olioiden rajapinta ja ka¨ytta¨ytyminen ovat jollain ta- valla yhteneva¨isia¨. Ta¨llaisia luokkia, joista ei ole mieleka¨sta¨ tehda¨ olioita, mutta joi- ta silti tarvitaan kuvaamaan ohjelmassa esiintyvien todellisten luok- kien rajapintoja ja niiden suhteita, kutsutaan abstrakteiksi kantaluo- kiksi (abstract base class). UML:ssa¨ abstraktit kantaluokat usein mer- kita¨a¨n merkinna¨lla¨ “{abstract}” kuten kuvassa. Samoin merkinna¨lla¨ 6.1. Periytyminen, luokkahierarkiat, polymorﬁsmi 145 Eliö {abstract} lisäänny() {abstract} Sieni {abstract} hajota() {abstract} Eläin {abstract} liiku() {abstract} Homesieni lisäänny() hajota() Lintu {abstract} laula() {abstract} pure() imetä() lisäänny() liiku() Koira Ihminen lisäänny() liiku() imetä() Kana lisäänny() liiku() laula() Satakieli lisäänny() liiku() laula() Nisäkäs {abstract} imetä() {abstract} KUVA 6.2: Elio¨ita¨ mallintavan ohjelman periytymishierarkiaa “{abstract}” voidaan viela¨ merkita¨ ne rajapinnan ja¨senfunktiot, joille abstrakti kantaluokka ei tarjoa toteutusta. Periytymista¨ ka¨yteta¨a¨n kuvaamaan luokkien va¨lisia¨ suhteita. Luokka Elio¨ kuvaa kaikkia ohjelmassa esiintyvia¨ elio¨ita¨. Sen rajapinta sisa¨lta¨a¨ kaikille elio¨ille yhteisen rajapinnan, ta¨ssa¨ esimerkissa¨ lisa¨a¨n- tymisen. Kaikkia esimerkin todellisia olioita voi pyyta¨a¨ lisa¨a¨ntyma¨a¨n, ja jokaisen elio¨n lisa¨a¨ntymispalvelu na¨ytta¨a¨ ulospa¨in ta¨sma¨lleen sa- manlaiselta. Sen sijaan lisa¨a¨ntymisen varsinainen toteutus saattaa hyvinkin vaihdella luokasta toiseen — on varsin todenna¨ko¨ista¨, etta¨ homesieni ja koira lisa¨a¨ntyva¨t eri tavalla! Esimerkissa¨ elio¨t jaetaan kahteen alikategoriaan, sieniin ja ela¨i- miin. Na¨ma¨ eroavat toisistaan ulkoisesti siina¨, etta¨ sienia¨ voi pyyta¨a¨ hajottamaan elopera¨ista¨ materiaalia, ela¨imia¨ puolestaan liikkumaan paikasta toiseen. Luokat Sieni ja Ela¨in on periytetty (derived) luokas- ta Elio¨. Ta¨llo¨in niiden rajapintaan kuuluu niiden omien palveluiden lisa¨ksi automaattisesti myo¨s luokan Elio¨ rajapinta — sienten ja ela¨in- ten rajapinta on siis laajennettu elio¨iden rajapinnasta. Ja¨lleen kysy- mys on vain rajapinnasta, ja jokainen todellinen sieniluokka voi to- teuttaa hajottamispalvelun haluamallaan tavalla. 6.1. Periytyminen, luokkahierarkiat, polymorﬁsmi 146 Samalla tavoin jaetaan ela¨imet viela¨ nisa¨kka¨isiin, joita voi ka¨skea¨ imetta¨ma¨a¨n, seka¨ lintuihin, joita voi pyyta¨a¨ laulamaan. Viimein na¨is- ta¨ luokista on periytetty todelliset ohjelmassa esiintyva¨t ela¨inluokat Koira, Ihminen, Kana ja Satakieli. Periytymisen hyo¨tyna¨ on, etta¨ hierarkia antaa mahdollisuuden puhua esimerkiksi kaikista nisa¨kka¨ista¨ luokkaa Nisa¨ka¨s ka¨ytta¨ma¨lla¨. Jos myo¨hemmin ohjelmassa tulee esimerkiksi tarve lisa¨ta¨ kaikkien nisa¨kka¨iden rajapintaan uusi palvelu — vaikkapa synnytta¨minen —, ta¨ma¨ ka¨y yksinkertaisesti lisa¨a¨ma¨lla¨ kyseinen operaatio luokan Nisa¨- ka¨s rajapintaan. Ta¨ma¨n ja¨lkeen ta¨ytyy tietysti viela¨ toteuttaa synnyt- ta¨minen kussakin nisa¨kka¨a¨ssa¨ lajille ominaisella tavalla. Vastaavasti jos ohjelmassa on funktio, jonka tehta¨va¨na¨ on siirta¨a¨ sille parametrina annettuja ela¨imia¨, ta¨ma¨ funktio voi ottaa parametri- naan yksinkertaisesti viitteen Ela¨in-luokan olioon. Koska seka¨ Koira, Ihminen, Kana etta¨ Satakieli ovat luokkahierarkiassa ela¨imia¨, siirty- misfunktiolle voi ta¨llo¨in antaa siirretta¨va¨ksi minka¨ tahansa ela¨imen. Siirtymisfunktion itsensa¨ ei tarvitse tieta¨a¨ yksityiskohtia siita¨, minka¨ ela¨inlajin edustajaa se on siirta¨ma¨ssa¨. Sille riitta¨a¨ tieto siita¨, etta¨ koska kyseessa¨ on ela¨in, sen julkisessa rajapinnassa on tarvittava palvelu ela¨imen siirta¨miseen. Ta¨llaista tilannetta, jossa funktio hy- va¨ksyy eri tyyppisia¨ parametreja, kutsutaan polymorﬁsmiksi (poly- morphism) eli “monimuotoisuudeksi”. Ka¨a¨nta¨ja¨ voi luokkahierarkian avulla lisa¨ksi tarkastaa, etta¨ siirtymisfunktiolle annetaan siirretta¨va¨k- si vain sellaisia olioita, joita todella voi siirta¨a¨ — ei esimerkiksi home- sienia¨. Kielissa¨, joissa ei ole vahvaa tyypitysta¨, tarkastetaan sopivan ja¨senfunktion lo¨ytyminen yleensa¨ vasta ajoaikana. Ta¨llaisissa kielis- sa¨ luokkahierarkian ka¨ytto¨ rajapintojen luokitteluun on tarpeetonta, koska esimerkiksi siirtymisfunktiolle voisi antaa parametrina minka¨ tahansa olion, ja jos kyseinen olio ei osaa siirtya¨, annetaan ajoaikai- nen virheilmoitus. Esimerkiksi Smalltalk kuuluu ta¨llaisiin oliokieliin. Luokkahierarkioiden etuna on viela¨ se, etta¨ jokaisessa todellises- sa luokassa rajapintafunktio voidaan tarvittaessa toteuttaa eri taval- la. Edella¨ mainittu siirtymisfunktio pystyy siirta¨ma¨a¨n mita¨ tahansa ela¨imia¨ kutsumalla na¨iden “liiku”-palvelua. Vaikka siirtymisfunktio ei va¨lita¨ka¨a¨n siita¨, minka¨ lajin ela¨imesta¨ on kysymys, voi jokainen ela¨in silti toteuttaa liikkumisen omalla tavallaan — linnut lenta¨ma¨l- la¨, koira jolkottamalla jne. Na¨in sama siirtymisfunktiossa oleva “lii- ku”-palvelun kutsu voi ajoaikana aiheuttaa palvelupyynno¨n vastaa- nottajaolion luokasta riippuen erilaisen toiminnon. Ta¨ta¨ kutsutaan 6.2. Periytyminen ja uudelleenka¨ytto¨ 147 puolestaan dynaamiseksi sitomiseksi (aliluku 6.5.2). Periytymiseen ja hierarkioihin perustuvassa kategorisoinnissa ko- rostuu olion tarjoaman palvelun ja sen toteutuksen ero. Kaikki Ela¨in- luokasta periytetyt luokat sisa¨lta¨va¨t palvelun “liiku” ja lupaavat, etta¨ sita¨ kutsumalla olio liikkuu paikasta toiseen. Rajapinta ei kuitenkaan lupaa mita¨a¨n siita¨, milla¨ tavalla liikkuminen tapahtuu. Ta¨ma¨n voi ajatella myo¨s niin, etta¨ hierarkiassa alempana olevat luokat erikoista- vat (specialize) ylempa¨na¨ ma¨a¨ra¨ttyjen palveluiden toimintaa. Erikois- taminen voi olla myo¨s useampivaiheista, esimerkiksi luokassa Lintu saatettaisiin erikoistaa liikkumista ma¨a¨ra¨a¨ma¨lla¨, etta¨ se tapahtuu len- ta¨en. Siita¨ huolimatta kanat ja satakielet voisivat viela¨ erikoistaa ta¨ta¨ lisa¨a¨ toteuttamalla lenta¨misen eri tavalla. 6.2 Periytyminen ja uudelleenka¨ytto¨ Luokkasuunnittelun edetessa¨ tulee vastaan tilanteita, joissa huoma- taan osan luokkaa (attribuuttien tai toiminnallisuuden) olevan sa- manlainen useassa luokassa. Esimerkiksi jos mietimme muita graﬁik- kaluokkia kuin aikaisemmin esimerkkina¨ ollut Pistetta¨, niin keksim- me ehka¨ ympyra¨n, viivan ja valokuvan. Na¨illa¨ kaikilla on yhteise- na¨ ominaisuutena tieto na¨kyvyydesta¨ (onko mallinnettu graﬁikkao- lio piirrettyna¨ na¨ytto¨laitteelle), joka ilmaistaan luokan attribuuttina ja sen ka¨ytto¨o¨n liittyvina¨ rajapintafunktioina. Attribuutin ilmaisema tilatieto ja siihen liittyva¨ rajapinta on kaikilla hierarkian luokilla sa- ma. Ohjelmiston ylla¨pidetta¨vyys paranee, jos kaikille luokille yhtei- set attribuutit ja rajapintafunktiot toteutetaan vain kerran ja sama to- teutus on kaikkien luokkien ka¨yto¨ssa¨. Oliosuunnittelussa ta¨ma¨ yleis- ta¨minen (generalization) saadaan aikaiseksi ma¨a¨rittelema¨lla¨ yhteiset osat yhteen luokkaan, josta muut ominaisuutta tarvitsevat luokat pe- riva¨t toteutuksen osaksi itsea¨a¨n. Yhteinen toiminnallisuus tavallaan nostetaan periytymishierarkiassa ylemma¨lle tasolle yhteiseen kanta- luokkaan. Kuvassa 6.3 seuraavalla sivulla on ma¨a¨ritelty kantaluokka Na¨ky- vyys, jonka vastuulle on merkitty tieta¨mys siita¨, onko graﬁikkaolio na¨kyvilla¨ na¨ytto¨laitteella (attribuutti), ja siihen liittyva¨t rajapinta- funktiot. Kantaluokasta periytetyt luokat periva¨t kaikki kantaluokan ominaisuudet osaksi itsea¨a¨n. Ta¨ma¨ tarkoittaa sita¨, etta¨ kun Ympy- 6.2. Periytyminen ja uudelleenka¨ytto¨ 148 ra¨-luokasta tehda¨a¨n olio, se sisa¨lta¨a¨ seka¨ Na¨kyvyys-luokan etta¨ ym- pyra¨n ominaisuudet. Tarkasteltaessa periytymista¨ periytetyn luokan kannalta se on erikoistettu versio kantaluokasta. Luokkien periytta¨mista¨ voidaan jatkaa periytymishierarkiassa eteenpa¨in. Jos Kirjastokomponentilla on hyvin samanlaiset ominai- suudet kuin na¨yto¨lla¨ esitetta¨va¨lla¨ valokuvalla, voimme periytta¨a¨ luo- kan Valokuva Kirjastokomponentista. Valokuva-luokalla on nyt kaik- ki samat ominaisuudet kuin sen molemmilla kantaluokilla — myo¨s na¨kyvyyteen liittyva¨t. Periytyminen liittyy la¨heisesti ohjelmakoodin uudelleenka¨ytto¨o¨n. Aikaisemmin olleesta luokasta (mahdollisesti ostetussa komponentis- sa tai aikaisemmassa projektissa) saadaan ka¨ytetyksi kaikki halutut ominaisuudet, ja johdetussa uudessa versiossa tarvitsee ainoastaan lisa¨ta¨ ja muuttaa tarvittavat uudet osuudet. Ta¨ma¨ vahva ominaisuus sisa¨lta¨a¨ myo¨s riskin: pitka¨t (ja huonosti dokumentoidut) periytymis- hierarkiat ovat usein perinteista¨ vaikealukuisempaa ohjelmakoodia, koska hierarkian eri osat (luokat) voivat sijaita eri puolilla ohjelma- koodia (esimerkiksi eri tiedostoissa). Viiva Ympyrä Kirjastokomponentti Valokuva Näkyvyys piirrä() onkoNäkyvissä: bool piilota() KUVA 6.3: Na¨kyvyytta¨ kuvaava kantaluokka ja periytettyja¨ luokkia 6.3. C++: Periytymisen perusteet 149 6.3 C++: Periytymisen perusteet C++:ssa periytyminen tarkoittaa, etta¨ kielessa¨ on mahdollista luoda uusi luokka jo olemassa olevaan luokkaan perustuen niin, etta¨ uuden luokan oliot periva¨t automaattisesti kaikki toisen luokan ominaisuu- det, niin rajapinnan kuin sisa¨isen toteutuksenkin. Kuva 6.4 na¨ytta¨a¨, mista¨ periytymisessa¨ yksinkertaistettuna on kyse. Jokainen aliluokka perii kaikki esi-isiensa¨ ominaisuudet ja voi lisa¨ksi laajentaa ja rajoite- tusti muokata niita¨. Kuvaan on piirretty myo¨s aliluokan olioita, joista na¨kyy olioiden “kerrosrakenne” — oliot ovat ika¨a¨n kuin kantaluokan olioita, joihin on liimattu kiinni aliluokkien vaatimat lisa¨osat. C++:ssa¨ on myo¨s mahdollista periytta¨a¨ luokka useammasta kuin yhdesta¨ luokasta. Ta¨ma¨ moniperiytyminen (multiple inheritance) on varsin kiistelty ominaisuus, ja sen ka¨ytto¨a¨ ei yleensa¨ suositella kuin tiettyihin tarkoituksiin. Moniperiytymista¨ ka¨sitella¨a¨n jonkin verran B B:n ominaisuudet (A:n ominaisuudet) A A:n ominaisuudet C C:n ominaisuudet (A:n ominaisuudet) D (A:n ominaisuudet) D:n ominaisuudet (C:n ominaisuudet) E (A:n ominaisuudet) E:n ominaisuudet (D:n ominaisuudet) (C:n ominaisuudet) F (A:n ominaisuudet) (C:n ominaisuudet) F:n ominaisuudet (D:n ominaisuudet) (a) Hierarkia A−osa C−Osa D−Osa F−Osa A−osa C−Osa F−Olio C−Olio (b) Aliluokan olioita KUVA 6.4: Periytymishierarkia ja oliot [Koskimies, 2000] 6.3. C++: Periytymisen perusteet 150 aliluvussa 6.7. Aliluvussa 6.9.2 ka¨sitella¨a¨n myo¨s yhta¨ moniperiyty- misen ka¨ytto¨tapaa — rajapintaluokkia. Periytymisen syntaksi on yksinkertainen: aliluokkaa esitelta¨essa¨ merkita¨a¨n luokan nimen ja¨lkeen kaksoispiste ja sen ja¨lkeen periyty- mistyyppi (la¨hes aina public) ja kantaluokan nimi (moniperiytymi- sen yhteydessa¨ na¨ita¨ periytymistyyppi-kantaluokka-pareja on useita pilkulla toisistaan erotettuina). Listauksessa 6.1 on esimerkkina¨ ku- van 6.4 luokkahierarkiaa C++:lla toteutettuna. C++:ssa on mahdollista public-periytymisen lisa¨ksi myo¨s private- ja protected-periytyminen, mutta niiden ka¨ytto¨ on varsin harvoin olio-ohjelmoinnissa tarpeellis- ta, eika¨ niita¨ ka¨sitella¨ ta¨ssa¨ teoksessa. 6.3.1 Periytyminen ja na¨kyvyys Periytymisen yhteydessa¨ aliluokka perii kantaluokasta niin ulospa¨in na¨kyva¨n rajapinnan kuin sisa¨isen toteutuksenkin. Perittyjen osien na¨- kyvyys aliluokan oliossa on kuitenkin osin erilainen kuin kantaluo- kassa. Kuva 6.5 seuraavalla sivulla kuvaa aliluokan pa¨a¨sya¨ kantaluo- kan eri osiin. Alla on luettelo, josta ka¨y ilmi eri na¨kyvyysma¨a¨reiden vaikutus periytymiseen. 1 class A 2 { 3 // Luokan A ominaisuudet 4 }; 5 class B : public A 6 { 7 // Luokan B A:han lisa¨a¨ma¨t ominaisuudet 8 }; 9 class C : public A 10 { 11 // Luokan C A:han lisa¨a¨ma¨t ominaisuudet 12 }; 13 class D : public C 14 { 15 // Luokan D C:hen lisa¨a¨ma¨t ominaisuudet 16 }; ... LISTAUS 6.1: Periytymisen syntaksi C++:lla 6.3. C++: Periytymisen perusteet 151 A−osa C−Osa private protected protected private public public C−Olio publfunkt protfunkt privmuutt uusifunkt uusiprivm uusiprotf KUVA 6.5: Periytyminen ja na¨kyvyys • public: Kantaluokan public-osat — sen ulkoinen rajapinta — ovat myo¨s aliluokassa public-puolella. Na¨in aliluokan ulkoises- sa rajapinnassa on kaikki se, mita¨ kantaluokassakin, seka¨ lisa¨ksi aliluokan ma¨a¨rittelema¨t uudet rajapintafunktiot. • private: Vaikka kantaluokan private-osa periytyykin aliluok- kaan, ei niihin pa¨a¨se ka¨siksi aliluokan ja¨senfunktioista. Ta¨ma¨ johtuu siita¨ na¨ko¨kannasta, etta¨ private-osat ovat kantaluokan si- sa¨isena¨ toteutuksena kantaluokan oma asia, eika¨ aliluokan tar- vitse pa¨a¨sta¨ niihin ka¨siksi — eiha¨n private-osa muutenkaan na¨y luokasta ulos. Aliluokan ja¨senfunktiot voivat tietysti va¨lillisesti ka¨ytta¨a¨ kantaluokan private-osia kutsumalla kantaluokan raja- pintafunktioita. 6.3. C++: Periytymisen perusteet 152 • protected: Ta¨ha¨n saakka protected-ma¨a¨reesta¨ ei ole mainittu juuri mita¨a¨n muuta, kuin etta¨ se on hyvin samantapainen kuin private. Protected-ma¨a¨reen ka¨ytto¨ liittyykin nimenomaan pe- riytymiseen. Kantaluokan protected-osa periytyy aliluokan pro- tected-osaksi. Na¨in protected-osa on kuin private-osa siina¨ mie- lessa¨, ettei se kuulu luokan ulkoiseen rajapintaan, mutta se on kuitenkin aliluokkien koodin ka¨ytetta¨vissa¨. Protected-osan teh- ta¨va¨ onkin laajentaa kantaluokan rajapintaa aliluokan suuntaan tarjoamalla ta¨lle erityisrajapinnan. Ta¨lla¨ tavoin kantaluokka voi tarjota aliluokille luokan laajentamiseen tarvittavia apufunk- tioita, joita ei kuitenkaan haluta yleiseen ka¨ytto¨o¨n. Kuten pub- lic-osaankin, protected-osaan tulee kirjoittaa vain ja¨senfunktioi- ta, vaikkei C++:n syntaksi ta¨ta¨ rajoitakaan. Edella¨ olevista na¨kyvyyssa¨a¨nno¨ista¨ aloittelijaa ihmetytta¨a¨ usein kantaluokan private-osan “erista¨minen” aliluokalta. Kapseloinnin kannalta ta¨ma¨ C++:n (ja Javan) ratkaisu on kuitenkin oikea, koska ali- luokkaa ei pita¨isi toteuttaa niin, etta¨ se riippuu kantaluokan sisa¨isesta¨ toteutuksesta. Mika¨li aliluokan ta¨ytyy saada suorittaa sellaisia kanta- luokan operaatioita, joita ei lo¨ydy kantaluokan julkisesta rajapinnas- ta, protected-ma¨a¨re tarjoaa ka¨yta¨nno¨llisen ratkaisun ongelmaan. Jotkut oppikirjat [Lippman ja Lajoie, 1997] ovat ottaneet sen kan- nan, etta¨ koska kantaluokan suunnittelija harvoin tieta¨a¨ tarkoin, mil- laisia luokkia kantaluokasta tullaan periytta¨ma¨a¨n, pannaan varmuu- den vuoksi koko kantaluokan sisa¨inen toteutus protected-puolelle — ja¨senmuuttujineen kaikkineen. Ta¨llo¨in kantaluokan ja aliluokan va¨- linen kapselointi kuitenkin murtuu ja aliluokka pa¨a¨see riippumaan kantaluokan sisa¨isesta¨ toteutuksesta. Paljon parempi ratkaisu on kir- joittaa protected-puolelle sopivat apuja¨senfunktiot, joiden avulla ali- luokka pa¨a¨see sopivasti ka¨siksi kantaluokan sisimpa¨a¨n. 6.3.2 Periytyminen ja rakentajat Kuten kaikki oliot, myo¨s aliluokan oliot ta¨ytyy alustaa kun ne luo- daan. Aliluvussa 3.4 ka¨siteltiin olion alustamista rakentajaja¨senfunk- tion avulla. Periytyminen tuo kuitenkin omat lisa¨nsa¨ olioiden alusta- miseen. Jokainen aliluokan olio koostuu kantaluokkaosasta tai -osis- ta seka¨ aliluokan lisa¨a¨mista¨ laajennuksista. Selva¨sti aliluokan ta¨ytyy ma¨a¨ritella¨ oma rakentajansa, jotta aliluokan uudet osat saadaan alus- 6.3. C++: Periytymisen perusteet 153 tetuksi, mutta miten pita¨isi toimia kantaluokan ja¨senmuuttujien alus- tamisen kanssa? Aliluokan ja¨senfunktiothan — rakentaja mukaanlu- kien — eiva¨t pa¨a¨se kantaluokan private-osiin ka¨siksi. Ratkaisu alustusongelmaan lo¨ytyy ja¨lleen luokkien vastuualueis- ta. Aliluokan vastuulla on aliluokan mukanaan tuomien uusien ja¨sen- muuttujien ja muiden tietorakenteiden alustaminen. Ta¨ta¨ tarkoitusta varten aliluokkaan kirjoitetaan oma rakentaja tai rakentajat. Kanta- luokan vastuulla on pita¨a¨ huoli siita¨, etta¨ aliluokan olion kantaluok- kaosa tulee alustetuksi oikein, aivan kuin se olisi irrallinen kanta- luokan olio. Ta¨ma¨n alustuksen hoitavat aivan normaalit kantaluokan rakentajat. Ainoaksi ongelmaksi ja¨a¨va¨t rakentajien parametrit. Aliluokan ra- kentaja saa kylla¨ parametrinsa aivan normaaliin tapaan aliluokan oliota luotaessa, mutta ongelmana on, miten kantaluokan rakentajalle saadaan va¨litetyksi sen tarvitsemat parametrit. C++:n tarjoama ratkaisu on, etta¨ aliluokan rakentajan alustuslistassa “kutsutaan” kantaluokan rakentajaa ja va¨liteta¨a¨n sille tarvittavat parametrit. Ta¨lla¨ tavoin ali- luokka voi itse pa¨a¨tta¨a¨, millaisia parametreja sen kantaluokalle va¨lite- ta¨a¨n olion luomisen yhteydessa¨. Listaus 6.2 seuraavalla sivulla na¨yt- ta¨a¨ esimerkin periytymisesta¨ ja rakentajien ka¨yto¨sta¨. Jos aliluokan rakentajan alustuslistassa ei kutsuta mita¨a¨n kanta- luokan rakentajaa, yritta¨a¨ ka¨a¨nta¨ja¨ olla ysta¨va¨llinen. Ta¨llaisessa tilan- teessa se kutsuu nimitta¨in kantaluokan oletusrakentajaa, joka ei tar- vitse parametreja. Ta¨ma¨ aiheuttaa kuitenkin sen, etta¨ rakentajakutsun unohtuessa alustuslistasta olion kantaluokkaosa alustetaan oletusar- voonsa, joka tuskin on haluttu lopputulos! Onkin eritta¨in ta¨rkea¨a¨, etta¨ aliluokan rakentajan alustuslistassa kutsutaan aina jotain kantaluo- kan rakentajaa. Aliluokan olion alustusja¨rjestys C++:ssa on sellainen, etta¨ ensim- ma¨isena¨ suoritetaan kantaluokan rakentaja kokonaisuudessaan, jonka ja¨lkeen suoritetaan aliluokan oma rakentaja. Mika¨li periytymishierar- kia on korkeampi kuin kaksi luokkaa, la¨hdeta¨a¨n rakentajia suoritta- maan aivan hierarkian huipusta la¨htien alaspa¨in, niin etta¨ olio ika¨a¨n kuin rakentuu va¨hitellen laajemmaksi ja laajemmaksi. Ta¨ma¨ alustus- ja¨rjestys takaa sen, etta¨ aliluokan rakentajassa voidaan jo turvallisesti kutsua kantaluokan ja¨senfunktioita, koska aliluokan rakentajan koo- diin pa¨a¨sta¨essa¨ kantaluokkaosa on jo alustettu kuntoon (poikkeuk- sena ovat aliluokan uudelleenma¨a¨rittelema¨t virtuaalifunktiot, joiden ka¨ytta¨ytymista¨ rakentajien kanssa ka¨sitella¨a¨n tarkemmin aliluvus- 6.3. C++: Periytymisen perusteet 154 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Kantaluokka . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1 class Lokiviesti 2 { 3 public: 4 Lokiviesti(string const& viesti); ... 5 private: 6 string viesti ; 7 }; 8 9 Lokiviesti::Lokiviesti(string const& viesti) : viesti (viesti) 10 { 11 } . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Aliluokka . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1 class PaivattyLokiviesti : public Lokiviesti 2 { 3 public: 4 PaivattyLokiviesti(Paivays const& pvm, string const& viesti); ... 5 private: 6 Paivays pvm ; 7 }; 8 9 PaivattyLokiviesti::PaivattyLokiviesti(Paivays const& pvm, 10 string const& viesti) : Lokiviesti(viesti), pvm (pvm) 11 { 12 } . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Olion luominen . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1 Lokiviesti viesti(\"Ka¨vin leffassa\"); 2 PaivattyLokiviesti pvmviesti(tanaan(), \"Huono oli\"); LISTAUS 6.2: Periytyminen ja rakentajat sa 6.5.7). 6.3.3 Periytyminen ja purkajat Alustamisen tapaan myo¨s aliluokan olion siivoustoimenpiteet vaa- tivat erikoiskohtelua luokan “kerrosrakenteen” vuoksi. Samoin kuin aliluokan olion alustaminen, myo¨s sen siivoaminen on jaettu vastuu- alueiden kesken. Kantaluokan purkajan tehta¨va¨na¨ on siivota kanta- luokkaolio sellaiseen kuntoon, etta¨ se voi rauhassa tuhoutua. Aliluo- kan purkajan vastuulla on vastaavasti pita¨a¨ huoli siita¨, etta¨ aliluokan 6.3. C++: Periytymisen perusteet 155 olion periytymisessa¨ lisa¨tty laajennusosa siivotaan tuhoutumiskun- toon. Kuten aiemminkin, ohjelmoijan ei itse tarvitse huolehtia purka- jien kutsumisesta vaan olion tuhoutuessa ka¨a¨nta¨ja¨ kutsuu automaat- tisesti tarvittavia purkajia. Periytymisen yhteydessa¨ olion purkajien kutsuja¨rjestys on pa¨invastainen rakentajien kutsuja¨rjestykseen na¨h- den eli ensin kutsutaan aliluokan purkajaa, sitten kantaluokan pur- kajaa ja tarvittaessa ta¨ma¨n kantaluokan purkajaa ja niin edelleen. Ta¨- ma¨ kutsuja¨rjestys varmistaa sen, etta¨ aliluokan purkajaa suoritettaes- sa olion kantaluokkaosa on viela¨ ka¨ytto¨kelpoinen, ja na¨in aliluokan purkaja voi viela¨ turvallisesti kutsua kantaluokan ja¨senfunktioita (ku- ten rakentajissakin, aliluokan uudelleenma¨a¨rittelema¨t virtuaalifunk- tiot ovat poikkeustapaus. Niita¨ ja purkajia ka¨sitella¨a¨n tarkemmin ali- luvussa 6.5.7). Kantaluokan purkaja kannattaa ma¨a¨ritella¨ la¨hes aina virtuaalisek- si, mutta ta¨ta¨ ka¨sitella¨a¨n vasta aliluvussa 6.5.5. 6.3.4 Aliluokan olion ja kantaluokan suhde Koska aliluokka perii kantaluokan kaikki ominaisuudet, tarjoaa ali- luokan olio ulospa¨in kaikki ne palvelut, jotka kantaluokan oliokin tar- joaa. Na¨in ollen aliluokan oliota voisi sen ulkoista rajapintaa ajatellen ka¨ytta¨a¨ kaikkialla, missa¨ kantaluokan oliotakin — periytymisessa¨ha¨n vain lisa¨ta¨a¨n ominaisuuksia. Ta¨ma¨ ajatus aliluokan olion kelpaamisesta kantaluokan olion pai- kalle on viety useissa oliokielissa¨ viela¨ pitemma¨lle ma¨a¨rittelema¨lla¨, etta¨ kielen tyypityksen kannalta aliluokan olio on tyypilta¨a¨n myo¨s kantaluokan olio. Na¨in aliluokan oliot kuuluvat ika¨a¨n kuin useaan luokkaan: aliluokkaan itseensa¨, kantaluokkaan, mahdollisesti kanta- luokan kantaluokkaan jne. Ta¨ma¨ is-a -suhde tulisi pita¨a¨ mielessa¨ ai- na, kun periytymista¨ ka¨yteta¨a¨n. Jos aliluokka on muuttunut vastuu- alueeltaan niin paljon, etta¨ se ei ena¨a¨ ole kantaluokan mukainen, pe- riytymista¨ on ka¨ytetty mita¨ ilmeisimmin va¨a¨rin. C++:n kannalta ta¨ma¨ ominaisuus tarkoittaa, etta¨ kielen tyypitykses- sa¨ aliluokan olio kelpaa kaikkialle minne kantaluokan oliokin. Erityi- sesti kantaluokkaosoittimen tai -viitteen voi laittaa osoittamaan myo¨s 6.4. C++: Periytymisen ka¨ytto¨ laajentamiseen 156 aliluokan olioon: class Kantaluokka { . . . }; class Aliluokka : public Kantaluokka { . . . }; void funktio(Kantaluokka& kantaolio); Kantaluokka* k p = 0; Aliluokka aliolio; k p = &aliolio; funktio(aliolio); Ta¨ma¨ tilanne, jossa osoittimen pa¨a¨ssa¨ olevan olion tyyppi ei ole sama kuin osoittimen tyyppi, ei aiheuta yleensa¨ ongelmia, koska jo- kainen aliluokan olio tarjoaa periytymisesta¨ johtuen kaikki kantaluo- kan tarjoamat palvelut. Ta¨ma¨ mahdollisuus ka¨ytta¨a¨ aliluokan olioita ohjelmassa kantaluokan sijaan on yksi ta¨rkeimpia¨ olio-ohjelmoinnin ja periytymisen tyo¨kaluja.\u0017 Sen ka¨ytto¨a¨ ka¨sitella¨a¨n aliluvussa 6.5.2. 6.4 C++: Periytymisen ka¨ytto¨ laajentamiseen Periytymisen kenties yksinkertaisin ka¨ytto¨tarkoitus on olemassa ole- van luokan laajentaminen. Siina¨ aliluokka laajentaa kantaluokan tar- joamia palveluita tarjoamalla kaikki kantaluokan tarjoamat palvelut identtisina¨ ja sen lisa¨ksi viela¨ uusia palveluita. Ta¨llainen periytymi- sen ka¨ytto¨ mahdollistaa koodin uudelleenka¨yto¨n, koska aliluokan ei tarvitse kirjoittaa uudelleen kantaluokan jo kertaalleen toteuttamia palveluita. Listaus 6.3 seuraavalla sivulla na¨ytta¨a¨ yksinkertaisen luokan Kirja, joka muistaa yksitta¨isen kirjan tiedot. Periaatteessa listauk- sessa esitetty luokka sopisi muuten hyvin kirjaston kortistoja¨rjestel- ma¨ssa¨ ka¨ytetyksi kirjaksi, mutta kirjaston kirjoilla on yksi olennai- nen lisa¨ominaisuus: niilla¨ on viimeinen palautuspa¨iva¨ma¨a¨ra¨. (Lis- tauksen rivilla¨ 5 on purkajan edessa¨ avainsana virtual. Silla¨ ei ole ta¨- ma¨n esimerkin kannalta merkitysta¨, mutta na¨in tehda¨a¨n yleensa¨ kai- kissa kantaluokissa. Virtuaalipurkajan merkitys seliteta¨a¨n aliluvus- sa 6.5.5.) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . \u0017C++:ssa vaaraksi muodostuu viipaloituminen (slicing). Kun esimerkiksi kantaluokan olioon sijoitetaan aliluokan olio (joka on tyypilta¨a¨n myo¨s kantaluokan olio), suoritetaan sijoituksessa vain kantaluokkaosan sijoitus ja aliluokkaosa ja¨a¨ sijoituksessa ka¨ytta¨ma¨tta¨. Ta¨ta¨ ka¨sitella¨a¨n aliluvussa 7.1.3. 6.4. C++: Periytymisen ka¨ytto¨ laajentamiseen 157 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Luokan esittely . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1 class Kirja 2 { 3 public: 4 Kirja(std::string const& nimi, std::string const& tekija); 5 virtual ~Kirja(); 6 std::string annaNimi() const; 7 std::string annaTekija() const; ... 11 private: ... 13 std::string nimi ; 14 std::string tekija ; 15 }; . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Luokan toteutus . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1 Kirja::Kirja(string const& n, string const& t) : nimi (n), tekija (t) 2 { 3 cout << \"Kirja \" << nimi << \" luotu\" << endl; 4 } 5 6 Kirja::~Kirja() 7 { 8 cout << \"Kirja \" << nimi << \" tuhottu\" << endl; 9 } 10 11 string Kirja::annaNimi() const 12 { 13 return nimi ; 14 } ... LISTAUS 6.3: Kirjan tiedot muistava luokka 6.4. C++: Periytymisen ka¨ytto¨ laajentamiseen 158 Jos olemassa olevaa kirjaluokkaa halutaan ka¨ytta¨a¨ uuden luokan pohjana periytymisessa¨, on ta¨rkea¨a¨ ensin varmistua siita¨, etta¨ uusi luokka tarjoaa sellaisenaan kaikki vanhan luokan palvelut ja lisa¨a¨ sii- hen lisa¨ksi uusia palveluita. Kirjan ja kirjaston kirjan tapauksessa na¨- ma¨ edellytykset toteutuvat, koska kaikki kirjan tarjoamat palvelut so- pivat sellaisenaan myo¨s kirjaston kirjalle. Listauksessa 6.4 on luokas- ta Kirja periytetty uusi laajempi luokka KirjastonKirja, joka tarjoaa kaikki kirjan palvelut ja lisa¨ksi palautuspa¨iva¨ma¨a¨ra¨n ka¨sittelyn. Periytta¨mista¨ ei koskaan kannata ka¨ytta¨a¨ turhaan, koska se moni- mutkaistaa ohjelmaa. Esimerkiksi ylla¨ oleva kirjastonkirjan periytta¨- minen kirjasta on perustelua vain, jos jossain todella tarvitaan myo¨s tavallista kirjaluokkaa tai jos kirjaluokka on saatu muualta. Mika¨li . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Luokan esittely . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1 class KirjastonKirja : public Kirja 2 { 3 public: 4 KirjastonKirja(std::string const& nimi, std::string const& tekija, 5 Paivays const& palpvm); 6 virtual ~KirjastonKirja(); 7 bool onkoMyohassa(Paivays const& tanaan) const; ... 9 private: 10 Paivays palpvm ; 11 }; . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Luokan toteutus . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1 KirjastonKirja::KirjastonKirja(string const& nimi, string const& tekija, 2 Paivays const& palpvm) : Kirja(nimi, tekija), palpvm (palpvm) 3 { 4 cout << \"Kirjastonkirja \" << nimi << \" luotu\" << endl; 5 } 6 7 KirjastonKirja::~KirjastonKirja() 8 { 9 cout << \"Kirjastonkirja \" << annaNimi() << \" tuhottu\" << endl; 10 } 11 12 bool KirjastonKirja::onkoMyohassa(Paivays const& tanaan) const 13 { 14 return palpvm .paljonkoEdella(tanaan) < 0; 15 } LISTAUS 6.4: Kirjaston kirjan palvelut tarjoava aliluokka 6.5. C++: Virtuaalifunktiot ja dynaaminen sitominen 159 sen sijaan ollaan kirjoittamassa alusta alkaen kirjaston lainausrekis- teria¨ ja tiedeta¨a¨n, etta¨ kaikki ohjelmassa esiintyva¨t kirjat ovat palau- tuspa¨iva¨ma¨a¨ra¨llisia¨ kirjaston kirjoja, kannattaa ehka¨ suoraan lisa¨ta¨ palautuspa¨iva¨ma¨a¨ra¨n ka¨sittely luokan Kirja sisa¨lle ja kenties nimeta¨ luokka uudelleen. Jos sen sijaan ohjelmassa tarvitaan seka¨ tavallisia kirjoja etta¨ kir- jaston kirjoja, kahden luokan ja periytymisen ka¨ytto¨ kannattaa. Ta- vallinen kirjaolio ainakin vaatii va¨hemma¨n muistia kuin kirjastonkir- jaolio, jonka ta¨ytyy myo¨s muistaa pa¨iva¨ma¨a¨ra¨. Lisa¨ksi kirjaston kirjaa on vaikea ka¨ytta¨a¨ tavallisena kirjana, koska esimerkissa¨ uutta kirjaa luotaessa rakentajalle ta¨ytyy aina antaa palautuspa¨iva¨ma¨a¨ra¨. 6.5 C++: Virtuaalifunktiot ja dynaaminen sitomi- nen Joskus aliluokan olion on tarpeen suorittaa kantaluokasta perima¨n- sa¨ palvelu hieman kantaluokasta poikkeavalla tavalla. Toisella taval- la ilmaistuna aliluokka saattaa haluta peria¨ kantaluokalta vain ja¨sen- funktion ulkoisen rajapinnan, mutta ei toteutusta. C++ tarjoaa aliluo- kalle mahdollisuuden tarjota oman toteutuksensa kantaluokalta peri- ma¨lleen ja¨senfunktiolle, jos kyseinen ja¨senfunktio on kantaluokassa ma¨a¨ritelty virtuaaliseksi. Ta¨llaista virtuaalista ja¨senfunktiota kutsu- taan yleensa¨ lyhyesti virtuaalifunktioksi (virtual function). 6.5.1 Virtuaalifunktiot Ja¨senfunktio ma¨a¨ritella¨a¨n kantaluokan esittelyssa¨ virtuaaliseksi lisa¨a¨- ma¨lla¨ ja¨senfunktion eteen avainsana virtual. Ta¨ma¨n ja¨lkeen kanta- luokasta periytetta¨villa¨ aliluokilla on kaksi mahdollisuutta: • Hyva¨ksya¨ kantaluokan tarjoama ja¨senfunktion toteutus. Ta¨llo¨in aliluokan ei tarvitse tehda¨ mita¨a¨n eli kantaluokan toteutus pe- riytyy automaattisesti myo¨s aliluokkaan. • Kirjoittaa oma toteutuksensa perima¨lleen ja¨senfunktiolle. Ta¨s- sa¨ tapauksessa aliluokan esittelyssa¨ esitella¨a¨n ja¨senfunktio uu- delleen, ja sen ja¨lkeen aliluokan toteutuksessa kirjoitetaan ja¨- senfunktiolle uusi toteutus aivan kuin normaalille aliluokan ja¨- 6.5. C++: Virtuaalifunktiot ja dynaaminen sitominen 160 senfunktiolle. Aliluokan esittelyssa¨ avainsanan virtual toista- minen ei ole pakollista, mutta kylla¨kin hyva¨n ohjelmointityylin mukaista. Olio-ohjelmoinnin kannalta on ta¨rkea¨a¨, etta¨ muutettu ja¨senfunk- tion toteutus tarjoaa kantaluokan kannalta saman palvelun kuin al- kupera¨inenkin. On myo¨s huomattava, etta¨ aliluokka voi muuttaa vain virtuaalifunktion toteutusta, ei esimerkiksi paluutyyppia¨ tai paramet- rien lukuma¨a¨ra¨a¨ tai tyyppia¨. ISO C++ sallii nykyisin, etta¨ jos alkupera¨i- nen paluutyyppi on osoitin tai viite luokkaan, niin uuden ja¨senfunk- tion paluutyyppi voi olla osoitin tai viite alkupera¨isen paluutyypin aliluokkaan. Ta¨lle paluutyypin kovarianssille (covariance) ei kuiten- kaan ole kovin usein ka¨ytto¨a¨ (mutta sita¨ tullaan kylla¨ ka¨ytta¨ma¨a¨n ali- luvussa 7.1.3).] Listauksessa 6.5 seuraavalla sivulla on kaksi aiemmin esitellyn luokan Kirja ja¨senfunktiota, jotka luokka ma¨a¨rittelee virtuaalisiksi, joten aliluokilla on mahdollisuus toteuttaa ne omalla tavallaan. Lis- tauksessa 6.6 sivulla 162 on luokka KirjastonKirja ma¨a¨ritellyt uu- delleen ja¨senfunktion tulostaTiedot niin, etta¨ se tulostaa myo¨s pa- lautuspa¨iva¨ma¨a¨ra¨n. Sen sijaan ja¨senfunktion sopiikoHakusana luok- ka perii kantaluokalta sellaisenaan toteutusta myo¨ten. Kirjaston kirjan tietojen tulostuksen toteutuksessa ta¨ytyy saa- da jotenkin tulostettua myo¨s kantaluokkaosassa olevat kirjan tie- dot. Ta¨ma¨n toteuttaa kantaluokan versio funktiosta tulostaTiedot, joten aliluokan uusi versio kutsuu sita¨ rivilla¨ 29. Pelkka¨ kutsu tulostaTiedot(virta) kutsuisi rekursiivisesti aliluokan funktiota it- sea¨a¨n, joten kyseisella¨ rivilla¨ ta¨ytyy kertoa erikseen, etta¨ halutaan kutsua kantaluokan versiota funktiosta. Ta¨ma¨ tehda¨a¨n lisa¨a¨ma¨lla¨ funktion eteen ma¨a¨re Kirja::, joka ma¨a¨ra¨a¨ minka¨ luokan versiota kutsutaan. 6.5.2 Dynaaminen sitominen Periytymishierarkiat ja virtuaalifunktiot saavat aikaan sen, etta¨ ja¨sen- funktion toteutus saattaa olla luokkahierarkiassa alempana, kuin mis- . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ]Periaatteessa oliokieli voi sallia vastaavantyyppisen asian myo¨s ja¨senfunktion parametreis- sa, mutta ta¨llo¨in periytymissuhteen ta¨ytyy menna¨ toiseen suuntaan – periytetyn luokan para- metrit ovat kantaluokan parametrien kantaluokkatyyppia¨. C++ ei kuitenkaan salli ta¨ta¨ kontra- varianssia (contravariance), eiva¨tka¨ salli monet muutkaan oliokielet. Na¨ista¨ asioista voi halu- tessaan lukea mm. kirjasta “Theory of Objects” [Abadi ja Cardelli, 1996]. 6.5. C++: Virtuaalifunktiot ja dynaaminen sitominen 161 . . . . . . . . . . . . . . . . . . . . . . . . . . . . Kantaluokan esittelyssa¨ . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1 class Kirja 2 { ... 8 virtual void tulostaTiedot(std::ostream& virta) const; 9 virtual bool sopiikoHakusana(std::string const& sana) const; 10 11 private: 12 void tulostaVirhe(std::string const& virheteksti) const; ... 15 }; . . . . . . . . . . . . . . . . . . . . . . . . . . Kantaluokan toteutuksessa . . . . . . . . . . . . . . . . . . . . . . . . . . 19 void Kirja::tulostaVirhe(string const& virheteksti) const 20 { 21 cerr << \"Virhe: \" << virheteksti << endl; 22 cerr << \"Kirjassa: \"; 23 tulostaTiedot(cerr); 24 cerr << endl; 25 } 26 27 void Kirja::tulostaTiedot(ostream& virta) const 28 { 29 virta << tekija << \" : \\\"\" << nimi << \"\\\"\"; 30 } 31 32 bool Kirja::sopiikoHakusana(string const& sana) const 33 { 34 return nimi .find(sana) != string::npos | | 35 tekija .find(sana) != string::npos; // Lo¨ytyiko¨ nimesta¨ tai tekija¨sta¨ 36 } LISTAUS 6.5: Luokan Kirja virtuaalifunktiot sa¨ ja¨senfunktio on alunperin otettu mukaan rajapintaan. Ta¨ma¨ ja pe- riytymisen “aliluokan-olio-kuuluu-kantaluokkaan” (is-a) -ilmio¨ saa- vat aikaan sen, etta¨ ka¨a¨nta¨ja¨ ei kaikissa tapauksissa pysty viela¨ ka¨a¨n- no¨saikana pa¨a¨ttelema¨a¨n, mita¨ rajapintafunktion toteutusta on tarkoi- tus kutsua, vaan pa¨a¨to¨s ta¨sta¨ siirtyy ajoaikaiseksi. Ta¨sta¨ ka¨yteta¨a¨n ni- mitysta¨ dynaaminen sitominen (dynamic binding). Listauksessa 6.7 sivulla 163 on funktio tulostaKirjat, joka ottaa parametrikseen taulukollisen kirjaosoittimia. Koska jokainen kirjas- ton kirja on periytymishierarkian mukaisesti myo¨s kirja, ta¨llainen taulukko voi sisa¨lta¨a¨ todellisuudessa osoittimia seka¨ kirjoihin etta¨ 6.5. C++: Virtuaalifunktiot ja dynaaminen sitominen 162 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Aliluokan esittelyssa¨ . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1 class KirjastonKirja : public Kirja 2 { ... 8 virtual void tulostaTiedot(std::ostream& virta) const; ... 11 }; . . . . . . . . . . . . . . . . . . . . . . . . . . . Aliluokan toteutuksessa . . . . . . . . . . . . . . . . . . . . . . . . . . . 27 void KirjastonKirja::tulostaTiedot(ostream& virta) const 28 { 29 Kirja::tulostaTiedot(virta); // Erikseen pyydeta¨a¨n kantaluokan palvelua 30 virta << \", palautus \" << palpvm ; 31 } LISTAUS 6.6: Luokan KirjastonKirja virtuaalifunktiot kirjastonkirjoihin, kuten listauksen riveilta¨ 24–28 na¨kyy. Mika¨a¨n ei tietysti esta¨ muita ohjelman osia periytta¨ma¨sta¨ Kirja-luokasta omia erikoistettuja kirjaluokkiaan, joita ne voivat myo¨s lisa¨ta¨ taulukkoon. Koska kirjan ja¨senfunktio tulostaTiedot on virtuaalinen, voidaan sen toteutus ma¨a¨ritella¨ uudelleen missa¨ tahansa periytetyssa¨ luokas- sa. Niinpa¨ rivilla¨ 14 ka¨a¨nta¨ja¨ tieta¨a¨ vain, etta¨ siina¨ kutsutaan jotain ja¨- senfunktion tulostaTiedot toteutusta. Ka¨a¨nta¨ja¨lla¨ ei kuitenkaan ky- seisella¨ rivilla¨ ole mita¨a¨n tietoa edes siita¨, mita¨ toteutuksia ja¨senfunk- tiolla voi olla, puhumattakaan siita¨, etta¨ ka¨a¨nta¨ja¨ pystyisi valitsemaan na¨ista¨ oikean. Niinpa¨ ka¨a¨nta¨ja¨ tuottaa kyseiseen kohtaan ohjelmaa koodin, joka ensin tarkastaa osoittimen pa¨a¨ssa¨ olevan olion todellisen luokan ja vasta sen ja¨lkeen kutsuu sille sopivaa ja¨senfunktion toteu- tusta. Kun nyt funktiota kutsutaan rivilla¨ 30, tapahtuu seuraavaa: Pa- rametrina annetun taulukon ensimma¨inen alkio osoittaa luokan Kirja olioon. Rivin 14 koodi kutsuu ta¨ma¨n vuoksi ja¨senfunktio- ta Kirja::tulostaTiedot. Silmukan seuraavalla kierroksella taulu- kon toinen alkio osoittaakin luokan KirjastonKirja olioon. Ta¨l- lo¨in ta¨sma¨lleen sama ohjelman rivi 14 kutsuukin ja¨senfunktiota KirjastonKirja::tulostaTiedot, koska se on oikea toteutus ta¨ma¨n- tyyppiselle oliolle. Ta¨lla¨ tavoin dynaaminen sitominen mahdollistaa sen, etta¨ sama ja¨senfunktiokutsu ka¨ytta¨ytyy eri tavalla riippuen siita¨, minka¨ tyyppi- 6.5. C++: Virtuaalifunktiot ja dynaaminen sitominen 163 10 void tulostaKirjat(vector<Kirja*> const& kirjat) 11 { 12 for (unsigned int i = 0; i != kirjat.size(); ++i) 13 { 14 kirjat[i]->tulostaTiedot(cout); 15 cout << endl; 16 } 17 } 18 19 int main() 20 { 21 vector<Kirja*> kirjaHylly; 22 23 // Huom! Alla olevasta puuttuu muistin loppumiseen varautuminen 24 kirjaHylly.push back( 25 new Kirja(\"Axiomatic\", \"Greg Egan\")); // [Egan, 1995] 26 kirjaHylly.push back( 27 new KirjastonKirja(\"Matemaattisia olioita\", \"Leena Krohn\", 28 Paivays(31,10,1999))); // [Krohn, 1992] 29 30 tulostaKirjat(kirjaHylly); // Tulostetaan kirjat 31 32 for (unsigned int i = 0; i != kirjaHylly.size(); ++i) 33 { 34 delete kirjaHylly[i]; kirjaHylly[i] = 0; 35 } 36 } LISTAUS 6.7: Dynaaminen sitominen C++:ssa nen olio osoittimen tai viitteen pa¨a¨ssa¨ on. Jos kantaluokasta Kirja periyteta¨a¨n jossain vaiheessa jokin toinen kirjaluokka, esimerkiksi MyytavaKirja, funktioon tulostaKirjat ei tarvitse tehda¨ mita¨a¨n muu- toksia, vaan se osaa automaattisesti kutsua myo¨s uuden kirjan tulos- tusja¨senfunktiota. Ta¨ssa¨ mielessa¨ funktion koodi on yleiska¨ytto¨inen eli geneerinen. Dynaaminen sitominen toimii myo¨s, jos virtuaalifunktiota kut- sutaan kantaluokan omasta ja¨senfunktiosta. Listauksessa 6.5 sivul- la 161 oli ma¨a¨ritelty riveilla¨ 19–25 luokan sisa¨inen ja¨senfunk- tio tulostaVirhe, jota kirjaluokan ja¨senfunktiot voivat ka¨ytta¨a¨ vir- heilmoitusten tulostamiseen. Ta¨ma¨ koodi kutsuu ja¨senfunktiota tulostaTiedot. Koska ta¨ma¨ ja¨senfunktio on virtuaalinen, ka¨yteta¨a¨n sen kutsumiseen dynaamista sitomista myo¨s luokan omien ja¨sen- 6.5. C++: Virtuaalifunktiot ja dynaaminen sitominen 164 funktioiden koodissa. Kun nyt virheilmoitusfunktiota kutsutaan jollekin kirjaoliolle tai siita¨ periytetylle oliolle, tutkitaan tulostaTiedot-kutsun yhteydessa¨, mika¨ on olion itsensa¨ todellinen luokka. Ta¨ma¨n ja¨lkeen kutsutaan ta¨- ma¨n todellisen luokan ma¨a¨ra¨a¨ma¨a¨ tulostusfunktiota. Na¨in kantaluo- kan omat ja¨senfunktiot voivat ka¨ytta¨a¨ hyva¨kseen aliluokissa ma¨a¨ritel- tyja¨ virtuaalifunktioiden toteutuksia, vaikka niilla¨ ei edes ole mita¨a¨n tietoa siita¨, millaisia aliluokkia ohjelmassa on olemassa! 6.5.3 Olion tyypin ajoaikainen tarkastaminen Kantaluokkaosoittimen pa¨a¨ssa¨ olevalle oliolle voi kutsua vain kanta- luokan rajapinnassa olevia funktioita, vaikka osoittimen pa¨a¨ssa¨ todel- lisuudessa olisikin aliluokan olio. Ta¨ma¨ johtuu siita¨, etta¨ ka¨a¨nno¨s- vaiheessa ka¨a¨nta¨ja¨lla¨ ei ole mita¨a¨n mahdollisuutta varmistua siita¨, minka¨ tyyppinen olio kantaluokkaosoittimen pa¨a¨ssa¨ on. Normaalisti kantaluokan rajapinnan ka¨ytta¨minen riitta¨a¨ ohjelmalle, ja dynaami- nen sitominen mahdollistaa sen, etta¨ aliluokan olio ka¨ytta¨ytyy sille ominaisella tavalla. Joskus tulee kuitenkin tarve pa¨a¨sta¨ ka¨siksi aliluokan rajapintaan. Jos aliluokan olio on kuitenkin kantaluokkaosoittimen pa¨a¨ssa¨, ei ali- luokan rajapinta ole na¨kyvissa¨. Ainoa vaihtoehto on luoda osoitin aliluokkaan ja laittaa se osoittamaan kantaluokkaosoittimen pa¨a¨ssa¨ olevaan olioon. Ta¨ma¨n ja¨lkeen aliluokan rajapintaan pa¨a¨see ka¨siksi ka¨ytta¨ma¨lla¨ saatua uutta osoitinta. ISOC++:ssa¨ on olion tyypin ajoaikaista tutkimista varten ominai- suus, jota yleisesti kutsutaan nimella¨ RTTI (Run-Time Type Informa- tion). Jotta olion tyyppia¨ voisi tutkia ajoaikana, ta¨ytyy olion luokassa olla va¨hinta¨a¨n yksi virtuaalinen ja¨senfunktio. Ta¨ma¨ vaatimus toteu- tuu ka¨yta¨nno¨ssa¨ aina, koska jokaisen kantaluokan purkajan tulisi aina olla virtuaalinen ja ta¨ma¨ virtuaalisuus periytyy myo¨s aliluokkiin. Kantaluokkaosoittimesta aliluokkaosoittimeksi Muunnos kantaluokkaosoittimesta aliluokkaosoittimeksi onnistuu tyyppimuunnoksella dynamic cast<Aliluokka*>(kluokkaosoitin). Sen toiminta on kaksivaiheinen. Ensin tarkastetaan, etta¨ kantaluok- kaosoittimen pa¨a¨ssa¨ oleva olio todella on aliluokan olio tai jonkin ali- luokasta edelleen periytetyn ja¨lkela¨isluokan olio. Na¨in varmistetaan, 6.5. C++: Virtuaalifunktiot ja dynaaminen sitominen 165 etta¨ olion voi turvallisesti sijoittaa aliluokkaosoittimen pa¨a¨ha¨n. Kan- taluokkaosoitinhan saattaa myo¨s osoittaa kantaluokan tai jonkin toi- sen kantaluokasta periytetyn aliluokan olioon. Jos kantaluokkaosoittimen pa¨a¨ssa¨ on va¨a¨ra¨n tyyppinen olio, pa- lautetaan tyhja¨ osoitin 0. Jos kantaluokkaosoittimen pa¨a¨ssa¨ on oikean tyyppinen olio, palautetaan kyseiseen olioon osoittava aliluokkaosoi- tin. Ta¨lla¨ tavoin dynamic cast-muunnoksen avulla ohjelma voi sa- malla kertaa tarkastaa, etta¨ olio todella on oikeaa tyyppia¨, ja viela¨ saada olion oikeantyyppisen osoittimen pa¨a¨ha¨n. Listaus 6.8 sisa¨lta¨a¨ esimerkin tyyppimuunnoksen ka¨yto¨sta¨. dynamic cast-muunnosta voi ka¨ytta¨a¨ myo¨s olioviitteisiin, siis tuottamaan aliluokkaviitteen, joka viittaa samaan olioon kuin annet- tu kantaluokkaviite. Ta¨ssa¨ tapauksessa ainoa ero osoitinmuunnok- seen on, etta¨ jos kantaluokkaviitteen pa¨a¨ssa¨ on va¨a¨ra¨n tyyppinen olio, dynamic cast heitta¨a¨ poikkeuksen (jonka tyyppi on std::bad cast). Syyna¨ poikkeuksen heittoon on, etta¨ ei ole olemassa “tyhja¨a¨ viitetta¨”, joka voitaisiin palauttaa virhetilanteessa. Olion luokan selvitta¨minen Edella¨ mainittu dynamic cast on ka¨teva¨, kun halutaan pa¨a¨sta¨ ka¨sik- si aliluokan laajennettuun rajapintaan silloin, kun aliluokan olio on kantaluokkaosoittimen pa¨a¨ssa¨. Samoin dynamic cast on riitta¨va¨, jos vain halutaan testata, toteuttaako kantaluokkaosoittimen pa¨a¨ssa¨ ole- 1 bool myohassako(Kirja* kp, Paivays const& tanaan) 2 { 3 KirjastonKirja* kkp = dynamic cast<KirjastonKirja*>(kp); 4 if (kkp != 0) 5 { // Jos tultiin ta¨nne, kirja on kirjastonkirja 6 return kkp->onkoMyohassa(tanaan); 7 } 8 else 9 { // Jos tultiin ta¨nne, kirja ei ole kirjastonkirja 10 return false; // Ei siis ole myo¨ha¨ssa¨ 11 } 12 } LISTAUS 6.8: Olion tyypin ajoaikainen tarkastaminen 6.5. C++: Virtuaalifunktiot ja dynaaminen sitominen 166 va olio tietyn aliluokan rajapinnan eli kuuluuko olio tiettyyn aliluok- kaan tai johonkin siita¨ periytettyyn ja¨lkela¨isluokkaan. Joissain eritta¨in harvinaisissa tilanteissa tulee kuitenkin tarve saada selville, mihin luokkaan olio kuuluu, ja kenties viela¨ tallettaa ta¨ma¨ tieto johonkin tietorakenteeseen. Ta¨ha¨n dynamic cast ei kelpaa, koska silta¨ voi vain kysya¨, kuuluuko olio tiettyyn luokkaan tai sen ja¨l- kela¨isluokkaan. Ta¨llaista luokan kysymista¨ varten C++:sta¨ lo¨ytyy ope- raattori typeid ja luokka type info. typeid-mekanismin ka¨ytto¨o¨n tulisi kuitenkin suhtautua eritta¨in suurella varauksella. La¨hes kaikissa tapauksissa virtuaalifunktioiden (tai joskus kenties dynamic castin) ka¨ytto¨ on parempi, uudelleenka¨y- tetta¨va¨mpi ja selkea¨mpi vaihtoehto. Esimerkiksi toteutus, jossa funk- tiossa kysyta¨a¨n typeid:lla¨ luokan tyyppi ja sitten if-lauseilla valitaan sopiva koodi, ei ole hyva¨a¨ suunnittelua. Se vaatisi, etta¨ luokkia lisa¨t- ta¨essa¨ lisa¨ta¨a¨n funktioon aina uusi vaihtoehto jokaista uutta luokkaa varten. Sen sijaan kannattaa lisa¨ta¨ luokkien yhteiseen kantaluokkaan virtuaalifunktio, jonka toteutukset periytetyissa¨ luokissa sitten suo- rittavat halutun toiminnallisuuden. Na¨in saadaan kaikki luokkaan liittyva¨t asiat kapseloitua samaan paikkaan. Luokka type info esitella¨a¨n komennolla #include <typeinfo>. Luokan oliot “edustavat” jokainen jotain ohjelman luokkaa. Olion luokan saa selville lausekkeella typeid(olio), joka palauttaa sellaisen type info-olion, joka edustaa olion luokkaa. Lauseke typeid(Luokka) palauttaa taas annettua luokkaa vastaavan type info-olion. Kahta type info-luokan oliota voi vertailla keskena¨a¨n normaaleil- la operaattoreilla == ja !=. Oliot ovat keskena¨a¨n yhta¨ suuria, jos ne edustavat samaa luokkaa, muuten erisuuria. Osoittimia na¨ihin olioi- hin voi sitten ohjelmassa ka¨ytta¨a¨ vertailuavaimina, jos esimerkiksi jostain tietorakenteesta pita¨a¨ etsia¨ haluttuun luokkaan kuuluva olio. Luokan type info rajapinnassa on myo¨s ja¨senfunktio name, joka pa- lauttaa oliota vastaavaa luokkaa kuvaavan merkkijonon. Ta¨ma¨n tark- ka muoto on ka¨a¨nta¨ja¨kohtainen eika¨ va¨ltta¨ma¨tta¨ pelkka¨ luokan nimi. Sen testaaminen, onko osoittimen kp pa¨a¨ssa¨ oleva olio kirjaston kirja, ka¨y periaatteessa vertailulla if (typeid(*kp) == typeid(KirjastonKirja)) Ta¨ma¨ kuitenkin testaa vain, onko osoittimen pa¨a¨ssa¨ ta¨sma¨lleen luok- kaan KirjastonKirja kuuluva olio. Olio-ohjelmoinnin mukaan myo¨s 6.5. C++: Virtuaalifunktiot ja dynaaminen sitominen 167 jokainen ta¨sta¨ luokasta periytetty olio on kirjaston kirja, joten ta¨l- lainen ta¨sma¨llinen testaus ei yleensa¨ ole se mita¨ halutaan. Ta¨ma¨n vuoksi dynamic cast on yleensa¨ oikea vaihtoehto ta¨llaisiin testeihin ja typeid-operaattorin ka¨ytto¨ kannattaa ja¨tta¨a¨ tilanteisiin, joissa tie- to olion luokasta talletetaan jonnekin tai va¨liteta¨a¨n parametrina. Lis- taus 6.9 na¨ytta¨a¨ esimerkin koodista, joka etsii taulukosta ensimma¨i- sen annettuun luokkaan kuuluvan olion. 6.5.4 Ei-virtuaalifunktiot ja peitta¨minen Dynaamista sitomista ka¨ytetta¨essa¨ ka¨a¨nta¨ja¨n on osattava ja¨senfunk- tiokutsun yhteydessa¨ tuottaa ylima¨a¨ra¨inen koodi, joka tarkastaa olion todellisen tyypin ajoaikana. Ta¨ma¨n vuoksi ka¨a¨nta¨ja¨n on ka¨sitelta¨va¨ 1 #include <typeinfo> 2 using std::type info; ... 13 vector<Kirja*> ktaulukko; 14 15 Kirja* haeEnsimmainen(type info const& kirjanTyyppi) 16 { 17 for (unsigned int i = 0; i < ktaulukko.size(); ++i) 18 { 19 if (typeid(*ktaulukko[i]) == kirjanTyyppi) 20 { 21 return ktaulukko[i]; 22 } 23 } 24 return 0; // Ei lo¨ytynyt 25 } 26 27 int main() 28 { 29 // Etsita¨a¨n ensimma¨inen Kirjastonkirja 30 Kirja* kp = haeEnsimmainen(typeid(KirjastonKirja)); 31 if (kp != 0) 32 { 33 kp->tulostaTiedot(cout); 34 } 35 } LISTAUS 6.9: Esimerkki typeid-operaattorin ka¨yto¨sta¨ 6.5. C++: Virtuaalifunktiot ja dynaaminen sitominen 168 virtuaalifunktioita ja tavallisia ja¨senfunktioita eri tavalla — tavallisen ja¨senfunktiokutsun yhteydessa¨ha¨n ka¨a¨nta¨ja¨ tieta¨a¨ jo ka¨a¨nno¨saikana, minka¨ luokan ja¨senfunktiota kutsutaan. C++ antaa ohjelmoijalle myo¨s mahdollisuuden ma¨a¨ritella¨ periyte- tyssa¨ luokassa uudelleen kantaluokan ei-virtuaalisia ja¨senfunktioita. Ta¨llaisten ja¨senfunktioiden kutsumisessa ei kuitenkaan ka¨yteta¨ dy- naamista sitomista vaan aliluokan uusi toteutus peitta¨a¨ (hide) kanta- luokan toteutuksen (tarkasti ottaen kaikki kantaluokan samannimi- set ja¨senfunktiot) aliluokassa. Ta¨ma¨ tarkoittaa sita¨, etta¨ mika¨li ky- seista¨ ja¨senfunktiota kutsutaan suoraan aliluokan oliolle, kutsutaan aliluokan toteutusta. Jos taas kutsu tapahtuu kantaluokkaosoittimen tai -viitteen kautta, kutsutaan kantaluokan toteutusta. Na¨in kutsutta- va ja¨senfunktio riippuu ta¨ysin siita¨, miten ja¨senfunktiota ohjelmassa kutsutaan. Ta¨llainen kutsutilanteesta riippuva ja¨senfunktion valinta ei ole olio-ohjelmoinnissa la¨hes koskaan toivottavaa. Se tapahtuu kuitenkin helposti vahingossa silloin, kun kantaluokan ja¨senfunktion esittelys- ta¨ unohtuu avainsana virtual. Vaikka avainsana olisikin paikallaan aliluokassa, kantaluokkaosoittimen la¨pi ei ta¨llo¨in ka¨yteta¨ dynaamista sitomista ja ohjelma valitsee ta¨llaisissa tilanteissa va¨a¨ra¨n toteutuksen ja¨senfunktiolle. Koska virtual-sanan unohtuminen kantaluokan esittelysta¨ ei ai- heuta ka¨a¨nno¨svirhetta¨ vaan va¨a¨ra¨n toiminnan, on eritta¨in ta¨rkea¨a¨, et- ta¨ kantaluokissa muistetaan merkita¨ virtual-sanalla kaikki sellaiset ja¨senfunktiot, joiden toteutus saatetaan ma¨a¨ritella¨ uudellen aliluokis- sa! Useimmissa muissa oliokielissa¨ dynaamista sitomista ka¨yteta¨a¨n oletusarvoisesti, joten niissa¨ ta¨llaista vaaratekija¨a¨ ei ole. 6.5.5 Virtuaalipurkajat Olio-ohjelmissa tulee varsin usein esiin tilanne, jossa kantaluokka- osoitin laitetaan osoittamaan aliluokan olioon, joka on luotu dynaa- misesti new’lla¨. Ta¨llainen tilanne vaatii hieman erikoistoimenpiteita¨, jos olio aiotaan ohjelmassa tuhota deletella¨ kantaluokkaosoittimen kautta. Jotta olioiden tuhoaminen kantaluokkaosoittien kautta toimisi oi- kein, kantaluokassa ta¨ytyy merkita¨ luokan purkaja virtuaaliseksi li- sa¨a¨ma¨lla¨ sen eteen avainsana virtual. Sama tehda¨a¨n hyva¨n tyylin mukaisesti myo¨s aliluokkien purkajissa, mutta kielen kannalta se ei 6.5. C++: Virtuaalifunktiot ja dynaaminen sitominen 169 ena¨a¨ ole va¨ltta¨ma¨to¨nta¨, vaan purkajan virtuaalisuus periytyy aliluok- kiin automaattisesti. Mika¨li olio tuhotaan kantaluokkaosoittimen kautta ilman, etta¨ kantaluokan purkaja on virtuaalinen, ohjelman toiminta on C++-stan- dardin mukaan ma¨a¨rittelema¨to¨nta¨. Ka¨yta¨nno¨ssa¨ on mahdollista, etta¨ ohjelma kaatuu tai sitten kutsuu tuhottavalle oliolle va¨a¨ria¨ purkajia tai toimii muuten va¨a¨rin. Na¨iden virheiden vuoksi on ta¨rkea¨a¨, etta¨ jokaisen kantaluokan purkaja ma¨a¨ritella¨a¨n virtuaaliseksi! Ongelma juontaa juurensa siita¨, etta¨ ka¨a¨nta¨ja¨ ei deleten koodia tuottaessaan pysty osoittimen tyypista¨ pa¨a¨ttelema¨a¨n, minka¨ tyyppi- nen olio osoittimen pa¨a¨ssa¨ on. Historiallisista ja optimointiteknisis- ta¨ syista¨ C++ ei oletusarvoisesti yrita¨ pa¨a¨tella¨ osoittimen pa¨a¨ssa¨ olevan olion todellista tyyppia¨ ajoaikaisesti, vaan ohjelman toiminta on ja¨tet- ty ma¨a¨rittelema¨tto¨ma¨ksi. Kantaluokan purkajan virtuaalisuus toimii vinkkina¨ ka¨a¨nta¨ja¨lle, jolloin ka¨a¨nta¨ja¨ generoi tuhoamisen yhteyteen koodin, joka tarkastaa olion todellisen tyypin ja tuhoaa sen asianmu- kaisella tavalla. Useimmissa muissa oliokielissa¨ aliluokan olion tuhoaminen kan- taluokkaviitteen la¨pi on tehty aina oikein toimivaksi tai olioiden tu- houtuminen tapahtuu aina automaattisesti. Niinpa¨ niissa¨ ei yleensa¨ ole mita¨a¨n C++:n tapaisia ansoja ta¨ssa¨ suhteessa. 6.5.6 Virtuaalifunktioiden hinta Mika¨a¨n hyva¨ ei ole koskaan ilmaista. Virtuaalifunktiot ja dynaami- nen sitominen tekeva¨t mahdollisiksi todella joustavat ohjelmaraken- teet, joissa ja¨senfunktion kutsujan ei tarvitse tieta¨a¨ yksityiskohtia sii- ta¨, mita¨ ja¨senfunktion toteutusta kutsutaan. Ta¨ma¨ parantaa ohjelman ylla¨pidetta¨vyytta¨, laajennettavuutta seka¨ luettavuutta. Dynaamisella sitomisella on kuitenkin hintansa. Mika¨li dynaamista sitomista ka¨yteta¨a¨n, ohjelman koodin ta¨ytyy aina virtuaalifunktion kutsun yhteydessa¨ tarkastaa kutsun kohteena olevan olion todellinen luokka ja valita oikea versio ja¨senfunktion toteutuksesta. Ta¨ma¨ valinta ja¨a¨ la¨hes aina ajoaikaiseksi, joten valin- nan tekeminen hidastaa aina ja¨senfunktion kutsumista hiukan. Ka¨y- ta¨nno¨n testit osoittavat, etta¨ ja¨senfunktion kutsuminen dynaamista sitomista ka¨ytta¨en on yleensa¨ noin 4 % hitaampaa kuin normaalisti [Driesen ja Ho¨lzle, 1996]. On kuitenkin muistettava, etta¨ mika¨li dy- naamista sitomista todella tarvitaan ohjelmassa, vaatisi ilman virtu- 6.5. C++: Virtuaalifunktiot ja dynaaminen sitominen 170 aalifunktioita kirjoitettu koodi joka tapauksessa ylima¨a¨ra¨ista¨ koodia dynaamisen sitomisen matkimiseen, joten virtuaalifunktioiden hinta on todellisuudessa jonkin verran pienempi. Suoritusnopeuden lisa¨ksi virtuaalifunktiot vaikuttavat olioiden muistinkulutukseen. Mika¨li luokassa tai sen kantaluokassa on yksi- kin virtuaalifunktio, ta¨ytyy luokan olioihin tallettaa jonnekin tieto siita¨, minka¨ luokan olioita ne ovat. Yleensa¨ ta¨ma¨ tapahtuu virtuaa- litaulujen ja virtuaalitauluosoittimien avulla (niista¨ voi lukea enem- ma¨n vaikkapa kirjoista “Inside the C++ Object Model” [Lippman, 1996] ja “The Design and Evolution of C++” [Stroustrup, 1994]). Ta¨ma¨ tieto vie useimmissa ka¨a¨nta¨jissa¨ muistia yhden osoittimen verran, joten virtuaalifunktioita ka¨ytta¨vien luokkien olioiden muistinkulutus kas- vaa useimmissa ja¨rjestelmissa¨ 4 tavun verran. Ta¨ma¨ lisa¨muistinkulutus ei riipu luokan virtuaalifunktioiden ma¨a¨ra¨sta¨, joten olioiden koko ei ena¨a¨ ensimma¨isen virtuaalifunktion lisa¨a¨misen ja¨lkeen kasva, vaikka virtuaalifunktioita olisikin useita. Koska jokaisella kantaluokalla tulisi joka tapauksessa olla virtuaali- purkaja, ei muiden ja¨senfunktioiden ma¨a¨rittely virtuaaliseksi ka¨yta¨n- no¨ssa¨ vaikuta olioiden muistinkulutukseen. Olioiden koon kasvun lisa¨ksi ka¨a¨nta¨ja¨ joutuu yleensa¨ varaamaan jonkin verran muistia jokaista virtuaalifunktioita sisa¨lta¨va¨a¨ luokkaa varten. Ta¨ta¨ lisa¨muistia tarvitaan tyypillisesti yhdesta¨ kolmeen osoit- timen verran jokaista luokassa olevaa virtuaalifunktiota varten. Kos- ka ylima¨a¨ra¨ista¨ muistia varataan kuitenkin vain kerran koko luokkaa kohti eika¨ sen ma¨a¨ra¨ riipu luokan olioiden ma¨a¨ra¨sta¨, sen vaikutus ohjelman muistinkulutukseen on yleensa¨ mita¨to¨n. On muistettava, etta¨ ka¨a¨nta¨ja¨lla¨ on aina lupa optimoida koodia. Edella¨ mainittu ohjelman hidastuminen on mahdollinen, mutta jos ka¨a¨nta¨ja¨ pystyy jo ka¨a¨nno¨saikana pa¨a¨ttelema¨a¨n, ettei dynaamista si- tomista tarvita jossain yhteydessa¨, se voi tietysti tuottaa myo¨s tehok- kaampaa koodia. 6.5.7 Virtuaalifunktiot rakentajissa ja purkajissa Normaalisti dynaamista sitomista ka¨yteta¨a¨n aina, kun virtuaalifunk- tiota kutsutaan, ja na¨in kutsutaan aina “oikeaa” versiota virtuaali- funktiosta. Ta¨sta¨ ovat kuitenkin poikkeuksena rakentajissa ja purka- jissa tapahtuvat virtuaalifunktioiden kutsut. 6.5. C++: Virtuaalifunktiot ja dynaaminen sitominen 171 Luvussa 6.3.2 selitettiin, kuinka aliluokan olion rakentaminen ta- pahtuu “kerroksittain” niin, etta¨ ensin suoritetaan kantaluokan ra- kentaja ja sitten aliluokan rakentaja. Ta¨ma¨ suoritusja¨rjestys aiheut- taa sen, etta¨ kantaluokan rakentajan koodissa olion “aliluokkaosaa” ei ole viela¨ alustettu, eika¨ se na¨in ollen ole ka¨ytto¨kunnossa. Asian voi ajatella niinkin, etta¨ ennen aliluokan rakentajan suorittamista luota- va olio ei viela¨ ole aliluokan olio, vaan vasta kantaluokan olio. Kun sitten kantaluokan rakentaja on saatu suoritetuksi, olion tyyppi “ta¨y- dentyy” aliluokan olioksi aliluokan rakentajaan siirrytta¨essa¨, kunnes lopulta kaikki rakentajat on saatu suoritetuksi ja koko olio alustetuk- si. Ta¨llainen kerroksittainen rakentuminen vaikuttaa siihen, miten virtuaalifunktiot ka¨ytta¨ytyva¨t. Kantaluokan rakentajan koodissa olio ei viela¨ oikeastaan ole aliluokan olio eika¨ na¨in ollen pysty suoritta- maan aliluokassa ma¨a¨riteltya¨ virtuaalifunktion toteutusta. Niinpa¨ ra- kentajassa kutsuttu virtuaalifunktio ka¨ytta¨ytyykin ika¨a¨n kuin olio oli- si vain kantaluokan olio. Sen toteutusta ei etsita¨ alempaa luokkahie- rarkiasta, vaan ainoastaan itse kantaluokasta tai sen omista kantaluo- kista. Ta¨ma¨ ei yleensa¨ ole se, mita¨ ohjelmoija haluaa, joten rakenta- jien koodissa ei yleensa¨ pita¨isi kutsua virtuaalifunktioita, ellei ole aivan varma siita¨, etta¨ haluaa kutsua nimenomaan kantaluokan omaa toteutusta. Ta¨sma¨lleen sama tilanne tapahtuu purkajissa, mutta pa¨invastai- sista syista¨. Kun aliluokan olio tuhotaan, kutsutaan ensin aliluokan omaa purkajaa, jonka tehta¨va¨na¨ on siivota aliluokan osa oliosta. Ta¨- ma¨n ja¨lkeen siirryta¨a¨n kantaluokan purkajaan ja niin edelleen, kun- nes pa¨a¨sta¨a¨n periytymishierarkian huipulle. Kantaluokan purkajan koodissa olio ei ena¨a¨ oikeastaan ole aliluokan olio, koska ta¨ma¨ osa oliosta on jo siivottu tuhoamiskuntoon. Niinpa¨ kantaluokan purka- jassa kutsutut virtuaalifunktiot ka¨ytta¨ytyva¨t samoin kuin kantaluo- kan rakentajissa, ja niita¨ kutsutaan aivan kuin olio olisi kantaluokan olio. Ta¨ma¨ tarkoittaa, etta¨ virtuaalifunktion toteutusta etsita¨a¨n vain kantaluokasta itsesta¨a¨n tai sen omista kantaluokista. Na¨in myo¨ska¨a¨n purkajien koodissa ei yleensa¨ pita¨isi kutsua virtuaalifunktioita. 6.6. Abstraktit kantaluokat 172 6.6 Abstraktit kantaluokat Aiemmin aliluvussa 6.1 mainittiin abstraktit kantaluokat ja ma¨a¨ri- teltiin ne luokkahierarkiassa oleviksi luokiksi, joiden ainoa merkitys on olla periytymisessa¨ kantaluokkina ja joista ei ole mieleka¨sta¨ teh- da¨ olioita. Abstraktin kantaluokan perustava ominaisuus on, etta¨ sii- na¨ ma¨a¨ritella¨a¨n rajapintafunktioita, joille ma¨a¨ritella¨a¨n toteutus vasta myo¨hemmin luokkahierarkiassa. C++:ssa abstrakteja kantaluokkia voi ma¨a¨ritella¨ ka¨ytta¨ma¨lla¨ puhtai- ta virtuaalifunktioita (pure virtual function). Puhtaalla virtuaalifunk- tiolla tarkoitetaan virtuaalifunktiota, jonka toteutus on pakko ma¨a¨- ritella¨ aliluokissa. Virtuaalifunktio tehda¨a¨n puhdas virtuaalifunktio lisa¨a¨ma¨lla¨ luokan esittelyssa¨ sen ja¨lkeen ma¨a¨re =0. Listaus 6.10 seu- raavalla sivulla sisa¨lta¨a¨ esimerkkina¨ luokkien Ela¨in ja Lintu esittelyt, joissa funktiot liiku ja laula on ma¨a¨ritelty puhtaiksi virtuaalifunk- tioiksi. Abstraktiksi kantaluokaksi ma¨a¨ritella¨a¨n C++:ssa mika¨ tahansa luok- ka, jossa on ainakin yksi puhdas virtuaalifunktio. Ta¨ma¨ funktio voi olla luokassa itsessa¨a¨n ma¨a¨ritelty tai peritty kantaluokalta. Periyte- tyissa¨ luokissa voidaan ta¨llainen puhdas virtuaalifunktio sitten ma¨a¨- ritella¨ taas tavalliseksi virtuaalifunktioksi ja kirjoittaa sille kyseiseen luokkaan sopiva toteutus. Na¨in periytymisen yhteydessa¨ puhtaat vir- tuaalifunktiot muuttuvat taas normaaleiksi virtuaalifunktioiksi. Lopulta periytymishierarkian alapa¨a¨ssa¨ ollaan tilanteessa, jossa luokissa ei ena¨a¨ ole yhta¨a¨n puhdasta virtuaalifunktiota vaan kaikil- le kantaluokissa ma¨a¨ritellyille puhtaille virtuaalifunktioille on ole- massa toteutus. Ta¨llaiset luokat ovat vihdoin “konkreettisia” luokkia, joista lo¨ytyy toteutus kaikille rajapintafunktioille ja joista on na¨in ja¨r- keva¨a¨ luoda oliota. Listauksen 6.10 esimerkissa¨ luokka Kana ma¨a¨rit- telee kaikki kantaluokkien puhtaat virtuaalifunktiot tavallisiksi vir- tuaalifunktioiksi ja tarjoaa niille toteutuksen, joten kanat edustavat luokkahierarkiassa “konkreettisia” olioita. Ka¨a¨nta¨ja¨ pita¨a¨ automaattisesti huolen siita¨, etta¨ abstrakteista kan- taluokista ei voi luoda oliota. Niiden ka¨ytto¨ ohjelmassa rajoittuukin siihen, etta¨ niita¨ voidaan ka¨ytta¨a¨ periytymisessa¨ kantaluokkana, ja siihen, etta¨ ohjelmassa voi olla osoittimia ja viitteita¨ ta¨llaiseen kanta- luokkaan. Na¨iden osoittimien ja viitteiden pa¨a¨ha¨n voi sitten sijoittaa abstrakteista kantaluokista periytettyjen luokkien olioita ja kutsua ali- 6.6. Abstraktit kantaluokat 173 11 class Elain : public Elio 12 { 13 public: 14 virtual ~Elain(); 15 virtual void liiku(Sijainti paamaara) = 0; 18 }; ... 19 class Lintu : public Elain 20 { 21 public: 22 virtual ~Lintu(); 23 virtual void laula() = 0; 24 }; ... 25 class Kana : public Lintu 26 { 27 public: 28 virtual ~Kana(); 29 virtual void lisaanny(); // Toteutus lisa¨a¨ntymisfunktiolle 30 virtual void liiku(Sijainti paamaara); // Toteutus liikkumiselle 31 virtual void laula(); // Toteutus laulamiselle 32 33 private: 34 // Ta¨nne tarvittava sisa¨inen toteutus 35 }; LISTAUS 6.10: Abstrakteja kantaluokkia ja puhtaita virtuaalifunktioita luokissa ma¨a¨riteltyja¨ puhtaiden virtuaalifunktioiden toteutuksia dy- naamista sitomista hyva¨ksika¨ytta¨en. Vaikka abstrakti kantaluokka pakottaakin aliluokat kirjoittamaan omat toteutuksensa puhtaalle virtuaalifunktiolle, voi kantaluokka C++:ssa ta¨sta¨ huolimatta sisa¨lta¨a¨ oman toteutuksensa ta¨lle funktiol- le. Ta¨llaisessa tapauksessa voidaan ajatella, etta¨ kantaluokka sisa¨l- ta¨a¨ osittaisen toteutuksen, joka toteuttaa rajapintafunktiosta sellai- sen osan, joka on yhteista¨ kaikille ta¨sta¨ luokasta periytetyille luokille. Aliluokkien toteutukset voivat sitten kutsua ta¨ta¨ kantaluokan tarjoa- maa toteutusta, ja ta¨lla¨ tavalla kaikissa aliluokissa toistuvaa yhteis- ta¨ osaa ei tarvitse kopioida erikseen jokaiseen aliluokkaan. Puhtaal- le virtuaalifunktiolle, jolla on toteutus, lo¨ytyy joskus myo¨s muutakin ka¨ytto¨a¨. Niista¨ lo¨ytyy tietoa esimerkiksi teoksesta “More Exceptional C++” [Sutter, 2002c]. 6.6. Abstraktit kantaluokat 174 Listauksessa 6.11 on esimerkki luokasta Ela¨in, joka sisa¨lta¨a¨ ja¨- senmuuttujanaan tiedon ela¨imen sijainnista. Ta¨ssa¨ luokassa ma¨a¨ri- tella¨a¨n myo¨s “kaikille ela¨imille yhteinen” toteutus liikkumiselle, ni- mitta¨in ela¨imen sijainnin pa¨ivitys. Ta¨ma¨ toteutus ei kuitenkaan ole riitta¨va¨, koska liikkumiseen liittyy paljon muutakin (reitin valinta, li- hasten liikuttaminen), joten aliluokissa on pakko ma¨a¨ritella¨ “tarkem- pi” toteutus liikkumiselle. On huomattava, etta¨ vaikka abstrakti kan- taluokka sisa¨lta¨isikin toteutuksen puhtaalle virtuaalifunktiolle, kon- kreettisten aliluokkien on silti pakko ma¨a¨ritella¨ funktiolle oma toteu- tuksensa. C++:n abstraktit kantaluokat voivat puhtaiden virtuaalifunktioiden lisa¨ksi sisa¨lta¨a¨ mita¨ tahansa muutakin. Abstrakti kantaluokka voi ma¨a¨ritella¨ kaikille aliluokille yhteisia¨ ja¨senmuuttujia, uusia ja¨sen- funktioita ja niin edelleen. Se voi myo¨s ma¨a¨ritella¨ toteutuksia omien kantaluokkiensa ma¨a¨rittelemille puhtaille virtuaalifunktioille ja na¨in tarjota na¨ille rajapintafunktiolle toteutuksen, joka periytyy myo¨s kai- kille aliluokille. Na¨ma¨ aliluokat voivat puolestaan joko hyva¨ksya¨ kan- 11 class Elain : public Elio 12 { 13 public: 14 virtual ~Elain(); 15 virtual void liiku(Sijainti paamaara) = 0; 16 private: 17 Sijainti paikka ; 18 }; ... 1 // Kaikille ela¨imille yhteinen osa liikkumista 2 void Elain::liiku(Sijainti paamaara) 3 { 4 paikka = paamaara; 5 } ... 1 // Kanan liikkuminen 2 void Kana::liiku(Sijainti paamaara) 3 { 4 // Ta¨ha¨n kanaan liittyva¨t erityistoimet, ka¨veleminen jne. 5 Elain::liiku(paamaara); // Kantaluokka suorittaa kaikille yhteiset toimet 6 } LISTAUS 6.11: Puhdas virtuaalifunktio, jolla on myo¨s toteutus 6.7. Moniperiytyminen 175 taluokan tarjoaman toteutuksen tai kirjoittaa omansa. Na¨in abstrak- tien kantaluokkien ka¨ytto¨mahdollisuudet ovat varsin laajat. 6.7 Moniperiytyminen Periytymisessa¨ uusi luokka luodaan periytta¨ma¨lla¨ siihen jonkin ole- massa olevan luokan ominaisuudet. Mika¨a¨n ei tietysti esta¨ laajenta- masta ta¨ta¨ mekanismia niin, etta¨ luokka periyteta¨a¨n useasta kanta- luokasta. Ta¨sta¨ ka¨yteta¨a¨n nimitysta¨ moniperiytyminen (multiple in- heritance). Moniperiytyminen on yksinkertaisesta ideastaan huolimatta var- sin kiistelty mekanismi, jonka ka¨ytto¨ oikein on joskus vaikeaa ja jo- hon liittyy paljon piilevia¨ vaaroja. Niinpa¨ moniperiytymista¨ ei ole otettu mukaan la¨heska¨a¨n kaikiin oliokieliin ainakaan ta¨ydellisesti to- teutettuna. Ta¨ma¨n aliluvun tarkoituksena on antaa yleiskatsaus mo- niperiytymiseen C++:ssa ja antaa muutama esimerkki sen vaaroista ja hyo¨tyka¨yto¨sta¨. On kuitenkin heti alkuun syyta¨ varoittaa, etta¨ moni- periytymisen ka¨ytto¨a¨ tulisi yleensa¨ va¨ltta¨a¨, koska se saattaa varsin usein johtaa ongelmiin. Silla¨ on kuitenkin oma paikkansa olio-ohjel- moinnissa, joten sen perusteet on syyta¨ ka¨sitella¨. 6.7.1 Moniperiytymisen idea Moniperiytymisessa¨ luokka perii ominaisuudet useammalta kuin yh- delta¨ kantaluokalta. Ta¨ssa¨ suhteessa moniperiytyminen ei tuo mita¨a¨n uutta periytymiseen. Aliluokan olio koostuu normaalissa moniperiy- tymisessa¨ kaikkien kantaluokkiensa kantaluokkaosista seka¨ aliluokan omasta laajennusosasta. Kuva 6.6 seuraavalla sivulla kuvaa ta¨ta¨ tilan- netta. Niin sanotussa yhdista¨va¨ssa¨ moniperiytymisessa¨ eli virtuaa- limoniperiytymisessa¨ (shared multiple inheritance, virtual multiple inheritance) tilanne on hiukan toinen, mutta sita¨ ka¨sitella¨a¨n vasta ali- luvussa 6.8.2. Tavallisessa periytymisessa¨ aliluokan oliota voi aina ajatella myo¨s kantaluokan oliona. Samalla tavalla moniperiytymisessa¨ aliluokan olio kuuluu ika¨a¨n kuin yhtaikaa kaikkiin kantaluokkiinsa. Toisin sa- nottuna aliluokan olio ka¨y aina minka¨ tahansa kantaluokkansa edus- tajaksi. Kuvan 6.6 esimerkissa¨ ta¨ma¨ toteutuu, koska kirjaston kirja on aina yhtaikaa seka¨ kirja etta¨ kirjaston teos. Ta¨ma¨ “aliluokan olio on 6.7. Moniperiytyminen 176 Kirja Nimi, tekijä yms. KirjastonKirja (Hyllyluokka, laina−aika yms.) Mahdolliset uudet ominaisuudet (Nimi, tekijä yms.) KirjastonTeos Hyllyluokka, laina−aika yms. (a) Moniperiytyminen Kirja−osa KirjastonKirja−osa KirjastonKirja−Olio KirjastonTeos− osa (b) Aliluokan olio KUVA 6.6: Moniperiytyminen ja sen vaikutus aina kaikkien kantaluokkiensa olio” -periaate on eritta¨in ta¨rkea¨a¨ mo- niperiytymisen oikean ka¨yto¨n kannalta. 6.7.2 Moniperiytyminen eri oliokielissa¨ Moniperiytyminen tuo mukanaan hyo¨tyjen lisa¨ksi monia ongelmia ja virhemahdollisuuksia. Niinpa¨ eri oliokielten kehitta¨ja¨t ovat suhtau- tuneet moniperiytymiseen varsin eri tavoin. Joissain kielissa¨ moni- periytymista¨ ei ole ollenkaan (Smalltalk), joissain sen ka¨ytto¨a¨ on ra- joitettu (Java) ja joissain (kuten C++:ssa) se on annettu ongelmineen kaikkineen ohjelmoijan ka¨ytto¨o¨n. C++:n suhtautuminen moniperiytymiseen on yksi sallivimmista oliokielten joukossa. C++:ssa on pyritty antamaan ohjelmoijalle mah- dollisuus ka¨ytta¨a¨ moniperiytymista¨ juuri niin kuin ohjelmoija halu- aa, ja samalla kaikki moniperiytymisen mukanaan tuomat riskit ja on- gelmat on ja¨tetty ohjelmoijan ratkaistavaksi. Ta¨ma¨n tuloksena C++:n moniperiytyminen on kylla¨ voimakas tyo¨kalu, mutta toisaalta sita¨ on eritta¨in helppo ka¨ytta¨a¨ va¨a¨rin niin, etta¨ sen aiheuttamat ongelmat ha- vaitaan vasta liian myo¨ha¨a¨n. Moniperiytymisen ka¨ytto¨ C++:ssa vaatii- kin ohjelmoijalta itsekuria ja olioajattelun hallitsemista. Javassa varsinaista luokkien moniperiytymista¨ ei ole lainkaan. Jo- kaisella luokalla on vain yksi kantaluokka, josta se periytyy. Sen si- 6.7. Moniperiytyminen 177 jaan Java sallii erilliset rajapintaluokat (interface class), joita voi yh- distella¨ moniperiytymisen tapaan. Ta¨ma¨ tekee mahdolliseksi sen, et- ta¨ luokkien rajapintoja voi yhdistella¨ monesta luokasta vapaasti, sen sijaan toteutuksen moniperiytyminen ei ole mahdollista. Rajapinta- luokkia ka¨sitella¨a¨n tarkemmin aliluvussa 6.9. Smalltalk ottaa viela¨ Javaakin tiukemman kannan moniperiytymi- seen. Ta¨ssa¨ kielessa¨ moniperiytymista¨ ei yksinkertaisesti ole. Jokai- sella luokalla on tasan yksi va¨lito¨n kantaluokka, eika¨ ta¨ha¨n voi vai- kuttaa milla¨a¨n tavalla. Ka¨yta¨nno¨ssa¨ Smalltalkin periytyminen sallii kuitenkin kaiken minka¨ Javankin. Smalltalkissa ei nimitta¨in ole tiuk- kaa ka¨a¨nno¨saikaista tyypitysta¨, vaan esimerkiksi kielen olioviitteet voivat aina viitata mihin tahansa olioon sen luokasta riippumatta ja mille tahansa oliolle voi yritta¨a¨ kutsua mita¨ tahansa palvelua. Ta¨sta¨ johtuen Smalltalkissa voi helposti tehda¨ usealle eri luokalle toimivaa koodia, vaikkei luokilla olisikaan yhteista¨ rajapintaa ma¨a¨ra¨a¨va¨a¨ kan- taluokkaa. 6.7.3 Moniperiytymisen ka¨ytto¨kohteita Kiistanalaisuudestaan huolimatta moniperiytymiselle on muutamia selkeita¨ ka¨ytto¨kohteita, joissa siita¨ on hyo¨tya¨. Heti alkuun on kuiten- kin syyta¨ huomauttaa, etta¨ missa¨a¨n seuraavassa esitelta¨vista¨ tilanteis- ta moniperiytyminen ei ole aivan va¨ltta¨ma¨to¨n. Kaikki esitetyt tilan- teet voidaan aina ratkaista myo¨s ilman moniperiytymista¨ — kylla¨- kin usein hieman tyo¨la¨a¨mmin ja monimutkaisemmin (samaan tapaan kuin kaikki ohjelmat on mahdollista koodata ilman koko olio-ohjel- mointia). Ehka¨ tyypillisin ja turvallisin ka¨ytto¨kohde moniperiytymiselle on rajapintojen yhdista¨minen. Jos sama luokka toteuttaa kerralla useita (yleensa¨ abstraktien) kantaluokkien ma¨a¨ra¨a¨mia¨ rajapintoja, tekee mo- niperiytyminen ta¨ma¨n ilmaisemisen helpoksi. Luokka moniperiyte- ta¨a¨n kaikista niista¨ kantaluokista (rajapintaluokista), joiden rajapin- nat se toteuttaa, ja puhtaiden virtuaalifunktioiden toteutukset kirjoi- tetaan aliluokkaan. Ta¨ma¨ moniperiytymisen muoto on juuri se, jota Java-kieli tukee, vaikkei siina¨ ta¨ydellista¨ moniperiytymista¨ olekaan. Moniperiytymisen ka¨ytto¨ ta¨ha¨n tarkoitukseen on suhteellisen mutka- tonta ja varsin hyo¨dyllista¨, joten sita¨ ka¨sitella¨a¨n tarkemmin aliluvus- sa 6.9. 6.7. Moniperiytyminen 178 Toinen ka¨ytto¨kohde moniperiytymiselle on kahden tai useamman olemassa olevan luokan yhdista¨minen. Varsin monet valmiit olio- kirjastot sisa¨lta¨va¨t luokkia, jotka on suunniteltu ka¨ytetta¨va¨ksi kanta- luokkina, joita ohjelmoija laajentaa ja tarkentaa periytta¨ma¨lla¨ niista¨ omat luokkansa. Ta¨ma¨ pa¨tee erityisesti sovelluskehyksissa¨, jotka esi- tella¨a¨n lyhyesti aliluvussa 6.11. Jos nyt ohjelmoijalle tulee tarve kir- joittaa luokka, jonka selkea¨sti pita¨isi laajentaa tai tarkentaa useaa val- mista kantaluokkaa, on moniperiytyminen usein ainoa ka¨yta¨nno¨lli- nen vaihtoehto. Ta¨llaisessa tilanteessa on kuitenkin syyta¨ olla tarkka- na, koska joissain tapauksissa moniperiytetta¨va¨t kantaluokat saatta- vat ha¨irita¨ toistensa toimintaa. Ta¨llaisiin tilanteisiin tutustutaan ali- luvuissa 6.8.1 ja 6.8.2. Kolmantena moniperiytymista¨ ka¨yteta¨a¨n joskus luokkien koosta- miseen valmiista ominaisuuskokoelmista. Ta¨sta¨ ka¨yteta¨a¨n englannin- kielisessa¨ kirjallisuudessa usein nimitysta¨ mixin tai ﬂavours.^ Esi- merkkina¨ ta¨llaisesta periytymisesta¨ voisi olla tilanne, jossa kaikki lai- naamiseen ja palautuspa¨iva¨ma¨a¨ra¨a¨n liittyva¨t rajapinnat ja toiminnal- lisuus on kirjoitettu erilliseen luokkaan Lainattava. Vastaavasti tuot- teen myymiseen liittyva¨t asiat ovat luokassa Myyta¨va¨. Na¨iden avulla luokka KirjastonKirja voitaisiin luoda moniperiytta¨ma¨lla¨ peruskanta- luokka Kirja ja ominaisuusluokka Lainattava. Samoin kaupan oleva CD-ROM saataisiin moniperiytta¨ma¨lla¨ luokat Cdrom ja Myyta¨va¨. Edella¨ esitettyja¨ moniperiytymisen ka¨ytto¨tapoja on viela¨ mahdol- lisuus yhdista¨a¨. On esimerkiksi mahdollista tehda¨ erillinen rajapinta ja useita sen eri tavalla toteuttavia ominaisuusluokkia. Na¨ita¨ ka¨ytta¨en ohjelmoija voi moniperiytta¨ma¨lla¨ ilmoittaa oman luokkansa toteutta- van tietyn rajapinnan ja viela¨ periytta¨a¨ mukaan valitsemansa toteu- tuksen. Ta¨llainen moniperiytymisen ka¨ytto¨ ei kuitenkaan ole ena¨a¨ aivan yksinkertaista, eika¨ sita¨ ka¨sitella¨ ta¨ssa¨ tarkemmin. 6.7.4 Moniperiytymisen vaaroja Suuri osa moniperiytymisen vaaroista johtuu siita¨, etta¨ moniperiyty- minen on houkuttelevan helpontuntuinen vaihtoehto sellaisissakin tapauksissa, joissa se ei olioajattelun kannalta ole perusteltua. Moni- . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ^Englanninkieliset termit ovat la¨hto¨isin amerikkalaisista ja¨a¨telo¨baareista, joissa asiakkaat saavat usein itse pa¨a¨tta¨a¨ ja¨a¨telo¨nsa¨ maun, ja ja¨a¨telo¨ valmistetaan paikan pa¨a¨lla¨ sekoittamalla (mixing) vaniljaja¨a¨telo¨o¨n marjoja, suklaata, pa¨hkino¨ita¨ tai muita makuaineita (ﬂavours). Ta¨sta¨ syysta¨ oliokirjallisuudessa “peruskantaluokasta” ka¨yteta¨a¨n joskus nimitysta¨ vanilla. 6.7. Moniperiytyminen 179 periytyminenkin on periytymista¨, joten periytetyn luokan olion ta¨y- tyy kaikissa tilanteissa olla ka¨sitteellisesti myo¨s kaikkien kantaluok- kiensa olio. Moniperiytymista¨ ei voi esimerkiksi ka¨ytta¨a¨ siihen, etta¨ aliluokan olio olisi va¨lilla¨ yhden, va¨lilla¨ toisen kantaluokan edusta- ja. Vaikkapa luokkaa Vesitaso ei voisi todenna¨ko¨isesti moniperiytta¨a¨ luokista Lentokone ja Vene, koska vesitaso ei kykene yhtaikaa liikku- maan seka¨ ilmassa etta¨ vedessa¨, toisin sanoen ilmassa ollessaan se ei ole vene ja vastaavasti vedessa¨ liikkuessaan se ei ole lentokone. Moniperiytyminen aiheuttaa normaalin periytymisen tapaan myo¨s sen, etta¨ kaikkien kantaluokkien julkiset rajapinnat na¨kyva¨t ali- luokasta ulospa¨in (poislukien C++:n yksityinen ja suojattu periytymis- tapa, jotka eiva¨t ole perinteisen olioajattelun mukaisia). Niinpa¨ mo- niperiytymista¨ ei tule ka¨ytta¨a¨ tilanteissa, joissa halutaan saada luok- kaan mukaan vain jonkin toisen luokan toiminnallisuus, muttei jul- kista rajapintaa. Esimerkiksi vaikka Paivays-luokka sisa¨lta¨a¨kin palau- tuspa¨iva¨ma¨a¨ra¨n ka¨sittelyyn tarvittavan toiminnallisuuden, ei luok- kaa KirjastonKirja voi moniperiytta¨a¨ luokista Kirja ja Paivays. Vaikka kirjaston kirja sisa¨lta¨a¨kin palautuspa¨iva¨ma¨a¨ra¨n, se ei ole pa¨iva¨ys, ei- ka¨ sen julkinen rajapinta sisa¨lla¨ Pa¨iva¨ys-luokan operaatioita. Ta¨ssa¨ Pa¨iva¨ys-ja¨senmuuttuja on selkea¨sti oikea ratkaisu. Vaikka moniperiytymista¨ ka¨ytetta¨isiinkin olioajattelun kannalta oikein, se tekee ohjelman luokkarakenteesta helposti vaikeaselkoisen. Lisa¨ksi moniperiytyminen aiheuttaa helposti ongelmia kuten rajapin- tojen moniselitteisyytta¨ ja vaikeuksia olion elinkaaren hallinnassa. Na¨ita¨ ongelmia ja niiden ratkaisemista ka¨sitella¨a¨n lyhyesti myo¨hem- missa¨ aliluvuissa. Kaikki ta¨ma¨ aiheuttaa kuitenkin sen, etta¨ monipe- riytymista¨ ei kannata ka¨ytta¨a¨, ellei sille ole painavia perusteita. 6.7.5 Vaihtoehtoja moniperiytymiselle Jos moniperiytymista¨ ka¨yteta¨a¨n todella mallintamaan sita¨, etta¨ aliluo- kan olio pysyva¨sti kuuluu useaan kantaluokkaan ja toteuttaa niiden rajapinnat, ei moniperiytymiselle ole helppoa vaihtoehtoa. Ta¨llo¨in on kuitenkin usein kyse rajapintaluokkien periytta¨misesta¨, joka ei juu- rikaan aiheuta ongelmia ja joka onnistuu myo¨s esimerkiksi Javassa, jossa varsinaista moniperiytymista¨ ei ole. Mika¨li tarve moniperiytymiseen sen sijaan tulee siita¨, etta¨ halu- taan yhdistella¨ jo olemassa olevien luokkien ominaisuuksia, voi mo- niperiytymisen usein kierta¨a¨. Ta¨llo¨in on nimitta¨in usein mahdollista 6.8. C++: Moniperiytyminen 180 periytymisen sijaan ka¨ytta¨a¨ koostetta eli ottaa valmiit luokat kirjoi- tettavan luokan ja¨senmuuttujiksi. Ta¨llo¨in valmiiden luokkien ja¨sen- funktiot eiva¨t kuitenkaan na¨y ulospa¨in. Jos na¨ita¨ ja¨senfunktioita pi- ta¨isi pystya¨ kutsumaan kirjoitettavan luokan ulkopuolelta, ta¨ytyy ta¨- ha¨n luokkaan viela¨ kirjoittaa “la¨pikutsufunktiot” (call-through func- tion). Na¨illa¨ tarkoitetaan ja¨senfunktioita, jotka yksinkertaisesti kutsu- vat vastaavaa ja¨senmuuttujan ja¨senfunktiota, va¨litta¨va¨t sille saaman- sa parametrit ja palauttavat ja¨senfunktiolta saamansa paluuarvon kut- sujalle. Moniperiytymisen korvaaminen koostamisella vaatii jonkin ver- ran ka¨sityo¨ta¨, mutta silla¨ va¨lteta¨a¨n yleensa¨ moniperiytymisen muka- naan tuomat ongelmat ja vaarat. Lisa¨ksi se on ainoa vaihtoehto kie- lissa¨, joissa moniperiytymista¨ ei ole. On ehka¨ viela¨ syyta¨ korostaa, etta¨ koostaminen ei ka¨y vaihtoehdoksi moniperiytymiselle, jos luok- kien va¨linen “is-a”-periytymissuhde on va¨ltta¨ma¨to¨n esimerkiksi sen takia, etta¨ uutta luokkaa on pystytta¨va¨ ka¨sittelema¨a¨n kantaluokkao- soittimien kautta. 6.8 C++: Moniperiytyminen C++:ssa moniperiytyminen tehda¨a¨n yksinkertaisesti luettelemalla luo- kan esittelyn yhteydessa¨ kaksoispisteen ja¨lkeen kaikki luokan kanta- luokat ja niiden periytymistavat (siis ka¨yta¨nno¨ssa¨ la¨hes aina public, kuten aliluvussa 6.3 todettiin). Kuvan 6.6 luokka KirjastonKirja esi- telta¨isiin seuraavasti: class KirjastonKirja : public KirjastonTeos, public Kirja { // Ta¨nne Kirjastonkirjan uudet lisa¨ominaisuudet }; Ominaisuuksiltaan moniperiytyminen ei C++:ssa eroa mitenka¨a¨n normaalista periytymisesta¨. Kantaluokilta periytyneiden osien na¨ky- vyydet ja muut ominaisuudet toimivat samoin kuin tavallisessa pe- riytymisessa¨kin. Sama kantaluokka ei voi kuitenkaan esiintya¨ periy- tymislistassa kahteen kertaan. Toisin sanoen aliluokkaa ei voi suo- raan periytta¨a¨ “kahteen kertaan” samasta kantaluokasta. Epa¨suorasti ta¨ma¨ kuitenkin onnistuu, jos kahdella eri kantaluokalla on keskena¨a¨n 6.8. C++: Moniperiytyminen 181 yhteisia¨ kantaluokkia. Ta¨llainen tilanne johtaa kuitenkin helposti on- gelmiin, ja sita¨ ka¨sitella¨a¨n myo¨hemmin tarkemmin toistuvan moni- periytymisen yhteydessa¨ (aliluku 6.8.2). Koska kantaluokkia on nyt useita, ta¨ytyy aliluokan rakentajan alustuslistassa luonnollisesti kutsua kaikkien kantaluokkien raken- tajia. Muuten moniperiytyminen ei tuo ongelmia olioiden elinkaa- reen (poislukien ja¨lleen pahamaineinen toistuva moniperiytyminen). Olion tuhoutumisen yhteydessa¨ kaikkia tarvittavia purkajia kutsu- taan edelleen automaattisesti. 6.8.1 Moniperiytyminen ja moniselitteisyys Moniperiytyminen tuo joitain lisa¨ongelmia verrattuna normaaliin periytymiseen. Kun kantaluokkia on useita, saattaa tietysti ka¨yda¨ niin, etta¨ samanniminen ja¨senfunktio periytyy aliluokan rajapintaan useasta kantaluokasta. Ta¨llo¨in ongelmaksi tulee pa¨a¨tta¨a¨, minka¨ kan- taluokan ja¨senfunktiota tulisi kutsua, kun ja¨senfunktiota kutsutaan aliluokan oliolle. C++:ssa ta¨ma¨ ongelma tulee esiin, kunhan ja¨senfunk- tio nimi vain on sama, vaikka parametreissa olisikin eroa. Ta¨llainen tilanne voisi esimerkiksi ilmeta¨, jos molemmat luokista KirjastonTeos ja Kirja ma¨a¨rittelisiva¨t ja¨senfunktion tulostaTiedot. Ta¨ma¨ tilanne on ongelmallinen jo olioajattelunkin kannalta. Ali- luokan olion tulisi rajapinnaltaan olla kaikkien kantaluokkiensa olio. Jos nyt vaikka pa¨a¨tetta¨isiin, etta¨ periytymislistalla ensimma¨inen kan- taluokka “voittaa” ja sen ja¨senfunktio valitaan, oltaisiin ristiriidassa sen kanssa, etta¨ aliluokan olion tulisi olla toistenkin kantaluokkien olio, ja niissa¨ ta¨ma¨ ja¨senfunktio on erilainen. C++ ratkaisee ongelman niin, etta¨ yritys kutsua kahdesta eri kantaluokasta periytynytta¨ ja¨sen- funktiota aiheuttaa ka¨a¨nno¨saikaisen virheilmoituksen siita¨, etta¨ ja¨- senfunktion kutsu on moniselitteinen (ambiguous). Joskus eri kantaluokkien samannimiset ja¨senfunktiot tekeva¨t sa- mantyyppisia¨ asioita (mihin samalla tavalla nimetty ja¨senfunktio tie- tysti saattaa viitata), ja aliluokan toteutuksen tulisi suorittaa kaik- kien kantaluokkien toteutus kyseisesta¨ ja¨senfunktiosta. Ta¨ma¨ onnis- tuu kohtalaisen helposti, jos kyseinen ja¨senfunktio on kaikissa kan- taluokissa virtuaalinen, jolloin aliluokka voi antaa sille oman toteu- tuksen. Aliluokka voi nyt omassa toteutuksessaan kutsua vuorollaan kaikkien kantaluokkien toteutusta ja¨senfunktiosta ja kenties viela¨ li- sa¨ta¨ ja¨senfunktioon omaa toiminnallisuuttaan. 6.8. C++: Moniperiytyminen 182 Kuten jo mainittiin, ta¨llainen ratkaisu on perusteltu vain, jos kan- taluokkien ma¨a¨rittelyt ja toteutukset ja¨senfunktiolle eiva¨t ole milla¨a¨n lailla ristiriidassa. Esimerkiksi KirjastonKirja-luokan tulostaTiedot- ja¨senfunktion voisi kenties toteuttaa listauksen 6.12 esitta¨ma¨lla¨ ta- valla. Ika¨va¨ kylla¨, ka¨yta¨nno¨ssa¨ moniselitteisyytta¨ ei usein voi ratkais- ta na¨in helposti, mika¨ va¨henta¨a¨ moniperiytymisen ka¨ytto¨mahdolli- suuksia. Jos moniselitteinen ja¨senfunktio tosiaan tekee erilaisia, keskena¨a¨n epa¨yhteensopivia asioita eri kantaluokissa, ei ole mita¨a¨n mahdol- lisuutta saada aliluokan oliota ka¨ytta¨ytyma¨a¨n ta¨ma¨n ja¨senfunktion osalta molempia kantaluokkia tyydytta¨va¨lla¨ tavalla. Jos moniperiy- 4 class KirjastonTeos 5 { 6 public: 7 virtual void tulostaTiedot(std::ostream& virta) const; ... 8 }; 9 10 class Kirja 11 { 12 public: 13 virtual void tulostaTiedot(std::ostream& virta) const; ... 14 }; 15 16 class KirjastonKirja : public KirjastonTeos, public Kirja 17 { 18 public: 19 virtual void tulostaTiedot(std::ostream& virta) const; ... 20 }; ... 21 void KirjastonKirja::tulostaTiedot(std::ostream& virta) const 22 { 23 Kirja::tulostaTiedot(virta); 24 KirjastonTeos::tulostaTiedot(virta); 25 // Ta¨nne mahdollisesti viela¨ lisa¨a¨ tulostusta 26 } LISTAUS 6.12: Moniselitteisyyden yksi va¨ltta¨mistapa 6.8. C++: Moniperiytyminen 183 tymista¨ kuitenkin halutaan ka¨ytta¨a¨, voi ta¨ta¨ ongelmaa ratkaista eri- laisilla tekniikoilla riippuen siita¨, ovatko ja¨senfunktiot virtuaalisia vai eiva¨t. Seuraavassa esiteta¨a¨n muutaman ta¨llaisen tekniikan pe- rusteet. Tarkemmin niita¨ on ka¨sitelty esimerkiksi kirjoissa “Effective C++” [Meyers, 1998] ja “More Exceptional C++” [Sutter, 2002c]. Mika¨li moniselitteisille ja¨senfunktioille ei ole tarkoitus antaa uusia toteutuksia moniperiytetyssa¨ aliluokassa, muodostuu ainoak- si ongelmaksi ja¨senfunktion kutsuminen. Ta¨ma¨kin on ongelma vain, kun ja¨senfunktiota kutsutaan suoraan aliluokan rajapinnan kautta — siis suoraan oliota ka¨ytta¨en tai aliluokkatyyppisen osoittimen tai viit- teen kautta. Kantaluokkaosoittimienhan kautta moniselitteisyytta¨ ei ole, koska kullakin kantaluokalla on vain yksi mahdollinen toteutus ja¨senfunktiolle. Ta¨llaisessa tilanteessa ratkaisuvaihtoehtoja on kol- me: • Kutsutaan moniselitteista¨ ja¨senfunktiota aina kantaluokkaosoit- timien kautta — tarvittaessa vaikkapa va¨liaikaisia osoitinmuut- tujia ka¨ytta¨en. Ta¨ma¨ on helpoin mutta ehka¨ ko¨mpelo¨in ratkai- su. • Moniselitteisen ja¨senfunktion kutsun yhteydessa¨ on mahdollis- ta erikseen kertoa, minka¨ kantaluokan versiota halutaan kut- sua. Ta¨ma¨ onnistuu ::-syntaksilla. Esimerkiksi Kirja-luokan tulostaTiedot-ja¨senfunktiota voi kutsua syntaksilla KirjastonKirja k; k.Kirja::tulostaTiedot(); Ta¨ma¨ syntaksi on kuitenkin myo¨s ehka¨ hieman oudon na¨ko¨i- nen ja vaatii kantaluokan nimen kirjoittamista na¨kyviin kutsun yhteyteen. • Kolmas vaihtoehto on kirjoittaa aliluokkaan uudet keskena¨a¨n erinimiset ja¨senfunktiot, jotka kutsuvat kunkin kantaluokan to- teutusta moniselitteiselle ja¨senfunktiolle ::-syntaksilla. Ta¨llo¨in aliluokan rajapintaan ta¨ytyy dokumentoida tarkasti, etta¨ kysei- set ja¨senfunktiot vastaavat kantaluokkien toisennimisia¨ ja¨sen- funktiota. Ta¨llainen rajapinnan osittainen uudelleennimea¨mi- nen on joskus tarpeen. Listaus 6.13 seuraavalla sivulla na¨ytta¨a¨ esimerkin ta¨sta¨ ratkaisusta. 6.8. C++: Moniperiytyminen 184 1 class KirjastonKirja : public KirjastonTeos, public Kirja 2 { 3 public: ... 15 void tulostaKTeostiedot(std::ostream& virta) const; 16 void tulostaKirjatiedot(std::ostream& virta) const; 17 }; ... 18 void KirjastonKirja::tulostaKTeostiedot(std::ostream& virta) const 19 { 20 KirjastonTeos::tulostaTiedot(virta); 21 } 22 23 void KirjastonKirja::tulostaKirjatiedot(std::ostream& virta) const 24 { 25 Kirja::tulostaTiedot(virta); 26 } LISTAUS 6.13: Ja¨senfunktiokutsun moniselitteisyyden eliminointi Joskus moniperiytetyssa¨ aliluokassa olisi tarpeen toteuttaa kulta- kin kantaluokalta periytynyt samanniminen (ja na¨in moniselitteinen) ja¨senfunktio niin, etta¨ toteutus olisi erilainen kullekin kantaluokalle. Toisin sanoen kaksi kantaluokkaa tai useampi haluaa periytetyn luo- kan tarjoavan toteutuksen samannimisille ja¨senfunktioille (samoilla parametreilla), mutta itse toteutuksien pita¨isi olla keskena¨a¨n erilai- sia. Ta¨llaisen pulman ratkaiseminen vaatii va¨ha¨n lisa¨kikkailua. On- gelman voi ratkaista lisa¨a¨ma¨lla¨ hierarkiaan kaksi abstraktia va¨likanta- luokkaa, jotka “uudelleennimea¨va¨t” ongelmallisen ja¨senfunktion. Ta¨- ma¨ tapahtuu niin, etta¨ va¨liluokat ma¨a¨ritteleva¨t keskena¨a¨n erinimiset puhtaat virtuaalifunktiot, joita ne sitten kutsuvat ongelmallisen ja¨- senfunktion toteutuksessa. Lopullinen aliluokka toteuttaa na¨ma¨ uu- det ja¨senfunktiot haluamallaan tavalla. Na¨ita¨ toteutuksia voi sitten kutsua uusilla nimilla¨ aliluokan rajapinnan kautta ja normaalisti al- kupera¨isella¨ nimella¨ kantaluokkaosoittimen tai -viitteen kautta. Lis- taus 6.14 seuraavalla sivulla na¨ytta¨a¨ esimerkin ta¨sta¨ tekniikasta. C++:ssa on myo¨s mahdollista ma¨a¨ra¨ta¨ aliluokka ka¨ytta¨ma¨a¨n yh- den kantaluokan toteutusta using-ma¨a¨reen avulla. Ta¨ma¨ ei kuiten- kaan ole olioajattelun kannalta oikea ratkaisu, koska ta¨llo¨in dy- naaminen sitominen ei tapahdu halutulla tavalla ja eri kantaluok- 6.8. C++: Moniperiytyminen 185 1 class KirjastonTeosApu : public KirjastonTeos 2 { 3 public: 4 // Rakentaja parametrien va¨litta¨miseksi kantaluokan rakentajalle 5 virtual void tulostaTiedot(std::ostream& virta) const; 6 virtual void tulostaKTeostiedot(std::ostream& virta) const = 0; 7 }; 8 9 void KirjastonTeosApu::tulostaTiedot(std::ostream& virta) const 10 { 11 tulostaKTeostiedot(virta); // Kutsutaan aliluokan toteutusta 12 } 13 14 15 class KirjaApu : public Kirja 16 { 17 public: 29 // Rakentaja parametrien va¨litta¨miseksi kantaluokan rakentajalle 30 virtual void tulostaTiedot(std::ostream& virta) const; 31 virtual void tulostaKirjatiedot(std::ostream& virta) const = 0; 32 }; 33 34 void KirjaApu::tulostaTiedot(std::ostream& virta) const 35 { 36 tulostaKirjatiedot(virta); // Kutsutaan aliluokan toteutusta 37 } 38 39 40 class KirjastonKirja : public KirjastonTeosApu, public KirjaApu 41 { 42 public: 43 virtual void tulostaKTeostiedot(std::ostream& virta) const; 44 virtual void tulostaKirjatiedot(std::ostream& virta) const; 45 }; 46 47 void KirjastonKirja::tulostaKTeostiedot(std::ostream& virta) const 48 { 49 // Ta¨nne KirjastonTeos-luokalle sopiva toteutus 50 } 51 52 void KirjastonKirja::tulostaKirjatiedot(std::ostream& virta) const 53 { 54 // Ta¨nne Kirja-luokalle sopiva toteutus 55 } LISTAUS 6.14: Moniselitteisyyden eliminointi va¨liluokilla 6.8. C++: Moniperiytyminen 186 kaosoittimien kautta kutsutaan eri toteutuksia moniselitteiselle ja¨- senfunktiolle. Esimerkin tapauksessa luokan KirjastonKirja esitte- lyyn lisa¨tta¨isiin ja¨senfunktion tulostaTiedot esittelyn sijaan rivi “using Kirja::tulostaTiedot;”, joka valitsee Kirja-luokan toteutuk- sen ka¨ytto¨o¨n. Ta¨llo¨in kuitenkin edelleen KirjastonTeos-osoittimen la¨- pi kutsuttaessa ka¨ytetta¨isiin KirjastonTeos-luokan toteutusta, mika¨ ei ole oikein. 6.8.2 Toistuva moniperiytyminen Moniperiytyminen ei salli sita¨, etta¨ sama kantaluokka esiintyisi pe- riytymisessa¨ suoraan kahteen kertaan. Ta¨sta¨ huolimatta kantaluokka voi silti periytya¨ mukaan useaan kertaan epa¨suorasti va¨lissa¨ olevien kantaluokkien kautta. Kuva 6.7 seuraavalla sivulla na¨ytta¨a¨ tilanteen, jossa kantaluokka Teos pa¨a¨tyy aliluokkaan KirjastonKirja kahta reit- tia¨ kantaluokkien Kirja ja KirjastonTeos kautta. Ta¨llaisesta ka¨yteta¨a¨n usein termia¨ toistuva moniperiytyminen [Koskimies, 2000] (repeated multiple inheritance [Meyer, 1997]). Tilanne, jossa kantaluokka periytyy aliluokkaan useaa eri reittia¨, on hieman ongelmallinen. Se nimitta¨in hera¨tta¨a¨ kysymyksen siita¨, millainen aliluokan olion rakenteen tulisi olla. Jokaisessa Kirjaston- Kirja-oliossa ta¨ytyy selva¨sti olla tyyppeja¨ Kirja ja KirjastonTeos olevat kantaluokkaosat. Kummassakin kantaluokan oliossa puolestaan ta¨y- tyy olla tyyppia¨ Teos oleva kantaluokkaosa. Ta¨sta¨ hera¨a¨ kysymys, on- ko KirjastonKirja-oliossa na¨ita¨ Teos-tyyppisia¨ kantaluokkaosia kaksi (yksi kummallekin va¨litto¨ma¨lle kantaluokalle) vai yksi. Kuva 6.8 si- vulla 188 havainnollistaa na¨ita¨ vaihtoehtoja. Kummassakin vaihtoehdossa on omat loogiset perustelunsa. Jos Teos-kantaluokkaosia on kaksi, muodostuu KirjastonKirja-olio selva¨s- ti kahdesta erillisesta¨ kantaluokkaosasta. Ta¨llo¨in puhutaan erottele- vasta moniperiytymisesta¨ (replicated multiple inheritance). Ta¨ssa¨ ta- vassa on se hyva¨ puoli, etta¨ yhdessa¨ kantaluokkaosassa tehdyt muu- tokset eiva¨t mitenka¨a¨n vaikuta toiseen kantaluokkaosaan ja kanta- luokkaosat eiva¨t ha¨iritse toisiaan. Lisa¨ksi ta¨ma¨ vaihtoehto on mah- dollista toteuttaa ilman, etta¨ olion muistinkulutus tai tehokkuus ka¨r- sii mitenka¨a¨n. Niinpa¨ C++:n “normaalissa” moniperiytymisessa¨ kanta- luokkaosia voi olla ta¨lla¨ tavoin epa¨suorasti useita kappaleita. Joissain tapauksissa ta¨ma¨ on myo¨s selkea¨sti “oikea” vaihtoehto. 6.8. C++: Moniperiytyminen 187 Kirja Sivumäärä, painovuosi yms. KirjastonKirja KirjastonTeos Hyllyluokka, laina−aika yms. Teos Nimi, tekijä KUVA 6.7: Toistuva moniperiytyminen Usean kantaluokkaosan vaihtoehdossa on kuitenkin haittana se, etta¨ la¨heska¨a¨n aina — ja varsinkaan esimerkin tapauksessa — ei ole ja¨rkeva¨a¨, etta¨ KirjastonKirja-oliossa on ika¨a¨n kuin kaksi erillista¨ Teos- oliota. Kirjaston kirjahan on kaikesta huolimatta nimenomaan yksi ainoa Teos! Erottelevassa moniperiytymisessa¨ kirjaston kirjan nimi ja tekija¨ talletetaan turhaan kahteen kertaan. Jos kirjaston kirjan ni- mea¨ nyt viela¨ muutetaan vain Kirja-luokan rajapinnan kautta, sisa¨lta¨- va¨t Teos-kantaluokkaosat eri nimet, mika¨ ei tietenka¨a¨n ole suotavaa. Kaiken kukkuraksi Teos-osoitinta ei voi laittaa osoittamaan Kirjaston- Kirja-olioon, koska ka¨a¨nta¨ja¨ ei voi tieta¨a¨, kumpaan kantaluokkaosaan osoittimen pita¨isi osoittaa. Ta¨ssa¨ syntyy siis samantapainen monise- litteisyys kuin aiemmin ja¨senfunktioiden tapauksessa. C++:ssa kahden kantaluokkaosan ongelma on ratkaistu niin, etta¨ pe- riytymisen yhteydessa¨ luokka voi “antaa luvan” siihen, etta¨ tarvittaes- sa moniperiytymisen yhteydessa¨ muut aliolion osat voivat pita¨a¨ ta¨ta¨ kantaluokkaosaa yhteisena¨. Ta¨ta¨ sanotaan virtuaaliseksi moniperiy- tymiseksi (virtual multiple inheritance) tai yhdista¨va¨ksi moniperiy- tymiseksi (shared multiple inheritance), ja se merkita¨a¨n C++:n periyty- 6.8. C++: Moniperiytyminen 188 Kirja−osa Teos−osa KirjastonKirja−osa osa KirjastonTeos− Teos−osa KirjastonKirja−Olio (a) Erilliset kantaluokkaosat Kirja−osa Teos−osa KirjastonKirja−osa osa KirjastonTeos− KirjastonKirja−Olio (b) Yhteinen kantaluokkaosa KUVA 6.8: Toistuva moniperiytyminen ja olion rakenne misen yhteydessa¨ avainsanalla virtual. Jos esimerkissa¨ luokka Kirja on sallinut kantaluokan Teos jakamisen syntaksilla class Kirja : public virtual Teos // Tai virtual public. . . ja luokka KirjastonTeos on tehnyt saman, tulee KirjastonKirja-olioon vain yksi Teos-kantaluokkaosa. Moniperiytymisen yhteydessa¨ siis kaikki luokat, jotka on periytetty virtuaaliperiytymisella¨ samasta kan- taluokasta, jakavat keskena¨a¨n ta¨ma¨n kantaluokkaosan. Virtuaaliperiytyminen tuo mukanaan yhden mutkan aliluokan olion elinkaareen. Aliluokan olion luomisen yhteydessa¨ kutsutaan aliluokan rakentajaa, joka kutsuu kantaluokan rakentajaa ja niin edel- leen. Virtuaaliperiytymisessa¨ ta¨ma¨ tarkoittaisi, etta¨ jaetun kantaluok- kaosan rakentaja suoritettaisiin useita kertoja, koska sita¨ luonnollises- ti kutsutaan kaikkien siita¨ periytettyjen luokkien rakentajissa. Ta¨ma¨ ei tietenka¨a¨n ole ja¨rkeva¨a¨, koska yksi olio voidaan alustaa vain ker- taalleen. 6.8. C++: Moniperiytyminen 189 C++ ratkaisee ta¨ma¨n ongelman niin, etta¨ virtuaalisesti periytetyn kantaluokan rakentajaa ta¨ytyy kutsua suoraan sen luokan rakenta- jassa, jonka tyyppista¨ oliota ollaan luomassa, ja muut kantaluokkien rakentajissa olevat virtuaalisen kantaluokan rakentajakutsut ja¨teta¨a¨n suorittamatta. Esimerkin tapauksessa ta¨ma¨ tarkoittaa sita¨, etta¨ Teos- luokan rakentajaa pita¨a¨ kutsua suoraan KirjastonKirja-luokan ra- kentajan alustuslistassa, vaikka Teos ei olekaan ta¨ma¨n luokan va¨lito¨n kantaluokka. Kun luokan KirjastonKirja oliota luodaan, Teos-luokan rakentajan kutsut luokissa KirjastonTeos ja Kirja ja¨teta¨a¨n suoritta- matta. Na¨in yhteinen kantaluokkaosa alustetaan vain kertaalleen suo- raan “alimman tason” luokan rakentajassa. Kuva 6.9 selventa¨a¨ tilan- netta. Jos yhteisen kantaluokkaosan rakentajan kutsu puuttuu alim- man tason rakentajasta, yritta¨a¨ C++ tyypilliseen tapaansa kutsua kan- taluokan oletusrakentajaa. Ka¨yta¨nno¨ssa¨ ta¨ma¨ vaatimus kutsua jaetun kantaluokan rakentajaa alimmassa aliluokassa “yli periytymishierarkian” hankaloittaa virtu- Kirja Kirja::Kirja(...) : Teos(...), ... KirjastonTeos KirjastonTeos::KirjastonTeos(...) : Teos(...), ... Teos Teos::Teos(...) ... KirjastonKirja KirjastonKirja::KirjastonKirja(...) : Teos(...), KirjastonTeos(...), Kirja(...), ... Ei suoriteta Ei suoriteta virtual virtual KUVA 6.9: Rakentajat virtuaalisessa moniperiytymisessa¨ 6.9. Periytyminen ja rajapintaluokat 190 aalisen moniperiytymisen ka¨ytto¨a¨ ja tekee luokkien va¨lisen vastuun- jaon vaikeammaksi. Se on kuitenkin va¨ltta¨ma¨to¨n rajoitus olioajatte- lun kannalta. Jos kaksi toisistaan tieta¨ma¨to¨nta¨ kantaluokkaa joutuu jakamaan yhteisen kantaluokkaosan, on luonnollista etta¨ na¨ma¨ luokat moniperiytta¨va¨ aliluokka ma¨a¨ra¨a¨, miten ta¨ma¨ yhteinen kantaluokka alustetaan. Helpommaksi tilanne muuttuu, jos jaetulla kantaluokal- la on vain oletusrakentaja, jolloin rakentajan parametreista ei tarvitse va¨litta¨a¨ aliluokissa. Olioiden tuhoamisen yhteydessa¨ vastaavia ongelmia ei tule, vaan jaetun kantaluokan purkajaa kutsutaan normaalisti kertaalleen ilman, etta¨ aliluokkien ta¨ytyy ottaa siihen kantaa._ Kaiken kaikkiaan toistuva kantaluokka aiheuttaa moniperiytymi- seen niin paljon monimutkaisuutta, rajoituksia ja ongelmia, etta¨ jot- kut ovat antaneet ta¨llaiselle periytymishierarkialle osuvan nimen “Dreaded Diamond of Death”. Varsinkin virtuaalista moniperiytymis- ta¨ pita¨isikin yleensa¨ va¨ltta¨a¨, ellei tieda¨ tarkkaan mita¨ on tekema¨ssa¨. Joskus se on kuitenkin kelvollinen apukeino ohjelmoijan tyo¨kalupa- kissa. 6.9 Periytyminen ja rajapintaluokat On varsin yleista¨, etta¨ abstrakteissa kantaluokissa ma¨a¨ritella¨a¨n lu- vun alun elio¨esimerkin tapaan pelka¨sta¨a¨n puhtaita virtuaalifunktioi- ta, jolloin abstraktit kantaluokat eiva¨t sisa¨lla¨ mita¨a¨n muuta kuin ra- japinnan ma¨a¨rittelyja¨. Ta¨llo¨in puhutaan usein rajapintaluokista (in- terface class). Rajapintaluokissa ei siis ole ja¨senmuuttujia eika¨ ja¨sen- funktioiden toteutuksia, vaan ainoastaan (yleensa¨ julkisen) rajapin- nan ma¨a¨rittely. Joissain oliokielissa¨, kuten Javassa, ta¨llaisille puhtaille rajapinnoille on oma syntaksinsa eika¨ niita¨ edes varsinaisesti lasketa luokiksi. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . _Tarkasti ottaen C++-standardi joutuu ottamaan kantaa virtuaaliseen periytymiseen purka- jienkin yhteydessa¨, kun se ma¨a¨rittelee purkajien keskina¨isen kutsuja¨rjestyksen. Ta¨lla¨ ei kui- tenkaan normaalisti ole mita¨a¨n merkitysta¨, ja purkajia edelleen kutsutaan ka¨a¨nteisessa¨ ja¨rjes- tyksessa¨ rakentajiin verrattuna. 6.9. Periytyminen ja rajapintaluokat 191 6.9.1 Rajapintaluokkien ka¨ytto¨ Rajapintaluokat ovat varsin ka¨tevia¨, koska niiden avulla voidaan ka¨ytta¨ja¨lle paljastaa luokkahierarkiasta pelkka¨ hierarkkinen rajapin- ta ja ka¨tkea¨ itse rajapintafunktioiden toteutus konkreettisiin luokkiin. Rajapintaluokkia ja dynaamista sitomista ka¨ytta¨ma¨lla¨ ohjelmoija voi lisa¨ksi itse pa¨a¨tta¨a¨, milla¨ hierarkiatasolla olioita ka¨sittelee. Joitain funktioita kiinnostaa vain, etta¨ niiden parametrit ovat mita¨ tahansa ela¨imia¨, johonkin tietorakenteeseen talletetaan mita¨ tahansa sienia¨ ja niin edelleen. Usein tulee eteen tilanne, jossa kaikkia haluttuja rajapintoja ei voi milla¨a¨n panna samaan luokkahierarkiaan, koska itse rajapinnat ovat toisistaan riippumattomia ja konkreettisten luokkien rajapinnat ovat erilaisia yhdistelmia¨ na¨ista¨ rajapinnoista. Ta¨llaisessa tapaukses- sa konkreettiset luokat pita¨isi pystya¨ koostamaan erilaisista rajapin- takomponenteista luokkahierarkiasta riippumatta. Kuva 6.10 na¨ytta¨a¨ esimerkin useiden toisistaan riippumattomien rajapintojen ka¨yto¨sta¨. Eri oliokielissa¨ ongelma on ratkaistu eri tavoilla. Ongelmaa ei ole niissa¨ kielissa¨, joissa ei ole ka¨a¨nno¨saikaista tyypitysta¨, koska itse ra- japintaluokan ka¨sitetta¨ ei tarvita. Esimerkiksi Smalltalkissa milta¨ ta- Eliö {abstract} lisäänny() {abstract} Eläin {abstract} hajota() liiku() Limasieni lisäänny() Nisäkäs {abstract} imetä() {abstract} Vesinokkaeläin muni() imetä() liiku() lisäänny() Ihminen lisäänny() liiku() imetä() Kana lisäänny() laula() liiku() Satakieli lisäänny() laula() liiku() Liikkuva Muniva Liikkuva <<interface>> liiku() {abstract} Lintu {abstract} laula() {abstract} muni() Muniva <<interface>> muni() {abstract} Sieni {abstract} hajota() {abstract} KUVA 6.10: Luokat, jotka toteuttavat erilaisia rajapintoja 6.9. Periytyminen ja rajapintaluokat 192 hansa oliolta voidaan pyyta¨a¨ mita¨ tahansa palvelua ja vasta ohjelman ajoaikana tarkastetaan, pystyyko¨ olio ta¨llaista palvelua tarjoamaan. Javan erilliset rajapinnat tarjoavat elegantin tavan yhdistella¨ raja- pintoja todellisissa luokissa. Java ei rajoita ta¨llaisten rajapintojen ma¨a¨- ra¨a¨ yhteen, vaan luokka voi luetella mielivaltaisen ma¨a¨ra¨n rajapin- toja, jotka se toteuttaa. Ta¨lla¨ tavoin luokat voivat luokkahierarkiasta riippumatta toteuttaa erilaisia rajapintoja. Na¨iden rajapintojen avul- la voidaan sitten ka¨sitella¨ kaikkia rajapinnan toteuttavia luokkia sa- massa koodissa, koska luokista ei tarvita muuta tietoa kuin se, etta¨ ne toteuttavat halutun rajapinnan. Listaus 6.15 na¨ytta¨a¨ osan kuvan 6.10 luokkien toteutuksesta Javalla. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Liikkuva.java . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1 public interface Liikkuva 2 { 3 public void liiku(Sijainti paamaara); 4 } . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Muniva.java . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1 public interface Muniva 2 { 3 public void muni(); 4 } . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Elain.java . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1 public abstract class Elain extends Elio implements Liikkuva 2 { ... 3 } . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Vesinokkaelain.java . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1 public class Vesinokkaelain extends Nisakas implements Muniva 2 { 3 public void lisaanny() { } 4 public void liiku(Sijainti paamaara) { } 5 public void imeta() { } 6 public void muni() { } ... 7 } LISTAUS 6.15: Erilliset rajapinnat Javassa 6.9. Periytyminen ja rajapintaluokat 193 6.9.2 C++: Rajapintaluokat ja moniperiytyminen C++:ssa rajapintaluokkia mallinnetaan abstrakteilla kantaluokilla ja moniperiytymisella¨. Mika¨li todellinen luokka toteuttaa useita toisis- taan riippumattomia rajapintoja, se periyteta¨a¨n kaikista rajapinnat ma¨a¨ra¨a¨vista¨ abstrakteista kantaluokista. Ta¨lla¨ tavoin saadaan aikaan useita juuriluokkia sisa¨lta¨va¨ luokkahierarkia, jossa rajapintaluokat ovat kaikkien rajapinnan toteuttavien todellisten luokkien kantaluok- kia. Listaus 6.16 na¨ytta¨a¨ osan kuvan 6.10 toteutuksesta C++:lla. Mika¨li abstraktit kantaluokat sisa¨lta¨va¨t ainoastaan puhtaita virtu- 17 class Liikkuva 18 { 19 public: 20 virtual ~Liikkuva(); 21 virtual void liiku(Sijainti paamaara) = 0; 22 }; ... 23 class Muniva 24 { 25 public: 26 virtual ~Muniva(); 27 virtual void muni() = 0; 28 }; ... 29 class Elain : public Elio, public Liikkuva 30 { 31 public: 32 private: 33 }; ... 47 class Vesinokkaelain : public Nisakas, public Muniva 48 { 49 public: 50 virtual ~Vesinokkaelain(); 51 virtual void lisaanny(); 52 virtual void liiku(Sijainti paamaara); 53 virtual void imeta(); 54 virtual void muni(); 55 }; LISTAUS 6.16: Rajapintaluokkien toteutus moniperiytymisella¨ C++:ssa 6.9. Periytyminen ja rajapintaluokat 194 aalifunktioita ja ovat na¨in pelkkia¨ rajapintaluokkia, ei moniperiyty- misen ka¨yto¨sta¨ aiheudu yleensa¨ ongelmia. Mika¨li moniperiytymises- sa¨ kantaluokat sen sijaan sisa¨lta¨va¨t myo¨s rajapintojen toteutuksia ja ja¨senmuuttujia, moniperiytyminen aiheuttaa yleensa¨ enemma¨n on- gelmia kuin ratkaisee, kuten aiemmin on todettu. C++:ssa moniperiy- tymisen ja¨rkeva¨ ka¨ytto¨ on ja¨tetty ohjelmoijan vastuulle, eika¨ kieli itse yrita¨ varjella ohjelmoijaa siina¨ esiintyvilta¨ vaaroilta. Rajapintaluokkien toteuttaminen C++:ssa moniperiytymisen avulla aiheuttaa kuitenkin jonkin verran ko¨mpelyytta¨ ohjelmaan. Ensinna¨- kin, koska seka¨ normaali periytyminen etta¨ rajapinnan toteuttaminen tehda¨a¨n kielessa¨ periytymissyntaksilla, ei luokan esittelysta¨ suoraan na¨e, mitka¨ sen kantaluokista ovat puhtaita rajapintoja ja mitka¨ sisa¨l- ta¨va¨t myo¨s toteutusta. Ta¨ha¨n ei C++:ssa ole muuta ratkaisukeinoa kuin nimeta¨ rajapintaluokat niin, etta¨ ka¨y selva¨sti ilmi niiden olevan pelk- kia¨ rajapintoja. Rajapintaluokat, rakentajat ja virtuaalipurkaja Rajapintaluokat ovat C++:ssa normaaleja luokkia, joten niilla¨kin on ra- kentaja, jota kutsutaan aliluokan rakentajasta. Ka¨yta¨nno¨ssa¨ ta¨ma¨ ra- kentaja on kuitenkin aina tyhja¨, koska rajapintaluokat nimensa¨ mu- kaan ma¨a¨ritta¨va¨t vain rajapinnan eiva¨tka¨ sisa¨lla¨ ja¨senmuuttujia tai toiminnallisuutta. Ta¨ma¨n vuoksi rakentajien kirjoittaminen rajapin- taluokille olisi turhauttavaa. Ta¨ssa¨ onneksi C++:n normaalisti vaaralli- nen automatiikka auttaa. Aliluvussa 3.4.1 todettiin, etta¨ jos luokalle ei kirjoiteta yhta¨a¨n ra- kentajaa, tekee ka¨a¨nta¨ja¨ sinne automaattisesti “tyhja¨n” oletusrakenta- jan. Vastaavasti aliluvussa 6.3.2 ka¨vi ilmi, etta¨ jos kantaluokan raken- tajan kutsu ja¨teta¨a¨n pois aliluokan rakentajan alustuslistasta, kutsuu ka¨a¨nta¨ja¨ automaattisesti kantaluokan oletusrakentajaa. Na¨iden omi- naisuuksien ansiosta rajapintaluokkaan ei tarvitse kirjoittaa rakenta- jaa ollenkaan, koska ta¨llo¨in ka¨a¨nta¨ja¨ automaattisesti luo tyhja¨n ole- tusrakentajan ja kutsuu sita¨ aliluokissa. Rajapintaluokat ovat C++:ssa la¨hes ainoa tilanne, jolloin luokalle ei tyyliohjeista poiketen kannata kirjoittaa rakentajaa. C++:ssa kantaluokkien purkajien tulisi olla virtuaalisia (alilu- ku 6.5.5), jotta niista¨ periytettyjen luokkien oliot tuhottaisiin aina oikein. Rajapintaluokat ovat kantaluokkia, joten ta¨ma¨ sa¨a¨nto¨ kos- kee myo¨s niita¨. Ika¨va¨ kylla¨, jos rajapintaluokan esittelyssa¨ esitella¨a¨n 6.9. Periytyminen ja rajapintaluokat 195 purkaja virtuaaliseksi, pita¨a¨ ta¨lle purkajalle kirjoittaa myo¨s toteutus, vaikka kyseessa¨ onkin pelkka¨ rajapintaluokka ilman toiminnallisuut- ta tai dataa, ja na¨in purkajan toteutus on aina tyhja¨. A¨rsytta¨va¨ksi ta¨ma¨n vaatimuksen virtuaalipurkajan toteutuksesta tekee se, etta¨ koska rajapintaluokassa ei muuten ole toiminnallisuut- ta, olisi luontevaa kirjoittaa sille pelkka¨ otsikkotiedosto (.hh) ja ja¨tta¨a¨ varsinainen kooditiedosto (.cc) kokonaan kirjoittamatta. Purkajan to- teutus taas normaalisti kirjoitettaisiin juuri kooditiedostoon. Ta¨ha¨n ongelmaan lo¨ytyy onneksi ratkaisu. C++ antaa mahdollisuuden kir- joittaa ja¨senfunktion toteutuksen suoraan luokkaesittelyn sisa¨lle (sa- maan tapaan kuin Javassa tehda¨a¨n). Ta¨ma¨ ei ole normaalisti hyva¨a¨ tyylia¨, koska luokan toteutusta ja esittelya¨ ei ole syyta¨ sotkea kes- kena¨a¨n. Lisa¨ksi ta¨llaiset luokan esittelyyn upotetut ja¨senfunktiot op- timoidaan aina kuten inline-funktiot (liitteen aliluku A.2), mika¨ ei yleisessa¨ tapauksessa ole va¨ltta¨ma¨tta¨ hyva¨ asia. Koska kuitenkin rajapintaluokan purkaja on aina tyhja¨, voi ta¨s- ta¨ tyylisa¨a¨nno¨sta¨ ainakin ta¨ma¨n kirjan kirjoittajien mielesta¨ poike- ta ta¨ssa¨ tilanteessa. Na¨in rajapintaluokan purkaja saadaan kirjoitet- tua kokonaan otsikkotiedostoon eika¨ erillista¨ toteutustiedostoa tarvi- ta. Listaus 6.17 na¨ytta¨a¨ esimerkkina¨ rajapintaluokan Liikkuva esitte- lyn na¨in kirjoitettuna. 6.9.3 Ongelmia rajapintaluokkien ka¨yto¨ssa¨ Erilliset rajapinnat tai rajapintaluokat selkeytta¨va¨t luokkahierarkiaa ja mahdollistavat ka¨teva¨sti sen, etta¨ saman rajapinnan toteuttavia olioita voidaan ka¨sitella¨ rajapintaosoittimen tai -viitteen avulla riip- pumatta luokkien sijainnista luokkahierarkiassa. Rajapintaluokat ei- va¨t kuitenkaan ratkaise kaikkia ongelmia. 1 class Liikkuva 2 { 3 public: 4 // Ka¨a¨nta¨ja¨ tuottaa automaattisesti tyhja¨n oletusrakentajan 5 virtual ~Liikkuva() {} // Upotettu tyhja¨ virtuaalipurkaja (inline) 6 virtual void liiku(Sijainti paamaara) = 0; 7 }; LISTAUS 6.17: Rajapintaluokan purkaja esittelyyn upotettuna 6.9. Periytyminen ja rajapintaluokat 196 Rajapintojen nimien pa¨a¨llekka¨isyys Moniperiytymisen yhteydessa¨ todettiin, etta¨ monen kantaluokan ta- pauksessa voi ka¨yda¨ niin, etta¨ usean kantaluokan rajapinnassa on ta¨s- ma¨lleen samanniminen ja¨senfunktio samoilla parametreilla. Ta¨sma¨l- leen sama ongelma voi ilmeta¨ myo¨s rajapintaluokkien tapauksessa, joten ongelma koskee myo¨s esimerkiksi Javaa, vaikka siina¨ ei varsi- naista moniperiytymista¨ olekaan. Samannimisten ja¨senfunktioiden erottaminen toisistaan ka¨y C++:ssa aliluvussa 6.8.1 sivulla 184 esitetylla¨ tekniikalla apukanta- luokkia ka¨ytta¨ma¨lla¨. Ta¨lla¨ tavoin rajapinnat toteuttava luokka voi an- taa eri toteutuksen kumpaakin rajapintaa varten. Ta¨ma¨ tekniikka vaa- tii kielessa¨ kuitenkin aitoa moniperiytymista¨, joten sita¨ ei, ika¨va¨ kyl- la¨, voi ka¨ytta¨a¨ Javassa. Ta¨ma¨n kirjan kirjoittajat eiva¨t valitettavasti on- nistuneet lo¨yta¨ma¨a¨n tai keksima¨a¨n yhta¨a¨n tapaa, jolla ongelman voisi ratkaista Javan keinoin. Rajapintaluokkien yhdista¨minen Rajapintaluokat voivat aiheuttaa myo¨s ongelmia itse periyty- misessa¨. Oletetaan esimerkiksi, etta¨ halutaan kirjoittaa funktio menePesaanJaMuni, joka ka¨skee elio¨ta¨ ensin liikkumaan pesa¨a¨nsa¨ ja sen ja¨lkeen munimaan sinne. Olisi luonnollista, etta¨ ta¨llaiselle funk- tiolle voisi antaa parametrina minka¨ tahansa olion, joka pystyy seka¨ liikkumaan etta¨ munimaan. Liikkuva ja Muniva ovat kuitenkin eril- lisia¨ rajapintaluokkia, eika¨ funktion parametrilla voi olla kuin yksi tyyppi. Yksi ratkaisuyritys olisi luoda uusi rajapintaluokka LiikkuvaJaMuniva, johon periytta¨ma¨lla¨ yhdisteta¨a¨n molemmat tarvittavat rajapinnat.\u0013 class LiikkuvaJaMuniva : public Liikkuva, public Muniva { }; . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . \u0013Rajapintaluokan LiikkuvaJaMuniva purkaja on automaattisesti virtuaalinen, koska ta¨ma¨ ominaisuus periytyy sen kantaluokilta. Ta¨ma¨n vuoksi virtuaalipurkajan esitteleminen ei ole va¨ltta¨ma¨to¨nta¨, vaikka ehka¨ kylla¨kin hyva¨a¨ tyylia¨. 6.9. Periytyminen ja rajapintaluokat 197 Ta¨ma¨n uuden rajapintaluokan avulla funktio olisi helppo esitella¨ seuraavasti: void menePesaanJaMuni(LiikkuvaJaMuniva& emo); Vaikka edella¨ esitetty tapa tuntuukin ja¨rkeva¨lta¨, se ei ylla¨tta¨en toi- mi. Jos funktiolle yritta¨a¨ esimerkiksi antaa parametrina luokan Kana oliota, valittaa ka¨a¨nta¨ja¨ ettei Kana-olio kelpaa funktion parametrik- si. Syyna¨ ta¨ha¨n on, etta¨ vaikka Kana toteuttaakin erikseen periyty- misen kautta seka¨ rajapinnan Liikkuva etta¨ Muniva, se ei luokka- hierarkian mukaan kuitenkaan ole periytetty na¨iden yhdistelma¨sta¨ LiikkuvaJaMuniva. Niinpa¨ Kana-olio ei kuulu ta¨ha¨n luokkaan eika¨ kel- paa parametriksi. Kuva 6.11 havainnollistaa tilannetta. Kaikki ta¨ma¨ seuraa suoraan periytymisen ominaisuuksista. Vaik- ka luokkaan LiikkuvaJaMuniva ei olekaan lisa¨tty yhta¨a¨n ylima¨a¨- ra¨ista¨ ja¨senfunktiota luokkiin Liikkuva ja Muniva verrattuna, saat- Eläin {abstract} Liikkuva <<interface>> liiku() {abstract} Lintu {abstract} laula() {abstract} muni() Muniva <<interface>> muni() {abstract} Kana lisäänny() laula() liiku() LiikkuvaJaMuniva <<interface>> PUUTTUU! KUVA 6.11: Ongelma yhdistettyjen rajapintojen kanssa 6.10. Periytyminen vai kooste? 198 taa olla etta¨ ta¨ma¨n luokan semantiikka (merkitys) sisa¨lta¨a¨ muutakin kuin kahden rajapinnan yhdista¨misen. Esimerkiksi rajapintaluokka LentavaJaMuniva na¨ytta¨isi ma¨a¨rittelylta¨a¨n samalta, mutta siina¨ voi- si olla dokumentoituna, etta¨ liikkuminen tapahtuu lenta¨en. Ta¨llo¨in Kanan ei tietysti pita¨isika¨a¨n kelvata toteuttamaan ta¨ta¨ rajapintaa. Ainoa ratkaisu ta¨ha¨n ongelmaan C++:n ja Javan tapaisissa kielis- sa¨ olisi luoda ohjelman luokkahierarkiaan varsinaisten rajapintaluok- kien lisa¨ksi myo¨s kaikki na¨iden yhdistelma¨t ja merkita¨ erikseen hie- rarkiaan, mitka¨ luokat toteuttavat myo¨s na¨ma¨ yhdistelma¨t. Ta¨ma¨ kui- tenkin tekisi luokkahierarkioista eritta¨in sotkuisia, ja yhdistelmien ma¨a¨ra¨ ra¨ja¨hta¨isi helposti todella suureksi. Na¨issa¨ kielissa¨ ei olekaan mita¨a¨n eleganttia ratkaisua ta¨ha¨n ongelmaan. Smalltalkissa puolestaan ta¨ta¨ ongelmaa ei tule, koska siina¨ ei ole tarvetta rajapintaluokille lai- sinkaan. Toisaalta ta¨ma¨ taas johtuu siita¨, etta¨ koko kieli ei tunne ra- japinnan ka¨sitetta¨, koska kaikki tyyppitarkastukset tehda¨a¨n kielessa¨ vasta ajoaikana. 6.10 Periytyminen vai kooste? Milloin luokkien va¨lilla¨ on olemassa periytymissuhde ja milloin on kyse koosteesta? Koosteessa on kyse kahden tai useamman olion muo- dostamasta kokonaisuudesta ja periytymisessa¨ taas yhdesta¨ oliosta, jonka luokkarakenne koostuu usean luokan ominaisuuksista. Vaik- ka kyse on kahdesta hyvin erilaisesta asiasta, niin suunnittelussa on usein vaikeata tehda¨ valintaa koosteen ja periytymisen va¨lilla¨. Perinteinen esimerkki oliosuunnittelussa on ihmisten (kohtuuton) yksinkertaistaminen mallintamalla heida¨n ominaisuuksiaan erillisi- na¨ luokkina: insino¨o¨ri, isa¨, viulisti ja niin edelleen. (Ei attribuutteina, silla¨ na¨ma¨ ominaisuudet itsessa¨a¨n ovat mutkikkaita, ja siten luokan arvoisia.) Halutessamme mallin Sibelius-akatemiasta ja TTY:lta val- mistuneesta ohjelmistosuunnittelijasta voimme katsoa ha¨nella¨ ole- van seka¨ Insino¨o¨rin ja Viulistin ominaisuudet, joten periytyminen na¨ista¨ luokista olisi perusteltua. Mutta mita¨pa¨ silloin kun ha¨n muu- taman vuoden kuluttua saa lapsia ja palkanmaksuja¨rjestelma¨mme pi- ta¨isi liitta¨a¨ ha¨neen myo¨s isa¨a¨ kuvaavan luokan ominaisuudet? Nyt joudummekin tekema¨a¨n ha¨nta¨ kuvaamaan kokonaan uuden olion, jonka rakenne on periytetty luokista Insino¨o¨ri, Viulisti ja Isa¨. Mi- ta¨ enemma¨n erilaisia vaihtoehtoja mallinnamme, sita¨ enemma¨n jou- 6.11. Sovelluskehykset 199 dumme tekema¨a¨n eri tavoin periytettyja¨ luokkia ja ohjelmassa vaih- tamaan ihmisia¨ kuvaavia olioita heida¨n ela¨ma¨ntilanteidensa muut- tuessa. Selva¨sti parempi vaihtoehto ta¨ssa¨ dynaamisessa tilanteessa on liitta¨a¨ tarvittava ma¨a¨ra¨ ihmiseen liittyvia¨ “ominaisuuksia” koos- teella. Kuva 6.12 esitta¨a¨ molemmat mahdollisuudet. Periytymisen “nyrkkisa¨a¨nno¨iksi” suositellaan: [Meyer, 1997] 1. Periyta¨ luokka B luokasta A vain silloin, kun pystyt perustele- maan, etta¨ luokan B oliot voidaan aina na¨hda¨ myo¨s luokan A olioina. 2. A¨la¨ ka¨yta¨ periytymista¨, jos periytymisella¨ saatu olion rakenne voi muuttua ohjelman ajoaikana (ka¨yta¨ ta¨llo¨in koostetta). 6.11 Sovelluskehykset Ohjelmistojen toteutuksessa voidaan hyo¨dynta¨a¨ aiemmissa projek- teissa tehtyja¨ tai projektin ulkopuolelta ostettuja ohjelmakomponent- VÄÄRIN! Isä Insinööri Viulisti Ohjelmistosuunnittelija Insinööri Viulisti Isä Ohjelmistosuunnittelija 0..1 0..1 0..1 KUVA 6.12: Insino¨o¨ri, viulisti ja isa¨ 6.11. Sovelluskehykset 200 teja. Na¨ma¨ komponentit ovat usein luokkakirjastoja, joiden avulla saadaan ka¨ytto¨o¨n valmiita olioita. Ohjelmiston toiminnallinen logiik- ka eli olioiden va¨linen yhteistyo¨ (kommunikointi) on edelleen ta¨ysin toteuttajan vastuulla ja hallinnassa. Ohjelmistoa voidaan ryhtya¨ rakentamaan myo¨s siten, etta¨ valmis komponentti ma¨a¨rittelee etuka¨teen myo¨s toiminnallisuuden kannal- ta olioiden kommunikointiin liittyvia¨ vaatimuksia. Ta¨llaista raken- netta sanotaan sovelluskehykseksi (framework). Olio-ohjelmoinnis- sa sovelluskehykset ovat luokkakirjastoja, joista on tarkoitus periyt- ta¨ma¨lla¨ erikoistaa oman sovelluksen toteuttavat luokkien versiot. Ku- vassa 6.13 na¨kyy ero sovelluskehysohjelman rakenteessa verrattuna “perinteiseen” aliohjelmakirjastorakenteeseen. Kirjastorutiineja kut- sutaan toteutetusta sovelluksesta, jolla on vastuu kokonaiskontrol- lista. Sovelluskehyksessa¨ kehys on ma¨a¨ritellyt perusrakenteen, jonka osaset kutsuvat kehyksen ma¨a¨rittelemissa¨ tapahtumissa sovelluksen operaatioita (jotka on toteutettu esimerkiksi kehyksen koodista periy- tetyissa¨ luokissa). Sovelluskehys tavallaan ma¨a¨rittelee koko sovelluksen “tunnel- man” tai arkkitehtuurin, jonka puitteissa ohjelmoijan on toimitta- va. Ikkunointija¨rjestelma¨n sovelluskehys voi ma¨a¨ritella¨ tarkasti, mi- ta¨ piirto-operaatioita ja tapahtumatietoja ohjelmoijan on toteutettava. Ja¨rjestelma¨ voi tarjota vaikkapa “perusikkunan”, jonka luokasta pe- riytta¨ma¨lla¨ toteutetaan varsinainen oma sovellus. Sovellus Kirjasto (a) Aliohjelma- kirjasto Sovellus Sovelluskehys (b) Sovelluskehys KUVA 6.13: Aliohjelmakirjasto ja sovelluskehys 201 Luku 7 Lisa¨a¨ olioiden elinkaaresta — Keita¨ ovat he? ha¨n kysyi. — Nimenomaan kenen luulet koettavan murhata sinut? – Jokaisen heista¨, Jossarian sanoi ha¨nelle. . . . Clevinger luuli olevansa oikeassa, mutta Jossarianilla oli todisteita, silla¨ vieraat ihmiset, joita ha¨n ei tuntenut, ampuivat ha¨nta¨ tykeilla¨ joka kerta kun ha¨n oli noussut il- maan pudottamaan pommeja heida¨n niskaansa, eika¨ se ollut hauskaa. – CATCH-22 [Heller, 1961] Tavallisia perustietotyyppeja¨ ka¨ytetta¨essa¨ tulee usein vastaan tilanne, jossa muuttujasta halutaan tehda¨ kopio tai vastaavasti yhden muuttu- jan arvo halutaan sijoittaa toiseen. Sama tilanne toistuu joskus olioi- den kanssa, varsinkin jos oliot edustavat abstrakteja tietotyyppeja¨, kuten pa¨iva¨yksia¨ tai kompleksilukuja. Olio-ohjelmoinnissa sijoituk- sen ja kopioinnin merkitys ei kuitenkaan ole yhta¨ selva¨ kuin perintei- sessa¨ ohjelmoinnissa, joten niita¨ on syyta¨ ka¨sitella¨ tarkemmin. C++:ssa varsinkin kopioimisen merkitys korostuu entisesta¨a¨n, kos- ka ka¨a¨nta¨ja¨ itse tarvitsee olioiden kopiointia esimerkiksi va¨litta¨essa¨a¨n olioita tavallisina arvoparametreina tai palauttaessaan niita¨ paluuar- voina. Koska olioiden kopioiminen ja sijoitus saattaa olla hyvin ras- kas operaatio, on ta¨rkea¨ta¨ etta¨ ohjelmoija tieta¨a¨, mita¨ kaikkea niihin liittyy ja missa¨ tapauksissa kopioiminen on automaattista. 7.1. Olioiden kopiointi 202 7.1 Olioiden kopiointi Olioiden kopiointia tarvitaan useisiin eri tarkoituksiin. Joskus olios- ta tulee tarve tehda¨ varmuuskopio, joskus taas taulukkoon halutaan tallettaa itsena¨inen kopio oliosta, ei pelka¨sta¨a¨n viitetta¨ alkupera¨iseen olioon. Olio-ohjelmoinnin kannalta oleellinen kysymys kopioinnissa kuitenkin on: “Mika¨ oikein on kopio?” Perustyyppien tapauksessa kopion ma¨a¨ritteleminen ei ole vaike- aa, koska kopio voidaan luoda yksinkertaisesti luomalla uusi muut- tuja ja kopioimalla vanhan muuttujan muisti uuteen bitti kerrallaan. Olioiden tapauksessa ta¨ma¨ ei kuitenkaan riita¨, koska olion tilaan voi kuulua paljon muutakin kuin olion ja¨senmuuttujat. Yksi tapa ma¨a¨ritella¨ olioiden kopiointi olisi sanoa, etta¨ uuden olion tulee olla identtinen vanhan kanssa. Ta¨ma¨kin ma¨a¨ritelma¨ tuot- taa kuitenkin ongelmia: jos kyseessa¨ kerran ovat ta¨ysin identtiset oliot, miten ne voi erottaa toisistaan, toisin sanoen miten ta¨llo¨in tie- deta¨a¨n, etta¨ kyseessa¨ todella on kaksi eri oliota? Jos oliot olisivat ta¨y- sin identtiset, pita¨isi periaatteessa toisen muuttamisen muuttaa myo¨s toista, mika¨ tuskin on yleensa¨ toivottavaa. Parempi tapa ma¨a¨ritella¨ kopiointi on sanoa, etta¨ oliota kopioitaes- sa uuden olion ja vanhan olion arvojen tai tilojen ta¨ytyy olla samat. Ta¨ssa¨ tilalla ei tarkoiteta yksinkertaisesti ja¨senmuuttujia vaan olion tilaa korkeammalla abstraktiotasolla ajatellen. Ta¨lla¨ tavoin ajateltuna esimerkiksi pa¨iva¨ysolion tila on se pa¨iva¨ys, jonka se sisa¨lta¨a¨. Vastaa- vasti merkkijono-olion tila on sen sisa¨lta¨ma¨ teksti ja dialogi-ikkunan tila sisa¨lta¨a¨ myo¨s ruudulla na¨kyva¨n ikkunan. Na¨in ajateltuna olion tilan ka¨site ei ena¨a¨ riipu sen sisa¨isesta¨ toteutuksesta vaan siita¨, mita¨ olio ohjelmassa edustaa. Edella¨ esitetty olioiden kopioinnin ma¨a¨ritelma¨ tarkoittaa, etta¨ eri tyyppisia¨ olioita kopioidaan hyvin eri tavalla. Kompleksilukuo- lion voi kenties kopioida yksinkertaisesti muistia kopioimalla, kun taas merkkijonon kopiointi saattaa toteutuksesta riippuen vaatia yli- ma¨a¨ra¨ista¨ muistinvarausta olion ulkopuolelta ja muita toimenpitei- ta¨. Na¨in ka¨a¨nta¨ja¨ ei pysty automaattisesti kopioimaan olioita, vaan luokan tekija¨n ta¨ytyy itse ma¨a¨ritella¨, mita¨ kaikkea oliota kopioitaessa ta¨ytyy tehda¨. Kaikkia olioita ei edes ole ja¨rkeva¨a¨ kopioida. Esimerkiksi hissin moottoria ohjaavan olion kopioiminen on todenna¨ko¨isesti ja¨rjeto¨n toimenpide, koska se vaatisi periaatteessa oliomallinnuksen mukai- 7.1. Olioiden kopiointi 203 sesti myo¨s itse fyysisen moottorin kopioimista. Ta¨ma¨n vuoksi olisi hyva¨, jos luokan kirjoittaja voisi myo¨s halutessaan esta¨a¨ kyseisen luo- kan olioiden kopioinnin kokonaan. 7.1.1 Erilaiset kopiointitavat Kirjallisuudessa jaotellaan erilaiset olioiden kopiointitavat usein kol- meen kategoriaan: viitekopiointiin, matalakopiointiin ja syva¨ko- piointiin. Tavallisesti olioiden kopiointi onkin mieleka¨sta¨ tehda¨ jol- lain edella¨ mainituista tavoista, mutta todellisuus on ja¨lleen kerran teoriaa ihmeellisempa¨a¨. Joskus saattaa tulla tarve kopioida osia olios- ta yhdella¨ tavalla ja toisia osia toisella. Perusperiaatteiltaan ta¨ma¨ jaot- telu on kuitenkin ja¨rkeva¨, joten ta¨ssa¨ aliluvussa ka¨yda¨a¨n la¨pi kaikki kolme kopiointitapaa. Viitekopiointi Viitekopiointi (reference copy) on kaikkein helpoin kopiointitavoista. Siina¨ ei yksinkertaisesti luoda ollenkaan uutta oliota, vaan “uutena oliona” ka¨yteta¨a¨n viitetta¨ vanhaan olioon. Kuva 7.1 havainnollistaa ta¨ta¨. Kuvan tilanteessa viitteen B pa¨a¨ha¨n “kopioidaan” olio A. Viitekopiointia ka¨yteta¨a¨n erityisesti oliokielissa¨, jossa itse muut- tujat ovat aina vain viitteita¨ olioihin, jotka puolestaan luodaan dy- Olion dataa Olio 3 A (a) Ennen kopiointia Olion dataa B Olio 3 A (b) Kopioinnin ja¨lkeen KUVA 7.1: Viitekopiointi 7.1. Olioiden kopiointi 204 naamisesti. Ta¨llaisia kielia¨ ovat esimerkiksi Java ja Smalltalk. Kun na¨is- sa¨ kielissa¨ luodaan uusi muuttuja ja alustetaan se olemassa olevalla muuttujalla, viittaavat molemmat muuttujat ta¨ma¨n ja¨lkeen samaan olioon. C++:ssa sen sijaan viitekopiointia ka¨yteta¨a¨n vain, kun erikseen luodaan viitteita¨ olioiden sijaan. Viitekopioinnin hyva¨na¨ puolena on sen nopeus. “Kopion” luomi- nen ei ka¨yta¨nno¨ssa¨ vaadi ollenkaan aikaa, koska mita¨a¨n kopioimista ei tarvitse tehda¨. Viitekopiointi toimiikin hyvin niin kauan, kun olion arvoa ei muuteta. Mika¨li sen sijaan oliota muutetaan toisen muuttu- jan kautta, vaikuttaa muutos tietysti myo¨s toiseen muuttujaan. Ta¨llai- nen ka¨ytta¨ytyminen on varsin haitallista, koska yksi ta¨rkea¨ kopioin- nin syy on tehda¨ oliosta “varmuuskopio”, joka sa¨ilytta¨a¨ arvonsa. Matalakopiointi Matalakopioinnissa (shallow copy) itse oliosta ja sen ja¨senmuuttujis- ta tehda¨a¨n kopiot. Jos kuitenkin ja¨senmuuttujina on viitteita¨ tai osoit- timia olion ulkopuolisiin tietorakenteisiin, ei na¨ita¨ tietorakenteita ko- pioida vaan matalakopioinnin lopputuloksena molemmat oliot jaka- vat samat tietorakenteet. Kuva 7.2 havainnollistaa matalakopioinnin vaikutuksia. Ohjelmointikielten toteutuksen kannalta matalakopiointi on sel- kea¨ operaatio, koska siina¨ kopioidaan aina kaikki olion ja¨senmuuttu- jat eika¨ mita¨a¨n muuta. Ta¨sta¨ syysta¨ esimerkiksi C++ ka¨ytta¨a¨ oletusar- Olion dataa Olio 3 A (a) Ennen kopiointia Olion dataa B Olio Olio 3 3 A (b) Kopioinnin ja¨lkeen KUVA 7.2: Matalakopiointi 7.1. Olioiden kopiointi 205 voisesti matalakopiointia, jos luokan kirjoittaja ei muuta ma¨a¨ra¨a¨. Ko- pioinnin tuloksena on ainakin pa¨a¨llisin puolin kaksi oliota, ja aika monessa tapauksessa matalakopiointi onkin hyva¨ksytta¨va¨ menetel- ma¨. Yleensa¨ viitekopiointia ka¨ytta¨vissa¨ oliokielissa¨ on myo¨s jokin ta- pa matalakopiointiin. Esimerkiksi Javassa jokaisesta luokasta lo¨ytyy ja¨senfunktio clone, joka oletusarvoisesti tuottaa oliosta matalakopioi- dun kopion. Samoin Smalltalk:n olioita voi pyyta¨a¨ suorittamaan toi- minnon copy, joka tekee matalakopioinnin. Ongelmaksi matalakopioinnissa muodostuu, etta¨ usein osa olion tilaan kuuluvasta tiedosta sijaitsee olion ulkopuolella viitteiden ja osoittimien pa¨a¨ssa¨. Jotta kopioituun olioon tehdyt muutokset eiva¨t heijastuisi alkupera¨iseen olioon ja pa¨invastoin, ta¨ytyisi myo¨s na¨ma¨ olion ulkopuoliset tietorakenteet kopioida. C++:ssa kaikki olion dynaa- misesti luomat oliot ja tietorakenteet sijaitsevat aina olion ulkopuo- lella, joten matalakopiointi ei yksinkertaisuudestaan huolimatta ole riitta¨va¨ la¨heska¨a¨n kaikille luokille. Syva¨kopiointi Syva¨kopiointi (deep copy) on kopiointimenetelma¨, jossa olion ja sen ja¨senmuuttujien lisa¨ksi kopioidaan myo¨s ne olion tilaan kuuluvat oliot ja tietorakenteet, jotka sijaitsevat olion ulkopuolella. Ta¨ma¨ na¨- kyy kuvassa 7.3. Olion dataa Olio 3 A (a) Ennen kopiointia Olion dataa Olion dataa B Olio Olio 3 3 A (b) Kopioinnin ja¨lkeen KUVA 7.3: Syva¨kopiointi 7.1. Olioiden kopiointi 206 Olioiden kannalta syva¨kopiointi on ehdottomasti paras kopiointi- tapa, koska siina¨ luodaan kopio kaikista olion tilaan kuuluvista asiois- ta. Na¨in kopioinnin ja¨lkeen uusi olio ja alkupera¨inen olio ovat ta¨ysin erilliset. Ohjelmointikielen kannalta syva¨kopiointi on kuitenkin on- gelmallinen. Varsin tyypillisesti oliot sisa¨lta¨va¨t osoittimia myo¨s sel- laisiin olioihin ja tietorakenteisiin, jotka eiva¨t varsinaisesti ole osa olion tilaa ja joita ei tulisi kopioida. Esimerkiksi kirjaston kirjat saat- taisivat sisa¨lta¨a¨ osoittimen siihen kirjastoon, josta ne on lainattu. Kir- jan tietojen kopioiminen ei kuitenkaan saisi aiheuttaa koko kirjaston kopioimista. Ka¨a¨nta¨ja¨n kannalta tilanne on ongelmallinen, koska useimmis- sa ohjelmointikielissa¨ ei ole mita¨a¨n tapaa ilmaista, mitka¨ osoittimet osoittavat olion tilaa sisa¨lta¨viin tietoihin ja mitka¨ osoittavat olion ul- kopuolisiin asioihin. Ta¨ma¨n vuoksi la¨hes mika¨a¨n ohjelmointikieli ei automaattisesti tue syva¨kopiointia. Poikkeuksen tekee Smalltalk, jossa oliolta lo¨ytyy myo¨s palvelu deepCopy. Yleensa¨ oliokielissa¨ annetaan ohjelmoijalle itselleen mahdolli- suus kirjoittaa syva¨kopioinnille toteutus, jota kieli sitten osaa auto- maattisesti ka¨ytta¨a¨. C++:ssa ohjelmoija kirjoittaa luokalle kopioraken- tajan, joka suorittaa kopioinnin ohjelmoijan sopivaksi katsomalla ta- valla. Samoin Javassa luokan kirjoittaja voi toteuttaa luokalle oman clone-ja¨senfunktion, joka matalakopioinnin sijaan suorittaa sopivan- laisen syva¨kopioinnin. Na¨in ka¨a¨nta¨ja¨ tarvitsee syva¨kopioinnin toteu- tukseen apua luokan kirjoittajalta. 7.1.2 C++: Kopiorakentaja C++:ssa olio kopioidaan ka¨ytta¨ma¨lla¨ kopiorakentajaa (copy construc- tor). Kopiorakentaja on rakentaja, joka ottaa parametrinaan viitteen toiseen samantyyppiseen olioon. Ideana on, etta¨ kun oliosta halutaan tehda¨ kopio, ta¨ma¨ olio luodaan kopiorakentajaa ka¨ytta¨en. Ta¨llo¨in ko- piorakentaja voi alustaa uuden olion niin, etta¨ se on kopio paramet- rina annetusta alkupera¨isesta¨ oliosta. Ka¨a¨nta¨ja¨ ka¨ytta¨a¨ itsekin auto- maattisesti kopiorakentajaa olion kopioimiseen tietyissa¨ tilanteissa, joita ka¨sitella¨a¨n tarkemmin aliluvussa 7.3. Listaus 7.1 seuraavalla sivulla na¨ytta¨a¨ osan yksinkertaisen merk- kijonoluokan esittelysta¨ ja sen kopiorakentajan toteutuksen. Kopiora- kentaja alustaa uuden merkkijonon koon suoraan alkupera¨isen merk- kijonon vastaavasta ja¨senmuuttujasta. Sen ja¨lkeen se varaa tarvittaes- 7.1. Olioiden kopiointi 207 sa tilaa uuden merkkijonon merkeille ja kopioi ne yksi kerrallaan van- hasta. Na¨in kopiorakentaja ei orjallisesti kopioi ja¨senmuuttujia, vaan merkkijono-olion “syvimma¨n olemuksen” eli itse merkit. Periytyminen ja kopiorakentaja Periytyminen tuo omat lisa¨nsa¨ kopion luomiseen. Aliluokan olio koostuu useista osista, ja kantaluokan osilla on jo omat kopioraken- tajansa, joilla kopion kantaluokkaosat saadaan alustetuksi. Aliluokan olion kopioiminen onkin jaettu eri luokkien kesken samoin kuin ra- kentajat yleensa¨: aliluokan kopiorakentajan vastuulla on kutsua kan- taluokan kopiorakentajaa ja lisa¨ksi alustaa aliluokan osa olioista ko- pioksi alkupera¨isesta¨. Listaus 7.2 seuraavalla sivulla na¨ytta¨a¨ esimer- kin pa¨iva¨tysta¨ merkkijonoluokasta, joka on periytetty luokasta Mjono. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . mjono.hh . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1 class Mjono 2 { 3 public: 4 Mjono(char const* merkit); 5 Mjono(Mjono const& vanha); // Kopiorakentaja 6 virtual ~Mjono(); ... 11 private: 12 unsigned long koko ; 13 char* merkit ; 14 }; . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . mjono.cc . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1 Mjono::Mjono(Mjono const& vanha) : koko (vanha.koko ), merkit (0) 2 { 3 if (koko != 0) 4 { // Varaa tilaa, jos koko ei ole nolla 5 merkit = new char[koko + 1]; 6 for (unsigned long i = 0; i != koko ; ++i) 7 { merkit [i] = vanha.merkit [i]; } // Kopioi merkit 8 merkit [koko ] = ’\\0’; // Loppumerkki 9 } 10 } LISTAUS 7.1: Esimerkki kopiorakentajasta 7.1. Olioiden kopiointi 208 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . pmjono.hh . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1 class PaivattyMjono : public Mjono 2 { 3 public: 4 PaivattyMjono(char const* merkit, Paivays const& paivays); 5 PaivattyMjono(PaivattyMjono const& vanha); // Kopiorakentaja 6 7 virtual ~PaivattyMjono(); ... 15 private: 16 Paivays paivays ; 17 }; . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . pmjono.cc . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1 // Olettaa, etta¨ Paivays-luokalla on kopiorakentaja 2 PaivattyMjono::PaivattyMjono(PaivattyMjono const& vanha) 3 : Mjono(vanha), paivays (vanha.paivays ) 4 { 5 } LISTAUS 7.2: Kopiorakentaja aliluokassa On huomattava, etta¨ jos aliluokan kopiorakentajassa unohdetaan kutsua kantaluokan kopiorakentajaa, pa¨teva¨t normaalit C++:n sa¨a¨n- no¨t, jotka sanovat etta¨ ta¨llo¨in kutsutaan kantaluokan oletusraken- tajaa. Oletusrakentaja puolestaan alustaa “kopion” kantaluokkaosan oletusarvoonsa, eika¨ olio na¨in kopioidu kunnolla! Jotkin ka¨a¨nta¨ja¨t an- tavat varoituksen, kun ne joutuvat kutsumaan kopiorakentajasta kan- taluokan oletusrakentajaa, mutta itse C++-standardi ei vaadi ta¨llaista varoitusta. Jos kantaluokalla ei ole oletusrakentajaa, ongelmia ei tule, koska ta¨llo¨in ka¨a¨nta¨ja¨ antaa virheilmoituksen, jos kantaluokan raken- tajan kutsu unohtuu aliluokan kopiorakentajasta. Ka¨a¨nta¨ja¨n luoma oletusarvoinen kopiorakentaja Jos luokalle ei ole ma¨a¨ritelty kopiorakentajaa, ka¨a¨nta¨ja¨ olettaa luokan olevan niin yksinkertainen, etta¨ voidaan ka¨ytta¨a¨ matalakopiointia eli kopioiminen voidaan suorittaa alustamalla kopion ja¨senmuuttu- jat alkupera¨isen olion ja¨senmuuttujista. Niinpa¨ ka¨a¨nta¨ja¨ kirjoittaa au- tomaattisesti luokkaan ta¨llaisen oletusarvoisen kopiorakentajan, jos luokasta ei lo¨ydy omaa kopiorakentajaa. Esimerkiksi aiemmin ta¨s- sa¨ teoksessa esiintyneella¨ Paivays-luokalla ei ollut kopiorakentajaa, 7.1. Olioiden kopiointi 209 mutta listauksen 7.2 rivilla¨ 3 voidaan alustuslistassa silti luoda kopio pa¨iva¨ysoliosta juuri ta¨ta¨ oletusarvoista kopiorakentajaa ka¨ytta¨en. Periaatteessa oletusarvoisen kopiorakentajan olemassaolo helpot- taa yksinkertaisten ohjelmien tekemista¨, koska aloittelevan ohjelmoi- jan ei tarvitse miettia¨ olioiden kopioimista. Ka¨yta¨nto¨ kuitenkin osoit- taa, etta¨ noin 90 %:ssa tosiela¨ma¨n olioista kopiointiin liittyy muuta- kin kuin ja¨senmuuttujien kopiointi. Erityisesti ta¨ma¨ tulee esille, jos luokassa on dataa osoittimien pa¨a¨ssa¨, kuten listauksen 7.1 merkki- jonoluokassa, jossa oletusarvoinen kopiorakentaja aiheuttaisi ohjel- maan vakavia toimintavirheita¨. Ta¨ma¨n vuoksi jokaiseen luokkaan tulisi erikseen kirjoittaa kopiorakentaja eika¨ luottaa oletusarvoi- sen kopiorakentajan toimintaan. Kopioinnin esta¨minen Ka¨a¨nta¨ja¨n automaattisesti kirjoittamasta oletusarvoisesta kopioraken- tajasta on haittaa, jos luokan olioita ei ole ja¨rkeva¨a¨ kopioida. Ta¨llo¨in- ha¨n luokan kirjoittaja ei halua kirjoittaa omaa kopiorakentajaansa, mutta ka¨a¨nta¨ja¨ siita¨ huolimatta tarjoaa oletusarvoisen kopiorakenta- jan. Kopioinnin esta¨minen vaatiikin luokan kirjoittajalta lisa¨kikkoja.\u0017 Kopioinnin esta¨minen tapahtuu esittelema¨lla¨ kopiorakentaja luo- kan private-puolella. Ta¨llo¨in luokan ulkopuolinen koodi ei pa¨a¨se kut- sumaan kopiorakentajaa eika¨ na¨in ollen kopioimaan olioita. Koska ko- piorakentaja on kuitenkin esitelty, ka¨a¨nta¨ja¨ ei yrita¨ tuputtaa oletusar- voista kopiorakentajaa. Na¨in kaikki ulkopuoliset kopiointiyritykset aiheuttavat ka¨a¨nno¨svirheen. Edella¨ esitetty kikka kuitenkin ratkaisee vasta puolet ongelmasta. Luokan omat ja¨senfunktiot pa¨a¨seva¨t nimitta¨in tietysti ka¨siksi private- osaan ja voivat na¨in kutsua kopiorakentajaa. Ta¨ma¨ esteta¨a¨n silla¨, etta¨ kopiorakentajan esittelysta¨ huolimatta kopiorakentajalle ei kirjoiteta ollenkaan toteutusta. Jos nyt luokan oma koodi yritta¨a¨ kopioida luo- kan olioita, linkkeri huomaa objektitiedostojen linkitysvaiheessa, et- ta¨ kopiorakentajalle ei lo¨ydy koodia ja aiheuttaa virheilmoituksen. Eri asia sitten on, kuinka helppo linkkerin virheilmoituksesta on pa¨a¨tel- la¨, mika¨ on mennyt vikaan ja missa¨ tiedostossa olevasta koodista vika aiheutuu. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . \u0017Kopioinnin — ja myo¨hemmin sijoituksen — esta¨misen vaikeus C++:ssa on yksi kielen suurimpia munauksia, ja esta¨miseen ka¨ytetta¨va¨ kikka vastaavasti yksi rumimpia tekniikoita, joihin jopa tunnollinen ohjelmoija joutuu turvautumaan. 7.1. Olioiden kopiointi 210 Vaikka ylla¨ esitetty kikka onkin esteettisyydelta¨a¨n kyseenalainen, on se ainoa tapa esta¨a¨ olioiden kopioiminen. Sita¨ tulisikin ka¨ytta¨a¨ aina, kun luokan olioiden kopioiminen ei ole mieleka¨sta¨. 7.1.3 Kopiointi ja viipaloituminen Periytyminen tarkoittaa, etta¨ jokainen aliluokan olio on myo¨s kanta- luokan olio. Ika¨va¨ kylla¨, ta¨ma¨ suhde ei ole ihan niin helppo ja yksin- kertainen, kuin mita¨ voisi toivoa. Esimerkiksi olioiden kopioiminen tuottaa tietyissa¨ tapauksissa ongelmia. Kun C++:ssa luodaan kopiorakentajalla oliosta kopio, ta¨ytyy kopio- ta luovan ohjelmanosan tieta¨a¨, minka¨ luokan oliota ollaan luomas- sa. Saman voi sanoa myo¨s niin, etta¨ C++:ssa ei ole suoraan mita¨a¨n ta- paa luoda oliota ilman, etta¨ ka¨a¨nno¨saikana tiedeta¨a¨n sen tyyppi (ta¨ma¨ koskee seka¨ kopiointia etta¨ muutakin olion luomista). Ta¨ma¨ vaatimus tulee siita¨, etta¨ ka¨a¨nta¨ja¨n on ka¨a¨nno¨saikana osattava varata jokaiselle oliolle riitta¨va¨sti muistia, kutsua oikeaa rakentajaa yms. Ta¨ma¨ rajoitus tarkoittaa, etta¨ kantaluokkaosoittimen tai -viitteen pa¨a¨ssa¨ olevasta oliosta ei voi luoda kunnollista kopiota, ellei osoitti- men pa¨a¨ssa¨ olevan olion tyypista¨ voida olla varmoja jo ka¨a¨nno¨saika- na. Kuten luvussa 6 todettiin, periytymista¨ ka¨ytetta¨essa¨ on varsin ta- vallista, etta¨ kantaluokkaosoittimen pa¨a¨ssa¨ voi olla kantaluokkaolion lisa¨ksi minka¨ tahansa periytetyn luokan olio. Ongelmalliseksi tilanteen tekee se, etta¨ ka¨a¨nta¨ja¨n kannalta jokai- nen aliluokan olio on myo¨s kantaluokan olio. Niinpa¨ aliluokan olio kelpaa parametriksi kantaluokan kopiorakentajalle, koska se ottaa pa- rametrinaan viitteen kantaluokkaan. Niinpa¨ koodissa PaivattyMjono pmj(\"Viipaloidun\", jokupaivays); Mjono mj(pmj); kutsutaan oliota mj luotaessa Mjono-luokan kopiorakentajaa ja sille annetaan viite pmj:hin parametrina. Kopiorakentajan koodissa ta¨ma¨ parametri na¨ytta¨a¨ tavalliselta Mjono-oliolta, joten kopiorakentaja luo kopion muuttujan pmj kantaluokkaosasta! Sen sijaan aliluokan lisa¨a¨- ma¨a¨n pa¨iva¨ykseen ei kosketa lainkaan, koska Mjono-luokan kopiora- kentaja ei ole siita¨ kuullutkaan! Kuva 7.4 seuraavalla sivulla havain- nollistaa tilannetta. Koodia katsottaessa on tietysti selva¨a¨, ettei mj voi olla kopio pmj:sta¨, koska se ei edes ole oikeaa tyyppia¨. Koodi kuiten- kin kelpaa ka¨a¨nta¨ja¨lle, koska se noudattaa periytymisen sa¨a¨nto¨ja¨. 7.1. Olioiden kopiointi 211 Mjono−osa koko merkit PaivattyMjono−osa paivays Mjono−osa koko merkit pmj mj Viipaloidun Viipaloidun Mjono−luokan kopiorakentaja Viipaloituminen! KUVA 7.4: Viipaloituminen olion kopioinnissa Ta¨ta¨ ilmio¨ta¨, jossa oliota kopioitaessa kopioidaankin erehdyksessa¨ vain olion kantaluokkaosa, kutsutaan nimella¨ viipaloituminen (slic- ing) [Budd, 2002, luku 12]. Viipaloituminen esiintyy paitsi kopioimi- sessa myo¨s sijoittamisessa (josta tarkemmin aliluvussa 7.2.3). Ilmio¨ osoittaa, kuinka ongelmallinen “aliluokan olio on kantaluokan olio” -suhde voi olla — aliluokan olion tulee olla kuin kantaluokan olio, paitsi etta¨ siita¨ ei pita¨isi voida tehda¨ kantaluokkakopiota. Viipaloitumisen esiintyminen Viipaloituminen on C++:ssa vaarana kulissien takana monessa tilan- teessa. Esimerkiksi allaolevassa koodissa tapahtuu viipaloituminen: vector<Mjono> mjonovektori; PaivattyMjono pmj(\"Metsa¨a¨n menna¨a¨n\", tanaan); mjonovektori.push back(pmj); Koodissa vektorin loppuun lisa¨ta¨a¨n kopio push back:lle annetusta oliosta. Mjono-vektori voi sisa¨lta¨a¨ vain Mjono-olioita, joten vektoriin lisa¨ta¨a¨n viipaloitunut kopio pmj:n kantaluokkaosasta. Ta¨ma¨n vuok- si C++:ssa periytymista¨ ja polymorﬁsmia ka¨ytetta¨essa¨ vektoreihin ja muihin tietorakenteisiin tulee aina tallettaa osoittimia olioihin, ei itse 7.1. Olioiden kopiointi 212 olioita. Viitteet puolestaan eiva¨t kelpaa STL:n tietorakenteiden kuten vektorin alkioiksi. Ta¨ta¨ ka¨sitella¨a¨n aliluvussa 10.2 sivulla 311. C++ kopioi olioita automaattisesti myo¨s muissa tilanteissa, ja sil- loinkin viipaloituminen on vaarana. Ta¨llaisia tilanteita ovat olioiden va¨litta¨minen arvoparametreina ja paluuarvoina, joista kerrotaan ali- luvussa 7.3. Na¨issa¨kin tilanteissa ongelma ilmenee, kun parametri tai paluuarvo on kantaluokkatyyppia¨, mutta ohjelmassa va¨liteta¨a¨n ali- luokan olioita. Ta¨llo¨in aliluokan oliosta va¨littyy vain viipaloitunut kantaluokkaosan kopio. Viipaloituminen ja muut oliokielet Kopioitumisen yhteydessa¨ viipaloituminen on oliokielissa¨ la¨hes yksi- nomaan C++:n ongelma. Ta¨ma¨ saattaa vaikuttaa ylla¨tta¨va¨lta¨ — aiheu- tuuhan viipaloituminen periytymisesta¨, joka toimii samalla tavalla la¨hes kaikissa oliokielissa¨. Syy ongelman C++-keskeisyyteen juontaa juurensa ta¨ma¨n aliluvun alussa mainitusta C++:n vaatimuksesta, etta¨ olion (myo¨s kopion) tyyppi ta¨ytyy tieta¨a¨ oliota luotaessa. Suurimmassa osassa muita oliokielia¨ olioita ka¨sitella¨a¨n aina C++:n osoittimia muistuttavien mekanismien kautta (joita na¨issa¨ kielissa¨ usein kutsutaan viitteiksi). Na¨in tehda¨a¨n muuan muassa Javassa ja Smalltalkissa. Na¨issa¨ kielissa¨ milla¨a¨n olioilla ei ole esimerkiksi omaa nimea¨ lainkaan, vaan niihin viitataan aina nimetyn olioviitteen kaut- ta. Ta¨ha¨n liittyy myo¨s se luvussa 3 mainittu asia, etta¨ olioiden elinkaari ei ole staattisesti ma¨a¨ra¨tty, vaan roskienkeruu pita¨a¨ huo- len olioiden tuhoamisesta. C++:n na¨ko¨kulmasta voitaisiin sanoa, etta¨ Javassa ja Smalltalkissa oliot luodaan aina dynaamisesti new’ta¨ vastaa- valla mekanismilla. Aliluvussa 7.1.1 todettiin, etta¨ Javassa ja Smalltalkissa oliot kopioi- daan kutsumalla kopioitavan olion kopiointipalvelua (Javan clone, Smalltalkin copy tai deepCopy). Ylla¨tta¨va¨a¨ kylla¨, ta¨ma¨ seikka yhdistet- tyna¨ siihen, etta¨ olioita ka¨sitella¨a¨n aina viitteiden kautta, esta¨a¨ vii- palointiongelman syntymisen kopioinnin yhteydessa¨. Kun nimitta¨in olio itse suorittaa kopioinnin, se pystyy luomaan oikeantyyppisen kopio-olion. Olio itse tieta¨a¨ oman todellisen tyyppinsa¨, joten se voi luoda oikeantyyppisen kopion itsesta¨a¨n, vaikka kopiointikutsu teh- ta¨isiinkin kantaluokkaviitteen kautta. Java ja Smalltalk eiva¨t lisa¨ksi tunne olioiden va¨litta¨mista¨ arvopara- metreina tai palauttamista paluuarvoina, vaan parametrit ja paluuar- 7.1. Olioiden kopiointi 213 vot ovat aina olioviitteita¨ (osoittimia). Samoin tietorakenteisiin talle- tetaan aina olioviitteita¨, koska muuta vaihtoehtoa ei kielissa¨ ole. Niin- pa¨ kopiointiviipaloitumista ei na¨issa¨ kielissa¨ pa¨a¨se syntyma¨a¨n muu- allakaan. Sen sijaan viipaloituminen saattaa kylla¨ aiheuttaa ongelmia muualla, esimerkiksi sijoituksessa (ta¨ta¨ ka¨sitella¨a¨n aliluvussa 7.2.3). Viipaloitumisen kierta¨minen C++:ssa Viipaloituminen ei muodostu C++:ssa ongelmaksi kovinkaan usein, koska varsin usein olioita kuljetetaan ohjelmassa osoittimia ja viittei- ta¨ ka¨ytta¨en ilman, etta¨ niita¨ ta¨ytyy missa¨a¨n vaiheessa kopioida. Ko- piointitapauksissakin tiedeta¨a¨n usein jo ohjelmointivaiheessa olion todellinen tyyppi, eika¨ viipaloitumista pa¨a¨se tapahtumaan. Aina silloin ta¨llo¨in ohjelmissa tulee kuitenkin vastaan tilanne, jossa kantaluokkaosoittimen pa¨a¨ssa¨ oleva olio pita¨isi pystya¨ kopioi- maan, eika¨ oliosta tiedeta¨ tarkasti, mihin luokkaan se kuuluu (paitsi etta¨ se on joko kantaluokan tai jonkin siita¨ periytetyn luokan olio). Ta¨llo¨in ongelman voi ratkaista matkimalla muiden oliokielten ko- piointitapaa ja antamalla kopioitavan olion kloonata itsensa¨. Kirjoi- tetaan kantaluokkaan virtuaalifunktio kloonaa, joka palauttaa osoitti- men dynaamisesti luotuun kopioon oliosta. Ta¨ma¨ kloonaa-ja¨senfunk- tio sitten toteutetaan jokaisessa hierarkian luokassa niin, etta¨ se luo new’lla¨ kopion itsesta¨a¨n kopiorakentajaa ka¨ytta¨en. Listaus 7.3 seuraavalla sivulla na¨ytta¨a¨ esimerkin ta¨llaisesta kloo- naamisesta. Viipaloitumista ei pa¨a¨se tapahtumaan, koska dynaami- nen sitominen pita¨a¨ huolen siita¨, etta¨ oliolle kutsutaan aina sen omaa kloonaa-toteutusta, vaikka itse kutsu olisikin tapahtunut kantaluok- kaosoittimen tai -viitteen kautta. Haittana ta¨ssa¨ kopiointitavassa on se, etta¨ jokainen kopio luodaan dynaamisesti. Ta¨sta¨ johtuen kopio- ta ei tuhota automaattisesti, vaan kopioijan tulee itse muistaa tuhota kopio deletella¨, kun sita¨ ei ena¨a¨ tarvita. Lisa¨ksi tyypillisesti olioiden dynaaminen luominen vie hieman enemma¨n muistia ja on va¨ha¨n hi- taampaa kuin staattinen. Ta¨sta¨ ei kuitenkaan yleensa¨ aiheudu ka¨y- ta¨nno¨ssa¨ tehokkuushaittaa. Ika¨va¨a¨ kloonausratkaisussa on myo¨s se, etta¨ jokaisen periytetyn luokan on muistettava itse toteuttaa kloonaa-ja¨senfunktio. Jos ta¨- ma¨ unohtuu, periytyy kantaluokan toteutus aliluokkaan, ja viipaloi- 7.1. Olioiden kopiointi 214 1 class Mjono 2 { 3 public: 10 Mjono(Mjono const& m); 11 virtual Mjono* kloonaa() const; ... 12 }; ... 13 Mjono* Mjono::kloonaa() const 14 { 15 return new Mjono(*this); 16 } ... 17 class PaivattyMjono : public Mjono 18 { 19 public: 21 PaivattyMjono(PaivattyMjono const& m); 22 virtual PaivattyMjono* kloonaa() const; ... 23 }; ... 24 PaivattyMjono* PaivattyMjono::kloonaa() const 25 { 26 return new PaivattyMjono(*this); 27 } ... 28 void kaytakopiota(Mjono const& mj) 29 { 30 Mjono* kopiop = mj.kloonaa(); // Tulos voi olla periytetyn kopio 31 // Ta¨a¨lla¨ sitten ka¨yteta¨a¨n kopiota 32 delete kopiop; kopiop = 0; // Pita¨a¨ muistaa myo¨s tuhota 33 } LISTAUS 7.3: Viipaloitumisen kierta¨minen kloonaa-ja¨senfunktiolla 7.2. Olioiden sijoittaminen 215 tuminen pa¨a¨see ja¨lleen tapahtumaan.] Abstrakteissa kantaluokissa kloonaa-ja¨senfunktiota ei voi toteuttaa, koska abstraktista kantaluo- kasta ei voi luoda olioita (eika¨ na¨in ollen myo¨ska¨a¨n kopiota). Ta¨llai- sessa kantaluokassa kloonaa esitella¨a¨nkin puhtaana virtuaalifunktio- na, jolloin sen toteuttaminen ja¨a¨ konkreettisten aliluokkien vastuulle. Listauksessa 7.3 saattaa ihmetytta¨a¨ se, etta¨ Mjono-luokassa kloonaa-funktiosta palautetaan Mjono-osoitin, kun taas periytetys- sa¨ luokassa ja¨senfunktion paluutyyppi onkin PaivattyMjono-osoitin. Ta¨ssa¨ on kyseessa¨ aliluvussa 6.5 mainittu paluutyypin kovarianssi. Se mahdollistaa sen, etta¨ kun kloonausta kutsutaan jonkin tyyppisen osoittimen kautta, saadaan paluuarvona samantyyppinen osoitin ko- pioon. Kloonausfunktio antaa mahdollisuuden kopiointiin ilman viipa- loitumista. Viipaloituminen on kuitenkin edelleen vaarana aiemmin ka¨sitellyissa¨ tietorakenteissa, parametrinva¨lityksessa¨ ja paluuarvois- sa. Na¨ihin vaaroihin paras apu on huolellinen suunnittelu ja ongel- man tiedostaminen. Yksi mahdollinen ratkaisu on myo¨s suunnitella ohjelman luokkahierarkiat niin, etta¨ kaikki kantaluokat ovat abstrak- teja. Ta¨llo¨in viipaloitumista ei pa¨a¨se syntyma¨a¨n, koska virheellista¨ kantaluokkakopiota on mahdotonta tehda¨ (abstrakteista kantaluokis- ta ei saa tehda¨ olioita) [Meyers, 1996, Item 33]. 7.2 Olioiden sijoittaminen Olioiden kopioimisen lisa¨ksi on toinenkin tapa saada aikaan kaksi keskena¨a¨n samanlaista oliota: sijoittaminen. Sijoittaminen on impe- ratiivisessa ohjelmoinnissa eritta¨in tavallinen toimenpide, ja se ope- tetaan useimmilla ohjelmointikursseilla la¨hes ensimma¨isena¨. Useim- miten sijoitettavat muuttujat ovat jotain kielen perustyyppia¨, mutta olio-ohjelmoinnissa luonnollisesti myo¨s sijoitetaan olioita toisiinsa. 7.2.1 Sijoituksen ja kopioinnin erot Olioiden sijoittaminen toisiinsa liittyy la¨heisesti olioiden kopiointiin, jopa siina¨ ma¨a¨rin, etta¨ joissain oliokielissa¨ ei sijoittamista tunneta ol- . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ]Ta¨ta¨ mekaanista kloonaa-funktion kopioimista voi jonkin verran helpottaa sopivalla esi- ka¨a¨nta¨ja¨makrokikkailulla, mutta se menee jo reilusti ohi ta¨ma¨n kirjan aihepiirin. Mutta kylla¨ #define:lla¨kin paikkansa on. . . , 7.2. Olioiden sijoittaminen 216 lenkaan vaan se on korvattu kopioinnilla. Sijoituksen ja kopioinnin lopputulos on sama: kaksi oliota, joiden “arvo” on sama. Kopioinnissa kuitenkin luodaan uusi olio, joka alustetaan olemassa olevan perus- teella, kun taas sijoituksessa on jo olemassa vanha olio, jonka “arvo” muutetaan vastaamaan toista oliota. Sijoitukseen liittyva¨t ongelmat liittyva¨t yleensa¨ juuri olion van- han sisa¨llo¨n ka¨sittelyyn. Usein sijoituksessa joudutaan ensin vapaut- tamaan vanhaa muistia ja muuten siivoamaan oliota va¨ha¨n purkajan tapaan ja ta¨ma¨n ja¨lkeen kopioimaan toisen olion sisa¨lto¨. Ta¨ma¨ aiheut- taa tiettyja¨ vaaratilanteita, joita uuden olion luomisessa ei ole. Erityisen hankalaksi sijoitus tulee, jos ohjelman ta¨ytyy varautua sijoituksessa mahdollisesti sattuviin virhetilanteisiin. Jos esimerkik- si halutaan, etta¨ sijoituksen epa¨onnistuessa olion vanha arvo sa¨ilyy, sijoitus ta¨ytyy tehda¨ varsin varovaisesti, jotta vanhaa arvoa ei heiteta¨ roskiin liian aikaisin. Na¨ita¨ ongelmia ka¨sitella¨a¨n jonkin verran virhe- ka¨sittelyn yhteydessa¨ aliluvussa 11.8. Ta¨llaisten vikasietoisten sijoi- tusten ka¨sittely on kuitenkin varsin hankalaa (sijoituksesta ja virheis- ta¨ C++:ssa on mainio artikkeli “The Anatomy of the Assignment Opera- tor” [Gillam, 1997]). On myo¨s tapauksia, joissa olion kopioiminen on mahdollista ja sallittua mutta sijoittaminen ei kuitenkaan ole mieleka¨sta¨. Esimerk- kina¨ voisi olla vaikka piirto-ohjelman ympyra¨olio: kopion luominen olemassa olevasta ympyra¨sta¨ on varmasti hyo¨dyllinen ominaisuus, mutta ympyra¨n sijoittaminen toiseen ympyra¨a¨n ei ole edes ajatukse- na ja¨rkeva¨. Niinpa¨ sijoittamisen ja kopioimisen salliminen tai esta¨mi- nen on syyta¨ pystya¨ tekema¨a¨n toisistaan riippumatta. 7.2.2 C++: Sijoitusoperaattori C++:ssa olioiden sijoittaminen tapahtuu erityisella¨ ja¨senfunktiolla, jo- ta kutsutaan sijoitusoperaattoriksi (assignment operator). Kun ohjel- massa tehda¨a¨n kahden olion sijoitus a = b, kyseisella¨ ohjelmarivilla¨ kutsutaan itse asiassa olion a sijoitusoperaattoria ja annetaan sille vii- te olioon b parametrina. Sijoitusoperaattorin tehta¨va¨na¨ on sitten tu- hota olion a vanha arvo ja korvata se olion b arvolla. Se, mita¨ kaikkia operaatioita ta¨ha¨n liittyy, riippuu ta¨ysin kyseessa¨ olevasta luokasta, kuten kopioinnissakin. Sijoitusoperaattorin syntaksi na¨kyy listauksesta 7.4 seuraavalla sivulla. Sijoitusoperaattori on ja¨senfunktio, joten se ta¨ytyy esitel- 7.2. Olioiden sijoittaminen 217 la¨ luokan esittelyssa¨. Sen nimi on “operator =” (sanava¨li yhta¨suu- ruusmerkin ja sanan operator va¨lissa¨ ei ole pakollinen, joten myo¨s “operator=” kelpaa). Merkkijonoluokan sijoitusoperaattori ottaa pa- rametrikseen vakioviitteen toiseen merkkijonoon, joka siis on sijoi- tuslauseessa yhta¨suuruusmerkin oikealla puolella oleva olio, jonka arvoa ollaan sijoittamassa. Ta¨ta¨ operaattoria ka¨ytta¨en sijoitus a = b aiheuttaa ja¨senfunktiokutsun a.operator =(b). Sijoitusoperaattori palauttaa paluuarvonaan viitteen olioon it- seensa¨. Syyna¨ ta¨ha¨n on C-kielesta¨ periytyva¨ mahdollisuus ketjusijoi- tukseen a = b = c, joka ensin sijoittaa c:n arvon b:hen ja sen ja¨lkeen b:n a:han. Kun nyt ensimma¨inen sijoitus palauttaa viitteen b:hen, ket- . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . mjono.hh . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1 class Mjono 2 { 3 public: 10 Mjono& operator =(Mjono const& vanha); ... 14 }; . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . mjono.cc . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1 Mjono& Mjono::operator =(Mjono const& vanha) 2 { 3 if (this != &vanha) 4 { // Jos ei sijoiteta itseen 5 delete[ ] merkit ; merkit = 0; // Vapauta vanha 6 koko = vanha.koko ; // Sijoita koko 7 if (koko != 0) 8 { // Varaa tilaa, jos koko ei nolla 9 merkit = new char[koko + 1]; 10 for (unsigned long i = 0; i != koko ; ++i) 11 { // Kopioi merkit 12 merkit [i] = vanha.merkit [i]; 13 } 14 merkit [koko ] = ’\\0’; // Loppumerkki 15 } 16 } 17 return *this; 18 } LISTAUS 7.4: Esimerkki sijoitusoperaattorista 7.2. Olioiden sijoittaminen 218 jusijoituksesti aiheutuu lopulta ja¨senfunktioketju a.operator =(b.operator =(c)) Sijoitusoperaattorin koodissa olion itsensa¨ palauttaminen tapah- tuu this-osoittimen avulla. Ta¨ma¨ osoitin osoittaa olioon itseensa¨, jo- ten *this on tapa sanoa “mina¨ itse”. Niinpa¨ rivi 17 palauttaa viitteen olioon itseensa¨. Sijoitusoperaattorin koodi on yleensa¨ jonkinlainen yhdistelma¨ purkajan ja kopiorakentajan koodeista. Rivilla¨ 3 olevaan ehtolausee- seen palataan myo¨hemmin. Rivi 5 tuhoaa merkkijonon vanhan sisa¨l- lo¨n, ja riveilla¨ 6–15 varataan tilaa kopiolle uudesta merkkijonosta ja kopioidaan merkkijono talteen. Ta¨ma¨n ja¨lkeen sijoitus onkin suori- tettu. Sijoitus itseen Sijoituksessa on tietysti periaatteessa mahdollista, etta¨ joku yritta¨a¨ si- joittaa olion itseensa¨, siis tyyliin a = a. Vaikka kukaan tuskin ta¨llaista sijoitusta ohjelmaansa kirjoittaakaan, saattaa itseen sijoitus piiloutua ohjelmaan tilanteissa, joissa parametreina tai osoittimien kautta saa- tuja olioita sijoitetaan toisiinsa. Itseen sijoitus tuntuu ta¨ysin vaaratto- malta operaatiolta, mutta sijoitusoperaattorin koodi on todella helppo kirjoittaa niin, etta¨ itseensa¨ sijoittamisella on vakavat seuraukset. Normaalisti sijoitusoperaattori toimii vapauttamalla ensin olion vanhaan arvoon liittyva¨n muistin ja mahdollisesti muut resurssit, jonka ja¨lkeen se varaa uutta muistia ja tekee varsinaisen sijoituksen. Jos nyt oliota ollaan sijoittamassa itseensa¨, olion vanha ja uusi arvo ovat itse asiassa samat. Ta¨llo¨in sijoitusoperaattori aloittaa toimintan- sa tuhoamalla olion arvon, jonka ja¨lkeen se yritta¨a¨ sijoittaa olion nyt jo tuhottua arvoa takaisin olioon itseensa¨. Esimerkin merkkijonoluokan tapauksessa ta¨ma¨ aiheuttaisi sen, et- ta¨ merkkijonon merkit sisa¨lta¨va¨ muisti vapautetaan, jonka ja¨lkeen si- joitusoperaattori varaa osoittimen pa¨a¨ha¨n uutta muistia ja sen ja¨lkeen tekee “tyhja¨n” kopioinnin, jossa uuden muistialueen alustamaton si- sa¨lto¨ kopioidaan itsensa¨ pa¨a¨lle. Joka tapauksessa olion sisa¨lto¨ on hu- kassa. Itseen sijoituksen ongelma ja¨a¨ helposti huomaamatta, koska sijoi- tusoperaattorin koodi na¨ytta¨a¨ silta¨, kuin meilla¨ olisi kaksi oliota: olio 7.2. Olioiden sijoittaminen 219 itse ja viiteparametrina saatu olio. Itseensa¨ sijoituksessa na¨ma¨ mo- lemmat ovat kuitenkin sama olio. Ongelmalle on kuitenkin onneksi helppo ratkaisu. Olion sijoitta- misen itseensa¨ ei tietenka¨a¨n pita¨isi tehda¨ mita¨a¨n, joten sijoitusope- raattorin koodissa pita¨a¨ vain tarkastaa, ollaanko oliota sijoittamassa itseensa¨. Jos na¨in on, voidaan koko sijoitus ja¨tta¨a¨ suorittamatta. Itseen sijoittamisen tarkastamisessa ta¨ytyy tutkia, ovatko olio itse ja viitepa- rametrina saatu olio samat. C++:ssa olioiden samuutta voidaan tutkia osoittimien avulla. Jos kaksi samantyyppista¨ osoitinta ovat yhta¨ suu- ret, osoittavat ne varmasti samaan olioon. Niinpa¨ listauksen 7.4 rivil- la¨ 3 tutkitaan, onko olioon itseensa¨ osoittava osoitin this yhta¨ suu- ri kuin osoitin parametrina saatuun olioon. Jos osoittimet ovat yhta¨ suuret eli ollaan suorittamassa sijoitusta itseen, hypa¨ta¨a¨n koko sijoi- tuskoodin yli ja poistutaan sijoitusoperaattorista. Periytyminen ja sijoitusoperaattori Aliluokan sijoituksessa ta¨ytyy pita¨a¨ huolta myo¨s olion kantaluokkao- san sijoittamisesta aivan kuten kopioinnissakin. Kopiorakentajassa ta¨ma¨ tapahtui kantaluokan kopiorakentajaa kutsumalla, sijoituksessa vastaavasti periytetyn luokan sijoitusoperaattorissa tulee kutsua kan- taluokan sijoitusoperaattoria. Listaus 7.5 seuraavalla sivulla na¨ytta¨a¨ pa¨iva¨tyn merkkijonon sijoitusoperaattorin, jossa merkkijono-osan si- joitus tapahtuu rivilla¨ 5 eksplisiittisesti kantaluokan sijoitusoperaat- toria kutsumalla. Ta¨ma¨n ja¨lkeen rivilla¨ 7 sijoitetaan aliluokan pa¨i- va¨ysja¨senmuuttuja sen omaa sijoitusta ka¨ytta¨en. Ka¨a¨nta¨ja¨n luoma oletusarvoinen sijoitusoperaattori Jos luokkaan ei kirjoiteta omaa sijoitusoperaattoria, ka¨a¨nta¨ja¨ lisa¨a¨ sii- hen automaattisesti oletusarvoisen sijoitusoperaattorin, joten ta¨ssa¨- kin suhteessa kopiorakentaja ja sijoitusoperaattori muistuttavat toi- siaan. Ta¨ma¨ oletusarvoinen sijoitusoperaattori yksinkertaisesti sijoit- taa kaikki olion ja¨senmuuttujat yksi kerrallaan. Ta¨sta¨ syysta¨ listauk- sen 7.5 rivin 7 pa¨iva¨ysolion sijoitus onnistuu, vaikka pa¨iva¨ysluokalle ei ole kirjoitettu sijoitusoperaattoria. Oletusarvoisen sijoitusoperaattorin ongelmat ovat samat kuin ole- tusarvoisen kopiorakentajankin. Jos luokassa on ja¨senmuuttujina osoittimia, ne sijoitetaan normaalia osoittimien sijoitusta ka¨ytta¨en, 7.2. Olioiden sijoittaminen 220 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . pmjono.hh . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1 class PaivattyMjono : public Mjono 2 { 3 public: 14 PaivattyMjono& operator =(PaivattyMjono const& vanha); ... 17 }; . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . pmjono.cc . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1 PaivattyMjono& PaivattyMjono::operator =(PaivattyMjono const& vanha) 2 { 3 if (this != &vanha) 4 { // Jos ei sijoiteta itseen 5 Mjono::operator =(vanha); // Kantaluokan sijoitusoperaattori 6 // Oma sijoitus, oletetaan etta¨ Paivays-luokalla on sijoitusoperaattori 7 paivays = vanha.paivays ; 8 } 9 return *this; 10 } LISTAUS 7.5: Sijoitusoperaattori periytetyssa¨ luokassa jolloin sijoituksen ja¨lkeen molempien olioiden osoittimet osoittavat samaan paikkaan ja oliot jakavat osoittimen pa¨a¨ssa¨ olevan datan. Useimmissa tapauksissa ta¨ma¨ ei kuitenkaan ole se, mita¨ halutaan, ja esimerkiksi merkkijonoluokassa oletusarvoinen sijoitusoperaattori toimisi pahasti va¨a¨rin. Ta¨ma¨n takia jokaiseen luokkaan tulisi erik- seen kirjoittaa sijoitusoperaattori. Sijoituksen esta¨minen Jos luokan olioiden sijoitus ei ole mieleka¨sta¨, voidaan sijoittaminen esta¨a¨ samalla tavoin kuin kopioiminen. Luokan sijoitusoperaattori esitella¨a¨n luokan esittelyssa¨ private-puolella, jolloin sita¨ ei pa¨a¨sta¨ kutsumaan luokan ulkopuolelta. Ta¨ma¨n ja¨lkeen sijoitusoperaattoril- le ei kirjoiteta ollenkaan toteutusta, jolloin sen ka¨ytto¨ luokan sisa¨lla¨ aiheuttaa linkitysaikaisen virheilmoituksen. Varsin usein sijoituksen ja kopioinnin mielekkyys ka¨yva¨t ka¨si ka¨- dessa¨, joten yleensa¨ luokalle joko kirjoitetaan seka¨ kopiorakentaja et- ta¨ sijoitusoperaattori tai sitten molempien ka¨ytto¨ esteta¨a¨n. Mutta ku- ten jo aiemmin todettiin, poikkeuksia ta¨ha¨n periaatteeseen on helppo keksia¨. 7.2. Olioiden sijoittaminen 221 7.2.3 Sijoitus ja viipaloituminen Aliluvussa 7.1.3 mainittu viipaloitumisongelma ei koske ainoastaan olioiden kopiointia. Koska sijoitus ja kopiointi ovat hyvin la¨hella¨ toi- siaan, on luonnollista, etta¨ viipaloituminen on vaarana myo¨s sijoituk- sessa. La¨hto¨kohdiltaan tilanne on sama kuin kopioimisessakin: jokai- nen aliluokan olio on myo¨s kantaluokan olio, joten aliluokan olion voi sijoittaa kantaluokan olioon. Ta¨llo¨in aliluokasta sijoitetaan ai- noastaan kantaluokkaosa ja periytta¨ma¨lla¨ lisa¨ttya¨ osaa ei huomioida mitenka¨a¨n. Ja¨lleen kerran ka¨a¨nta¨ja¨ ei varoita ta¨sta¨ mitenka¨a¨n, koska periytymisen tyypityksen kannalta asia on niin kuin pita¨a¨kin. Sijoitus tuo viipaloitumiseen viela¨ lisa¨a¨ kiemuroita. Olion, johon sijoitus tapahtuu, ei va¨ltta¨ma¨tta¨ tarvitse olla kantaluokan olio, vaan se voi olla myo¨s jonkin toisen samasta kantaluokasta periytetyn luo- kan olio. Ta¨llaisen erityyppisten olioiden sijoituksen ei tietenka¨a¨n pi- ta¨isi olla edes mahdollista. Sijoitus ja viipaloituminen tulevat kuiten- kin mahdolliseksi, jos sijoittaminen tapahtuu olioon osoittavan kan- taluokkaosoittimen tai -viitteen kautta. Ka¨a¨nta¨ja¨ valitsee ka¨ytetta¨va¨n sijoitusoperaattorin osoittimen tyypin perusteella, joten valittua tulee kantaluokan sijoitus. Ta¨llo¨in kantaluokan sijoitusoperaattori sijoit- taa erityyppisten olioiden kantaluokkaosat, ja aliluokkien lisa¨osiin ei kosketa. Listaus 7.6 seuraavalla sivulla na¨ytta¨a¨ esimerkin molemmis- ta viipaloitumismahdollisuuksista. Kuva 7.5 sivulla 223 havainnol- listaa listauksessa tapahtuvaa viipaloitumista. Koska sijoitusviipaloitumisessa on kysymys va¨a¨ra¨n sijoitusope- raattorin valitsemisesta, luonnolliselta ratkaisulta saattaisi tuntua si- joitusoperaattorin muuttaminen virtuaaliseksi. Ta¨ma¨ ei kuitenkaan ole mahdollista. Syyna¨ on se, etta¨ virtuaalifunktion parametrien tu- lee pysya¨ luokasta toiseen samana, kun taas sijoitusoperaattorin para- metrin tulee olla aina vakioviite luokkaan itseensa¨. Toisekseen virtu- aalisuus ei muutenkaan ratkaisisi koko ongelmaa, koska sijoituksessa aliluokasta kantaluokkaan kantaluokan sijoitusoperaattori on ainoa mahdollinen. Viipaloitumisvaaran esta¨miskeinoja C++:ssa on muutama. Helpoin on tehda¨ luokkahierarkia, jossa kaikki kantaluokat ovat abstrakte- ja, jolloin niilla¨ ei ole sijoitusoperaattoria ollenkaan. Ta¨llo¨in viipa- loitumista ei voi tapahtua [Meyers, 1996, Item 33]. Toinen ratkaisu on tarkastaa aina sijoituksen yhteydessa¨, etta¨ olio itse ja sijoitettava olio kuuluvat samaan luokkaan kuin suoritettava sijoitusoperaatto- 7.2. Olioiden sijoittaminen 222 ... 1 class NumMjono : public Mjono 2 { 3 public: 4 NumMjono(char const* merkit, int numero); ... 5 NumMjono& operator =(NumMjono const& nmj); 6 7 private: 8 int numero ; 9 }; ... 10 void sijoita(Mjono& mihin, Mjono const& mista) 11 { 12 mihin = mista; 13 } 14 15 int main() 16 { 17 Mjono mj(\"Tavallinen\"); 18 PaivattyMjono pmj(\"Pa¨iva¨tty\", tanaan); 19 NumMjono nmj(\"Numeroitu\", 12); 20 21 sijoita(pmj, nmj); // Viipaloituminen funktion sisa¨lla¨! 22 sijoita(mj, pmj); // Ta¨a¨lla¨ myo¨s! 23 } LISTAUS 7.6: Viipaloitumismahdollisuudet sijoituksessa ri. Ta¨ma¨ onnistuu listauksen 7.7 seuraavalla sivulla osoittamalla ta- valla typeid-operaattorin avulla. Ta¨ma¨ tarkastus on kuitenkin vasta ajoaikainen, joten mahdollisten virheiden havaitseminen ja¨a¨ ohjel- man testausvaiheeseen. Sijoitukseen liittyva¨ viipaloituminen ei sina¨nsa¨ ole pelka¨sta¨a¨n C++:n ongelma, vaan se ilmenee myo¨s muissa oliokielissa¨ kuten Javassa ja Smalltalkissa, jos olioita (eika¨ pelka¨sta¨a¨n olio-osoittimia) voi sijoit- taa toisiinsa. Na¨issa¨ kielissa¨ ei kuitenkaan yleensa¨ ole erillista¨ sijoi- tusoperaattoria, vaan ohjelmoijan ta¨ytyy itse kirjoittaa sijoita-pal- velu tms. Ta¨llaisissa tilanteissa viipaloituminen tulee mahdolliseksi na¨issa¨kin kielissa¨. 7.2. Olioiden sijoittaminen 223 Mjono−osa koko merkit PaivattyMjono−osa paivays Mjono−osa koko merkit NumMjono−osa numero Mjono−osa koko merkit pmj nmj mj Viipaloituminen! Viipaloituminen! Mjono−luokan sijoitusoper. Mjono−luokan sijoitusoper. Vain varjostettu osa näkyy kantaluokkaviitteiden kautta KUVA 7.5: Viipaloituminen olioiden sijoituksessa 1 #include <typeinfo> 2 Mjono& Mjono::operator =(Mjono const& m) 3 { 4 if (typeid(*this) != typeid(MJono) | | typeid(m) != typeid(MJono)) 5 { 6 /* Virhetoiminta */ 7 } 8 // Tai assert(typeid(*this) == typeid(MJono) && typeid(m) == typeid(MJono)); 9 // (ks. aliluku 8.1.4) 10 if (this != &m) 11 { ... 12 } 13 return *this; 14 } LISTAUS 7.7: Viipaloitumisen esta¨minen ajoaikaisella tarkastuksella 7.3. Oliot arvoparametreina ja paluuarvoina 224 7.3 Oliot arvoparametreina ja paluuarvoina Kun C:ssa¨ ja C++:ssa kokonaislukumuuttuja va¨liteta¨a¨n funktioon nor- maalina parametrina, ka¨yteta¨a¨n arvonva¨litysta¨ (call by value). Ta¨ma¨ tarkoittaa, etta¨ funktioon ei va¨liteta¨ itse muuttujaa vaan sen arvo. Kun nyt C++:ssa halutaan va¨litta¨a¨ olioita funktioon vastaavanlaisina arvo- parametreina, joudutaan ja¨lleen miettima¨a¨n, mika¨ oikein on se olion “arvo”, joka funktioon va¨liteta¨a¨n. Kopioinnin yhteydessa¨ olion arvon ka¨sitetta¨ on jo pohdittu, joten on luonnollista ka¨ytta¨a¨ samaa ideaa ta¨ssa¨kin tapauksessa. Kun olio va¨liteta¨a¨n funktiolle arvoparametrina, funktion sisa¨lle luodaan kopio parametrioliosta ka¨ytta¨ma¨lla¨ luokan kopiorakentajaa. Koska kopiora- kentaja tulee ma¨a¨ritella¨ niin, etta¨ alkupera¨isen olion ja uuden olion “arvot” ovat samat, na¨in saadaan samalla va¨litetyksi parametrin arvo funktion sisa¨lle. Funktion sisa¨lla¨ parametrin kopio ka¨ytta¨ytyy kuten tavallinen funktion paikallinen olio, ja sita¨ pystyy ka¨ytta¨ma¨a¨n normaalisti para- metrin nimen avulla. Parametriin tehta¨va¨t muutokset muuttavat tie- tysti vain kopiota, joten alkupera¨inen olio sa¨ilyy muuttumattomana. Kun lopulta funktiosta palataan, parametrikopio tuhotaan aivan ku- ten normaalit paikalliset muuttujatkin. Olioiden ka¨ytto¨ arvoparametreina on siis ta¨ysin mahdollista C++:ssa, kunhan luokalla vain on toimiva kopiorakentaja. Arvopara- metrien ka¨ytto¨ ei kuitenkaan yleensa¨ ole suositeltavaa, koska olion kopioiminen ja kopion tuhoaminen funktion lopussa ovat mahdolli- sesti hyvin raskaita toimenpiteita¨. Toinen vaara on, etta¨ funktion si- sa¨ssa¨ muutetaan parametrikopiota ja luullaan, etta¨ muutos tapahtuu- kin parametrina annettuun alkupera¨iseen olioon. Viimeiseksi periy- tyminen ja arvonva¨litys saavat aikaan viipaloitumisen vaaran (alilu- ku 7.1.3). Kaiken ta¨ma¨n vuoksi olioiden parametrinva¨lityksessa¨ kan- nattaa aina ka¨ytta¨a¨ viitteita¨ — ja erityisesti const-viitteita¨ — aina kun se on mahdollista. Kun funktiosta palautetaan olio paluuarvona, tilanne on viela¨ mutkikkaampi. Olio, joka palautetaan return-lauseessa, on todenna¨- ko¨isesti funktion paikallinen olio, joten se tuhoutuu heti funktiosta palattaessa. Ta¨ma¨ kuitenkin tarkoittaa sita¨, etta¨ olio tuhoutuu, ennen kuin paluuarvoa ehdita¨a¨n ka¨ytta¨a¨ kutsujan puolella! Niinpa¨ funk- tion paluuarvo ta¨ytyykin saada talteen kutsujan puolelle, ennen kuin funktion paikalliset oliot tuhotaan. 7.3. Oliot arvoparametreina ja paluuarvoina 225 Sana “arvo” esiintyy ta¨ssa¨kin yhteydessa¨, joten ongelma ratkeaa kopioinnilla. Kun funktiosta palautetaan return-lauseella olio, funk- tion kutsujan puolelle luodaan nimeto¨n va¨liaikaisolio (temporary object), joka alustetaan kopiorakentajalla kopioksi return-lauseessa esiintyva¨sta¨ oliosta. Ta¨ma¨n ja¨lkeen funktiosta voidaan palata ja tu- hota kaikki paikalliset oliot, mukaanlukien alkupera¨inen paluuarvo. Kutsujan koodissa va¨liaikaisolio toimii funktion “paluuarvona”, ja jos paluuarvo esimerkiksi sijoitetaan talteen toiseen olioon, va¨liaikaiso- lio toimii sijoituksen alkuarvona. Kun va¨liaikaisoliota ei lopulta ena¨a¨ tarvita, se tuhotaan automaattisesti. Na¨in va¨liaikaisoliot eiva¨t voi ai- heuttaa muistivuotoja. C++ antaa ka¨a¨nta¨ja¨lle olioita palautettaessa mahdollisuuden opti- mointeihin ja tehokkaan koodin tuottamiseen. Paluuarvojen tapauk- sessa ka¨a¨nta¨ja¨ voi esimerkiksi ka¨ytta¨a¨ va¨liaikaisolion sijaan jotain ta- vallista oliota, jos se huomaa etta¨ va¨liaikaisolio va¨litto¨ma¨sti sijoitet- taisiin tai kopioitaisiin johonkin toiseen olioon. Kuten arvoparametrienkin tapauksessa, olioiden va¨litta¨minen pa- luuarvona onnistuu C++:ssa ilman erityisia¨ ongelmia, kunhan luokan kopiorakentaja toimii oikein. Paluuarvon kopiointi ja tuhoaminen ai- heuttavat kuitenkin ja¨lleen sen, etta¨ olion palauttaminen saattaa olla hyvin raskas operaatio. Myo¨s viipaloituminen on vaarana, jos kanta- luokkaolion palauttavasta funktiosta yriteta¨a¨nkin palauttaa aliluokan oliota (aliluku 7.1.3). Olioiden palauttamista paluuarvoina kannattaa- kin va¨ltta¨a¨, jos mahdollista. Olion palauttamiselle ei kuitenkaan ole 1 Paivays kuukaudenAlkuun(Paivays p) 2 { 3 Paivays tulos(p); 4 tulos.asetaPaiva(1); // Kuukauden alkuun 5 return tulos; 6 } 7 8 int main() 9 { 10 Paivays a(13, 10, 1999); 11 Paivays b(a); // Kopiorakentaja 12 b = kuukaudenAlkuun(a); 13 } LISTAUS 7.8: Olio arvoparametrina ja paluuarvona 7.3. Oliot arvoparametreina ja paluuarvoina 226 aivan yhta¨ itsesta¨a¨nselva¨a¨ vaihtoehtoa kuin viitteet parametrinva¨li- tyksen tapauksessa. Ei pida¨ koskaan tehda¨ sita¨ virhetta¨, etta¨ yritta¨a¨ va¨ltta¨a¨ olion pa- lauttamista paluuarvona ka¨ytta¨ma¨lla¨ viitetta¨ samalla tavoin kuin ar- voparametrien yhteydessa¨. Jos ohjelma palauttaa viitteen paikalli- seen olioon, viitteen pa¨a¨ssa¨ oleva olio tuhoutuu funktiosta palattes- sa. Ta¨llo¨in ohjelmaan ja¨a¨ viite olemattomaan olioon, jonka ka¨ytta¨mi- nen todenna¨ko¨isesti aiheuttaa varsin vinkeita¨ virhetilanteita. Alilu- vussa 11.7 esitelta¨va¨t automaattiosoittimet ovat yksi vaihtoehto, mut- ta usein paluuarvon korvaaminen viiteparametrilla on helpoin ratkai- su. Listauksessa 7.8 edellisella¨ sivulla on funktio, joka ka¨ytta¨a¨ olioita seka¨ arvoparametrina etta¨ paluuarvona. Kuva 7.6 na¨ytta¨a¨ vaihe ker- rallaan, mita¨ funktiota kutsuttaessa tapahtuu. Funktiokutsun vaiheet ovat seuraavat: 1. Funktiokutsun yhteydessa¨ parametrista a tehda¨a¨n automaatti- a : Paivays 1. 4. 5. b : Paivays ? : Paivays p : Paivays 2. 3. tulos : Paivays main() kuukaudenAlkuun() <<create>> <<create>> kuukaudenAlkuun(a) Paivays(a) Paivays(p) asetaPaiva(1) return \"?\" operator =(?) <<destroy>> <<destroy>> <<destroy>> Paivays(a) <<create>> <<create>> <<destroy>> <<destroy>> Paivays(tulos) <<create>> KUVA 7.6: Oliot arvoparametreina ja paluuarvoina 7.4. Tyyppimuunnokset 227 sesti kopiorakentajaa ka¨ytta¨en kopio kutsuttavan funktion puo- lelle. Na¨in funktion parametri p saa saman “arvon” kuin a. 2. Funktion sisa¨lla¨ luodaan paluuarvoa varten olio tulos kopioi- malla se parametrista p. Ta¨ma¨n ja¨lkeen tulosolion pa¨iva¨ys siir- reta¨a¨n kuukauden alkuun. 3. Funktio palauttaa paluuarvonaan olion tulos. Siita¨ luodaan ko- piorakentajalla automaattisesti nimeto¨n va¨liaikaiskopio kutsu- jan puolelle. Ta¨ma¨n ja¨lkeen funktiosta poistutaan ja seka¨ tulos etta¨ p tuhotaan. 4. Funktion paluuarvo sijoitetaan olioon b. Ta¨ssa¨ ka¨yteta¨a¨n sijoi- tusoperaattoria, jolle annetaan parametriksi a¨sken saatu va¨liai- kaisolio. Na¨in funktion paluuarvo saadaan talteen olioon b. 5. Rivin 12 lopussa va¨liaikaisoliota ei ena¨a¨ tarvita, joten se tuhou- tuu automaattisesti. 7.4 Tyyppimuunnokset Tyyppimuunnos (type cast) on operaatio, jota ohjelmoinnissa tar- vitaan silloin ta¨llo¨in, kun ka¨sitelta¨va¨ tieto ei ole jotain operaatio- ta varten oikean tyyppista¨. Olio-ohjelmoinnin kannalta suomenkieli- nen termi “tyyppimuunnos” on harhaanjohtava. Jos tyyppia¨ int oleva muuttuja i “muunnetaan” liukuluvuksi double, muuttujan i tyyppi ei tietenka¨a¨n muutu miksika¨a¨n, vaan se on edelleenkin kokonaisluku- muuttuja. Tarkempaa olisikin sanoa, etta¨ tyyppimuunnoksessa koko- naislukumuuttujan i arvon perusteella tuotetaan uusi liukulukuarvo, joka jollain tavalla vastaa muuttujan i arvoa. Ta¨ssa¨ suhteessa tyyppi- muunnos muistuttaa suuresti kopiointia, jossa siina¨kin tuotetaan ole- massa olevan olion perusteella uusi olio. Kopioinnissa uusi ja vanha olio ovat samantyyppiset, tyyppimuunnoksessa taas eiva¨t. Edellisessa¨ esimerkissa¨ vanhan ja uuden arvon vastaavuus tietysti on, etta¨ seka¨ i:n etta¨ tuotetun liukuluvun numeerinen arvo on sama. On kuitenkin olemassa tyyppimuunnoksia, joissa ta¨ma¨ vastaavuus ei ole yksi-yhteen. Esimerkiksi seka¨ liukuluvut 3.0 etta¨ 3.4 tuottavat ko- konaisluvuksi muunnettaessa arvon 3. Vastaavuus voi olla muutakin 7.4. Tyyppimuunnokset 228 kuin numeerinen. Jos osoitin muunnetaan kokonaisluvuksi, tulokse- na on useimmissa ympa¨risto¨issa¨ kokonaisluku, joka ilmoittaa sen ko- neen muistiosoitteen, jossa osoittimen osoittama olio sijaitsee. Olio-ohjelmoinnissa on tavallista, etta¨ ohjelmoija kirjoittaa omia abstrakteja tietotyyppeja¨a¨n, kuten murtolukuja, kompleksilukuja ja niin edelleen. Ta¨ma¨n vuoksi C++:ssa on mahdollisuus kirjoittaa omia tyyppimuunnosoperaattoreita, joiden avulla on mahdollisuus tehda¨ tyyppimuunnoksia omien tietotyyppien ja muiden tyyppien va¨lilla¨. Ta¨ma¨n lisa¨ksi standardoinnin yhteydessa¨ C++:aan tuli tyyppimuun- noksille uusi syntaksi, jota nykyisin suositellaan ka¨ytetta¨va¨ksi van- han C:sta¨ periytyva¨n muunnossyntaksin sijaan. 7.4.1 C++:n tyyppimuunnosoperaattorit C-kielessa¨ tyyppimuunnokset tehtiin syntaksilla (uusiTyyppi)vanhaArvo. Ta¨ma¨n syntaksin ongelma on, etta¨ su- lut ovat hieman epa¨loogisessa paikassa — eiva¨t parametrin vaan itse operaation ympa¨rilla¨. Ta¨sta¨ johtuen C++-kieleen lisa¨ttiin vaihtoehtoi- nen syntaksi uusiTyyppi(vanhaArvo). Kummankin na¨iden tyyppimuunnoksen ongelma on, etta¨ niita¨ voidaan ka¨ytta¨a¨ suorittamaan kaikentyyppisia¨ muunnoksia — niin muunnoksia kokonaisluvusta liukuluvuksi kuin muunnoksia olio- osoittimesta kokonaisluvuksi. Ka¨a¨nta¨ja¨ ei tarkasta la¨hes ollenkaan muunnoksen mielekkyytta¨ vaan luottaa siihen, etta¨ ohjelmoija tieta¨a¨ mita¨ on tekema¨ssa¨. Ka¨yta¨nno¨ssa¨ ta¨ma¨ on johtanut siihen, etta¨ tyyp- pimuunnoksiin ja¨a¨ helposti kirjoitusvirheita¨. Ta¨ma¨n lisa¨ksi tyyppi- muunnoksista johtuvien virheiden lo¨yta¨minen on usein vaikeaa, kos- ka itse muunnokset na¨ytta¨va¨t aivan funktiokutsuilta, joten niiden lo¨y- ta¨minen koodista ei ole aivan helppoa. ISOC++ on pyrkinyt korjaamaan ta¨ta¨ ongelmaa lisa¨a¨ma¨lla¨ kieleen nelja¨ aivan uutta tyyppimuunnosoperaattoria, joiden syntaksi poik- keaa jonkin verran aiemmista: static cast<uusiTyyppi>(vanhaArvo) const cast<uusiTyyppi>(vanhaArvo) dynamic cast<uusiTyyppi>(vanhaArvo) reinterpret cast<uusiTyyppi>(vanhaArvo) 7.4. Tyyppimuunnokset 229 Syntaksi saattaa ensikatsomalta tuntua oudolta, mutta se on yhteen- sopiva mallien ka¨ytta¨ma¨n syntaksin kanssa (malleista kerrotaan lu- vussa 9.5). Jokainen na¨ista¨ operaattoreista on tarkoitettu vain tietynlaisten mielekka¨iden muunnosten tekemiseen, ja ka¨a¨nta¨ja¨ antaa virheilmoi- tuksen, jos niita¨ yriteta¨a¨n ka¨ytta¨a¨ va¨a¨rin. Myo¨s vanhat tyyppimuun- nossyntaksit on sa¨ilytetty kielessa¨ yhteensopivuussyista¨, mutta nii- den ka¨ytto¨a¨ tulisi va¨ltta¨a¨, ja suosia uusia operaattoreita. static_cast Tyyppimuunnoksen static cast tehta¨va¨na¨ on suorittaa kaikkia sel- laisia tyyppimuunnoksia, joiden mielekkyydesta¨ ka¨a¨nta¨ja¨ voi varmis- tua jo ka¨a¨nno¨saikana. Ta¨ma¨ tarkoittaa esimerkiksi muunnoksia eri kokonaislukutyyppien va¨lilla¨, muunnoksia enum-luettelotyypeista¨ ko- konaisluvuiksi ja takaisin seka¨ muunnoksia kokonaislukutyyppien ja liukulukutyyppien va¨lilla¨. Esimerkiksi kahden kokonaisluvun kes- kiarvon voi laskea liukulukuna seuraavasti: double ka = (static cast<double>(i1) + static cast<double>(i2)) / 2.0; Muunnos static cast ei suostu suorittamaan sellaisia muunnok- sia, jotka eiva¨t ole mielekka¨ita¨. Esimerkiksi muunnos Paivays* pvmp = new Paivays(); int* ip = static cast<int*>(pvmp); // KA¨A¨NNO¨ SVIRHE! antaa ka¨a¨nno¨saikana virheilmoituksen, koska pa¨iva¨ysosoittimen pa¨a¨ssa¨ ei voi olla kokonaislukua ja na¨in ollen muunnos pa¨iva¨ysosoit- timesta kokonaislukuosoittimeksi on ja¨rjeto¨n. Tavallisten tyyppimuunnosten lisa¨ksi static cast on mahdolli- nen myo¨s silloin, kun kantaluokkaosoittimen pa¨a¨ssa¨ tiedeta¨a¨n ole- van jonkin aliluokan olio, johon halutaan aliluokkaosoitin ilman, etta¨ olion tyyppia¨ testataan ajoaikana. Aiemmin aliluvussa 6.5.3 ka¨sitel- ty dynamic cast on normaalisti oikea va¨line ta¨ha¨n, mutta jos kanta- luokkaosoittimen pa¨a¨ssa¨ olevan olion tyyppi on todella tiedossa, voi nopeuskriittisissa¨ ohjelmissa olla tarve pa¨a¨sta¨ eroon olion tyypin tes- tauksesta. Ta¨llo¨in muunnoksen voi korvata nopeammalla mutta tur- vattomammalla static cast-operaatiolla. 7.4. Tyyppimuunnokset 230 const_cast Normaalisti const-sanan ka¨ytto¨ ohjelmissa lisa¨a¨ ohjelman luotetta- vuutta, koska se pakottaa ohjelmoijan miettima¨a¨n, mita¨ olioita muu- tetaan ja mita¨ ei. Vastaavasti se esta¨a¨ sellaisten olioiden muuttami- sen, jotka on aiemmin ma¨a¨ritelty vakioiksi. Joskus on kuitenkin pak- ko ka¨ytta¨a¨ sellaista ohjelmakoodia, joka on suunniteltu “va¨a¨rin” ja jossa const-sanan ka¨ytto¨ tuo ongelmia. Ta¨llo¨in const cast antaa oh- jelmoijalle mahdollisuuden poistaa const-sanan vaikutuksen, eli silla¨ voi tehda¨ vakio-osoittimesta ja -viitteesta¨ ei-vakio-osoittimen tai -viit- teen. Yksi tyypillinen esimerkki na¨kyy listauksessa 7.9. Listauksen oh- jelmassa ka¨yteta¨a¨n jonkin toisen tahon ohjelmoimaa kirjastoa kirjo- jen ka¨sittelyyn. Ta¨sta¨ kirjastosta lo¨ytyy funktio kauankoPalautukseen, joka laskee annetusta kirjasta, kuinka pitka¨ aika viimeiseen palautus- pa¨iva¨a¨n on. Ongelmana on, etta¨ ta¨ma¨n funktion tekija¨ ei ole kuullut- kaan const-sanasta, joten funktio ottaa parametrinaan normaaliviit- teen kirjaan vakioviitteen sijaan. Funktio tulostaPalautusaika sen sijaan on koodattu oikein ja ot- taa parametrinaan vakioviitteen. Jos funktiossa nyt yritetta¨isiin kut- sua virheellista¨ funktiota, ka¨a¨nta¨ja¨ ei hyva¨ksyisi ta¨ta¨ kutsua, kos- ka vakio-oliota ei voi laittaa ei-vakioviitteen pa¨a¨ha¨n. On kuitenkin selva¨a¨, etta¨ kyseinen virhe ei johdu varsinaisesti virheellisesta¨ oh- jelmasuunnittelusta vaan siita¨, etta¨ toinen ohjelmoija on unohtanut 1 int kauankoPalautukseen(KirjastonKirja& kirja) 2 { 3 // Funktion toteutus ei muuta kirja-oliota mitenka¨a¨n ... 14 } . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1 void tulostaPalautusaika(KirjastonKirja const& k) 2 { 3 // int paivia = kauankoPalautukseen(k) aiheuttaisi ka¨a¨nno¨svirheen! 4 int paivia = kauankoPalautukseen(const cast<KirjastonKirja&>(k)); 5 cout << \"Kirjan \" << k.annaNimi() << \" palautukseen on \"; 6 cout << paivia << \" pa¨iva¨a¨.\" << endl; 7 } LISTAUS 7.9: Esimerkki const_cast-muunnoksesta 7.4. Tyyppimuunnokset 231 const-sanan. Ta¨ma¨n vuoksi rivilla¨ 4 on ka¨ytetty const cast-muun- nosta tuottamaan ei-vakioviite, joka kelpaa parametriksi huolimatto- masti kirjoitetulle funktiolle. Koska const cast-muunnoksen ka¨ytto¨ rikkoo C++:n “vakiota ei voi muuttaa” -periaatetta vastaan, sen ka¨ytto¨ on periaatteessa aina osoi- tus siita¨, etta¨ jokin osa ohjelmasta on suunniteltu huonosti. Ta¨sta¨ syysta¨ sen ka¨ytto¨a¨ tulisi va¨ltta¨a¨ aina kun mahdollista ja mieluummin korjata se ohjelmanosa, jossa const-sanan ka¨ytto¨ on laiminlyo¨ty. dynamic_cast Tyyppimuunnosta dynamic cast ka¨yteta¨a¨n muuttamaan kantaluokka- osoitin tai -viite aliluokkaosoittimeksi tai -viitteeksi ja samalla tarkas- tamaan, etta¨ osoittimen tai viitteen pa¨a¨ssa¨ oleva olio on todella sopi- vaan luokkaan kuuluva. Sen toiminta on jo ka¨sitelty aliluvussa 6.5.3 sivulla 164. Ajoaikaisesta tarkastuksesta johtuen dynamic cast on ainoa ISOC++:n uusista tyyppimuunnoksista, jota ei voi mitenka¨a¨n toteuttaa vanhaa tyyppimuunnossyntaksia ka¨ytta¨en. reinterpret_cast Kaikki ta¨ha¨n saakka esitellyt tyyppimuunnosoperaattorit ovat suos- tuneet tekema¨a¨n vain sellaisia muunnoksia, jotka ovat jollain taval- la ka¨ytettyjen tyyppien kannalta mielekka¨ita¨ (ellei const cast-muun- noksen tekema¨a¨ vakioisuuden poistoa katsota “mieletto¨ma¨ksi”). Sil- loin ta¨llo¨in ohjelmissa saattaa kuitenkin joutua ka¨sittelema¨a¨n tietoa tavalla, joka ei ole sen todellisen tyypin mukainen. Esimerkiksi osoitinta voi joskus joutua ka¨sittelema¨a¨n muistiosoit- teena — siis kokonaislukuna. Vastaavasti monissa ka¨ytto¨liittyma¨kir- jastoissa painonappeihin voi liitta¨a¨ “tunnistustietoa”, joka va¨liteta¨a¨n ohjelmalle nappia painettaessa. Ta¨ma¨ tieto voi olla vaikkapa tyyppia¨ void*, mutta silti siihen pita¨isi saada talletettua (ja myo¨hemmin pa- lautettua) osoitin johonkin olioon. Ta¨llaisia tiedon esitystavan muuttamiseen liittyvia¨ muunnoksia varten on operaattori reinterpret cast. Kuten sen nimikin ilmai- see, se “tulkitsee uudelleen” sille annetun tiedon toisen tyyppisena¨. Muunnoksen la¨hes ainoa ka¨ytto¨kohde on muuntaa tieto ensin toisen- 7.4. Tyyppimuunnokset 232 tyyppiseksi ja myo¨hemmin takaisin. ISOC++ -standardi luettelee seu- raavat muunnostyypit, joihin reinterpret cast kelpaa: • Osoittimen voi muuntaa kokonaisluvuksi, jos kokonaisluku- tyyppi on niin suuri, etta¨ osoitin mahtuu siihen. • Vastaavasti kokonaisluvun (tai luettelotyypin arvon) voi muun- taa takaisin osoittimeksi. • Tavallisen osoittimen voi muuntaa toisentyyppiseksi osoitti- meksi. • Viitteen voi muuntaa toisentyyppiseksi viitteeksi. • Funktio-osoittimen voi muuntaa toisentyyppiseksi funktio- osoittimeksi (mutta tavallista osoitinta ei tarkasti ottaen voi muuntaa funktio-osoittimeksi tai pa¨invastoin, ainakaan niin et- ta¨ koodi olisi siirretta¨va¨a¨). Sama pa¨tee ja¨senfunktio- ja -muuttu- jaosoittimille. Kaikissa na¨issa¨ muunnoksissa muunnettua arvoa ei pita¨isi ka¨yt- ta¨a¨ mihinka¨a¨n muuhun kuin tiedon tallettamiseen ja myo¨hemmin ta- kaisin muuntamiseen. Ainoana poikkeuksena on osoittimen muun- taminen kokonaisluvuksi, jonka tuottaman tuloksen pita¨isi olla “sel- lainen, etta¨ se ei ha¨mma¨styta¨ niita¨, jotka tuntevat kyseisen koneen muistiosoitteiden rakenteen”.^ Listauksessa 7.10 seuraavalla sivulla on esimerkki tilanteesta, jos- sa ka¨ytto¨liittyma¨kirjasto tarjoaa funktion luoNappula, jolle annetaan nappulan nimi ja siihen liittyva¨ kokonaisluku. Kun nappulaa paine- taan, ka¨ytto¨liittyma¨ kutsuu funktiota nappulaaPainettu ja antaa sil- le parametriksi nappulaan liittyva¨n kokonaisluvun. Esimerkki ka¨yt- ta¨a¨ riveilla¨ 4–5 reinterpret cast-muunnosta tallettamaan nappuloi- den sisa¨lta¨miin lukuihin osoittimia kirjoihin ja myo¨hemmin rivilla¨ 10 muuntamaan saadut kokonaisluvut taas takaisin osoittimiksi. 7.4.2 Ohjelmoijan ma¨a¨rittelema¨t tyyppimuunnokset Kun ohjelmoija kirjoittaa oman tietotyyppinsa¨, on mahdollisesti tar- vittavia tyyppimuunnoksia kahdenlaisia: tyyppimuunnoksia, jotka . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ^“It [the mapping function] is intended to be unsurprising to those who know the address- ing structure of the underlying machine.” [ISO, 1998, kohta 5.2.10.4] 7.4. Tyyppimuunnokset 233 ... 1 void luoNappula(string const& nimi, unsigned long int luku); ... . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1 // Ta¨ma¨ funktio luo ka¨ytto¨liittyma¨n 2 void luoKayttoliittyma(KirjastonKirja* kirja1, KirjastonKirja* kirja2) 3 { 4 luoNappula(\"Kirja1\", reinterpret cast<unsigned long int>(kirja1)); 5 luoNappula(\"Kirja2\", reinterpret cast<unsigned long int>(kirja2)); 6 } ... 7 // Ta¨ta¨ funktiota kutsutaan, kun nappulaa painetaan 8 void nappulaaPainettu(unsigned long int luku) 9 { 10 KirjastonKirja* kp = reinterpret cast<KirjastonKirja*>(luku); 11 cout << \"Painettu kirjaa \" << kp->annaNimi() << \".\" << endl; 12 } LISTAUS 7.10: Tiedon esitystavan muuttaminen ja reinterpret_cast muuntavat muun tyyppisia¨ arvoja ohjelmoijan kirjoittamaksi tyypik- si, ja muunnoksia, jotka muuttavat ohjelmoijan oman tyypin arvo- ja muuntyyppisiksi. C++ antaa ohjelmoijalle mahdollisuuden kirjoit- taa omia tyyppimuunnoksia molempiin suuntiin. Tyyppimuunnok- set kirjoitetaan oman tietotyyppiluokan ja¨senfunktioina, ja kumpaan- kin suuntaan tapahtuvilla muunnoksilla on oma syntaksinsa. Rakentaja tyyppimuunnoksena Oleellisilta osiltaan tyyppimuunnos on uuden erityyppisen arvon luomista olemassa olevan arvon pohjalta. C++:ssa uusia “arvoja” (olioi- ta) luodaan rakentajan avulla, joten on luontevaa yhdista¨a¨ tyyppi- muunnokset ja rakentajat. Ta¨ma¨ tapahtuu niin, etta¨ kielen mukaan jokainen yksiparametrinen rakentaja on myo¨s tyyppimuunnos raken- tajan parametrin tyypista¨ luokan olioksi. Esimerkiksi listauksessa 7.11 seuraavalla sivulla on osa luokan Murtoluku ma¨a¨rittelya¨. Luokalla on rakentaja, jonka avulla uuden murtoluvun voi luoda kokonaisluvun avulla. Ta¨llo¨in luodaan uusi murtoluku, joka on arvoltaan sama kuin parametrina oleva kokonais- 7.4. Tyyppimuunnokset 234 1 class Murtoluku 2 { 3 public: 4 Murtoluku(int kokonaisarvo); // Myo¨s tyyppimuunnos ... 6 private: 7 int osoittaja ; 8 int nimittaja ; 9 }; ... 10 Murtoluku::Murtoluku(int kokluku) : osoittaja (kokluku), nimittaja (1) 11 { 12 } LISTAUS 7.11: Esimerkki rakentajasta tyyppimuunnoksena luku. C++ osaa nyt automaattisesti ka¨ytta¨a¨ ta¨ta¨ rakentajaa seka¨ ekspli- siittisena¨ etta¨ implisiittisena¨ tyyppimuunnoksena. Kun tyyppimuun- nos suoritetaan, ohjelma luo uuden nimetto¨ma¨n murtoluvun rakenta- jan avulla ja ka¨ytta¨a¨ sita¨ tyyppimuunnoksen “lopputuloksena”. Ka¨a¨n- ta¨ja¨ pita¨a¨ myo¨s huolen siita¨, etta¨ ta¨ma¨ va¨liaikaisolio tuhotaan auto- maattisesti, kun sita¨ ei ena¨a¨ tarvita. Ta¨ssa¨ suhteessa tyyppimuunnos- ten va¨liaikaisoliot ka¨ytta¨ytyva¨t ta¨sma¨lleen samoin kuin paluuarvon va¨lityksessa¨ ka¨ytetyt va¨liaikaisoliot (aliluku 7.3). Rakentajamuunnosta voi ka¨ytta¨a¨ kaikkialla missa¨ normaaliakin tyyppimuunnosta. Esimerkiksi muunnos static cast<Murtoluku>(3) tuottaa lukua kolme vastaavan murto- lukuolion luokan rakentajan avulla. Myo¨s vanhat muunnossyntaksit Murtoluku(3) ja (Murtoluku)3 toimivat. Na¨ista¨ ensimma¨inen muis- tuttaa jo syntaksinsakin puolesta rakentajan kutsumista. Myo¨s implisiittiset tyyppimuunnokset toimivat normaalisti. Esimerkiksi koodissa void kasittele(Murtoluku const& m); ... kasittele(5); ka¨a¨nta¨ja¨ huomaa, etta¨ funktion kutsumiseksi kokonaisluku 5 pita¨a¨ muuttaa murtoluvuksi. Se tuottaa ta¨llaisen murtoluvun rakentajan 7.4. Tyyppimuunnokset 235 avulla ja va¨litta¨a¨ funktiolle viitteen luotuun va¨liaikaisolioon. Funk- tiokutsun ja¨lkeen va¨liaikaisolio tuhotaan. Rakentajatyyppimuunnoksen esta¨minen Rakentajan ka¨ytto¨ tyyppimuunnoksena saattaa aluksi tuntua ka¨teva¨l- ta¨, mutta kun ta¨ma¨ ominaisuus lisa¨ttiin C++:aan, huomattiin pian, etta¨ ominaisuudella oli epa¨toivottuja sivuvaikutuksia. Ongelmana on, et- ta¨ ka¨a¨nta¨ja¨n kannalta kaikki yksiparametriset rakentajat ovat mahdol- lisia tyyppimuunnoksia. On kuitenkin paljon tilanteita, joissa raken- tajan suorittamaa uuden olion luomista ei mitenka¨a¨n ja¨rkeva¨sti voi tulkita tyyppimuunnokseksi. Jos ohjelmassa esimerkiksi ma¨a¨ritella¨a¨n oma taulukkotyyppi, sen rakentaja voi hyvinkin saada parametrinaan tiedon siita¨, kuinka mo- nen alkion taulukko halutaan luoda. Kuitenkaan ei ole mitenka¨a¨n ja¨rkeva¨a¨ ajatella, etta¨ kyseessa¨ olisi tyyppimuunnos kokonaisluvus- ta taulukoksi. Ta¨ssa¨ tapauksessa rakentajan saama parametri on vain olion luomiseen tarvittava lisa¨tieto eika¨ rakentaja ma¨a¨ra¨a¨ minka¨a¨n- laista tyyppimuunnosta. Ka¨a¨nta¨ja¨ ei kuitenkaan ta¨ta¨ tieda¨ ja saattaa ka¨ytta¨a¨ taulukkoluokan rakentajaa myo¨s implisiittisena¨ tyyppimuun- noksena tilanteissa, joissa (esimerkiksi kirjoitusvirheen vuoksi) ko- konaisluvusta ta¨ytyisi tehda¨ taulukko. Ta¨ma¨ johtaa virheisiin, joiden lo¨yta¨minen saattaa olla hyvin hankalaa. Myo¨hemmin C++:ssa on yritetty paikata syntynytta¨ virhela¨hdetta¨ lisa¨a¨ma¨lla¨ kieleen avainsana explicit. Jos yksiparametrisen rakenta- jan edessa¨ on luokan esittelyssa¨ sana explicit, kyseista¨ rakentajaa ei ka¨yteta¨ implisiittisena¨ tyyppimuunnoksena. Ka¨ytta¨ja¨ voi kuitenkin halutessaan pyyta¨a¨ tyyppimuunnosta normaalilla tyyppimuunnos- syntaksilla, mutta ta¨ma¨ tuskin tapahtuu vahingossa. Edella¨ mainitun taulukkotyypin rakentaja olisi esimerkiksi syyta¨ varustaa explicit- sanalla seuraavaan tapaan: class Taulukko { public: explicit Taulukko(int koko); // EI implisiittinen tyyppimuunnos ... }; 7.4. Tyyppimuunnokset 236 Vaikka explicit-sana ratkaiseekin syntyneen ongelman, se on edelleen varsin hankala ka¨ytta¨a¨, koska ohjelmoijan ta¨ytyy erikseen muistaa merkita¨ ne rakentajat, joita ha¨n ei halua ka¨ytta¨a¨ tyyppimuun- noksena. Kielen kannalta avainsana “implicit” olisi saattanut olla pa- rempi, mutta se olisi aiheuttanut epa¨yhteensopivuutta kielen vanho- jen ja uusien murteiden va¨lilla¨. Muunnosja¨senfunktiot Rakentajien ka¨ytto¨ tyyppimuunnoksena tekee mahdolliseksi sen, et- ta¨ muita tyyppeja¨ voi muuntaa ohjelmoijan kirjoittamaksi tyypiksi. Tyyppimuunnoksen tekeminen toiseen suuntaan ei onnistu samalla menetelma¨lla¨, koska ohjelmoija ei tietenka¨a¨n voi kirjoittaa uusia ra- kentajia esimerkiksi int-tyypille tai kaupallisista kirjastoista lo¨ytyvil- le tyypeille. Tyyppimuunnos omasta tyypista¨ johonkin toiseen tyyppiin tapah- tuu erityisten muunnosja¨senfunktioiden (conversion member func- tion) avulla. Na¨ma¨ ovat parametrittomia vakioja¨senfunktioita, joiden “nimi” muodostuu avainsanasta operator ja lisa¨ksi muunnoksen koh- detyypista¨. Esimerkiksi kokonaisluvuksi muuntavan muunnosja¨sen- funktion nimi on operator int. Muunnosja¨senfunktio palauttaa pa- luuarvonaan muunnoksen lopputuloksen. Ja¨senfunktion esittelyssa¨ ei paluuarvoa merkita¨ na¨kyviin, koska se ka¨y ilmi jo nimesta¨. Listauksessa 7.12 seuraavalla sivulla on esimerkki murtolukuluo- kan muunnosja¨senfunktiosta, jonka avulla murtoluvun voi muun- taa liukuluvuksi. Kyseisen ma¨a¨rittelyn ja¨lkeen ka¨a¨nta¨ja¨ osaa ka¨ytta¨a¨ muutosta seka¨ implisiittisena¨ etta¨ eksplisiittisena¨ muunnoksena. Jos siis m on murtolukuolio, niin muunnokset static cast<double>(m) ja double d = m; toimivat normaalisti. Implisiittiset tyyppimuunnokset ovat tietysti mahdollinen vir- hela¨hde muunnosja¨senfunktioidenkin tapauksessa. Jostain syysta¨ explicit-avainsanaa ei voi ka¨ytta¨a¨ muunnosja¨senfunktioiden yhtey- dessa¨ luomaan muunnoksia, joita voisi ka¨ytta¨a¨ vain eksplisiittisesti. Jos ta¨llaista halutaan, luokkaan ta¨ytyy kirjoittaa tavallinen ja¨senfunk- tio, jota kutsutaan tyyppimuunnoksen sijaan. 7.5. Rakentajat ja struct 237 1 class Murtoluku 2 { 3 public: ... 5 operator double() const; // Muunnosja¨senfunktio ... 9 }; ... 13 Murtoluku::operator double() const 14 { 15 return static cast<double>(osoittaja )/static cast<double>(nimittaja ); 16 } LISTAUS 7.12: Esimerkki muunnosja¨senfunktiosta 7.5 Rakentajat ja struct C++:ssa luokkia voi ka¨ytta¨a¨ aivan samoissa paikoissa kuin kielen sisa¨a¨nrakennettuja tyyppeja¨kin. Niinpa¨ on ta¨ysin normaalia tehda¨ struct-tietorakenteita, joiden tietokenttina¨ on olioita, kuten pa¨iva¨yk- sia¨, string-merkkijonoja ja niin edelleen. Listauksessa 7.13 on esi- merkkina¨ ta¨llainen tietorakenne. Kun struct-tietorakenne luodaan, sisa¨ltyy ta¨ha¨n kaikkien structin kenttien luominen. Luokkatyyppia¨ olevilla kentilla¨ ta¨ma¨ tarkoittaa kenttien rakentajien kutsumista. Jos tietorakenne luodaan “normaaliin tapaan” syntaksilla Henkilotiedot henk;, kutsutaan ken- tille automaattisesti oletusrakentajia, koska mita¨a¨n alustamiseen tar- vittavia parametreja ei ole saatavilla. Jo C-kielessa¨ oli myo¨s mahdollista alustaa struct syntaksilla, jos- 1 struct Henkilotiedot 2 { 3 string nimi; 4 Paivays syntymapvm; 5 int ika; 6 }; LISTAUS 7.13: struct-tietorakenne, jossa on olioita 7.5. Rakentajat ja struct 238 sa aaltosuluissa luetellaan luomisen yhteydessa¨ kenttien alkuarvot. Ta¨ma¨ on mahdollista myo¨s C++:ssa. Mika¨li kenttina¨ on luokkia, na¨i- den alustamisessa on kaksi vaihtoehtoa. Mika¨li luokalla on yksipara- metrinen rakentaja, joka kelpaa implisiittiseksi tyyppimuunnokseksi, voi aaltosulkulistaan laittaa ta¨lle rakentajalle kelpaavan parametrin. Ta¨llo¨in kenta¨n alustus tapahtuu tyyppimuunnosrakentajaa ka¨ytta¨en. Toinen tapa on kutsua aaltosulkulistassa suoraan jotain luokan raken- tajaa ja antaa sille tarvittavat parametrit. Alla on esimerkki molemmista tapauksista: Henkilotiedot henk = {\"K. Dari\", Paivays(1, 1, 1970), 33}; Kentta¨ nimi alustetaan char*-merkkijonoliteraalista string-luokan yksiparametrista rakentajaa ka¨ytta¨en. Sen sijaan kentta¨ syntymapvm alustetaan alkuarvoonsa erikseen Paivays-luokan rakentajaa kutsu- malla. Olioiden tapauksessa aaltosulkualustus ei ika¨va¨ kylla¨ ole va¨lt- ta¨ma¨tta¨ kovin tehokas ratkaisu. C++-standardi nimitta¨in ma¨a¨rittelee alustuksen niin, etta¨ olioiden tapauksessa ensin luodaan aaltosul- kulistassa olevien tietojen perusteella irralliset va¨liaikaisoliot ja na¨- ma¨ sitten kopioidaan struct-tietorakenteen sisa¨a¨n. Ta¨ma¨ tapahtuu luomalla kenta¨t kopiorakentajilla, joille annetaan luodut va¨liaikaiso- liot parametreina. Esimerkissa¨ luodaan ensin va¨liaikaiset merkkijono string(\"K. Dari\") ja pa¨iva¨ys, ja kenta¨t nimi ja syntymapvm aluste- taan sitten na¨ista¨ kopiorakentajien avulla. Tehokas ka¨a¨nta¨ja¨ saa opti- moida va¨liaikaisoliot pois, jos se siihen kykenee, mutta kopiorakenta- jan on silti oltava olemassa. Aaltosulkualustuksen rajoituksena on myo¨s, etta¨ sita¨ voi ka¨ytta¨a¨ vain, jos ollaan luomassa nimettya¨ paikallista tai globaalia muuttujaa. Sen sijaan syntaksia ei voi ka¨ytta¨a¨ va¨liaikaisolion tai new’lla¨ luodun structin luomiseen. Ta¨llaisia tilanteita varten on joskus ka¨teva¨a¨ kir- joittaa struct-tietorakenteellekin rakentaja, joka alustaa tietoraken- teen kenta¨t haluttuihin alkuarvoihin. Listaus 7.14 seuraavalla sivulla na¨ytta¨a¨ esimerkin ta¨llaisesta ra- kentajasta. Rakentaja koostuu pelka¨sta¨ alustuslistasta, joka alustaa tietorakenteen kenta¨t annettujen parametrien avulla. Poikkeukselli- sesti koko rakentaja on kirjoitettu struct-ma¨a¨rittelyn sisa¨lle, koska rakentaja ei sisa¨lla¨ mita¨a¨n toiminnalisuutta ja ta¨lla¨ tavoin sen koo- dia ei tarvitse kirjoittaa erilleen .cc-tiedostoon. Kyseessa¨ on saman- 7.5. Rakentajat ja struct 239 1 struct Henkilotiedot 2 { 3 string nimi; 4 Paivays syntymapvm; 5 int ika; 6 // Rakentaja, sisa¨lta¨a¨ VAIN kenttien alustuksen 7 Henkilotiedot(string const& n, int p, int k, int v, int i) 8 : nimi(n), syntymapvm(p, k, v), ika(i) {} 9 }; LISTAUS 7.14: struct, jolla on rakentaja lainen tilanne kuin rajapintaluokkien virtuaalipurkajien yhteydessa¨ aliluvussa 6.9.2. Rakentajan sisa¨lta¨va¨n tietorakenteen luominen tapahtuu nyt ai- van kuten olionkin. Se onnistuu milla¨ tahansa seuraavista syntak- seista: Henkilotiedot henk(\"K. Dari\", 1, 1, 1970, 33); Henkilotiedot* hp = new Henkilotiedot(\"Dari\", 1, 1, 1970, 33); Henkilotiedot(\"K. Dari\", 1, 1, 1970, 33); // Va¨liaikaisolio Vaikka periaatteessa tietorakenteen rakentajan runkoon tai alus- tuslistaan voisi C++:ssa kirjoittaa koodia, ei ta¨ta¨ tyylillisesti missa¨a¨n nimessa¨ pita¨isi tehda¨. Ohjelmissa on totuttu pita¨ma¨a¨n struct-tietora- kenteita “tyhmina¨” tietovarastoina, joihin ei sisa¨lly ylima¨a¨ra¨ista¨ toi- minnalisuutta. Niinpa¨ structin rakentaja onkin syyta¨ pita¨a¨ vain syn- taktisena temppuna, jolla helpotetaan kenttien alustamista. Periaat- teessa C++ sallisi myo¨s purkajien ja ja¨senfunktioiden kirjoittamisen struct-tietorakenteille, mutta na¨ita¨ka¨a¨n ei kannata ka¨ytta¨a¨, koska ne tekisiva¨t ohjelmasta vain vaikealukuisemman tavalliselle ohjelmoi- jalle. Jos tietorakenteeseen on todella tarve upottaa toiminnallisuutta, kannattaa siita¨ yleensa¨ kirjoittaa luokka struct-tietorakenteen sijaan. 240 Luku 8 Lisa¨a¨ rajapinnoista “There are things known and things unknown and in be- tween are The Doors.” – Jim Morrison [The Doors FAQ, 2002] Rajapinnan tehta¨va¨ on kertoa siihen liittyva¨sta¨ komponentista (mo- duuli tai olio) ka¨ytta¨ja¨lle vastaus kysymykseen “Miten ta¨ma¨n on tar- koitus toimia?”. Kun ka¨ytta¨ja¨ tieta¨a¨ vastauksen ta¨ha¨n kysymykseen, ha¨n pystyy hyo¨dynta¨ma¨a¨n rajapinnan ma¨a¨rittelema¨a¨ palvelua va¨lit- ta¨ma¨tta¨ sen taakse kapseloidusta toteutuksesta. Samalla komponen- tilla voi olla useita erilaisia rajapintoja. Oliolla voi olla rajapinta (tai sen osa) erikseen vakio-olioille, “tavallisille” instansseille ja aliluokil- le. Rajapinnan toteuttajan pita¨isi pita¨a¨ mielessa¨a¨n lause “Tekeeko¨ toteutus ta¨sma¨lleen sen, mita¨ rajapinnan ma¨a¨rittely sanoo?”. Taito ja kokemus ovat ta¨ssa¨ tyo¨ssa¨ korvaamattomia. Ohjelmointikieli seka¨ suunnittelu- ja tyylisa¨a¨nno¨t voivat ainakin ohjata kohti oikein toimi- vaa lopputulosta. 8.1 Sopimus rajapinnasta Sopimussuunnittelu (Design By Contract, [Meyer, 1997]) on rajapin- tasuunnittelun ja -toteutuksen menetelma¨, jossa palveluiden ominai- suuksia kuvataan matematiikan keinoin. Lakimiesten viilaaman kir- 8.1. Sopimus rajapinnasta 241 jallisen kahden osapuolen sopimuksen tapaan sopimussuunnittelus- sa ajatellaan syntyva¨n rajapinnan ka¨ytta¨ja¨n ja sen toteutuksen va¨lille sopimus, jossa kummallakin osapuolella on tarkkaan ma¨a¨ritellyt vas- tuunsa: • Rajapinnan toteutus lupaa jokaisen rajapinnan palvelun osalta toimia tietylla¨ tavalla, kun rajapintaa ka¨yteta¨a¨n sovitulla taval- la. Ma¨a¨rittelyssa¨ on siis mukana tieto siita¨, mitka¨ ovat rajapin- nan sallittuja ka¨ytto¨tapoja (funktioiden parametrit, kutsuja¨rjes- tykset, tietotyyppien arvorajat yms.). • Rajapintaa ka¨ytta¨va¨ ohjelmoija lupaa ka¨ytta¨a¨ palveluita ai- noastaan niiden ma¨a¨rittelyn mukaisesti. Sopimussuunnittelu on nimensa¨ mukaisesti ennen kaikkea raja- pintojen suunnittelua auttava ajattelutapa, joka pakottaa ja ohjaa se- ka¨ miettima¨a¨n etta¨ dokumentoimaan rajapinnan huolellisesti. Tois- sijaisena hyo¨tyna¨ menetelma¨ yksinkertaistaa komponenttien toteut- tamista. Ta¨ma¨ saavutetaan siten, etta¨ rajapinnan toteutuksen ulko- puolelle on sopimuksessa rajattu hankalat ja ei-toivotut ka¨ytto¨tavat, jolloin niihin ei tarvitse toteutuksessa varautua. Voidaan ajatella et- ta¨ “villin la¨nnen” rajapintaa saa ka¨ytta¨a¨ miten huvittaa ja toteutuk- sen pita¨isi osata toimia kaikissa tilanteissa jotenkin “oikein” (mini- missa¨a¨n laadukas koodi ei ainakaan kaadu kummallisissa tilanteis- sa). Sopimussuunnittelussa jaetaan vastuuta toiminnasta kutsujan ja toteutuksen kesken, jolloin kokonaisohjelmakoodima¨a¨ra¨ on yleensa¨ pienempi kuin kaikkeen varautuneissa toteutuksissa. 8.1.1 Palveluiden esi- ja ja¨lkiehdot Rajapinnan yksitta¨isen palvelun sopimus tehda¨a¨n ma¨a¨rittelema¨lla¨ jo- kaiselle palvelulle esiehto (precondition, P) ja ja¨lkiehto (postcondi- tion, Q). Molemmat ovat (predikaattilogiikan) loogisia lausekkeita, joista esiehdon tulee olla totta ennen palvelun ka¨ynnista¨mista¨ ja ja¨l- kiehdon palvelun suorittamisen ja¨lkeen: {P} palvelu() {Q} Ta¨ma¨n “oikeellisuuskaavan” (correctness formula) mukaisesti pal- velu lupaa, etta¨ kaikki funktion palvelu suoritukset, joiden alkaessa ehto P on totta, pa¨a¨ttyva¨t tilaan, jossa ehto Q on totta. Jos ehto P ei 8.1. Sopimus rajapinnasta 242 toteudu, niin palvelu toimii ma¨a¨rittelema¨tto¨ma¨sti ja ja¨lkiehdon ei tar- vitse toteutua (haluttua palvelua ei saada). Yksinkertaisimmillaan ehdoilla voidaan rajata parametrien arvoa- lueita: { p >= kirja.lainauspa¨iva¨ } kirja.palauta(palautuspa¨iva¨ p) { kirja.tila = PALAUTETTU } Ka¨ytetta¨essa¨ predikaattilogiikan ominaisuuksia voidaan kertoa esimerkiksi rutiinista, joka ja¨rjesta¨a¨ taulukon, joka ei sisa¨lla¨ yhta¨ka¨a¨n tyhja¨a¨ alkiota: {∀i | 1 ≤i ≤n : ARRAY[i] ̸= TYHJA¨} qsort(ARRAY) {∀i | 1 ≤i < n : ARRAY[i] ≤ARRAY[i + 1]} Sopimussuunnittelun ta¨rkein ominaisuus on palvelun kutsujan ja toteuttajan vastuiden ma¨a¨rittely — samalla suunnittelun suurin vai- keus on pa¨a¨tta¨a¨, mitka¨ ominaisuudet kuuluvat vastuurajan millekin puolelle. • Kutsuja. Palvelun kutsujan vastuulla on pita¨a¨ huoli siita¨, etta¨ esiehto toteutuu. Jos kirja yriteta¨a¨n palauttaa pa¨iva¨ma¨a¨ra¨lla¨ jo- ka on ennen kirjattua lainauspa¨iva¨a¨, niin palvelun mukainen sopimus ei ole voimassa, ja suoritus voi tehda¨ mita¨ tahansa. Oh- jelman testausvaiheessa voidaan esiehtoja tarkastaa, mutta pa¨a¨- ma¨a¨ra¨na¨ on, etta¨ na¨ita¨ tarkastuksia ei ole ena¨a¨ lopullisessa oh- jelmistossa mukana (huolellinen testaus on lo¨yta¨nyt esiehdon rikkovat palvelun kutsut). • Toteuttaja. Palvelun toteuttajan vastuulla on pita¨a¨ huoli siita¨, etta¨ palvelun suorituksen ja¨lkeen ja¨lkiehto on aina voimassa (kunhan esiehto on ollut voimassa). Jos sopimuksessa on ma¨a¨- ritelty rutiinin parametripa¨iva¨yksen olevan jotain pa¨iva¨ma¨a¨ra¨a¨ myo¨hemma¨n, niin palvelun toteutuksessa ei ena¨a¨ tehda¨ tar- kastusta, joka on ma¨a¨ritelty kutsujan vastuulle. Jos palvelu ei pysty toteuttamaan sopimuksen mukaista palvelua (ja¨lkiehto ei ta¨yty) kyseessa¨ on virhetilanne, joka on jollain tavalla ilmaista- va. Ta¨ma¨ tehda¨a¨n yleensa¨ poikkeusten (luku 11) avulla. 8.1. Sopimus rajapinnasta 243 Yleiska¨ytto¨isten komponenttien rajapinnoissa on usein vaikea pa¨a¨tta¨a¨, minka¨lainen sopimus halutaan muodostaa kutsujan ja toteu- tuksen va¨lille. Joskus voi olla tarkoituksenmukaista tarjota samasta perustoiminnallisuudesta sopimukseltaan erilaisia versioita. Yksi esimerkki ta¨llaisesta toiminnasta on vector-taulukon indek- sointi, josta lo¨ytyy kaksi versiota. Operaatio at() hyva¨ksyy parametri- naan minka¨ tahansa kokonaisluvun ja tarkistaa toteutuksessaan osuu- ko ta¨ma¨ luku indeksina¨ tulkittuna taulukon sisa¨lle. Jos indeksi on kelvollinen, niin kyseessa¨ oleva alkio palautetaan, muutoin kutsujal- le kerrotaan virheesta¨ (poikkeusmekanismilla). Toinen versio indek- soinnista on hakasulkuoperaattori (v[i]), joka ei suorita indeksin lail- lisuustarkistusta. Sopimussuunnittelun kannalta ta¨ma¨ versio on ma¨a¨- ritellyt kutsujan vastuulle (esiehdoksi), etta¨ operaatiota kutsutaan ai- noastaan laillisilla taulukon indekseilla¨. Jos kutsuja ei ta¨yta¨ esiehtoa, niin toteutus tekee ma¨a¨rittelema¨tto¨ma¨n operaation (joka ohjelmassa na¨kyy esimerkiksi sen kaatumisena tai muistin roskaantumisena). 8.1.2 Luokkainvariantti Luokkien tapauksessa voidaan palveluiden esi- ja ja¨lkiehtojen lisa¨k- si ma¨a¨ritella¨ pysyva¨isva¨itta¨ma¨ eli invariantti (invariant), joka kertoo luokan olion ylla¨pita¨ma¨n ehdon palvelukutsujen va¨lilla¨. Esimerkiksi: class KirjastonKirja { // Invariantti: sijaintitieto on aina // LAINASSA, PAIKALLA, HUOLTO tai POISTETTU . . . Esimerkin KirjastonKirja-luokan olioiden tilan tiedeta¨a¨n olevan ai- na invariantin mukainen silloin, kun ohjelman suoritus ei ole keskel- la¨ jotain luokan tarjoamaa palvelua. Luokkainvariantin on oltava voimassa heti olion synnyttya¨. Jos oliota luotaessa kutsutaan alustusrutiinia (kuten C++:n rakentajaja¨sen- funktio), invariantin on oltava voimassa, kun ta¨ma¨ rutiini on lopet- tanut suorituksensa. Ta¨ma¨n ja¨lkeen invariantin on oltava voimassa aina ennen ja ja¨lkeen jokaista julkisen rajapinnan rajapintafunktion kutsua: {LUOKKAINVARIANTTI∧P} olio.palvelu() {LUOKKAINVARIANTTI∧Q} 8.1. Sopimus rajapinnasta 244 Kun suoritus on ja¨senfunktion koodissa olion “sisa¨lla¨”, puhu- taan epa¨stabiilista tilasta, jossa luokkainvariantti saa va¨liaikisesti olla epa¨tosi. Ta¨ma¨n sa¨a¨nno¨n noudattaminen ja tarkastaminen tulee han- kalaksi silloin kun ja¨senfunktio kutsuu toisia ja¨senfunktioita osana omaa toiminnallisuuttaan. 8.1.3 Sopimussuunnittelun ka¨ytto¨ Yksinkertaisissa esimerkeissa¨ kauniilta na¨ytta¨va¨ sopimussuunnitte- lu ei valitettavasti skaalaudu varsinkaan ohjelmiston ylimma¨n tason moduulien rajapintoihin. Esi- ja ja¨lkiehtojen tarkkaan ma¨a¨rittelyyn kuuluu usein niin paljon informaatiota, etta¨ sen kuvaaminen mate- maattisen tarkasti on ainakin ta¨lla¨ hetkella¨ liian tyo¨la¨sta¨. Formaalei- hin ma¨a¨rittely- ja suunnittelumenetelmiin liittyva¨a¨ tutkimusta teh- da¨a¨n paljon, ja sita¨ sovelletaan useissa erityisen suurta varmuutta vaativissa ohjelmistoprojekteissa. Tavallisimmissa ohjelmistoprojek- teissa matemaattista ma¨a¨rittelya¨ ei kuitenkaan yleensa¨ juuri ka¨yteta¨. Sopimussuunnittelun periaatteiden tunteminen ja jopa osittainenkin noudattaminen on tietysti pelkka¨a¨ sanallista rajapintakuvausta ek- saktimpi menetelma¨. Olio-ohjelmointi aiheuttaa myo¨s omat hankaluutensa sopimus- suunnitteluun. Periytymisen yhteydessa¨ rajapinnan sopimuksen pi- ta¨isi usein osata sanoa jotain myo¨s aliluokan rajapinnasta, joka usein on hyvin hankalaa. Joskus kantaluokka ei tieda¨ edes karkeasti minka¨- laisia versioita siita¨ on tarkoitus periytta¨a¨, jolloin liian tiukasti ma¨a¨- ritellyt rajapintasopimukset voivat pahimmillaan tehda¨ periytettyjen versioiden tekemisen mahdottomiksi siten, etta¨ kantaluokan rajapin- tasopimus pideta¨a¨n voimassa. 8.1.4 C++: luokan sopimusten tarkastus Ohjelmiston kehitysvaiheessa on hyo¨dyllista¨ tarkastaa, etta¨ rajapin- noille ma¨a¨riteltyja¨ sopimuksia noudatetaan. Va¨a¨rinymma¨rryksista¨ tai huonosta dokumentoinnista johtuvat sopimuksen vastaiset kut- sut saadaan heti na¨kyville ohjelmiston testausvaiheessa. Ylima¨a¨ra¨i- set testit hidastavat ohjelman toimintaa, mutta testausvaiheessa silla¨ ei useinkaan ole merkitysta¨. Poikkeuksena ovat reaaliaikavaatimuk- sia omaavat ohjelmistot, joissa ei voida ka¨ytta¨a¨ testeissa¨ka¨a¨n ylima¨a¨- 8.1. Sopimus rajapinnasta 245 ra¨isia¨ tarkastuksia, jos niiden aiheuttama ylima¨a¨ra¨inen prosessointi vaikuttaa ohjelmiston ajoituksiin. assert-makro Ohjelmissa olevilla sopimustarkastuksilla on pitka¨t perinteet jo ajal- ta ennen olio-ohjelmoinnin yleistymista¨. C-kielessa¨ on ma¨a¨riteltyna¨ varmistusrutiini assert [Kerninghan ja Ritchie, 1988], joka ilmoittaa virheesta¨ ja pysa¨ytta¨a¨ ohjelman suorituksen, jos sen parametrina ole- va lauseke on arvoltaan epa¨tosi. Kun ohjelmiston julkaisuversio ka¨a¨n- neta¨a¨n siten, etta¨ esika¨a¨nta¨ja¨symboli NDEBUG on ma¨a¨riteltyna¨, assert- makron arvo asettuu tyhja¨ksi, jolloin na¨ma¨ kehitysvaiheen tarkas- tukset eiva¨t ole mukana lopullisessa ohjelmistossa. Ta¨ma¨ ka¨yta¨nto¨ (assert-testit ovat mukana vain ohjelman testausvaiheessa) noudat- taa sopimussuunnittelun periaatetta, jonka mukaan oikein toimivas- sa ohjelmassa seka¨ rajapinnan kutsu etta¨ toteutus noudattavat aina sopimusta. Koska assert on ma¨a¨ritelty makroksi, se ei ole C++:n std-nimia- varuuden sisa¨lla¨. Rajapintafunktion alussa voidaan testauksessa var- mistaa kutsujan noudattavan sopimusta funktion parametreista: #include <cassert> ... int palvelu(int a, int b) { assert( a < 2 && b > 40 ); ... Funktion va¨a¨ra¨ ka¨ytto¨ voi na¨kya¨ testaajalle esimerkiksi seuraavas- sa muodossa: assertkoe.cc:4: failed assertion ‘a < 2 && b > 40’ (program aborted) Ta¨ssa¨ assertkoe.cc on la¨hdekooditiedoston nimi. C++-standardi ja¨t- ta¨a¨ tarkoituksella ma¨a¨rittelema¨tta¨ tulostettavan virheen muodon. Jos- sain ympa¨risto¨ssa¨ ta¨llainen virheilmoitus voidaan ilmaista esimer- kiksi ka¨ytto¨liittyma¨n virheikkunalla. Koska assert-tarkastukset eiva¨t ole mukana lopullisessa ohjelma- koodissa, on oltava tarkkana, etta¨ ka¨ytetty varmistusehto ei sisa¨lla¨ 8.1. Sopimus rajapinnasta 246 ohjelman toiminnallisuutta: 1 Paivays* pNyt = 0; 2 assert(pNyt = Paivays::NytOsoitin()); // Sijoitus assertissa! 3 pNyt->Tulosta(); Vaikka assert-makron parametrina oleva sijoitus (rivi 2) saakin C++:ssa arvon, joka tulkitaan myo¨s totuusarvoksi, kyseessa¨ on silti va¨a¨- ra¨ tapa ka¨ytta¨a¨ varmistusrutiinia.\u0017 Edellinen koodinpa¨tka¨ voi toimia ta¨ysin oikein ohjelmiston testausvaiheessa, mutta kun lopullisessa versiossa assert-makro ma¨a¨ritella¨a¨n tyhja¨ksi, rivin 2 sijoitus ja¨a¨ ko- konaan suorittamatta ja koodin toiminta muuttuu. C-kielen assert on ma¨a¨ritelty siten, etta¨ ohjelman suoritus kes- keyteta¨a¨n kutsumalla kielen funktiota abort. Ta¨ta¨ funktiota ei pideta¨ C++:ssa¨ suositeltavana, koska sita¨ kutsuttaessa ei suoriteta mita¨a¨n lope- tustoimenpiteita¨. Erityisesti ohjelmassa kutsuhetkella¨ olevien olioi- den purkajat ja¨a¨va¨t suorittamatta (niissa¨ voi olla resurssien vapau- tukseen liittyvia¨ toimintoja). C++:ssa¨ suositeltavampi tapa on ka¨yt- ta¨a¨ poikkeusmekanismia (luku 11) myo¨s “assertiovirheen” ilmaisemi- seen. Listauksessa 8.1 seuraavalla sivulla on esimerkki poikkeuksilla toteutetusta Assert-makrosta C++:lla. Luokkainvariantti Koska luokkainvariantin tarkastuksessa on tarkoituksena tarkastaa luokan vastuualueen sopimus jokaisen rajapintafunktion suorituk- sen ja¨lkeen, kannattaa ta¨ma¨ tarkastus kirjoittaa omaksi ja¨senfunktiok- seen, jota kutsutaan aina muiden ja¨senfunktioiden alussa ja lopussa. (Myo¨s alussa, jotta voidaan varmistua siita¨, etta¨ invariantin ma¨a¨ra¨a¨- ma¨ sopimus on voimassa myo¨s rutiinin suorituksen alkaessa, katso ohjelmalistaus 8.2 sivulla 248). Ta¨ssa¨ esitetty suoraviivainen toteutus ei huomioi mitenka¨a¨n sita¨ tilannetta, etta¨ invariantti saa olla epa¨tosi jos tarkistuksen kohteena olevaa ja¨senfunktiota on kutsuttu toisesta (saman olion) ja¨senfunk- tiosta. Ta¨llainen kutsuketjun seuraaminen mutkistaisi ohjelmakoodia jonkin verran, ja sen toteuttamiseen ei C++:ssa¨ valitettavasti ole valmii- ta apukeinoja. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . \u0017Hyvin tyypillinen virhe C++:ssa on ka¨ytta¨a¨ yhta¨suuruutta tarkoitettaessa kielen (huonosti valittua) sijoitusmuotoa [assert( x = 7 ) vs. assert( x == 7 )]. 8.1. Sopimus rajapinnasta 247 1 #ifndef ASSERT HH 2 #define ASSERT HH 3 4 #include \"varmistuspieleen.hh\" /* poikkeusluokan esittely */ 5 #include <iostream> 6 7 #ifdef NDEBUG 8 #define Assert(x) /* tyhja */ 9 #else 10 #define Assert(x) Assert toteutus( x, #x, FILE , LINE ) 11 #endif 12 13 inline void Assert toteutus( bool varmistus, char const* lauseke, 14 char const* tiedosto, 15 unsigned int rivinumero ) 16 { 17 if( varmistus == false ) { 18 std::cerr << tiedosto << \":\" << rivinumero << \":\"; 19 std::cerr << \"Varmistus epa¨onnistui: \" << lauseke << std::endl; 20 throw VarmistusPieleen(lauseke, tiedosto, rivinumero); 21 } 22 } 23 24 #endif /* ASSERT HH */ LISTAUS 8.1: Varmistusrutiinin toteutus Koska C++ ei automaattisesti tue invarianttien tarkastusta, niiden ka¨ytto¨ on ta¨ysin ohjelmoijan vastuulla. Ta¨ma¨ tulee ottaa huomioon myo¨s periytymishierarkioissa, joissa on pidetta¨va¨ huoli siita¨, etta¨ ali- luokan ja¨senfunktiot tarkastavat myo¨s kantaluokan invariantin sa¨ily- misen: // Kun ja¨rjestetty taulukko on periytetty luokasta Taulukko inline void Ja¨rjestettyTaulukko::Invariantti() { #ifndef NDEBUG Taulukko::Invariantti(); ... #endif } 8.2. Luokan rajapinta ja olion rajapinta 248 1 inline void JarjestettyTaulukko::Invariantti() 2 { 3 #ifndef NDEBUG 4 // Invariantti: alkiot ovat aina suuruusja¨rjestyksessa¨, siten etta¨ 5 // indeksissa¨ 1 on pienin alkio ja indeksissa¨ KOKO suurin. 6 for( int i = 1; i < KOKO; i++ ) 7 { 8 if( alkio[ i ] > alkio[ i+1 ] ) 9 throw JarjestettyTaulukko::InvarianttiRikottu(); 10 } 11 #endif 12 } 13 14 void JarjestettyTaulukko::EtsiJaMuutaAlkio( 15 Alkio const& etsittava, Alkio const& korvaava ) 16 { 17 Invariantti(); 18 // Ja¨senfunktion toteutus 19 Invariantti(); 20 } LISTAUS 8.2: Esimerkki luokkainvariantin toteutuksesta ja¨senfunktiona 8.2 Luokan rajapinta ja olion rajapinta Luokkasuunnittelussa tulee silloin ta¨llo¨in vastaan tilanne, jossa jo- kin luokan vastuualueeseen kuuluva asia ei oikeastaan kuulu min- ka¨a¨n luokan olion tehta¨viin vaan ika¨a¨n kuin “koko luokalle”, toisaal- ta kaikille, toisaalta ei milleka¨a¨n oliolle. Esimerkkeja¨ ta¨llaisista luo- kan “yhteisista¨” asioista ovat esim. pa¨iva¨ysluokalla taulukko, jossa pideta¨a¨n muistissa kuukausien pituudet, tai merkkijonoluokalla tieto siita¨, mika¨ on merkkijonojen maksimipituus. Samoin ohjelman tes- tausvaiheessa saattaisi olla mukavaa pita¨a¨ kirjaa siita¨, montako jon- kin luokan oliota ohjelman ajon aikana on luotu, seka¨ tarjota funktio, jolla tuon tiedon saa tulostettua. Ta¨llaiset luokalle yhteiset asiat pita¨isi tietysti jotenkin sitoa itse luokkaan ja kapseloida niin, etta¨ vain ka¨ytta¨ja¨lle tarkoitettu “luokan rajapinta” na¨kyy ulospa¨in. Ta¨ma¨n saavuttamiseen eri oliokielissa¨ on ka¨ytetty erilaisia mekanismeja. Ta¨ssa¨ teoksessa tutustutaan lyhyes- ti kahteen eri mekanismiin: Smalltalkin metaluokkiin ja luokkaolioi- hin seka¨ C++:n luokkafunktioihin ja -muuttujiin. Jotkin ohjelmointi- 8.2. Luokan rajapinta ja olion rajapinta 249 kielet sisa¨lta¨va¨t myo¨s kompromisseja na¨iden kahden va¨lilla¨, esimer- kiksi Javan luokkaolioiden ka¨site on yhdistelma¨ Smalltalkin ja C++:n mekanismeja. 8.2.1 Metaluokat ja luokkaoliot Puhtaasti oliopohjainen la¨hestymistapa luokan yhteisiin asioihin on, etta¨ luokan yhteisen datan ja operaatioiden ta¨ytyy olla jonkin olion attribuutteja ja operaatioita. Ta¨ma¨ johtaa luontevasti ajatukseen, etta¨ jokaista luokkaa vastaa yksika¨sitteinen olio, jonka vastuulla luokan yhteiset asiat ovat. Viela¨ va¨ha¨n pidemma¨lle vietyna¨ saadaan aikaan oliomalli, jossa jokainen luokka on olio, luokkaolio (class object), joka hoitaa kaikki koko luokan vastuulla olevat asiat. Smalltalkissa on otettu juuri ta¨ma¨ la¨hestymistapa luokkiin, ja se on itse asiassa oleellinen osa kielen toteutusta. Uusien olioiden luomi- nen on tietysti yksi asia, joka kuuluu luokan vastuualueeseen. Niin- pa¨ Smalltalkin operaatio new, jolla luodaan uusi olio, on itse asiassa luokkaa vastaavan luokkaolion ja¨senfunktio, joka luo uuden olion ja palauttaa sen paluuarvonaan. Smalltalkissa luokkaolion nimi on sama kuin itse luokan nimi ja luokkaoliota voi ka¨ytta¨a¨ kaikkialla missa¨ normaaleitakin olioita. Luokkaolion voi esimerkiksi antaa parametrina toiselle oliolle, joka ka¨ytta¨a¨ sita¨ luomaan luokasta uusia olioita. Ongelmaksi ja¨a¨, etta¨ jokaisen olion pita¨a¨ kuulua johonkin luok- kaan. Jos kerran luokkakin on vain olio, mihin luokkaan se sitten kuuluu? Smalltalkin ratkaisu on sanoa, etta¨ jokainen luokkaolio kuu- luu omaan metaluokkaansa (metaclass), jonka ainoa olio se on. Kaik- ki metaluokat puolestaan on periytetty luokasta Class, joka sisa¨lta¨a¨ kaikki kaikille luokille yhteiset ominaisuudet. Na¨in metaluokat muo- dostavat keskena¨a¨n luokkahierarkian, joka on rakenteeltaan ta¨sma¨l- leen samanlainen kuin tavallisten luokkien muodostama hierarkia. Kuvassa 8.1 seuraavalla sivulla on esimerkkina¨ osa metaluokkien ja tavallisten luokkien muodostamaa hierarkiaa. Smalltalkin metaluokkahierarkia antaa mahdollisuuden varsin mielenkiintoisiin olioratkaisuihin, ja sen avulla voidaan saada aikaan mm. samanlaisia rakenteita kuin C++:n mallien (aliluku 9.5) avulla. Ta¨ssa¨ teoksessa ei aiheeseen kuitenkaan keskityta¨ enempa¨a¨, mutta asiasta kiinnostuneelle sopivaa kirjallisuutta on mm. “An Introduc- 8.2. Luokan rajapinta ja olion rajapinta 250 Object Eläin Nisäkäs \"MetaEläin\" \"MetaNisäkäs\" Class 1 1 Olion tiedot ja jäsenfunktiot kuuluu kuuluu kuuluu Jyke Metaluokkahierarkia Luokkahierarkia Olio Luokkaolio Luokan yhteiset tiedot ja palvelut KUVA 8.1: Luokka- ja metaluokkahierarkiaa Smalltalkissa tion to Object-oriented Programming, 3rd edition” [Budd, 2002, luku 25]. 8.2.2 C++: Luokkamuuttujat C++:n kehitta¨ja¨t pa¨a¨ttiva¨t, etta¨ metaluokista ja luokkaolioista saatava hyo¨ty oli turhan pieni niiden toteutusvaivaan na¨hden. Ta¨ma¨n vuoksi C++:ssa on ka¨yto¨ssa¨ erilainen mekanismi luokan yhteisia¨ asioita var- ten. Jos jokin muuttuja on luonteeltaan sellainen, etta¨ se kuuluu koko luokalle eika¨ vain yhdelle oliolle (siis muuttujan halutaan olevan luo- kan kaikille olioille yhteinen), muuttuja esitella¨a¨n luokan esittelyssa¨ luokkamuuttujana (static data member). Luokkamuuttujan esittely on muuten samanlainen kuin ja¨senmuuttujankin, mutta esittely al- 8.2. Luokan rajapinta ja olion rajapinta 251 kaa avainsanalla static: class X { ... int jasenmuuttuja ; static int luokkamuuttuja ; }; Luokkamuuttuja on luonteeltaan hyvin la¨hella¨ normaalia globaa- lia muuttujaa: sen elinkaari on ohjelman alusta ohjelman loppuun saakka. Luokkamuuttuja on siis olemassa, vaikka luokasta ei olisi viela¨ luotu ainuttakaan oliota. Koska luokkamuuttuja ei ole minka¨a¨n olion oma, sen alustamista ei voi tehda¨ rakentajan alustuslistassa ja¨- senmuuttujien tapaan, vaan jossain kooditiedostossa ta¨ytyy olla erik- seen luokkamuuttujan ma¨a¨rittely, jonka yhteydessa¨ muuttuja aluste- taan: // Yleensa¨ samassa kooditiedostossa kuin ja¨senfunktioiden koodi int X::luokkamuuttuja = 2; Luokan omassa koodissa luokkamuuttujaan voi viitata aivan ku- ten ja¨senmuuttujaankin, suoraan sen nimella¨. Luokan ulkopuo- lelta niihin voi viitata syntaksilla Luokka::lmuuttuja (esimerkissa¨ X::luokkamuuttuja ). Toinen (harvemmin ka¨ytetty) tapa on viitata luokkamuuttujiin luokan olion kautta ika¨a¨n kuin luokkamuuttuja oli- si ja¨senmuuttuja: X xolio; int arvo = xolio.luokkamuuttuja ; Luokkamuuttujille pa¨teva¨t hyvin pitka¨lle samat tyylisa¨a¨nno¨t kuin ja¨senmuuttujillekin, mm. luokkamuuttujat olisi hyva¨ pita¨a¨ luokan private-puolella. Ta¨ha¨n on kuitenkin yksi yleinen poikkeus — luok- kavakiot. Varsin usein luokkaan liittyy epa¨lukuinen ma¨a¨ra¨ rajoituk- sia, maksimiarvoja yms. lukuja, joihin perinteisesti olisi ka¨ytetty #define-vakioita tai globaaleja vakioita. Huomattavasti parempi ka¨y- ta¨nto¨ on kuitenkin upottaa ta¨llaiset vakiot luokan sisa¨a¨n, jolloin on selva¨a¨, mita¨ osaa ohjelmasta ne koskevat. Samalla myo¨s nimikonﬂik- tien vaara pienenee. 8.2. Luokan rajapinta ja olion rajapinta 252 Luokkavakioita saa tehtya¨ yksinkertaisesti luomalla luokkamuut- tuja, joka on ma¨a¨ritelty vakioksi const-ma¨a¨reella¨. Ongelmaksi ja¨a¨- va¨t vain tilanteet, joissa ta¨llaisen vakion tulee olla ka¨a¨nno¨saikainen vakio. Mika¨li vakion alustaminen ja¨teta¨a¨n mielivaltaiseen kooditie- dostoon, ka¨a¨nta¨ja¨ ei lo¨yda¨ sita¨ eika¨ pysty ka¨ytta¨ma¨a¨n vakion arvoa ka¨a¨nno¨saikana. Ta¨ma¨ ratkaisemiseksi C++-standardiin otettiin mu- kaan mahdollisuus alustaa kokonaislukutyyppiset luokkavakiot suo- raan luokan esittelyssa¨. Tyypillisesti ta¨llaiset vakiot sijoitetaan luo- kan public-osaan, koska niiden ka¨ytto¨tarkoituksena on juuri na¨kya¨ luokasta ulospa¨in: class Y { public: static int const MAX PITUUS = 80; ... }; Vaikka ta¨llaiset luokkavakiot alustetaankin luokan esittelyssa¨, ne ta¨ytyy silti edelleen ma¨a¨ritella¨ jossain kooditiedostossa (mutta alus- tusta ei ena¨a¨ toisteta): // Yleensa¨ samassa kooditiedostossa kuin ja¨senfunktioiden koodi int const Y::MAX PITUUS; Luokkavakioita voi ka¨ytta¨a¨ luokan omassa koodissa tavallisten luokkamuuttujien tapaan. 8.2.3 C++: Luokkafunktiot Luokkafunktiot (static member function) muistuttavat ja¨senfunktioita samalla tavalla kuin luokkamuuttujat ja¨senmuuttujia. Luokkafunktiot edustavat sellaisia luokan palveluja ja operaatioita, jotka eiva¨t koh- distu mihinka¨a¨n yksitta¨iseen olioon, vaan “koko luokkaan”. Ta¨llaisia operaatioita ovat mm. luokkamuuttujien ka¨sittely, mutta on tietysti mahdollista etta¨ luokan vastuualueeseen kuuluu muitakin “luokan- laajuisia” operaatioita. Luokkafunktiot esitella¨a¨n luokan esittelyssa¨ 8.3. Tyypit osana rajapintaa 253 lisa¨a¨ma¨lla¨ niiden eteen sana static: class X { public: int jasenfunktio(); static int luokkafunktio(); ... }; Luokkafunktioiden ma¨a¨rittely kooditiedostossa puolestaan ei eroa ja¨senfunktion ma¨a¨rittelysta¨. Ainoat rajoitukset ovat, etta¨ koska luok- kafunktio ei kohdistu mihinka¨a¨n olioon, sen koodissa ei voi viitata ja¨senmuuttujiin, ja¨senfunktioihin eika¨ this-osoittimeen. Sen sijaan luokkafunktion koodissa voi vapaasti ka¨ytta¨a¨ luokkamuuttujia ja toi- sia luokkafunktioita. Listaus 8.3 seuraavalla sivulla sisa¨lta¨a¨ esimerkkina¨ luokan, joka pita¨a¨ kirjaa siita¨, montako luokan oliota ohjelman aikana on luotu ja tuhottu. Laskurit on talletettu luokkamuuttujina luotu ja tuhottu ja ne pystyy tulostamaan luokkafunktion tulosta tilasto avulla. 8.3 Tyypit osana rajapintaa Listauksessa 8.4 sivulla 255 on osa tyypillista¨ pa¨iva¨ysluokan rajapin- taa. Aiemmin ta¨llaista rajapintaa mainostettiin erityisesti sen vuoksi, etta¨ sen sanottiin piilottavan luokan toteutuksen ka¨ytta¨ja¨lta¨, jolloin luokan toteutusperiaatteen muuttaminen on helppoa. Kielta¨ma¨tta¨ listauksen pa¨iva¨ysluokassa on nyt mahdollista esitta¨a¨ pa¨iva¨ys sisa¨isesti mita¨ erilaisimmilla tavoilla, mutta luokan rajapinta on myo¨s lupaus. Rajapinnalla luokka lupaa, etta¨ sita¨ voi ka¨ytta¨a¨ raja- pinnan mukaan, ja ta¨sta¨ lupauksesta on pidetta¨va¨ kiinni. Toisaalta ta¨- ma¨ lupaus sitoo myo¨s luokan mahdollisia toteutuksia, toisin sanoen muutettiinpa luokan toteutusta milla¨ tavoin hyva¨nsa¨, niin rajapinnan on pysytta¨va¨ ta¨sma¨lleen samana. Kuvitellaanpa esimerkiksi, etta¨ pa¨iva¨ysluokkaa on onnistuneesti ka¨ytetty jo pitka¨a¨n sukupuuohjelmassa ja kaikki on toiminut mai- niosti. Myo¨hemmin huomataan, etta¨ periaatteessa mika¨a¨n ei esta¨ pa¨i- va¨yksen vuosiluvun menemista¨ negatiiviselle puolelle, jos suvun jo- 8.3. Tyypit osana rajapintaa 254 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . oliolaskuri.hh . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1 #ifndef LASKIJA HH 2 #define LASKIJA HH 3 4 class Laskija 5 { 6 public: 7 Laskija(int luku); 8 ~Laskija(); ... 9 static void tulosta tilasto(); 10 private: 11 int luku ; ... 12 static unsigned int luotu ; 13 static unsigned int tuhottu ; 14 }; 15 16 #endif . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . oliolaskuri.cc . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1 #include \"oliolaskuri.hh\" 2 3 #include <iostream> 4 using std::cout; 5 using std::endl; 6 7 Laskija::Laskija(int luku) : luku (luku) 8 { 9 ++luotu ; 10 } 11 12 Laskija::~Laskija() 13 { 14 ++tuhottu ; 15 } 16 17 void Laskija::tulosta tilasto() 18 { 19 cout << \"Olioita luotu: \" << luotu << endl; 20 cout << \"Olioita tuhottu: \" << tuhottu << endl; 21 } 22 23 unsigned int Laskija::luotu = 0; 24 unsigned int Laskija::tuhottu = 0; LISTAUS 8.3: Esimerkki luokkamuuttujien ja -funktioiden ka¨yto¨sta¨ 8.3. Tyypit osana rajapintaa 255 1 class Paivays 2 { 3 public: 4 Paivays(unsigned int p, unsigned int k, unsigned int v); 5 ~Paivays(); 6 7 void asetaPaiva(unsigned int paiva); 8 void asetaKk(unsigned int kuukausi); 9 void asetaVuosi(unsigned int vuosi); 10 11 unsigned int annaPaiva() const; 12 unsigned int annaKk() const; 13 unsigned int annaVuosi() const; 14 15 void etene(int n); 16 int paljonkoEdella(Paivays const& p) const; 17 18 private: 19 unsigned int paiva ; 20 unsigned int kuukausi ; 21 unsigned int vuosi ; 22 }; LISTAUS 8.4: Tyypillinen pa¨iva¨ysluokan esittely takin haaraa voidaan seurata todella pitka¨a¨n (todellisuudessa ta¨llai- nen mahdollisuus lienee korkeintaan joillain kuningassuvuilla). Eipa¨ ha¨ta¨a¨, ohjelma on tehty olio-ohjelmointia ka¨ytta¨en, joten pa¨iva¨ysluo- kan sisa¨isen toteutuksen muuttaminen on helppoa. Ohjelman tekija¨ lupaa puhelimessa korjata ohjelman tunnissa ja alkaa tutkia koodia. On kylla¨ totta, etta¨ luokan sisa¨isen toteutuksen muuttaminen on todella helppoa — ainoa muutos on muuttaa rivi 21 muo- toon “int vuosi ;”. Ta¨ma¨ ei kuitenkaan ratkaise ongelmaa, kos- ka luokan rajapinnassa ka¨yteta¨a¨n vuosilukujen ka¨sittelyyn tyyppia¨ unsigned int. Seuraava ratkaisuyritys on luonnollisesti muuttaa uusi tyyppi myo¨s rajapintaan. Ta¨ma¨ vaatii jonkin verran tarkkuutta (jotta kaikista rajapinnan unsigned int -tyypeista¨ muutetaan vain ja ainostaan tar- peelliset), mutta on suhteellisen helppo huomata, etta¨ rakentajan ja asetaVuosi-ja¨senfunktion vuosi-parametrin tyyppi pita¨a¨ korjata, sa- moin annaVuosi-ja¨senfunktion paluutyyppi. Helpotuksesta huokais- ten ohjelmoija ka¨a¨nta¨a¨ ohjelman uudelleen ja aloittaa testauksen. 8.3. Tyypit osana rajapintaa 256 Testaus osoittaa, etta¨ ohjelma toimii va¨a¨rin! Vikana on, etta¨ alkupera¨inen pa¨iva¨ysluokan rajapinta vihjaa ka¨yt- ta¨ja¨lle varsin voimakkaasti, etta¨ pa¨iva¨yksista¨ saatavia vuosilukuja saa ja tuleekin tallettaa unsigned int -tyyppisiin muuttujiin. Niinpa¨ oh- jelmasta saattaa lo¨ytya¨ paljonkin koodia, joka na¨ytta¨a¨ vaikkapa seu- raavalta: unsigned int syntymaVuosi = henkilo.syntymapvm.annaVuosi(); if (syntymaVuosi % 10 == 0) { cout << \"Syntynyt tasakymmenluvulla\" << endl; } Kun nyt vuodet muutetussa luokassa palautetaankin int- tyyppisina¨, tehda¨a¨n ensimma¨isella¨ rivilla¨ tyyppimuunnos int ⇒unsigned int. Mika¨li syntyma¨vuosi sattuu olemaan ne- gatiivinen, sita¨ ei voi esitta¨a¨ etumerkitto¨ma¨lla¨ luvulla, joten syntymaVuosi-muuttujaan tallettuu va¨a¨ra¨ arvo!] Esimerkin kaltaisten tapausten vuoksi on ta¨rkea¨ ymma¨rta¨a¨, etta¨ myo¨s rajapinnan tyypit ovat osa rajapintaa. Ta¨ma¨ koskee niin pa- rametrien tyyppeja¨ kuin paluutyyppeja¨kin. Mika¨li rajapinta ma¨a¨ra¨a¨ tyypin tietyksi, alkaa rajapintaa ka¨ytta¨va¨ koodi helposti luottaa tyy- pin sa¨ilymiseen samana. Mika¨li alkupera¨inen pa¨iva¨ysluokka olisi “ti- laa sa¨a¨sta¨a¨kseen” ka¨ytta¨nyt unsigned char-tyyppia¨ vuosiluvun va¨lit- ta¨miseen, olisi vuosi 2000 -ongelma pa¨a¨ssyt syntyma¨a¨n olio-ohjel- moinnista huolimatta. Ratkaisu ongelmaan on toistaa tuttua kapselointiperiaatetta uu- delleen: kapseloidaan rajapinnan tyypit omien tyyppinimien taakse, ja vaaditaan ka¨ytta¨jia¨ ka¨ytta¨ma¨a¨n tarjottuja tyyppeja¨. Listaus 8.5 seu- raavalla sivulla sisa¨lta¨a¨ parannellun pa¨iva¨ysluokan esittelyn. Riveilla¨ 5–8 esitella¨a¨n kaikki luokan rajapinnassa ka¨ytetyt tyypit ja annetaan niille nimi. Ta¨ma¨n ja¨lkeen kaikkialla luokassa ka¨yteta¨a¨n pa¨iva¨nume- ron yhteydessa¨ tyyppia¨ PaivaNro jne. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ]C++-standardissa sanotaan, etta¨ negatiivinen luku muuntuu etumerkitto¨ma¨ksi luvuksi niin, etta¨ uusiluku ≡vanhaluku (mod 2n), missa¨ n on etumerkitto¨ma¨n luvun bittien luku- ma¨a¨ra¨. 8.3. Tyypit osana rajapintaa 257 1 class Paivays 2 { 3 public: 4 // Luokan ka¨ytta¨ma¨t tyypit 5 typedef unsigned char PaivaNro; 6 typedef unsigned char KkNro; 7 typedef unsigned long int VuosiNro; 8 typedef long int Erotus; 9 10 Paivays(PaivaNro p, KkNro k, VuosiNro v); 11 ~Paivays(); 12 13 void asetaPaiva(PaivaNro paiva); 14 void asetaKk(KkNro kuukausi); 15 void asetaVuosi(VuosiNro vuosi); 16 17 PaivaNro annaPaiva() const; 18 KkNro annaKk() const; 19 VuosiNro annaVuosi() const; 20 21 void etene(Erotus n); 22 Erotus paljonkoEdella(Paivays p) const; 23 24 private: 25 PaivaNro paiva ; 26 KkNro kuukausi ; 27 VuosiNro vuosi ; 28 }; LISTAUS 8.5: Parannettu pa¨iva¨ysluokan esittely Luokan ka¨ytta¨ja¨n tulee nyt ka¨ytta¨a¨ nimettyja¨ tyyppeja¨: Paivays::VuosiNro syntymaVuosi = henkilo.syntymapvm.annaVuosi(); if (syntymaVuosi % 10 == 0) { cout << \"Syntynyt tasakymmenluvulla\" << endl; } Jos nyt ta¨ma¨n pa¨iva¨ysluokan kanssa tulee tarve sallia negatiiviset vuodet, on muutoksen tekeminen helppoa: muutetaan rivi 7 muotoon typedef long int VuosiNro; Ta¨ma¨ vaihtaa vuosinumeroiden ka¨sittelyn kaikkialla: 8.4. Ohjelmakomponentin sisa¨iset rajapinnat 258 • Luokan rajapinnassa kaikki vuosinumerot muuttuvat etumer- killisiksi. • Mika¨li luokan ka¨ytta¨ja¨t ovat ka¨ytta¨neet tyyppia¨ Paivays::VuosiNro omassa koodissaan, heida¨nkin koodin- sa muuttuu muutoksen mukaiseksi. • Mika¨li luokan toteutuksessa on ka¨ytetty VuosiNro-tyyppia¨, par- haassa tapauksessa toteutuksen koodiin ei tarvitse koskea ollen- kaan, koska muutos tapahtuu automaattisesti. Luokan sisa¨lla¨ ma¨a¨riteltyihin tyyppeihin liittyy yksi C++:n omi- tuisuus, joka on hyva¨ tieta¨a¨. Luokan ja¨senfunktioiden toteutuksissa (siis itse koodissa) ka¨a¨nta¨ja¨ tieta¨a¨ vasta ja¨senfunktion nimen luet- tuaan, minka¨ luokan ja¨senfunktiosta on kysymys. Niinpa¨ ka¨a¨nta¨ja¨ ei ja¨senfunktion paluutyyppia¨ lukiessaan viela¨ tieda¨, mita¨ luokkaa ollaan ka¨sittelema¨ssa¨, eika¨ se na¨in ollen myo¨ska¨a¨n osaa automaat- tisesti hyva¨ksya¨ luokan ma¨a¨rittelemia¨ tyyppeja¨. Ta¨ma¨n vuoksi ta¨y- tyy ja¨senfunktioiden toteutuksissa paluutyypit kirjoittaa muodossa Luokkanimi::Tyyppi, jos ne ovat luokan sisa¨isia¨ tyyppeja¨. Sen sijaan ja¨senfunktioiden parametrilistassa ja itse koodissa voi luokan omia tyyppeja¨ ka¨ytta¨a¨ suoraan ilman luokan nimea¨. Listaus 8.6 na¨ytta¨a¨ esi- merkkina¨ luokan Paivays ja¨senfunktion annaPaiva toteutuksen. 8.4 Ohjelmakomponentin sisa¨iset rajapinnat Joskus ohjelmiston rakenteesta tulee kaikesta yritta¨misesta¨ huolimat- ta sellainen, etta¨ kaikkea luokan sisa¨iseen toteutukseen liittyva¨a¨ toi- minnallisuutta ei saada luokan sisa¨a¨n kapseloiduksi, vaan osa siita¨ joudutaan — usein ohjelmointikielen syntaksista johtuvista syista¨ — toteuttamaan luokan ulkopuolisissa funktioissa. Ta¨llo¨in ongelmaksi tulee, etta¨ vain luokan omilla funktioilla on pa¨a¨sy luokan sisa¨iseen 1 Paivays::PaivaNro Paivays::annaPaiva() const 2 { 3 return paiva ; 4 } LISTAUS 8.6: Luokan ma¨a¨rittelema¨ tyyppi paluutyyppina¨ 8.4. Ohjelmakomponentin sisa¨iset rajapinnat 259 toteutukseen, joten luokan ulkopuolinen funktio joutuu periaatteessa ka¨ytta¨ma¨a¨n luokkaa sen julkisen rajapinnan kautta. Yleensa¨ tilanne on sellainen, etta¨ vaikka ongelmallinen funktio onkin luokan ulkopuolinen, se kuitenkin kuuluu ka¨sitteellisesti sa- maan moduuliin tai komponenttiin kuin luokkakin. Ta¨ten ongelmana ei niinka¨a¨n ole se, etta¨ ohjelmassa haluttaisiin rikkoa luokan tarjoa- maa kapselointia vaan se, etta¨ ohjelmassa yhdelle funktiolle haluttai- siin sallia muuta ohjelmaa laajempi rajapinta luokan olioiden ka¨ytta¨- miseen. Vastaavasti komponentin sisa¨lla¨ olevilla luokilla saattaa olla tarve ka¨ytta¨a¨ toisiaan tavoilla, joita ei haluta sallia komponentin ulko- puolella. Komponentin luokat tarvitsisivat ta¨llo¨in toisiinsa normaalia julkista rajapintaa laajemman rajapinnan. Mekanismit ta¨llaisille komponentin sisa¨isille rajapinnoille vaih- televat suuresti ohjelmointikielesta¨ toiseen. Joissain ohjelmointikie- lissa¨ (kuten C) tukea ei ole juuri lainkaan, jolloin ohjelmoija joutuu turvautumaan dokumentointiin, sopimuksiin ja koodauska¨yta¨nto¨ihin tarjotakseen moduulin sisa¨lle ulkomaailmalle na¨kyva¨a¨ laajemman ra- japinnan. Sen sijaan Java tarjoaa pakkauksen (package)ka¨sitteen, joka on tarkoitettu moduulien kirjoittamiseen (ta¨ta¨ ka¨siteltiin lyhyesti ali- luvussa 1.6.2). Javan luokissa on puolestaan rajapintojen public, protected ja private lisa¨ksi mahdollisuus “pakkauksen sisa¨iseen” ra- japintaan, joka saadaan aikaan kirjoittamalla ja¨senfunktio tai -muut- tuja ilman na¨kyvyysma¨a¨retta¨. Na¨ihin ja¨senfunktioihin ja -muuttujiin pa¨a¨see ka¨siksi myo¨s muista saman pakkauksen luokista. C++:ssa vastaava rakenne saadaan aikaan ysta¨va¨funktioiden (friend function) ja ysta¨va¨luokkien (friend class) avulla. Niita¨ ka¨sitella¨a¨n tar- kemmin seuraavissa aliluvuissa. 8.4.1 C++: Ysta¨va¨funktiot C++ tarjoaa varsin karkean mekanismin laajemman rajapinnan tarjoa- miseen: luokka voi julistaa joukon funktioita “ysta¨vikseen”. Luokan ysta¨va¨funktioiden koodi pa¨a¨see ka¨siksi myo¨s luokan olioiden private- osiin, eli ka¨yta¨nno¨ssa¨ silla¨ on samat oikeudet luokan olioihin kuin 8.4. Ohjelmakomponentin sisa¨iset rajapinnat 260 luokan omilla ja¨senfunktioilla. Esittelyn syntaksi on class Luokka { ... friend paluutyyppi funktionnimi(parametrit); }; On huomattava, etta¨ kyseinen esittely ei tee ysta¨va¨funktiosta luokan ja¨senfunktiota vaan kyseessa¨ on ta¨ysin erillinen normaali funktio, jol- le vain sallitaan pa¨a¨sy luokan olioiden private-osaan. Listauksessa 8.7 seuraavalla sivulla on esimerkki luokasta, jonka rakentaja on private-puolella. Ta¨ma¨ tarkoittaa sita¨, etta¨ luokan nor- maali ka¨ytta¨ja¨ ei pa¨a¨se ollenkaan luomaan luokan olioita. Sen sijaan luokka ma¨a¨ra¨a¨ funktion luoLukko ysta¨va¨kseen, jolloin funktio voi luo- da olioita ja palauttaa niita¨ osoittimen pa¨a¨ssa¨ paluuarvonaan. Na¨in luoLukko-funktiolla on ka¨yto¨ssa¨a¨n laajempi rajapinta kuin tavallisel- la ka¨ytta¨ja¨lla¨. Vaikka ysta¨va¨mekanismi salliikin periaatteessa ysta¨va¨funktioille oikeuden ka¨pa¨lo¨ida¨ olioiden private-osia aivan miten tahansa, kan- nattaa kuitenkin muistaa kapselointiperiaate. Yleensa¨ koodin selkeyt- ta¨ parantaa, jos ysta¨va¨funktiotkaan eiva¨t ka¨yta¨ suoraan luokan sisa¨is- ta¨ toteutusta (ja¨senmuuttujia), vaan luokan private-puolelle kirjoite- taan sopivat ja¨senfunktiot, joiden kautta ysta¨va¨funktio saa ka¨ytto¨o¨n- sa¨ laajemmat oikeudet. Ta¨lla¨ tavoin luokan rajapinta sa¨ilyy edelleen funktioista koostuvana. 8.4.2 C++: Ysta¨va¨luokat Ysta¨va¨funktioiden avulla voidaan yhdelle funktiolle sallia normaalia vapaampi pa¨a¨sy tietyn luokan olioon. Jos ohjelmakomponentissa on kaksi toisiinsa kiintea¨sti liittyva¨a¨ luokkaa, voi ka¨yda¨ niin, etta¨ luo- kassa on useita ja¨senfunktioita, joiden ta¨ytyy saada vapaampi pa¨a¨sy toisen luokan sisa¨lle. Ta¨ma¨ olisi mahdollista toteuttaa luettelemal- la kaikki luokan ja¨senfunktiot toisen luokan ysta¨va¨funktioiksi, mutta ta¨ma¨ on ko¨mpelo¨a¨. C++ antaa myo¨s luokalle mahdollisuuden julistaa toinen luokka ysta¨va¨kseen. Ta¨ma¨ tarkoittaa sita¨, etta¨ kaikki kyseisen luokan ja¨senfunktiot ovat automaattisesti ysta¨va¨funktioita. 8.4. Ohjelmakomponentin sisa¨iset rajapinnat 261 1 class Lukko 2 { 3 public: ... 4 private: 5 Lukko(); // Rakentaja private-puolella: olioiden luominen mahdotonta! 6 ~Lukko(); // Purkaja private-puolella: olioiden tuhoaminen mahdotonta! ... 7 friend Lukko* luoLukko(); // luo Lukko-olioita 8 friend void poistaLukko(Lukko* lp); // tuhoaa Lukko-olioita 9 }; 10 11 Lukko* luoLukko() 12 { 13 Lukko* lp = new Lukko; // Mahdollista, koska ysta¨va¨ ... 14 return lp; 15 } 16 void poistaLukko(Lukko* lp) 17 { 18 delete lp; lp = 0; // Mahdollista, koska ysta¨va¨ 19 } 20 LISTAUS 8.7: Laajemman rajapinnan salliminen ysta¨va¨funktioille Luokkaysta¨vyys saadaan aikaan ma¨a¨reella¨ friend class Luokkanimi sen luokan esittelyssa¨, joka haluaa sal- lia toisen luokan ja¨senfunktioille vapaan pa¨a¨syn omien olioidensa sisa¨lle. Listaus 8.8 seuraavalla sivulla na¨ytta¨a¨ esimerkin ta¨llaisesta esittelysta¨. 8.4. Ohjelmakomponentin sisa¨iset rajapinnat 262 1 class LainausJarjestelma 2 { ... 3 }; 4 5 class KirjastonKirja 6 { 7 public: ... 8 Paivays const& annaPalautusPvm() const; 9 private: 10 void asetaPalautusPvm(Paivays const& uusiPvm); 11 Paivays palautusPvm ; ... 12 friend class LainausJarjestelma; 13 // LainausJarjestelman oliot kutsuvat funktiota asetaPalautusPvm 14 }; LISTAUS 8.8: Ysta¨va¨luokat 263 Luku 9 Geneerisyys ISOC++ 14.7.3/7: The placement of explicit specialization dec- larations for function templates, class templates, member func- tions of class templates, static data members of class templates, mem- ber classes of class templates, member class templates of class templates, member function templates of class templates, member functions of mem- ber templates of class templates, member functions of member templates of non-template classes, member function templates of member classes of class templates, etc., and the placement of partial specialization declara- tions of class templates, member class templates of non-template classes, member class templates of class templates, etc., can affect whether a program is well-formed according to the relative positioning of the explicit specialization declarations and their points of instantiation in the translation unit as speciﬁed above and below. When writing a specialization, be careful about its location; or to make it compile will be such a trial as to kindle its self-immolation. – International Standard 14882 [ISO, 1998] Mallit kertovat meille, millainen jonkin asian pita¨isi olla, milta¨ jonkin tulisi na¨ytta¨a¨ tai miten jokin saadaan rakennetuksi. Jokaisella suoma- laisella on ka¨sitys siita¨, mika¨ sauna on. Saunan mallinen rakennus sisa¨lta¨a¨ lo¨ylytilan, pukuhuoneen ja vilvoittelualueen mielella¨a¨n ve- den a¨a¨rella¨. Saunojen rakentamiseen on paljon perinnetietoa, ohjeita 9.1. Yleiska¨ytto¨isyys, pysyvyys ja vaihtelevuus 264 ja jopa tutkimustuloksia. Voidaan sanoa, etta¨ suomalaisten saunojen rakentaminen on dokumentoitu (mallinnettu) huolellisesti vuosisato- jen saatossa. Vaikka ohjelmistojen tekeminen on hyvin nuori ala saunojen ra- kentamiseen verrattuna, ta¨lla¨kin alalla pyrita¨a¨n hyo¨dynta¨ma¨a¨n aikai- sempia kokemuksia. Ohjelmistomallit pyrkiva¨t kertomaan yleisia¨ lin- joja (geneerisen mallin) siita¨, minka¨lainen ratkaisu parhaiten (koke- mukseen perustuen) sopisi ka¨silla¨ olevaan ongelmaan. 9.1 Yleiska¨ytto¨isyys, pysyvyys ja vaihtelevuus Uudelleenka¨ytto¨a¨ mainostetaan yhtena¨ olio-ohjelmoinnin suurena etuna. Sen saavuttaminen ei kuitenkaan ole niin helppoa kuin pinta- puolisesti voisi luulla. Kun luokkaa tai ohjelmakomponenttia suunni- tellaan, se ra¨a¨ta¨lo¨ida¨a¨n ja kehiteta¨a¨n usein enemma¨n tai va¨hemma¨n tietoisesti juuri tiettyyn ka¨ytto¨ympa¨risto¨o¨n. Ta¨llo¨in on todenna¨ko¨is- ta¨, ettei tuo komponentti kuitenkaan sovi ta¨sma¨lleen samanlaisena toiseen ka¨ytto¨tarkoitukseen — uudelleenka¨ytto¨ ei onnistukaan puh- taassa muodossaan. Syyna¨ ta¨ha¨n on, etta¨ vaikka yleisella¨ tasolla jokin luokka tai kom- ponentti vaikuttaisikin “uudelleenka¨ytetta¨va¨lta¨”, on sen ka¨ytto¨koh- teilla kuitenkin hieman erilaisia tarpeita, jotka sitten vaikuttavat luo- kan rajapintaan tai toteutukseen. Ta¨ma¨n vuoksi ollaan yha¨ enemma¨n sita¨ mielta¨, etta¨ uudelleenka¨ytto¨a¨ on eritta¨in vaikea saada aikaan “sat- tumalta”. Uudelleenka¨ytto¨ vaatii sen, etta¨ luokkaa tai komponenttia ensimma¨ista¨ kertaa suunniteltaessa on edes jonkinlainen ka¨sitys sii- ta¨, missa¨ kaikissa ympa¨risto¨issa¨ luokkaa tai moduulia tarvitaan ja ka¨yteta¨a¨n. Ta¨ssa¨ mielessa¨ olisikin ehka¨ paras puhua yleiska¨ytto¨isyy- desta¨ (genericity) mielummin kuin uudelleenka¨yto¨sta¨ (re-usability). Yleiska¨ytto¨isyyden suunnittelussa oleellinen asia on lo¨yta¨a¨ kom- ponentin mahdolliset ka¨ytto¨kohteet (tai edes arvata “todenna¨ko¨ises- ti kattava” osa niista¨) ja analysoida na¨iden ka¨ytto¨kohteiden tarpei- ta. Ta¨llo¨in saadaan kuva siita¨, missa¨ osissa komponenttia eri ka¨yt- to¨kohteiden tarpeet pysyva¨t ta¨sma¨lleen samoina ja missa¨ asioissa ne vaihtelevat. Ta¨llainen pysyvyys- ja vaihtelevuusanalyysi (common- ality and variability analysis) on ta¨rkea¨ osa yleiska¨ytto¨isen kompo- nentin suunnittelua, koska se kertoo, missa¨ ma¨a¨rin mahdollisuutta 9.1. Yleiska¨ytto¨isyys, pysyvyys ja vaihtelevuus 265 yleiska¨ytto¨isyyteen sovellusalueessa on ja missa¨ osissa komponent- tia se ilmenee. [Coplien, 1999] A¨a¨rimma¨isessa¨ tapauksessa pysyvyys- ja vaihtelevuusanalyysi saattaa antaa tulokseksi, etta¨ kaikkien ka¨ytto¨kohteiden tarpeet suun- niteltavan komponentin kannalta ovat ta¨sma¨lleen samat. Ta¨llo¨in yleiska¨ytto¨isyys on eritta¨in helppoa saada aikaan, koska ta¨sma¨lleen samanlainen komponentti kelpaa kaikkialle, ja tehta¨va¨ksi ja¨a¨ ena¨a¨ komponentin toteuttaminen perinteisin keinoin. Ta¨ma¨ on tietysti ihannetilanne, joka esiintyy varsin harvoin. Yleensa¨ ka¨ytto¨kohteiden tarpeissa on jonkinlaisia eroja, ainakin jos suunniteltava komponentti ei ole todella pieni. Toisessa a¨a¨ripa¨a¨ssa¨ saatetaan huomata, etta¨ vaikka aluksi eri ka¨yt- to¨kohteet na¨yttiva¨tkin tarvitsevan samanlaista ohjelmakomponenttia, ovat niiden tarpeet niin erilaiset, ettei ta¨ydellisia¨ yhta¨la¨isyyksia¨ ka¨yt- to¨kohteiden va¨lilla¨ kuitenkaan ole. Ta¨llaisessa tapauksessa vaihtele- vuus on siis niin suurta, etta¨ ka¨yta¨nno¨ssa¨ suunniteltava komponent- ti jouduttaisiin ra¨a¨ta¨lo¨ima¨a¨n kokonaan jokaista ka¨ytto¨kohdetta koh- ti. Ta¨llo¨in yleiska¨ytto¨isyytta¨ ei tietenka¨a¨n voida saavuttaa ollenkaan. Onneksi ta¨llaiset tapaukset ovat myo¨s eritta¨in harvinaisia. Tyypillinen tulos pysyvyys- ja vaihtelevuusanalyysista¨ on, etta¨ osa komponentista voidaan pita¨a¨ ta¨sma¨lleen samanlaisena, osa taas ta¨ytyy toteuttaa eri tavalla eri ka¨ytto¨kohteisiin. Haasteena on toteut- taa yleiska¨ytto¨inen komponentti niin, etta¨ pysyva¨t osat ja vaihtelevat osat saadaan kokonaan eriytettya¨ toisistaan. Ta¨llo¨in komponentin ka¨ytta¨minen helpottuu, koska ohjelmoijalle on selva¨a¨, mitka¨ osat ha¨nen ta¨ytyy ra¨a¨ta¨lo¨ida¨ tai kirjoittaa uudelleen juuri tiettya¨ ka¨ytto¨kohdetta varten, mitka¨ osat taas ka¨yteta¨a¨n aina sel- laisinaan. Mika¨li analyysi on tehty kunnolla, on viela¨ lisa¨ksi toden- na¨ko¨ista¨, etta¨ kun tulevaisuudessa ka¨ytto¨kohteita tulee lisa¨a¨, pysy- va¨t osat kelpaavat sellaisenaan niihinkin. Kuva 9.1 seuraavalla sivul- la esitta¨a¨ pysyvyys- ja vaihtelevuusanalyysin toimintaa graaﬁsesti. Oman kiemuransa ta¨llaiseen yleiska¨ytto¨isen komponentin suun- nitteluun tuo kaiken lisa¨ksi se, etta¨ varsin usein “ka¨ytto¨kohteiden” valinta on varsin mielivaltaista. Tyypillisesti mita¨ suppeammaksi komponentin ka¨ytto¨kohdevalikoima ma¨a¨ritella¨a¨n, sita¨ enemma¨n py- syvyytta¨ komponenttiin saadaan, koska ka¨ytto¨kohteiden keskina¨isia¨ eroja on va¨hemma¨n. Ta¨llo¨in suuri osa komponentin koodista on ta¨y- dellisesti uudelleenka¨ytetta¨va¨a¨, mutta sen ka¨ytto¨alue on suppea. 9.1. Yleiska¨ytto¨isyys, pysyvyys ja vaihtelevuus 266 Sovelluskohde 1 Sovelluskohde 2 (Tuleva sovelluskohde) Pysyvä osa Vaihtelevat osat (räätälöinti) Pysyvyys− ja vaihtelevuus− analyysi KUVA 9.1: Pysyvyys- ja vaihtelevuusanalyysi ja yleiska¨ytto¨isyys Toisaalta, jos ka¨ytto¨kohteiden kirjo on suuri, vaihtelevuuden ma¨a¨- ra¨ kasvaa suureksi. Tuloksena on komponentti, joka on kylla¨ todella yleiska¨ytto¨inen, mutta jossa samanlaisena uudelleenka¨ytetta¨va¨n koo- din ma¨a¨ra¨ on kuitenkin va¨ha¨inen. Kuten niin usein ohjelmistoteknii- kassa ta¨ssa¨kin taitoa vaatii sopivan kompromissin lo¨yta¨minen, jotta hyo¨dyt saadaan maksimoitua. Olennaiseksi kysymykseksi yleiska¨ytto¨isyydessa¨ muodostuu yleensa¨ se, miten komponentin pysyvyys ja vaihtelevuus saadaan erotettua toisistaan selkea¨sti. Ta¨ha¨n on olemassa lukematon ma¨a¨ra¨ erilaisia vaihtoehtoja tilanteesta riippuen. Niita¨ ka¨sitella¨a¨n tarkem- min esimerkiksi kirjassa “Generative Programming” [Czarnecki ja Eisenecker, 2000]. Aliluvussa 9.2 esitelta¨va¨t suunnittelumallit ovat 9.2. Suunnittelun geneerisyys: suunnittelumallit 267 era¨a¨nlainen esimerkki suunnittelutason pysyvyyden ja vaihtelevuu- den hallinnasta. Aliluvussa 9.4 kuvataan lyhyesti pysyvyyden ja vaihtelevuuden hallintaa periytymisen avulla. Aliluvussa 9.5 taas ka¨sitella¨a¨n C++:n mallit (template), jotka antavat mahdollisuuden periytymisesta¨ poikkeavaan yleiska¨ytto¨isyyteen. 9.2 Suunnittelun geneerisyys: suunnittelumallit Olio-ohjelmassa useat oliot toteuttavat yhdessa¨ ohjelmiston toimin- nallisuuden. Hyvin suunniteltu luokka mahdollistaa sen toteutta- man rakenteen uudelleenka¨yto¨n myo¨s muualla kuin alkupera¨isessa¨ ka¨ytto¨kohteessa. Vastaavasti voimme uudelleenka¨ytta¨a¨ usean luokan muodostaman toiminnallisen kokonaisuuden ja jopa yleista¨a¨ ta¨ma¨n uudelleenka¨yto¨n sa¨a¨nno¨sto¨ksi, joka kertoo, miten jokin yleisempi on- gelma voidaan ratkaista kyseisen oliojoukon avulla. Ta¨llaisia sa¨a¨n- no¨sto¨ja¨ nimiteta¨a¨n olio-ohjelmoinnin suunnittelumalleiksi (design pattern). Suunnittelumalleja voi esiintya¨ usealla tasolla. Yleisen tason mal- li (arkkitehtuurimalli) voi kertoa periaatteen koko ohjelmiston raken- teesta (esimerkiksi WWW-sovelluksen osien jako http-palvelimen, “cgi-skriptien” ja tietokannan ohjauksen kesken). Keskitaso on mo- duulisuunnittelussa, jossa suunnittelumalli antaa mallin muutaman luokan (tyypillisesti 3–7) muodostamasta toiminnallisesta kokonai- suudesta. La¨hinna¨ toteutusta ovat mallit (toteutusmalli, idiomi), jot- ka liittyva¨t tietyn ongelman ratkaisemiseen ohjelmointikielen tasol- la (esimerkiksi funktionaalisten kielten rakenne map, joka kohdis- taa ma¨a¨ra¨tyn funktion kutsun yksitellen listan jokaiselle alkiolle) [Haikala ja Ma¨rija¨rvi, 2002]. Puhuttaessa suunnittelumalleista ilman lisa¨ma¨a¨reita¨ tarkoitetaan yleensa¨ keskitason oliosuunnitteluun liitty- via¨ malleja. 9.2.1 Suunnittelumallien edut ja haitat Edella¨ mainitusta seuraa helposti, etta¨ suunnittelumalliksi voidaan julistaa melkein mika¨ tahansa “ei-triviaali” ohjelmakoodin tai suun- nitelman osa. Ta¨ma¨ ei kuitenkaan ole tarkoitus. Yleisiksi suunnit- telumalleiksi on tarkoitus kera¨ta¨ ka¨yta¨nno¨ssa¨ usein ka¨ytettyja¨ mal- liratkaisuja. Ensimma¨inen suunnittelumallien kokoelma oli niin sa- 9.2. Suunnittelun geneerisyys: suunnittelumallit 268 nottu “Gang of Four” (GoF) -kirja [Gamma ja muut, 1995], johon on valittu ainoastaan sellaisia suunnittelumalleja, joille on lo¨ytynyt va¨- hinta¨a¨n kaksi toisistaan riippumatonta ka¨ytta¨ja¨kuntaa (ohjelmistota- loa tai -projektia). Ta¨llaiset yleiset suunnittelumallit tulisi pikkuhil- jaa saada osaksi jokaisen ohjelmistosuunnittelijan tieta¨mysta¨ — ta¨lla¨ hetkella¨ on vain hyvin vaikea arvioida, mitka¨ mallit ohjelmistosuun- nittelijan yleissivistykseen tulisi kuulua. Parhaimmillaan suunnittelumalli on abstrakti suunnitteludoku- mentti, joka voidaan ottaa ka¨ytto¨o¨n ohjelmiston suunnittelussa he- ti, kun suunnittelija tunnistaa ongelmasta kohdan, johon malli sopii. Yleista¨ tieta¨mysta¨ (osa ohjelmoijan yleisivistysta¨) olevien mallien li- sa¨ksi suunnittelumallit ovat erinomainen tapa kera¨ta¨ talteen organi- saatiossa olevaa sovellusaluekohtaista tietoa, joka na¨in sa¨ilyy, vaikka ihmiset vaihtuvatkin. Jotta suunnittelumalleista olisi hyo¨tya¨, ne tulisi dokumentoida huolellisesti ja niiden tulisi olla helposti omaksuttavissa. Dokumen- tointiin on ma¨a¨ra¨muotoisten dokumentointipohjien lisa¨ksi kehitetty muodollisempia menetelmia¨. Mallikieli (pattern language) on nimi- tys kokoelmalle saman sovellusalueen toisiinsa liittyvia¨ suunnittelu- malleja ja tiedolle siita¨, miten na¨ita¨ malleja voidaan sovellusalueella hyo¨dynta¨a¨ ja yhdistella¨. Suunnittelumallit itsessa¨a¨n eiva¨t auta mita¨a¨n, elleiva¨t niita¨ ka¨yt- ta¨va¨t suunnittelijat tieda¨, mita¨ mikin malli tarkoittaa. Nykyisin uusien suunnittelumallien lo¨yta¨minen ja julkaiseminen vaikuttaa hieman riista¨ytyneen ka¨sista¨, eika¨ kukaan yksitta¨inen ohjelmoija eh- di opetella kaikkia uusia malleja. Mallien muistamista ja opettelua haittaa myo¨s usein niiden sidonnaisuus englannin kieleen — vaa- tii erinomaista kielitaitoa saada oikea mielleyhtyma¨, jos suunnittelu- mallin nimena¨ on esimerkiksi Memento [Gamma ja muut, 1995, s. 283–291] tai The Percolation Pattern [Binder, 1999]. 9.2.2 Suunnittelumallin rakenne Yksi olio-ohjelmien perustoimenpiteista¨ on ka¨sitella¨ joukkoa olioita vaikkapa kutsumalla joukon jokaiselle oliolle jotain palvelurutiinia. Polymorﬁsmin yhteydessa¨ (aliluku 6.5.2) na¨imme esimerkin oliojou- kon ka¨sittelysta¨ luokkahierarkian avulla siten, etta¨ ka¨sittely tapahtui yhteisen kantaluokan kautta. Joukkojen ka¨sittelyssa¨ seuraava luon- teva askel on tarve ma¨a¨ritella¨ alijoukkoja. Na¨ita¨ uusia oliokokoelmia 9.2. Suunnittelun geneerisyys: suunnittelumallit 269 olisi luontevaa ka¨sitella¨ samoissa paikoissa kuin kantaluokan olioita, koska muutoin olioita ka¨sitteleva¨n asiakasohjelmakoodin tulisi toi- mia aina eri tavoin sen mukaan, onko silla¨ ka¨sittelyssa¨ hierarkian pe- rusolio vai kokoelmaolio. Ratkaisuna ma¨a¨rittelemme hierarkiaan uu- den luokan Kokoelma, joka osaa ka¨sitella¨ joukkoa alkupera¨isia¨ olioita kantaluokan rajapinnan mukaisesti (katso kuva 9.2). Kokoelman avulla voidaan luoda esimerkiksi kuvassa 9.3 seuraa- valla sivulla esitetty rakenne. Kun luokan Kokoelma palvelu “piirra¨” ma¨a¨ritella¨a¨n kutsumaan samaa palvelua jokaiselle kokoelmassa ole- valle oliolle, saamme sa¨ilytetyksi myo¨s alkupera¨isen toiminnallisuu- den, jossa taulukon kaikki graﬁikkaoliot piirtyva¨t na¨ytto¨laitteelle ka¨s- ketta¨essa¨. Nyt silmukka, joka kutsuu palvelua “piirra¨” taulukon nel- ja¨lle alkiolle, aiheuttaa palvelun kutsumisen kokoelman kautta kai- kille rakenteessa oleville perusolioille (kuvassa id-tunnisteet 1, 2, 5, 6, 7, 8 ja 9). Kuvan 9.2 rakenne on vasta tiettyyn tarkoitukseen laadittu suun- nitelma. Siita¨ saadaan kuitenkin yleiska¨ytto¨isempi, kun ja¨ta¨mme rakenteesta pois graﬁikkaesimerkkimme ja keskitymme ainoastaan piirrä() piilota() Viiva Näkyvyys piirrä() { abstract } onkoNäkyvissä: bool piilota() { abstract } piirrä() piilota() poista( index ) palauta_jäsen( index ) Kokoelma lisää( obj : Näkyvyys ) piirrä() piilota() Ympyrä ∗ KUVA 9.2: Kokoelman toteuttava luokka 9.2. Suunnittelun geneerisyys: suunnittelumallit 270 id01: Viiva id02: Ympyrä id03: Kokoelma id04: Kokoelma id07: Ympyrä id05: Viiva id06: Viiva id08: Viiva id09: Ympyrä taulukko[4] : Näkyvyys KUVA 9.3: Taulukko joka sisa¨lta¨a¨ kokoelmia olioiden kokoelmaan luokkahierarkiassa ja kantaluokan rajapinnan sa¨ilytta¨miseen. Rakenteen tarkoituksena on tarjota saman kantaluo- kan kaksi eri variaatiota: ma¨a¨ra¨tyilla¨ operaatioilla toimivat oliot (kan- taluokka on rajapintakuvaus) ja kokoelma na¨ita¨ olioita. Rakenteen normaali ka¨ytto¨tarkoitus on ainoastaan rajapinnan ka¨ytto¨ ilman tie- toa siita¨, onko ka¨yto¨ssa¨ kokoelma vaiko perusolio. Na¨illa¨ ehdoilla saamme kuvassa 9.5 sivulla 272 na¨kyva¨n rakenteen. Kyseessa¨ on suunnittelumalli nimelta¨ Kokoelma (Composite) [Gamma ja muut, 1995, s. 163–173], joka on tarkemmin kuvattu aliluvussa 9.3.1. UML ma¨a¨rittelee suunnittelumalleja varten oman piirrossymbo- linsa, jossa kerrotaan ainoastaan mallin nimi (ta¨ma¨ erityisesti koros- taa sita¨, etta¨ suunnittelijan ja toteuttajan oletetaan tieta¨va¨n heti mal- lin nimesta¨ kaiken siihen liittyva¨n). Malliin liiteta¨a¨n tavallaan pa- rametreina olioita, jotka toteuttavat suunnittelumallin ma¨a¨rittelema¨t osat. Na¨iden todellisten ohjelman olioiden sanotaan esiintyva¨n suun- nittelumallin ma¨a¨rittelema¨ssa¨ roolissa (role). Esimerkki suunnittelu- mallin kuvaamisesta UML:lla¨ on kuvassa 9.4 seuraavalla sivulla, jos- sa luokka Na¨kyvyys on roolissa Kantaluokka, Ympyra¨ on Perusolio ja Na¨kyvyysLista on Kokoelma. 9.3. Valikoituja esimerkkeja¨ suunnittelumalleista 271 Kokoelma Kokoelma Perusolio Kantaluokka (a) Suunnittelumalli NäkyvyysLista Ympyrä Kokoelma Näkyvyys { abstract } Kokoelma Perusolio Kantaluokka (b) Mallin ka¨ytto¨ KUVA 9.4: Suunnittelumallin UML-symboli ja sen ka¨ytto¨ 9.3 Valikoituja esimerkkeja¨ suunnittelumalleis- ta Ta¨ssa¨ aliluvussa esitella¨a¨n kolme melko yksinkertaista mutta hyvin yleisessa¨ ka¨yto¨ssa¨ olevaa suunnittelumallia. Niista¨ na¨kyy GoF-kirjan ensimma¨isena¨ ka¨ytto¨o¨n ottama suunnittelumallien dokumentointi- muoto, jossa esitella¨a¨n mallin ka¨ytto¨tarkoitus, perustelu sille miksi malli on yleiska¨ytto¨inen ratkaisu useaan ongelmaan ja mallin luok- karakenteen kuvaus UML:n avulla. 9.3.1 Kokoelma (Composite) Tarkoitus: Esitta¨a¨ olioita hierarkkisesti toisistaan koostuvina siten, etta¨ koosteolioita ja niiden osia voidaan ka¨ytta¨a¨ samalla tavalla. [Gamma ja muut, 1995, s. 163–173] 9.3. Valikoituja esimerkkeja¨ suunnittelumalleista 272 Perustelu: Oliokokoelmissa tarvitaan usein ominaisuutta tallettaa kokoelmaan toisia kokoelmia. Ta¨llo¨in ongelmaksi tulee kokoelmien ja perusolioiden erilainen ka¨ytta¨ytyminen. Kun kokoelmaoliot ja pe- rusoliot sisa¨lta¨va¨t saman toiminnallisen rajapinnan, ta¨ta¨ ongelmaa ei synny. Soveltuvuus: Tarvitaan sisa¨kka¨isia¨ kokoelmia tai halutaan, etta¨ olioita ka¨sittelevien asiakkaiden ei tarvitse va¨litta¨a¨ perusolioiden ja kokoelmien va¨lisista¨ eroista. Rakenne: Katso kuva 9.5. Osallistujat: • Asiakas: Ka¨ytta¨a¨ olioita kantaluokan tarjoaman rajapinnan kautta. • Kantaluokka: Ma¨a¨rittelee (abstraktin) rajapinnan, jonka operaa- tiot aliluokkien on toteutettava. • Perusolio: Primitiiviolio (joita voi olla useita), joka toteuttaa kantaluokan rajapinnan. palvelu() { abstract } Kantaluokka palvelu() Perusolio Kokoelma lisää() poista() palvelu() palauta_jäsen() Toteutus: kutsu palvelu():a jokaiselle kokoelman oliolle. ∗ Asiakas Käyttää KUVA 9.5: Suunnittelumalli Kokoelma 9.3. Valikoituja esimerkkeja¨ suunnittelumalleista 273 • Kokoelma: Koosteluokka, joka tallettaa viittaukset osaolioihin. Kantaluokan rajapinta toteutetaan siten, etta¨ kukin kutsu va¨lite- ta¨a¨n jokaiselle koosteessa mukana olevalle oliolle. Seuraukset: Asiakkaan ohjelmakoodi yksinkertaistuu, koska eroa perusolioiden ja koosteiden va¨lille ei tarvitse tehda¨. Uusien kooste- ja perusluokkien lisa¨a¨minen on helppoa. 9.3.2 Iteraattori (Iterator) Tarkoitus: Tarjoaa kokoelman alkioihin viittaamiseen rajapinnan, joka on erilla¨a¨n itse kokoelman toteutuksesta. [Gamma ja muut, 1995, s. 257–271] Tunnetaan myo¨s nimella¨: Kohdistin (Cursor). Perustelu: Listan tai muun alkiokokoelman rajapinta kasvaa helpos- ti hyvin suureksi, jos siihen liiteta¨a¨n seka¨ rakenteen muokkaukseen etta¨ la¨pika¨yntiin kuuluvat operaatiot. Yksi joukko la¨pika¨ynnin ope- raatioita (alkuun, seuraava, edellinen ynna¨ muut) mahdollistaa vain yhden la¨pika¨ynnin olemassaolon kerrallaan, koska alkiokokoelman toteuttava olio sa¨ilytta¨a¨ itse operaatioiden vaatiman tilatiedon. Kun vastuu alkiokokoelman la¨pika¨ynnista¨ irrotetaan erilliseksi (mutta ko- koelmaan la¨heisesti liittyva¨ksi) olioksi, saadaan ratkaistuksi molem- mat ongelmat. Soveltuvuus: Tarvitaan useita yhta¨aikaisia la¨pika¨ynteja¨ kokoelmaan tietoa tai halutaan tarjota yhtena¨inen rajapinta useiden eri tavalla to- teutettujen kokoelmien la¨pika¨yntiin. Rakenne: Katso kuva 9.6 seuraavalla sivulla. Osallistujat: • Asiakas: Ka¨ytta¨a¨ rajapintaa Iteraattori luokan Kokoelma sisa¨lla¨ olevien alkioiden osoittamiseen ja la¨pika¨yntiin. 9.3. Valikoituja esimerkkeja¨ suunnittelumalleista 274 Kokoelma { abstract } LuoIteraattori() Asiakas Iteraattori { abstract } Ensimmäinen() Seuraava() bool OnkoLopussa() OsoitettuAlkio() Käyttää Käyttää KokoelmaToteutus LuoIteraattori() IteraattoriToteutus Ensimmäinen() Seuraava() bool OnkoLopussa() OsoitettuAlkio() Luo uuden tähän kokoelmaan osoittavan iteraattorin KUVA 9.6: Suunnittelumalli Iteraattori • Kokoelma: Ma¨a¨rittelee alkiokokoelman rajapinnan (erityisesti tavan, jolla kokoelmaan saadaan luoduksi iteraattori). • KokoelmaToteutus: Toteuttaa edella¨ mainitun rajapinnan. Tar- vitsee keinon luoda iteraattoriolion. • Iteraattori: Ma¨a¨rittelee rajapinnan alkiokokoelman la¨pika¨yn- tiin. • IteraattoriToteutus: Toteuttaa edella¨ mainitun rajapinnan. (Ko- koelmaToteutus palauttaa ta¨ma¨n luokan instanssin.) Seuraukset: Kokoelma pystyy tarjoamaan useita erilaisia tapoja al- kioiden la¨pika¨yntiin tarjoamalla useita iteraattoreita. Erilla¨a¨n oleva iteraattori yksinkertaistaa kokoelman rajapintaa. Kokoelman ka¨ytta¨ja¨ pystyy pita¨ma¨a¨n tarvittaessa useita osoituksia eli la¨pika¨ynteja¨ yhta¨- aikaa ka¨ynnissa¨. 9.3.3 Silta (Bridge) Tarkoitus: Erottaa rajapinnan toteutuksesta siten, etta¨ toteutuk- sen muutokset eiva¨t vaikuta mitenka¨a¨n rajapinnan ohjelmakoodiin [Gamma ja muut, 1995, s. 151–161]. 9.3. Valikoituja esimerkkeja¨ suunnittelumalleista 275 Tunnetaan myo¨s nimella¨: Handle/Body, Envelope/Letter [Coplien, 1992, luku 5.5] ja myo¨s hieman omituisella nimella¨ Cheshire Cat\u0017 [Meyers, 1998, s.148]. La¨hes samasta rakenteesta on useita eri vari- aatioita, joita on esitelty artikkelissa “C++ Idioms Patterns” [Coplien, 2000]. Perustelu: Rajapinnan ja toteutuksen erotteleminen toisistaan oh- jelmakoodin tasolla on joissain ohjelmointikielissa¨ toteutettuna kie- len rakenteilla (katso Modula-3, aliluku 1.6.1). C++:n luokkaesittelyssa¨ on myo¨s puhtaasti toteutukseen liittyvia¨ osia (private-osuus), jolloin niissa¨ tehdyt muutokset na¨kyva¨t myo¨s julkisen rajapinnan ka¨ytta¨jil- le (esimerkiksi ka¨a¨nno¨sriippuvuuksina). Silta on toteutustekniikka, jolla myo¨s C++:n tapaisissa kielissa¨ saadaan rajapinta ja toteutus ero- telluksi paremmin toisistaan (ortogonaalisempi yhteys osien va¨lille). Soveltuvuus: Halutaan poistaa kiintea¨ yhteys rajapinnan ja toteu- tuksen va¨lilta¨ vaikkapa siksi, etta¨ toteutus voi muuttua ohjelman suo- rituksen aikana. Seka¨ rajapinnasta etta¨ toteutuksesta halutaan periyt- ta¨a¨ toisistaan riippumatta uusia versioita. Toteutuksen muutosten ei haluta aiheuttavan mita¨a¨n muutoksia rajapintaa ka¨ytta¨vissa¨ asiakas- koodeissa (niita¨ ei haluta ka¨a¨nta¨a¨ uudelleen). Rakenne: Katso kuva 9.7 seuraavalla sivulla. Osallistujat: • Asiakas: Ka¨ytta¨a¨ palveluita luokasta Rajapinta tehdyn olion kautta. • Rajapinta: Ma¨a¨rittelee julkisen rajapinnan ja viittaa olioon, joka toteuttaa ta¨ma¨n rajapinnan. • Toteutus: Kantaluokka, josta periytetyt oliot ovat rajapinnan to- teutusten eri versioita. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . \u0017Viittaa Lewis Carrollin teoksessa “Alice’s Adventures in Wonderland” [Carroll, 1865] ole- vaan kissaan, joka suomennoksessa “Liisa ihmemaassa” esiintyy nimella¨ Irvikissa. Ta¨ma¨ kissa pystyi erottamaan irvistyksensa¨ muusta ruumiistaan niin, etta¨ vain irvistys na¨kyi. 9.3. Valikoituja esimerkkeja¨ suunnittelumalleista 276 Asiakas Käyttää Rajapinta palvelu() palvelu() Toteutus { abstract } palvelu() Toteutus_A palvelu() Toteutus_B imp−>palvelu() imp 1 KUVA 9.7: Suunnittelumalli Silta Seuraukset: Toteutus ei ole pysyva¨sti kytketty rajapintaan, minka¨ vuoksi toteutus voi ma¨a¨ra¨ytya¨ ajoaikana ohjelman konﬁguraatiosta tai jopa muuttua kesken ohjelman suorituksen. Rajapinnan ja toteu- tusten laajentaminen periytta¨ma¨lla¨ on yksinkertaisempaa. Rajapin- nan ka¨ytta¨ja¨lle ei na¨y epa¨oleellista tietoa toteutusyksityiskohdista (C++). 9.3.4 C++: Esimerkki suunnittelumallin toteutuksesta Tyypillinen esimerkki ajoaikaisesta toteutuksen valinnasta on ny- kyaikainen graaﬁnen ka¨ytto¨liittyma¨. Erityisesti UNIX-ja¨rjestelmissa¨ ka¨ytta¨ja¨lla¨ on valittavana useita erityylisia¨ ka¨ytto¨liittymia¨, jotka tar- joavat erilaisia vaihtoehtoja ka¨ytetta¨vyydessa¨ ja ulkoasussa. Jos ha- luamme tehda¨ ohjelman, joka tarjoaa todella joustavan ka¨ytto¨liitty- ma¨n, voimme valita ka¨ytto¨o¨n erilaisella ka¨ytto¨liittyma¨kirjastolla teh- dyn toteutuksen ka¨ytta¨ja¨n mieltymysten mukaan. Valinta voi olla etu- ka¨teen merkittyna¨ ohjelman alustustiedoissa, jolloin ka¨ytto¨liittyma¨- kirjasto valitaan ohjelman ka¨ynnistyessa¨, tai erityisen joustava oh- jelma voi pystya¨ ka¨ynnista¨ma¨a¨n koko ka¨ytto¨liittyma¨nsa¨ uudelleen 9.4. Geneerisyys ja periytymisen rajoitukset 277 ajoaikana tehdyn valinnan perusteella (joitain ta¨llaisia ja¨rjestelmia¨ on olemassa). Ta¨llaisessa tilanteessa voimme hyo¨dynta¨a¨ ohjelman ra- kenteessa suunnittelumallia silta (aliluku 9.3.3 sivulla 274). Otamme esimerkiksi ka¨ytto¨liittyma¨n komponentin, jonka avul- la voidaan na¨ytta¨a¨ ka¨ytta¨ja¨lle virhetiedotteita avaamalla na¨yto¨lle ik- kuna, jossa ilmoitusteksti sijaitsee. Tiedotteita ka¨ytta¨ville ohjelman osille tarjotaan abstrakti virheikkunan rajapinta, joka on esitetty lis- tausessa 9.1 seuraavalla sivulla. Luokka VirheIkkuna on suunnit- telumallin roolissa Rajapinta. Koska virheikkunan julkisen rajapin- nan esittely ei tarvitse toteutuksen luokkaesittelya¨, siita¨ on olemas- sa vain luokan ennakkoesittely (VirheIkkunaToteutus). Virheikkunan rajapinnan toteutuksessa ainoastaan delegoidaan eli va¨liteta¨a¨n raja- pintakutsut varsinaiselle toteutusoliolle, joka on rajapinnan ainoan ja¨senmuuttujan (imp ) pa¨a¨ssa¨ (tiedoston virheikkuna.cc rivit 11–14). Toteutukselle ma¨a¨ritella¨a¨n kantaluokka (listaus 9.2 sivulla 279), josta periytta¨ma¨lla¨ tehda¨a¨n todellisen toiminnallisuuden toteutta- via versioita (listaus 9.3 sivulla 279). Luokka VirheIkkunaToteutus on suunnittelumallin roolissa Toteutus. Nyt ta¨ma¨n luokkarakenteen avulla voidaan tehda¨ luokan VirheIkkuna instansseja erilaisilla toteu- tuksilla ja¨rjestelma¨n konﬁguraation mukaan (listaus 9.4 sivulla 279). 9.4 Geneerisyys ja periytymisen rajoitukset Aliluvussa 9.1 todettiin, etta¨ geneerisen ja yleiska¨ytto¨isen komponen- tin suunnittelussa oleellinen asia on pysyvyyden ja vaihtelevuuden erottaminen toisistaan, jotta pysyva¨t osat voidaan toteuttaa vain ker- ran. Ta¨ma¨n tavoitteen saavuttamiseen lo¨ytyy ohjelmointikielitasolta erilaisia mekanismeja. Olio-ohjelmoinnissa periytyminen on meka- nismi, jolla luokkien yhteiset ominaisuudet voidaan toteuttaa kertaal- leen kantaluokassa. Niinpa¨ onkin luonnollista etta¨ periytyminen an- taa mahdollisuuden yleiska¨ytto¨isyyteen. Ta¨ssa¨ aliluvussa tutkitaan sita¨, millaiseen yleiska¨ytto¨isyyteen periytyminen antaa mahdollisuu- den ja missa¨ sen rajat tulevat vastaan. On varsin helppoa keksia¨ tilanteita, joissa periytyminen on ihan- teellinen mekanismi pysyvien ja vaihtelevien osien erottamiseen toi- sistaan. Jokainen periytymishierarkia vastaa tilannetta, jossa kanta- luokkien toteutus periytyy aliluokille — kantaluokat ovat hierarkian 9.4. Geneerisyys ja periytymisen rajoitukset 278 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . virheikkuna.hh . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1 #ifndef VIRHEIKKUNA HH 2 #define VIRHEIKKUNA HH 3 4 #include <string> 5 6 class VirheIkkunaToteutus; // ennakkoesittely 7 8 class VirheIkkuna 9 { 10 public: 11 VirheIkkuna( VirheIkkunaToteutus const* p ); 12 virtual ~VirheIkkuna(); 13 14 // Na¨ytta¨a¨ viestin ikkunassa ja odottaa ka¨ytta¨ja¨n kuittausta 15 void NaytaIlmoitus( const std::string& viesti ) const; 16 17 private: 18 VirheIkkunaToteutus const* imp ; // osoitin toteutukseen 19 }; 20 #endif . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . virheikkuna.cc . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1 // Julkisen rajapinnan esittely 2 #include \"virheikkuna.hh\" 3 4 // Esittely rajapinnan toteutusten kantaluokasta 5 #include \"virheikkunatoteutus.hh\" 6 7 VirheIkkuna::VirheIkkuna( VirheIkkunaToteutus const* p ) : imp (p) {}; 8 VirheIkkuna::~VirheIkkuna() { imp = 0; } 9 10 // Palveluiden siirto toteutukselle: 11 void VirheIkkuna::NaytaIlmoitus( string const& viesti ) const 12 { 13 imp ->NaytaIlmoitus( viesti ); 14 } LISTAUS 9.1: Virhetiedoterajapinnan esittely ja toteutus 9.4. Geneerisyys ja periytymisen rajoitukset 279 1 #ifndef VIRHEIKKUNATOTEUTUS HH 2 #define VIRHEIKKUNATOTEUTUS HH 3 4 #include <string> 5 6 class VirheIkkunaToteutus 7 { 8 public: 9 VirheIkkunaToteutus(); 10 virtual ~VirheIkkunaToteutus(); 11 12 virtual void NaytaIlmoitus( const std::string& viesti ) const = 0; 13 }; 14 #endif LISTAUS 9.2: Toteutusten kantaluokka, virheikkunatoteutus.hh 1 // virheikkunaversiot.hh 2 #include \"virheikkunatoteutus.hh\" 3 4 class Gtk Virheikkuna : public VirheIkkunaToteutus { ... 5 6 class Motif Virheikkuna : public VirheIkkunaToteutus { ... LISTAUS 9.3: Toteutus virheikkunoista, virheikkunaversiot.hh 1 #include \"virheikkuna.hh\" 2 #include \"virheikkunaversiot.hh\" ... 3 VirheIkkuna* UusiVirheIkkuna() 4 { 5 VirheIkkunaToteutus* w; 6 if( konfiguraatio.kaytossaMotif() ) { 7 w = new Motif VirheIkkuna(); 8 } else { // GTK 9 w = new Gtk VirheIkkuna(); 10 } 11 return new VirheIkkuna( w ); 12 } LISTAUS 9.4: Eri virheikkunatoteutusten valinta 9.4. Geneerisyys ja periytymisen rajoitukset 280 pysyva¨ osa, kun taas aliluokat voivat jokainen edustaa rajapinnal- taan tai toteutukseltaan erilaista “erikoistettua” versiota kantaluokan kuvaamasta asiasta. Tyypillisesti ta¨lla¨ tavalla toteutettu yleiska¨ytto¨i- nen kirjasto tarjoaa valmiina luokkahierarkian yla¨osan (pysyva¨t osat), joista jokainen sovelluskohde sitten periytta¨a¨ itselleen sopivat aliluo- kat, joissa toteutetaan vaihtelevat osat. Aliluvussa 6.11 mainitut so- velluskehykset ovat hyva¨ esimerkki periytymisen ka¨ytta¨misesta¨ ta¨lla¨ tavoin. Monissa oliokielissa¨ periytyminen onkin pa¨a¨asiallinen yleis- ka¨ytto¨isyyden mekanismi. Periytymisella¨ on kuitenkin muutama ominaisuus, joka rajoittaa sen ka¨ytto¨kelpoisuutta yleiska¨ytto¨isyydessa¨. Ta¨rkein niista¨ on se, et- ta¨ kaikki aliluokat periva¨t kantaluokan rajapinnan ja toteutukset ko- konaisuudessaan ja ta¨sma¨lleen samanlaisena. Ta¨ma¨ edellytta¨a¨, etta¨ yleiska¨ytto¨isen komponentin pysyvyys koskee kokonaisia palveluita (ja¨senfunktioita) parametreineen ja paluuarvoineen. Na¨in ei kuiten- kaan aina ole. Ongelmaa on ehka¨ helpoin kuvata esimerkin avulla. Oletetaan, et- ta¨ halutaan koodata periytymista¨ ka¨ytta¨en yleiska¨ytto¨inen taulukko- tyyppi, suunnilleen samanlainen kuin C++:n vector-tyyppi. Ta¨llaises- sa taulukkotyypissa¨ pysyvina¨ osina ovat itse vektorin toteutus ja ka¨- sittely, vaihtelevana osana puolestaan on taulukon alkioiden tyyppi, joka luonnollisesti riippuu ka¨ytto¨kohteesta. Periytymista¨ ka¨ytta¨en ta¨- ma¨ tarkoittaa, etta¨ suunnitellaan kaikille taulukoille yhteinen kanta- luokka Taulukko, johon koodataan kaikille taulukoille yhteiset asiat. Ta¨sta¨ kantaluokasta sitten tarvittaessa periyteta¨a¨n kaikki erilaiset tau- lukot omiksi aliluokikseen, joihin sitten koodataan taulukon alkiotyy- pista¨ riippuvat asiat. Kuva 9.8 seuraavalla sivulla na¨ytta¨a¨ na¨in synty- va¨n luokkahierarkian. Seuraavaksi ta¨ha¨n luokkahierarkiaan pita¨isi lisa¨ta¨ tarvittavat jul- kisen rajapinnan palvelut. Kantaluokkaan Taulukko kuuluvat kaikil- le taulukoille ta¨sma¨lleen samanlaiset palvelut, kun taas toisistaan eroavat ja¨senfunktiot toteutetaan kussakin aliluokassa. Yksinkertai- suuden vuoksi ta¨ssa¨ keskityta¨a¨n pelka¨sta¨a¨n palveluihin annaKoko, lisaaLoppuun, poistaLopusta ja haeAlkio. Na¨ma¨ vastaavat vector- tyypin operaatioita size, push back, pop back ja at. Na¨ista¨ palveluista annaKoko ei selva¨sti milla¨a¨n tavalla riipu alkioi- den tyypista¨, se kun vain palauttaa niiden lukuma¨a¨ra¨n. Sen voisi siis laittaa yhteiseen kantaluokkaan. Samalla tavoin poistaLopusta pois- taa taulukon viimeisen alkion tyypista¨ riippumatta, joten sen voisi ai- 9.4. Geneerisyys ja periytymisen rajoitukset 281 Taulukko {abstract} TaulukkoInt TaulukkoString ... KUVA 9.8: Yleiska¨ytto¨isen taulukon toteutus periytta¨ma¨lla¨ nakin rajapintansa puolesta ma¨a¨ritella¨ kantaluokassa. Sen sijaan pal- velu lisaaLoppuun ottaa parametrikseen lisa¨tta¨va¨n alkion, joten pal- velun parametrin tyyppi riippuu alkion tyypista¨. Vastaavasti palvelu haeAlkio palauttaa paluuarvonaan halutun alkion, joten sen paluuar- von tyyppi riippuu alkiotyypista¨. Na¨ma¨ palvelut (ja kaikki muut vas- taavat palvelut) pita¨isi toteuttaa erikseen jokaisessa aliluokassa, jotta rajapinnan tyypit voivat erota toisistaan. Kun ta¨ta¨ jaottelua jatketaan, huomataan etta¨ varsin suuressa osas- sa taulukon rajapintaa esiintyy alkion tyyppi jossain muodossa, joten varsin suuri osa palveluista siirtyisi aliluokkiin ja kantaluokan Tau- lukko pysyva¨ osa ja¨isi varsin pieneksi. Yleiska¨ytto¨isyyden kannalta ta¨ma¨ on tietysti huono asia, koska uuden taulukkotyypin luomisessa ta¨ytyisi ma¨a¨ritella¨ suuri joukko alkiotyypista¨ riippuvia palveluja, ja valmiina periytyva¨n toiminnallisuuden ma¨a¨ra¨ ja¨isi pieneksi. Yksi ratkaisu ta¨ha¨n ongelmaan on lisa¨ta¨ itse sovellusalueen py- syvyytta¨. Vaaditaan, etta¨ kaikkien mahdollisten alkiotyyppien on pe- riydytta¨va¨ yhteisesta¨ kantaluokasta Alkio. Ta¨ma¨ sina¨nsa¨ mita¨tto¨ma¨lta¨ tuntuva lisa¨vaatimus muuttaa tilannetta palveluiden kannalta oleelli- sesti. Nyt palvelun lisaaLoppuun parametrin tyyppi voikin olla Alkio, jolloin sama palvelu voi ottaa vastaan minka¨ tahansa luokasta Alkio periytetyn alkion. Samoin palvelun haeAlkio paluuarvon tyyppi voi olla Alkio, jol- loin sama palvelu pystyy palauttamaan minka¨ tahansa tyyppisen al- kion. Ta¨lla¨ tavoin ajateltuna kaikki (tai ainakin la¨hes kaikki) taulukon palvelut saadaan toteutettua ta¨sma¨lleen samalla rajapinnalla, ja ne 9.4. Geneerisyys ja periytymisen rajoitukset 282 voidaan toteuttaa yhteisessa¨ kantaluokassa. Ta¨llo¨in periytettyja¨ tau- lukkoluokkia ei edes tarvita, koska vaihtelevuutta ei ena¨a¨ ole ja¨ljella¨! Ta¨ma¨n tyyppista¨ ratkaisua ka¨yteta¨a¨n esimerkiksi Java-kielen alku- pera¨isissa¨ sa¨ilio¨luokissa. Javassa kaikki luokat ovat periytettyina¨ yh- dessa¨ suuressa luokkahierarkiassa, jonka huipulla on kaikkien luok- kien kantaluokka Object. Na¨in on luonnollista, etta¨ Javan taulukko- luokassa alkioiden tyyppi on aina Object, joten sama taulukkotyyp- pi kelpaa minka¨ tahansa tyyppisten alkioiden tallettamiseen.] Yleis- ka¨ytto¨isyyden kannalta ta¨llainen tilanne on hyva¨, koska sama luokka kelpaa sellaisenaan kaikkiin taulukoihin. Yhdessa¨ yleiska¨ytto¨isessa¨ taulukkoluokassa on myo¨s huonot puo- lensa. Taulukkoa luotaessa ei ena¨a¨ ilmoiteta, minka¨ tyyppisia¨ alkioita taulukkoon on tarkoitus tallettaa. Koska palvelun lisaaLoppuun para- metriksi kelpaa mika¨ tahansa yhteisesta¨ kantaluokasta periytetty olio, voidaan samaan taulukkoon tallettaa sekaisin erityyppisia¨ alkioita, vaikkapa kokonaislukuja, merkkijonoja ja pa¨iva¨yksia¨. Joskus ta¨llainen heterogeeninen useantyyppisia¨ alkioita sisa¨lta¨va¨ taulukko on ka¨teva¨, mutta silla¨ on haittapuolensa. Samoin kuin lisaaLoppuun-palvelun parametri, palvelun haeAlkio paluutyyppi on myo¨s yhteista¨ kantaluokkatyyppia¨. Ta¨ma¨ tarkoittaa, etta¨ kun taulukosta haetaan alkio, ei ka¨a¨nta¨ja¨ pysty paluutyypin pe- rusteella sanomaan, minka¨ tyyppinen alkio taulukosta saatiin, koska paluuarvona on osoitin tai viite yhteiseen kantaluokkaan. Ka¨yta¨nno¨s- sa¨ taulukon ka¨ytta¨ja¨n ta¨ytyy itse tieta¨a¨, minka¨ tyyppinen alkio taulu- kosta olisi pita¨nyt tulla, ja tehda¨ tarvittava tyyppimuunnos, jotta alkio saadaan oikeantyyppisen osoittimen tai viitteen pa¨a¨ha¨n. Yhteisesta¨ kantaluokasta aiheutuvat ominaisuudet johtavat sii- hen, etta¨ ka¨a¨nta¨ja¨ ei pysty tekema¨a¨n taulukon ka¨yto¨sta¨ tarvitta- via tyyppitarkastuksia, vaan ne ja¨a¨va¨t taulukon ka¨ytta¨ja¨n vastuulle. Javan tyyppimuunnoksissa kantaluokasta aliluokkaan tehda¨a¨n tarkas- tus siita¨, etta¨ kantaluokkaviitteen pa¨a¨ssa¨ on oikeantyyppinen olio. Sa- moin olisi mahdollista lisa¨ta¨ taulukkoluokkaan tarkastus siita¨, etta¨ kaikki taulukkoon lisa¨tta¨va¨t alkiot ovat keskena¨a¨n samaa tyyppia¨. Ta¨llaiset tarkastukset ovat kuitenkin va¨ista¨ma¨tta¨ ajoaikaisia eiva¨t- ka¨ ka¨a¨nno¨saikaisia. Niinpa¨ helposta yleiska¨ytto¨isyydesta¨ on joudut- tu maksamaan se hinta, etta¨ entista¨ suurempi osa ohjelman virheis- ta¨ huomataan vasta testausvaiheessa (jos silloinkaan) sen sijaan, etta¨ . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ]Tarkalleen ottaen ta¨ma¨ ei ole aivan totta, koska Javan perustyypit kuten int eiva¨t ole olioita eiva¨tka¨ na¨in ollen Object-luokasta periytta¨ma¨lla¨ luotuja. 9.5. C++: Toteutuksen geneerisyys: mallit (template) 283 ka¨a¨nta¨ja¨ ilmoittaisi niista¨ jo ohjelmaa ka¨a¨nnetta¨essa¨. Samoin tauluk- kotyypin ka¨yto¨sta¨ on tullut tyo¨la¨a¨mpa¨a¨ tarvittavien tyyppimuunnos- ten takia. On syyta¨ korostaa, etta¨ edella¨ mainitut ongelmat tulevat esil- le etupa¨a¨ssa¨ ka¨a¨nno¨saikaisesti tyypitetyssa¨ kielessa¨ kuten Javassa. Smalltalk-kielen taulukkotyyppi on samantapainen kuin Javan. Kos- ka kielessa¨ ei kuitenkaan ole ollenkaan ka¨a¨nno¨saikaista tyypitysta¨, ovat kaikki tyyppitarkastukset joka tapauksessa aina ajoaikaisia, joten Smalltalkissa taulukkoluokka ei milla¨a¨n lailla huononna kielen tyyp- piturvallisuutta (tai sen puutetta). Pohjimmiltaan koko ongelman syy on se, etta¨ vaikka erityyppisilla¨ taulukoilla onkin todella paljon yhteista¨, ei ta¨ma¨ pysyvyys muodos- ta yhtena¨ista¨ kokonaisuutta, jonka saisi luontevasti toteutettua kanta- luokkana. Taulukon alkioiden tyyppi vaikuttaa taulukon rajapintaan ja toteutukseen kauttaaltaan, eika¨ sen vaikutusta pysty eliminoimaan periytymista¨ ka¨ytta¨en ilman, etta¨ tyyppiturvallisuudesta ja ka¨ytto¨mu- kavuudesta joudutaan tinkima¨a¨n. Ta¨llaisten ongelmien vuoksi pysyvyyden ja vaihtelevuuden hal- lintaan tarvitaan jokin toinenkin mekanismi, joka antaa perinteises- ta¨ periytymisesta¨ poikkeavan tavan yleiska¨ytto¨isyyden hallintaan. C++:ssa ta¨ma¨ mekanismi on mallit (templates), jota ka¨sitella¨a¨n seu- raavassa aliluvussa. Enemma¨n tai va¨hemma¨n samantapaisia meka- nismeja on joissain muissakin kielissa¨. Muun muassa Java-kieleen tu- li template-mekanismia muistuttava generics-mekanismi version 5.0 myo¨ta¨. 9.5 C++: Toteutuksen geneerisyys: mallit (tem- plate) Suunnittelumallien tarkoituksena on tarjota yleisia¨ suunnittelurat- kaisuja, joista ohjelmiston suunnittelija sitten tuottaa oman suunni- telmansa korvaamalla mallissa esiintyva¨t roolit ja luokat oman oh- jelmistonsa ka¨sitteilla¨. Samaan tapaan myo¨s ohjelmiston toteutusvai- heessa on usein tilanteita, joissa sama geneerinen koodirunko olisi ka¨yta¨nno¨llinen useassa kohtaa ohjelmaa. Periaatteessa jo parametreja saavat funktiot ovat a¨a¨rimma¨isen yk- sinkertainen esimerkki ta¨llaisesta geneerisyydesta¨. Funktioissa “auki ja¨tettyja¨” tietoja edustavat parametrit, joita funktion kirjoittaja ka¨ytta¨a¨ 9.5. C++: Toteutuksen geneerisyys: mallit (template) 284 funktion koodissa varsinaisten arvojen sijaan. Funktiota kutsuttaessa na¨ille parametreille sitten annetaan varsinaiset arvot. Funktioiden tarjoama toteutuksen geneerisyys rajoittuu C++:ssa kuitenkin vain parametrien arvoihin. Kaikki muut asiat, kuten muut- tujien tyypit ja taulukoiden koot, on kiinnitetta¨va¨ jo koodausvaihees- sa. Usein periaatteessa yleiska¨ytto¨isessa¨ koodissa kuitenkin myo¨s muuttujien, parametrien yms. tyypit vaihtelevat. Esimerkiksi luku- jen keskiarvo lasketaan samalla tavalla riippumatta siita¨, ovatko luvut tyypilta¨a¨n int, double vai kenties jokin itse luokkana ma¨a¨ritelty luku- tyyppi. Samoin itse tehty taulukkotyyppi on rakenteeltaan samanlai- nen riippumatta siita¨, millaisia otuksia taulukkoon on tarkoitus tal- lettaa. Niissa¨ ohjelmointikielissa¨, joissa ei ole vahvaa (ka¨a¨nno¨saikaista) tyypitysta¨, ei ole myo¨ska¨a¨n na¨ita¨ ongelmia. Ilman tyypitysta¨ sama ohjelmakoodi pystyy laskemaan keskiarvon minka¨ tahansa tyyppisil- le arvoille ja taulukossa voi olla mita¨ asioita tahansa. Samalla tietysti ja¨a¨da¨a¨n ilman ka¨a¨nno¨saikaisia tyyppitarkastuksia ja mahdolliset vir- heet havaitaan vasta ajoaikana. Esimerkkeja¨ ta¨llaisista kielista¨ ovat Smalltalk, Python ja Scheme. 9.5.1 Mallit ja tyyppiparametrit Ka¨a¨nno¨saikaisesti tyypitetyissa¨ kielissa¨, kuten C++:ssa, ta¨ytyy kaikkien parametrien ja muuttujien tyyppien olla selvilla¨ jo ohjelmaa ka¨a¨nnet- ta¨essa¨, joten tyyppien geneerisyys ei onnistu ilman lisa¨kikkoja. Mallit (template) ovat C++:n tapa kirjoittaa yleiska¨ytto¨isia¨ funktioita (funktio- mallit, function template) ja luokkia (luokkamallit, class template). Malleissa tyyppeja¨ ja joitain muitakin asioita voidaan ja¨tta¨a¨ ma¨a¨ra¨a¨- ma¨tta¨ (ta¨sta¨ tarkemmin aliluvussa 9.5.5). Auki ja¨tetyt asiat “sidotaan” vasta myo¨hemmin ka¨yto¨n yhteydessa¨. Ta¨lla¨ tavoin saadaan sa¨ilyte- tyksi C++:n vahva ka¨a¨nno¨saikainen tyypitys, mutta annetaan silti mah- dollisuus kirjoittaa geneerista¨ koodia. C++:n kirjaston vector on esimerkki luokkamallista. C++:ssa jokaisen taulukon sisa¨lta¨mien alkioiden tyyppi ta¨ytyy tieta¨a¨ jo ka¨a¨nno¨saikana. Kuitenkin jokaisen taulukon toteutus on alkioiden tyyppia¨ lukuunot- tamatta sama. Niinpa¨ vector itsessa¨a¨n on vasta luokkamalli, joka ker- too taulukoiden yleisen toteutuksen ja ja¨tta¨a¨ auki alkioiden tyypin. Vektoreita ka¨ytetta¨essa¨ ohjelmoija sitten kiinnitta¨a¨ alkioiden tyypin ja luo kokonaislukuvektoreita (vector<int>), liukulukuvektoreita 9.5. C++: Toteutuksen geneerisyys: mallit (template) 285 (vector<double>) tai pa¨iva¨ysosoitinvektoreita (vector<Paivays*>). Auki ja¨tetyt asiat selvia¨va¨t jo ka¨a¨nno¨saikana, mutta kuitenkin vasta vektoreiden ka¨ytto¨tilanteessa, ei itse vektorin ohjelmakoodissa. Funktioissa parametrien nimia¨ ka¨yteta¨a¨n koodissa korvaamaan parametrien “puuttuvia” arvoja, jotka selvia¨va¨t kutsun yhteydessa¨. Samalla tavoin malleissa auki ja¨tetyt tyypit nimeta¨a¨n ja ohjelmakoo- dissa ka¨yteta¨a¨n na¨ita¨ nimia¨ auki ja¨tettyjen tyyppien tilalla. Yhdes- sa¨ mallissa voi auki ja¨tettyja¨ tyyppeja¨ olla useita, aivan samoin kuin funktio voi saada useita parametreja. Koska funktion parametrit ja mallin auki ja¨tetyt tyypit muistuttavat suuresti toisiaan, puhutaankin usein mallien tyyppiparametreista. Koska mallien koodissa on auki ja¨tettyja¨ asioita, ka¨a¨nta¨ja¨ ei pysty mallin tavatessaan tuottamaan siita¨ koodia. Vasta kun kyseista¨ mallia ka¨yteta¨a¨n ohjelmassa ja annetaan tyyppiparametreille arvot, ka¨a¨nta¨ja¨ ka¨a¨nta¨a¨ mallista koodin, jossa mallin auki ja¨tetyt tyypit on korvattu annetuilla tyypeilla¨. Ta¨ta¨ kutsutaan mallin instantioimiseksi (instan- tiation). Ka¨a¨nta¨ja¨ ka¨a¨nta¨a¨ mallista oman versionsa jokaista sellaista ka¨ytto¨kertaa kohti, jossa tyyppiparametrien arvot ovat erilaiset. Na¨in mallien ka¨ytta¨minen ei yleensa¨ va¨henna¨ tuotetun konekoodin ma¨a¨- ra¨a¨, vaikka mallit tietysti va¨henta¨va¨tkin ohjelmoijan koodausurakkaa. Mallien syntaksi C++:ssa on yksinkertainen: template<typename tyyppiparam1, typename tyyppiparam2, . . .> // Ta¨ha¨n normaali funktion tai luokan ma¨a¨rittely Malli alkaa avainsanalla template, jonka ja¨lkeen nimeta¨a¨n kulmasul- keissa kaikki mallissa auki ja¨tetyt tyyppiparametrit. Ta¨ma¨n ja¨lkeen seuraa itse mallin koodi, joka funktiomallin tapauksessa on tavalli- nen funktion ma¨a¨rittely ja luokkamallin tapauksessa luokan. Koodis- sa tyyppiparametreja voi ka¨ytta¨a¨ aivan kuin normaaleja C++:n tyyp- peja¨. C++ sallii myo¨s syntaksin, jossa avainsanan typename tilalla ka¨y- teta¨a¨n tyyppiparametrilistassa avainsanaa class. Ta¨ma¨ “vanha” syn- taksi on ta¨ysin identtinen ylla¨mainitun kanssa, mutta sita¨ tuskin kan- nattaa ka¨ytta¨a¨, koska auki ja¨tetyt tyypit voivat malleissa aina olla mita¨ tahansa tyyppeja¨, eiva¨t va¨ltta¨ma¨tta¨ luokkia. Seuraavassa esitella¨a¨n C++:n template-mekanismin perusperiaat- teet ja niita¨ ka¨yteta¨a¨n jonkin verran hyva¨ksi mm. luvussa 10. Tem- plate-mekanismi on kuitenkin varsin monimutkainen ja -puolinen niin syntaksiltaan kuin ka¨ytto¨tavoiltaan ja rajoituksiltaankin. C++:n 9.5. C++: Toteutuksen geneerisyys: mallit (template) 286 geneerisyydesta¨ ja geneerisesta¨ ohjelmoinnista kiinnostuneen kan- nattaakin tutustua esimerkiksi kirjoihin “C++ Templates — The Com- plete Guide” [Vandevoorde ja Josuttis, 2003] ja “Modern C++ De- sign” [Alexandrescu, 2001]. 9.5.2 Funktiomallit Funktiomallit (function template) ovat geneerisia¨ malleja, joista ka¨a¨n- ta¨ja¨ voi generoida eri tyypeilla¨ toimivia funktioita. Kaikilla mallis- ta generoiduilla funktioilla on sama nimi, mutta ne toimivat yleensa¨ erityyppisilla¨ parametreilla. Listauksessa 9.5 on funktiomallin min toteutus. Rivi 1 kertoo, etta¨ kyseessa¨ on malli ja etta¨ siina¨ on yksi auki ja¨tetty tyyppi, jota mallin koodissa kutsutaan nimella¨ T. Riveilla¨ 2–12 on sitten funktion ma¨a¨- rittely, jossa tyyppiparametria T ka¨yteta¨a¨n kuin mita¨ tahansa tyyppia¨ kertomaan, etta¨ mallista luodut funktiot ottavat kaksi samantyyppis- ta¨ parametria ja palauttavat viela¨ samaa tyyppia¨ olevan paluuarvon. Tyyppia¨ T ka¨yteta¨a¨n myo¨s funktion rungossa tuloksen sisa¨lta¨va¨n pai- kallisen muuttujan luomiseen. Kun funktiomalli on ma¨a¨ritelty, voidaan siita¨ luoda todellisia funktioita yksinkertaisesti kutsumalla niita¨. Mika¨li kaikki tyyppipa- rametrit esiintyva¨t funktiomallin parametrilistassa, ka¨a¨nta¨ja¨ osaa au- tomaattisesti pa¨a¨tella¨ kutsusta tyyppiparametrien arvot. Esimerkiksi kutsusta min(1,2) ka¨a¨nta¨ja¨ pa¨a¨ttelee, etta¨ T:n on oltava int. Sen ja¨l- 1 template <typename T> // Tai template <class T> (identtinen) 2 T min(T p1, T p2) 3 { 4 T tulos; 5 if (p1 < p2) 6 { 7 tulos = p1; 8 } else { 9 tulos = p2; 10 } 11 return tulos; 12 } LISTAUS 9.5: Parametreista pienemma¨n palauttava funktiomalli 9.5. C++: Toteutuksen geneerisyys: mallit (template) 287 keen ka¨a¨nta¨ja¨ ka¨a¨nta¨a¨ automaattisesti funktiomallista koodin, jossa T on korvattu tyypilla¨ int, ja kutsuu ta¨ta¨ koodia. Vastaavasti kutsun min(2.3, 5.7) na¨hdessa¨a¨n ka¨a¨nta¨ja¨ tuottaa funktiomallista koodin, jossa T on double (liukulukuvakiot ovat C++:ssa tyyppia¨ double). Vaikka jokaisesta funktiomallin kutsusta periaatteessa tuotetaan- kin oma koodinsa, nykyiset ka¨a¨nta¨ja¨t ovat niin a¨lykka¨ita¨, etta¨ ne osaa- vat samantyyppisissa¨ kutsuissa ka¨ytta¨a¨ yhteista¨ koodia. Jos ka¨a¨nta¨ja¨ on jo kutsun min(1,2) yhteydessa¨ tuottanut funktiomallista int-tyyp- pia¨ ka¨ytta¨va¨n toteutuksen, se kutsuu ta¨ta¨ samaa toteutusta myo¨hem- min tavatessaan kutsun min(5,9). Na¨in mallien ka¨ytto¨ ei kasvata oh- jelmakoodin kokoa tarpeettomasti. Vaikka ka¨a¨nta¨ja¨ osaakin pa¨a¨tella¨ tyyppiparametreille arvot kutsun parametrien tyypeista¨, voidaan kutsun yhteydessa¨ myo¨s antaa tyyp- piparametreille eksplisiittiset arvot kirjoittamalla kyseiset arvot kul- masulkeissa mallin nimen pera¨a¨n. Esimerkiksi kutsu float f = min<float>(3.2, 6); ka¨skee ka¨a¨nta¨ja¨a¨ tuottamaan koodin, jossa T on float, vaikka para- metrien tyypit ovatkin double ja int. Ka¨a¨nta¨ja¨ tuottaa mallista min- funktion, joka ottaa parametreikseen kaksi float-arvoa ja palauttaa myo¨s float-arvon. Koska kutsussa annetut parametrit ovat eri tyyp- pia¨, sovelletaan niihin implisiittisia¨ tyyppimuunnoksia aivan kuten normaalistikin funktioita kutsuttaessa. Ta¨llainen tyyppiparametrien eksplisiittinen ma¨a¨ra¨a¨minen on joskus hyo¨dyllista¨, kun halutaan pa- kottaa juuri tietynlainen toteutus mallista. Se on myo¨s va¨ltta¨ma¨to¨nta¨, jos funktiomallissa on tyyppiparametreja, jotka eiva¨t ka¨y ilmi mallin parametrilistasta (ta¨llo¨in ka¨a¨nta¨ja¨ ei tietenka¨a¨n voi itse pa¨a¨tella¨ nii- den arvoja). 9.5.3 Luokkamallit Luokkamalli (class template) toimii mallina luokille, jotka ovat muu- ten samanlaisia, mutta joissa jotkin tyypit voivat erota toisistaan. Lis- taus 9.6 seuraavalla sivulla esittelee luokkamallin Pari, joka kuvaa mallin luokille, joihin voi tallettaa kaksi mielivaltaista tyyppia¨ olevaa alkiota ja joista na¨ma¨ alkiot voi lukea kutsuilla annaEka ja annaToka. Mallissa on kaksi auki ja¨tettya¨ tyyppia¨ T1 ja T2, jotka esitella¨a¨n mallin alussa ja joita sitten ka¨yteta¨a¨n itse luokan ma¨a¨rittelyssa¨. 9.5. C++: Toteutuksen geneerisyys: mallit (template) 288 1 template <typename T1, typename T2> 2 class Pari 3 { 4 public: 5 Pari(T1 eka, T2 toka); 6 T1 annaEka() const; 7 T2 annaToka() const; ... 11 private: 12 T1 eka ; 13 T2 toka ; 14 }; LISTAUS 9.6: Tietotyypin “pari” ma¨a¨ritteleva¨ luokkamalli Itse Pari ei viela¨ ole luokka, vaan vasta malli kokonaiselle “perheelle” luokkia. Luokkamallista saadaan instantioitua todellinen luokka ma¨a¨ra¨a¨ma¨lla¨ tyyppiparametreille arvot. Ta¨ma¨ tapahtuu luet- telemalla tyyppiparametrien arvot kulmasulkeissa luokkamallin ni- men ja¨lkeen. Esimerkiksi rivi Pari<int, double> p(1, 3.2); tuottaa ensin luokkamallista luokan, jossa T1 on int ja T2 on vastaa- vasti double. Ta¨ma¨n ja¨lkeen ta¨sta¨ luokasta luodaan olio p. Ta¨ma¨ olio sisa¨lta¨a¨ sitten tyyppeja¨ int ja double olevat ja¨senmuuttujat, sen ja¨sen- funktion annaEka paluutyyppi on int ja niin edelleen. Olioiden luomisen lisa¨ksi luokkamallista voi instantioida luokan missa¨ tahansa kohtaa ohjelmaa. Syntaksi Pari<int, double> toimii luokkamallista luodun luokan nimena¨, ja sita¨ voi ka¨ytta¨a¨ missa¨ ta- hansa kuten tavallista luokan nimea¨. Esimerkiksi funktioesittely void f(Pari<int, int>& i, Pari<float, int*> d); esittelee tavallisen funktion, joka ottaa parametreikseen viitteen kaksi kokonaislukua sisa¨lta¨va¨a¨n pariin ja parin, johon on talletettu liukulu- ku ja osoitin kokonaislukuun. Jokainen erilainen luokkamallista luotu luokka on oma erillinen luokkansa, eika¨ silla¨ ole mita¨a¨n sukulaisuussuhdetta muihin samas- ta mallista luotuihin luokkiin, joissa tyyppiparametrit ovat erilaiset. Esimerkiksi edella¨ olleet luokat Pari<int,int> ja Pari<float,int*> 9.5. C++: Toteutuksen geneerisyys: mallit (template) 289 ovat ta¨ysin omia luokkiaan, eiva¨tka¨ niista¨ luodut oliot ole keskena¨a¨n vaihtokelpoisia. Na¨in ta¨ytyy tietysti ollakin, koska seka¨ luokkien ra- japinta etta¨ sisa¨inen toteutus eroavat toisistaan tyyppien osalta. Listaus 9.6 vasta esitteli luokan. Ta¨ma¨n lisa¨ksi ta¨ytyy luokka- mallille kirjoittaa viela¨ ja¨senfunktioiden toteutukset. Ta¨ma¨ tapahtuu C++:ssa niin, etta¨ ja¨senfunktiot kirjoitetaan ika¨a¨n kuin omiksi malleik- seen, ja jokaiseen ja¨senfunktion toteutukseen tulee oma template- ma¨a¨reensa¨ tyyppiparametreineen kaikkineen. Listauksessa 9.7 on esi- merkkina¨ kaksi Pari-luokkamallin ja¨senfunktion toteutusta. Ne alka- vat samanlaisella template-ma¨a¨reella¨ kuin itse luokkamallin esittely- kin. Lisa¨ksi niiden koodissa on ennen na¨kyvyystarkenninta :: “luo- kan nimena¨” merkinta¨ Pari<T1, T2>, jossa mallin tyyppiparametrit on “sijoitettu paikoilleen”. Luokkamallin ja¨senfunktiot ka¨ytta¨ytyva¨t kuten funktiomallit myo¨s siina¨ mielessa¨, etta¨ ka¨a¨nta¨ja¨ instantioi luokkamallista instan- tioidulle luokalle vain ne ja¨senfunktiot, joita luokan olioille todella kutsutaan. Normaalin luokan tapauksessahan ka¨a¨nta¨ja¨ ka¨a¨nta¨a¨ kaik- ki ja¨senfunktiot riippumatta siita¨, kutsutaanko niita¨. Ta¨ssa¨ suhteessa luokkamallit voivat jopa va¨henta¨a¨ tuotetun konekoodin ma¨a¨ra¨a¨, jos luokkamallissa on paljon ja¨senfunktioita, joita ei ka¨yteta¨. C++:ssa on myo¨s mahdollista kirjoittaa ja¨senfunktiomalleja (mem- ber function template). Ja¨senfunktiomalli muistuttaa muuten tavallis- ta funktiomallia, mutta se on ma¨a¨ritelty jonkin luokan ja¨senfunktiok- si. Niinpa¨ siihen pa¨teva¨t kaikki asiat, jotka on selitetty funktiomal- leista aliluvussa 9.5.2. Kaikki ka¨a¨nta¨ja¨t eiva¨t viela¨ (kesa¨lla¨ 2000) tue ja¨senfunktiomalleja, mutta uusimmissa¨ ka¨a¨nta¨ja¨versioissa ta¨ma¨ omi- naisuus jo yleensa¨ toimii. 1 template <typename T1, typename T2> 2 Pari<T1, T2>::Pari(T1 eka, T2 toka) : eka (eka), toka (toka) 3 { 4 } 5 6 template <typename T1, typename T2> 7 T1 Pari<T1, T2>::annaEka() const 8 { 9 return eka ; 10 } LISTAUS 9.7: Esimerkki luokkamallin Pari ja¨senfunktioista 9.5. C++: Toteutuksen geneerisyys: mallit (template) 290 Mielenkiintoinen yhdistelma¨ syntyy, kun luokkamallille ma¨a¨ritel- la¨a¨n ja¨senfunktiomalli. Ta¨llo¨in siis jo itse luokkamallissa on tietty ma¨a¨ra¨ auki ja¨tettyja¨ tyyppeja¨ ja sen ja¨senfunktiomallissa on na¨iden tyyppien lisa¨ksi viela¨ joukko omia auki ja¨tettyja¨ tyyppeja¨, jotka ma¨a¨- ra¨ytyva¨t ja¨senfunktiomallin kutsun yhteydessa¨. Koska ta¨ssa¨ yhdistel- ma¨ssa¨ on ika¨a¨n kuin malli mallin sisa¨lla¨, se on syntaksiltaan varsin erikoinen. Listaus 9.8 sisa¨lta¨a¨ esimerkin ta¨llaisesta luokkamallin ja¨- senfunktiomallista. Ta¨ma¨n ja¨senfunktiomallin avulla mihin tahansa pariin voi “summata” minka¨ tahansa tyyppisen toisen parin, kunhan vain parien alkiot voi laskea yhteen keskena¨a¨n. 9.5.4 Tyyppiparametreille asetetut vaatimukset Mallin ma¨a¨rittelyssa¨ tyyppiparametreille ei suoraan aseteta mita¨a¨n vaatimuksia vaan auki ja¨tetyt tyypit vain nimeta¨a¨n mallin alussa, ja sen ja¨lkeen niita¨ ka¨yteta¨a¨n itse mallin koodissa. On kuitenkin selva¨a¨, ettei mallille voi antaa tyyppiparametreiksi mita¨ tahansa. Eri tyyp- pien ja olioiden rajapinnat eroavat toisistaan, ja na¨in ollen myo¨s nii- den ka¨ytto¨ on erilaista. 1 template <typename T1, typename T2> 2 class Pari 3 { ... 9 template <typename T3, typename T4> 10 void summaa(Pari<T3, T4> const& toinenPari); ... 14 }; ... 27 template <typename T1, typename T2> 28 template <typename T3, typename T4> 29 void Pari<T1, T2>::summaa(Pari<T3, T4> const& toinenPari) 30 { 31 eka += toinenPari.annaEka(); 32 toka += toinenPari.annaToka(); 33 } LISTAUS 9.8: Luokkamallin sisa¨lla¨ oleva ja¨senfunktiomalli 9.5. C++: Toteutuksen geneerisyys: mallit (template) 291 C++:n malleissa ei milla¨a¨n tavalla erikseen kerrota, millainen ra- japinta tyyppiparametreihin sijoitettavilla todellisilla tyypeilla¨ tulisi olla. Mallille annetuilta tyypeilta¨ vaaditaan vain, etta¨ mallin koodin on ka¨a¨nnytta¨va¨, kun tyyppiparametrien tilalle sijoitetaan instantioin- nissa todelliset tyypit. Esimerkiksi listauksen 9.5 funktiomalli min vertailee parametre- jaan pienempi kuin -operaattorilla <. Na¨in ollen min-mallia voi ka¨yt- ta¨a¨ vain tyypeille, joiden arvoja voi vertailla <-operaattorilla. Samoin jos mallin koodi kutsuu auki ja¨tettya¨ tyyppia¨ olevalle oliolle jotain ja¨senfunktiota, voi kyseista¨ mallia ka¨ytta¨a¨ vain sellaisten tyyppien kanssa, joista kyseinen ja¨senfunktio lo¨ytyy. Vaikka edella¨mainittu periaate on sina¨nsa¨ hyvin yksinkertainen, se aiheuttaa helposti ongelmia mallien kirjoittamisessa, koska tyyp- piparametrien vaatimukset on piilotettu ja siroteltu mallin koodin se- kaan. Ta¨ma¨n vuoksi geneerisessa¨ ohjelmoinnissa (generic program- ming) pita¨isi ottaa aina huomioon seuraavat seikat: • Tyyppiparametreihin kohdistuvat vaatimukset tulisi aina doku- mentoida ja kera¨ta¨ yhteen paikkaan, jotta mallin ka¨ytta¨ja¨ pystyy helposti na¨kema¨a¨n, millaisten tyyppien kanssa mallia voi ka¨yt- ta¨a¨. • Jotta malli olisi mahdollisimman yleiska¨ytto¨inen, sen suunnit- telussa tulisi kiinnitta¨a¨ huomiota siihen, ettei malli vaadi tyyp- piparametreiltaan yhta¨a¨n enempa¨a¨ kuin on tarpeen. Edella¨ mainituista varsinkin ja¨lkimma¨inen kohta vaatii mallin kir- joittajalta huolellisuutta ja taitoa. C++:ssa implisiittiset tyyppimuun- nokset, arvonva¨litys yms. aiheuttavat sen, etta¨ sina¨nsa¨ viattomalta na¨ytta¨va¨ koodi saattaa kulissien takana tehda¨ varsin monimutkaisia asioita. Ta¨llo¨in mallin koodista tulee helposti sellaista, etta¨ se vaatii tyyppiparametreiltaan ominaisuuksia, joita ohjelmoija ei ole ollen- kaan suunnitellut. Esimerkiksi listauksen 9.5 sina¨nsa¨ viattoman na¨ko¨inen min-mal- li vaatii selva¨sti tyyppiparametriltaan pienemmyysvertailun. Sen li- sa¨ksi kuitenkin auki ja¨tettya¨ tyyppia¨ T olevat parametrit va¨liteta¨a¨n funktion sisa¨a¨n normaalia arvonva¨litysta¨ ka¨ytta¨en, joka vaatii kopio- rakentajan ka¨ytto¨a¨ (aliluku 7.3). Samoin paluuarvon palauttaminen tehda¨a¨n kopiorakentajalla. Koodissa on rivi T tulos;, joten tyypilta¨ T vaaditaan oletusrakentajaa. Kaiken kukkuraksi tulos-muuttujaan si- 9.5. C++: Toteutuksen geneerisyys: mallit (template) 292 joitetaan kahdessa kohtaa uusi arvo, joten malli vaatii sijoitusoperaat- torin olemassaolon. Na¨iden kaikkien vaatiminen rajoittaa min-mallin ka¨ytto¨a¨ aika lailla, koska oletusrakentaja ja sijoitus ovat operaatioita, joita la¨heska¨a¨n kaikille luokille ei haluta kirjoittaa. Pienella¨ suunnittelulla min-malli saadaan huomattavasti ka¨ytta¨ja¨- ysta¨va¨llisemma¨ksi. Ka¨ytta¨ma¨lla¨ viitteenva¨litysta¨ arvonva¨lityksen si- jaan pa¨a¨sta¨a¨n eroon kopiorakentajan vaatimisesta. Poistamalla sina¨n- sa¨ tarpeeton tulos-olio saadaan lisa¨ksi oletusrakentajan ja sijoitus- operaattorin ka¨ytto¨ poistettua. Listaus 9.9 sisa¨lta¨a¨ paremman version min-mallista. Sen koodi vaatii tyyppiparametrilta ena¨a¨ ainostaan pie- nempi kuin -operaattorin olemassaoloa. 9.5.5 Erilaiset mallien parametrit Ta¨ha¨n mennessa¨ mallien yhteydessa¨ on ka¨sitelty vain yksinkertaisia tyyppiparametreja. Mallien parametreissa on lisa¨ksi joitain lisa¨omi- naisuuksia, jotka joskus helpottavat mallien ka¨ytto¨a¨. Mallien oletusparametrit C++:ssa voidaan tavallisten funktioiden parametreille antaa oletusar- voja, jolloin funktiokutsun lopusta alkaen voi ja¨tta¨a¨ parametreja an- tamatta. Esimerkiksi funktioesittely void f(int i = 1, double j = 3.14}; 1 template <typename T> 2 T const& min(T const& p1, T const& p2) 3 { 4 if (p1 < p2) 5 { 6 return p1; 7 } 8 else 9 { 10 return p2; 11 } 12 } LISTAUS 9.9: Parempi versio listauksen 9.5 funktiomallista 9.5. C++: Toteutuksen geneerisyys: mallit (template) 293 kertoo, etta¨ funktiolla f on kaksi parametria, joista ensimma¨isella¨ on oletusarvo 1 ja toisella 3.14. Funktiota voi kutsua kolmella eri tavalla: • Kutsussa f(5, 0.0) ei ole mita¨a¨n ihmeellista¨, ja i saa arvon 5 ja j:lle tulee arvo 0.0. • Kutsu f(3) aiheuttaa sen, etta¨ i saa arvon 3 ja j:lle ka¨yteta¨a¨n oletusarvoa 3.14. • Kutsu f() antaa molemmille parametreille oletusarvon, eli i on 1 ja j on 3.14. Mallien tyyppiparametreille pa¨teva¨t samat sa¨a¨nno¨t. Tyyppipara- metrilla voi olla oletusarvo, jota ka¨yteta¨a¨n jos tyyppiparametria ei instantioinnin yhteydessa¨ anneta. Listaus 9.10 sisa¨lta¨a¨ luokkamallin Pari2, jonka ensimma¨inen tyyppiparametri on oletusarvoisesti int ja toisen tyyppiparametrin oletusarvo on sama kuin ensimma¨inen para- metri. Mallin voi nyt instantioida kolmella eri tavalla: • Syntaksilla Pari2<double, string> kaikki toimii kuten ennen- kin, eli tuloksena on liukuluku-merkkijono-pari. • Syntaksi Pari2<double> tuottaa parin, jossa parin molemmat arvot ovat liukulukuja. • Pari2<> tuottaa kokonaislukuparin. 1 template <typename T1 = int, typename T2 = T1> 2 class Pari2 3 { 4 public: 5 Pari2(T1 eka, T2 toka); 6 T1 annaEka() const; 7 T2 annaToka() const; ... 8 }; LISTAUS 9.10: Mallin oletusparametrit 9.5. C++: Toteutuksen geneerisyys: mallit (template) 294 Vakioparametrit Malleissa voi tyyppien lisa¨ksi ja¨tta¨a¨ auki myo¨s tiettyja¨ ka¨a¨nno¨saikai- sia vakioita, jotka ja¨a¨va¨t mallin parametreiksi tyyppiparametrien ta- paan. Auki ja¨tetta¨va¨ksi kelpaavia vakioita ovat • kokonaislukuvakiot ja luettelotyyppien arvot (enum) • osoitin globaaliin olioon tai funktioon • viite globaaliin olioon tai funktioon • osoitin ja¨senfunktioon tai -muuttujaan. Listauksessa 9.11 on luokkamalli MJono. Siita¨ luoduilla merkkijo- noluokilla on kiintea¨ maksimipituus, joka annetaan mallin paramet- rina. Mallille on lisa¨ksi ma¨a¨ra¨tty oletusarvoinen maksimipituus 80, jota ka¨yteta¨a¨n, jos maksimipituutta ei erikseen anneta. Koska esimer- kissa¨ jokainen eri maksimipituudella luotu merkkijonoluokka on oma erillinen luokkansa, eripituiset merkkijono-oliot eiva¨t kuulu samaan luokkaan eika¨ niita¨ na¨in ollen normaalisti voi esimerkiksi sijoittaa toisiinsa. Malliparametrit Viimeisena¨ mallissa voi ja¨tta¨a¨ auki myo¨s toisen mallin, joka annetaan vasta instantioinnin yhteydessa¨. Ta¨ma¨ mahdollisuus mallin malli- parametreihin (template template parameter) on varsin harvoin nor- maalikoodissa tarvittava ominaisuus, mutta se tekee mahdolliseksi varsin monimutkaisenkin geneerisen ohjelmoinnin. 1 template <unsigned long SIZE = 80> 2 class MJono 3 { 4 public: 5 MJono(char const* arvo); 6 char const* annaArvo() const; 7 private: 8 char taulukko[SIZE+1]; 9 }; 10 MJono<12> s1(\"Tuli ta¨yteen\"); LISTAUS 9.11: Malli, jolla on vakioparametri 9.5. C++: Toteutuksen geneerisyys: mallit (template) 295 Malliparametreja ei juuri ka¨sitella¨ ta¨ssa¨ teoksessa, mutta listauk- sessa 9.12 on esimerkki funktiomallista, jossa on ja¨tetty auki yksi kak- si tyyppiparametria saava malli seka¨ lisa¨ksi yksi tavallinen tyyppipa- rametri. Mallin summaa avulla voi nyt summata mista¨ tahansa kaksi- parametrisesta luokkamallista instantioituja olioita, kunhan luokka- mallin tyyppiparametrit ovat samat, ja se tarjoaa operaatiot annaEka ja annaToka. 9.5.6 Mallien erikoistus Mallit ovat varsin tehokas tapa kirjoittaa yleista¨ koodia, joka on tar- koitettu toimimaan tyypeista¨ riippumatta. Joskus tulee kuitenkin vas- taan tilanne, jossa juuri tietyn tyypin tapauksessa mallin koodi tu- lisikin toteuttaa eri tavalla. Syyna¨ ta¨ha¨n saattaa olla tehokkuus- tai tilaoptimointi, tai kenties kyseinen tyyppi eroaa jollain olennaisella tavalla muista tyypeista¨. Mallien erikoistus (template specialization) antaa mahdollisuu- den ta¨llaisiin erikoistapauksiin. Kun malli antaa jollekin asialle ylei- sen toteutuksen, erikoistukset ma¨a¨ritteleva¨t ta¨ha¨n poikkeuksia. Ka¨yt- ta¨ja¨n kannalta kaikki sa¨ilyy ennallaan, ja mallia voi ka¨ytta¨a¨ normaa- listi, mutta mallia instantioidessa ka¨a¨nta¨ja¨ saattaakin valita normaa- lin mallin koodin sijaan erikoistuksen tarjoaman koodin. Luokkamallien yhteydessa¨ listauksessa 9.6 ollut malli Pari on toteutettu siten, etta¨ sen molemmat alkiot on talletettu omiin ja¨- senmuuttujiinsa eka ja toka . Jos halutaan tehda¨ totuusarvopari Pari<bool,bool>, ta¨ma¨ toteutustapa tuhlaa muistia. Jokainen ja¨sen- 1 template < template <typename T1, typename T2> class X, typename S> 2 S summaa(X<S, S> const& x) 3 { 4 return x.annaEka() + x.annaToka(); 5 } 6 7 void kayta() 8 { 9 Pari<int, int> p(1, 2); 10 int tulos = summaa(p); 11 } LISTAUS 9.12: Mallin malliparametri 9.5. C++: Toteutuksen geneerisyys: mallit (template) 296 muuttuja vie va¨ltta¨ma¨tta¨ va¨hinta¨a¨n yhden tavun muistia, vaikka pe- riaatteessa kaksi totuusarvoa saisi helposti puristettua yhteenkin ta- vuun. Ta¨ma¨ tilaoptimointi voidaan toteuttaa Pari-mallin erikoistuk- sena. Listaus 9.13 sisa¨lta¨a¨ alkupera¨isen mallin erikoistuksen, jossa on vain yksi ja¨senmuuttuja ekaJaToka , johon molemmat totuusarvot voidaan tallettaa C++:n bittioperaatioita ka¨ytta¨en. Luokkamallien erikoistuksen syntaksissa ja¨teta¨a¨n template-avain- sanan ja¨lkeen kulmasulkeet tyhjiksi (merkkina¨ siita¨, etta¨ erikoistuk- sessa ei auki ja¨tettyja¨ tyyppeja¨ ena¨a¨ ole), ja ne tyypit, joita erikois- tus koskee, merkita¨a¨n mallin nimen ja¨lkeen kulmasulkeisiin. Lis- tauksessa on myo¨s yhden ja¨senfunktion erikoistuksen toteutus. Sii- na¨ template-avainsanaa ei tarvita ollenkaan vaan erikoistuksen tyypit merkita¨a¨n suoraan luokan nimen yhteyteen. Funktiomallin erikoistus on syntaksiltaan vastaava kuin luokka- mallinkin. Siina¨ template-avainsanan ja¨lkeiset kulmasulkeet ovat ja¨l- leen tyhja¨t, ja erikoistuksen kohteena olevat tyypit merkita¨a¨n kulma- sulkeisiin funktiomallin nimen ja¨lkeen. Jos kaikki erikoistuksen tyyp- piparametrien arvot voi pa¨a¨tella¨ parametrilistan avulla, funktion ni- men ja¨lkeisen tyyppilistan voi ja¨tta¨a¨ halutessaan pois. Listaus 9.14 seuraavalla sivulla sisa¨lta¨a¨ min-mallin erikoistuksen pa¨iva¨ysolioille. Luokkamallien tapauksessa C++ sallii viela¨ luokkamallin osittais- 1 template <> 2 class Pari<bool, bool> 3 { 4 public: 5 Pari(bool eka, bool toka); 6 bool annaEka() const; 7 bool annaToka() const; 8 // summaa() puuttuu erikoistuksesta! 9 private: 10 unsigned char ekaJaToka ; // Sa¨a¨sta¨a¨ muistia 11 }; 12 13 bool Pari<bool, bool>::annaEka() const 14 { 15 return (ekaJaToka & 1) != 0; 16 } LISTAUS 9.13: Luokkamallin Pari erikoistus totuusarvoille 9.5. C++: Toteutuksen geneerisyys: mallit (template) 297 31 template<> 32 Paivays const& min<Paivays>(Paivays const& p1, Paivays const& p2) 33 { 34 if (p1.paljonkoEdella(p2) > 0) 35 { 36 return p2; 37 } else { 38 return p1; 39 } 40 } LISTAUS 9.14: Funktiomallin min erikoistus pa¨iva¨yksille erikoistuksen (class template partial specialization). Ta¨ssa¨ varsin harvoin tarvitussa mekanismissa osa luokkamallin tyyppiparamet- reista sidotaan tiettyihin arvoihin mutta lopputuloksessa on viela¨ avoimia tyyppeja¨. Listaus 9.15 na¨ytta¨a¨ Pari-mallin osittaiserikoistuk- sen, jota ka¨yteta¨a¨n, kun molemmat parin tyyppiparametrit ovat sa- mat. 9.5.7 Mallien ongelmia ja ratkaisuja C++:n mallien varsin omalaatuisen syntaksin lisa¨ksi mallien suunnit- teleminen on varsin vaativaa puuhaa. Geneerinen ohjelmointi itses- sa¨a¨n on hankalaa, ja useimpien C++-ka¨a¨nta¨jien la¨hes lukukelvottomat virheilmoitukset eiva¨t mitenka¨a¨n auta asiaa. Ta¨ma¨n lisa¨ksi itse mal- 1 template <typename T> 2 class Pari<T, T> 3 { 4 public: 5 Pari(T eka, T toka); 6 T annaEka() const; 7 T annaToka() const; 8 private: 9 T alkiot [2]; 10 }; 11 template <typename T> 12 T Pari<T, T>::annaEka() const 13 { return alkiot [0]; } LISTAUS 9.15: Luokkamallin Pari osittaiserikoistus 9.5. C++: Toteutuksen geneerisyys: mallit (template) 298 lien koodaamisessa on tiettyja¨ C++-riippuvia asioita, jotka on hyva¨ tie- ta¨a¨. Ta¨ha¨n alilukuun on kera¨tty joitain ta¨llaisia seikkoja. Mallin koodin sijoittelu ja export Tavallisista funktioista poiketen ka¨a¨nta¨ja¨ ka¨a¨nta¨a¨ mallin koodin vas- ta instantioinnin yhteydessa¨, eli kun mallia ka¨yteta¨a¨n. Ka¨yta¨nno¨ssa¨ ta¨ma¨ aiheuttaa vaatimuksen, etta¨ ka¨a¨nta¨ja¨lla¨ ta¨ytyy olla tiedossaan mallin koodi silloin, kun mallia ka¨ytta¨va¨a¨ koodia ka¨a¨nneta¨a¨n. Normaalien funktioiden yhteydessa¨ ta¨ta¨ rajoitusta ei ole vaan pelkka¨ funktion esittely riitta¨a¨ funktion kutsumiseen. Ta¨ma¨ tekee mahdolliseksi sen, etta¨ funktioista vain esittelyt laitetaan otsikkotie- dostoihin, jotka sitten luetaan #include-komennolla jokaiseen tiedos- toon, jossa funktioita kutsutaan. Funktioiden varsinainen ma¨a¨rittely (koodi) voi sitten olla omassa kooditiedostossaan, joka voidaan ka¨a¨n- ta¨a¨ erikseen. Funktiota kutsuttaessa pelkka¨ funktion esittely riitta¨a¨ ka¨a¨nta¨ja¨lle funktiokutsun tekeva¨n koodin tuottamiseen (ta¨ta¨ ka¨sitel- tiin jo aiemmin aliluvussa 1.4, katso kuva 1.6 sivulla 41). Koska mallin koodin on oltava ka¨a¨nta¨ja¨n tiedossa mallia ka¨ytet- ta¨essa¨, normaalin mallin koodia ei voi ja¨tta¨a¨ omaan erikseen ka¨a¨nnet- ta¨va¨a¨n kooditiedostoonsa, vaan mallin koodi on luettava sisa¨a¨n joka paikassa jossa mallia ka¨yteta¨a¨n. Ka¨yta¨nno¨ssa¨ ta¨ma¨ tarkoittaa sita¨, etta¨ yleensa¨ mallin koko koodi kirjoitetaan otsikkotiedostoon. Ohjelman modulaarisuuden kannalta mallin koodin sijoittaminen otsikkotiedostoon on huono asia. Modulaarisuuden perusperiaattei- tahan on, etta¨ eri ohjelmamoduulien ei tarvitse tieta¨a¨ toistensa to- teutusta, vaan pelka¨n rajapinnan (eli esittelyiden) lukeminen riitta¨a¨. Ta¨ma¨n vuoksi C++:aan lisa¨ttiin standardoinnin yhteydessa¨ avainsana export, jonka avulla mallien modulaarisuutta voidaan lisa¨ta¨. Ika¨va¨ kylla¨ ta¨ma¨ avainsana on ta¨ta¨ kirjoitettaessa (keva¨a¨lla¨ 2003) toteutet- tu vain eritta¨in harvoissa ka¨a¨nta¨jissa¨. Lisa¨ksi ka¨yta¨nno¨n kokemukset era¨iden lehtiartikkeleiden [Sutter, 2002b] mukaan viittaavat siihen, etta¨ export ei kuitenkaan ratkaise kaikkia ka¨a¨nno¨sriippuvuusongel- mia toivotulla tavalla, vaikka se mahdollistaakin mallien esittelyn ja toteutuksen kirjoittamisen eri tiedostoihin. Jos mallista annetaan vain esittely ja sen alussa ennen template- sanaa esiintyy avainsana export, ta¨ma¨ kertoo ka¨a¨nta¨ja¨lle etta¨ mallin koodi on muualla. Ta¨llo¨in ka¨a¨nta¨ja¨ ei mallia ka¨ytetta¨essa¨ viela¨ varsi- naisesti instantioi mallia, vaan pista¨a¨ ainoastaan muistiin, millaista 9.5. C++: Toteutuksen geneerisyys: mallit (template) 299 instantiointia tarvitaan. Mallin koodi kirjoitetaan nyt toiseen koodi- tiedostoon ja varustetaan myo¨s avainsanalla export. Ta¨ta¨ kooditiedostoa ka¨a¨nta¨essa¨a¨n ka¨a¨nta¨ja¨ pista¨a¨ muistiin sen, et- ta¨ mallin koodi lo¨ytyy tarvittaessa kyseisesta¨ tiedostosta. Kun lopulta koko ohjelman objektitiedostoja linkiteta¨a¨n yhteen, linkkeri etsii tar- vittavia mallien instansseja vastaavat mallien koodit sisa¨lta¨va¨t koodi- tiedostot ja ka¨a¨nta¨a¨ niista¨ tarvittavan koodin. Ta¨lla¨ tavoin mallit voidaan export-avainsanan avulla kirjoittaa sa- maan tapaan kuin muukin koodi ja laittaa otsikkotiedostoihin vain mallien esittelyt. Listauksessa 9.16 on esimerkki export-avainsanan ka¨yto¨sta¨. Tyyppi vai arvo — avainsana typename Mallin koodissa ei auki ja¨tetyista¨ tyypeista¨ tiedeta¨ mita¨a¨n. Normaa- listi ta¨ma¨ ei haittaa, koska ka¨a¨nta¨ja¨ tuottaa mallista konekoodia vasta mallin ka¨yto¨n yhteydessa¨, jolloin tyyppiparametreja vastaavat tyypit- . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . max.hh . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1 export template <typename T> 2 T const& max(T const& p1, T const& p2); . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . main.cc . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1 #include \"max.hh\" 2 int main() 3 { 4 int m = max(4, 8); ... 5 } . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . max.cc . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1 export template <typename T> 2 T const& max(T const& p1, T const& p2) 3 { 4 if (p1 > p2) 5 { 6 return p1; 7 } else { 8 return p2; 9 } 10 } LISTAUS 9.16: Avainsanan export ka¨ytto¨ 9.5. C++: Toteutuksen geneerisyys: mallit (template) 300 kin ovat jo tiedossa. Tietyissa¨ tilanteissa ka¨a¨nta¨ja¨lle ta¨ytyy kuitenkin kertoa enemma¨n tyyppiparametreista. C++:ssa luokan sisa¨lla¨ voi ma¨a¨ritella¨ ja¨senfunktioita, ja¨senmuuttu- jia ja luokan sisa¨isia¨ tyyppeja¨ (aliluku 8.3). Ongelmaksi tulee, etta¨ jos T on luokka, voidaan syntaksilla T::x viitata ja¨senmuuttujaan tai -funktioon nimelta¨ x tai luokan sisa¨lla¨ ma¨a¨riteltyyn tyyppiin nimelta¨ x. Jos nyt T on mallin tyyppiparametri, ei ka¨a¨nta¨ja¨lla¨ ole mallin koo- dia lukiessaan mita¨a¨n tapaa tieta¨a¨, onko x ja¨senfunktio tai -muuttuja vai tyyppi. Ta¨ma¨ tieto taas on tarpeen jo mallin koodin sisa¨a¨nluku- vaiheessa, jotta ka¨a¨nta¨ja¨ ylipa¨a¨ta¨a¨n pystyy ymma¨rta¨ma¨a¨n koodin ja selvitta¨ma¨a¨n, onko siina¨ syntaksivirheita¨. Ongelma on ratkaistu C++:ssa ma¨a¨ritta¨ma¨lla¨, etta¨ edella¨ mainittua muotoa olevat luokan sisa¨lle tapahtuvat viittaukset tulkitaan aina niin, etta¨ ne viittaavat joko ja¨senfunktioon tai -muuttujaan. Jos mallin koodissa halutaan, etta¨ T::x onkin tyyppi, sen eteen ta¨ytyy kirjoittaa avainsana typename. Listaus 9.17 valottaa ta¨ta¨ esimerkin avulla. Siina¨ funktiomalli summaa olettaa saavansa tyyppiparametrina tietorakenteen, jossa on kenta¨t eka ja toka seka¨ lisa¨ksi sisa¨inen tyyppi arvotyyppi, joka ker- too minka¨ tyyppisia¨ arvoja eka ja toka ovat. Funktiomalli haluaa pa- lauttaa ta¨ma¨ntyyppisen arvon, joten sen paluutyypiksi on merkit- 1 template<typename T> 2 typename T::arvotyyppi summaa(T const& pari) 3 { 4 return pari.eka + pari.toka; 5 } 6 7 struct IntPari 8 { 9 typedef int arvotyyppi; 10 arvotyyppi eka; 11 arvotyyppi toka; 12 }; 13 14 void kaytto(IntPari const& p) 15 { 16 int summa = summaa(p); 17 } LISTAUS 9.17: Tyypin ma¨a¨ra¨a¨minen avainsanalla typename 9.5. C++: Toteutuksen geneerisyys: mallit (template) 301 ty T::arvotyyppi. Oletusarvoisesti ka¨a¨nta¨ja¨ tulkitsee ta¨ma¨n T:n ja¨- senfunktioksi tai -muuttujaksi, joten merkinna¨n eteen ta¨ytyy lisa¨ta¨ typename. Listauksessa na¨kyy myo¨s esimerkki ta¨ma¨n funktiomallin ka¨yto¨sta¨. 302 Luku 10 Geneerinen ohjelmointi: STL ja metaohjelmointi We deﬁne abstraction as selective ignorance — concentrat- ing on the ideas that are relevant to the task at hand, and ignoring everything else — and we think that it is the most important idea in modern programming. The key to writing a successful program is knowing which parts of the prob- lem to take into account, and which parts to ignore. Every programming language offers tools for creating useful ab- stractions, and every successful programmer knows how to use those tools. – From the preface of Accelerated C++ [Koenig ja Moo, 2000] Hyva¨ esimerkki mallien ka¨yto¨sta¨ ja geneerisesta¨ ohjelmoinnista on C++-standardiin kuuluva kirjasto STL (Standard Template Library). STL ma¨a¨rittelee joukon tavallisimpia tietorakenteita ja niiden ka¨yt- to¨o¨n tarkoitettuja algoritmeja. Tarkoituksena on, etta¨ ohjelmoijan ei tarvitsisi keksia¨ pyo¨ra¨a¨ uudelleen ja kirjoittaa aina omia tietoraken- teitaan vaan ohjelmissa voisi suoraan ka¨ytta¨a¨ valmiiksi kirjoitettuja, tehokkaita ja optimoituja vakiokomponentteja. STL perustuu la¨hes kokonaan malleihin. Se on toteutettavissa kokonaan C++:n vakio-ominaisuuksia ka¨ytta¨en eika¨ vaadi ka¨a¨nta¨ja¨lta¨ mita¨a¨n “erityisominaisuuksia”. Niinpa¨ STL on hyva¨ esimerkki siita¨, kuinka geneerisia¨ ratkaisuja C++:lla on mahdollista saada aikaan. 10.1. STL:n perusperiaatteita 303 Usein geneerisessa¨ ohjelmoinnissa tulee vastaan tilanteita, jos- sa geneerisen kirjaston olisi tarpeen mukauttaa omaa rakennettaan auki ja¨tetyista¨ tyypeista¨ riippuen, valita parhaiten tilanteeseen sopi- van algoritmi tai muuten toimia “a¨lykka¨a¨sti”. Ta¨llaisesta pa¨a¨ttelyja¨ ka¨ytta¨va¨sta¨ geneerisyydesta¨, jossa geneerinen komponentti vaikuttaa omaan rakenteeseensa, ka¨yteta¨a¨n yleensa¨ nimitysta¨ metaohjelmointi (metaprogramming) ja sita¨ ka¨sitella¨a¨n jonkin verran aliluvussa 10.6. 10.1 STL:n perusperiaatteita Kaikkein ta¨rkein geneerisyyden muoto STL:ssa¨ on, etta¨ kaikissa sen tietorakenteissa alkioiden tyyppi on ja¨tetty auki eli tietorakenteet on kirjoitettu ottamatta kantaa alkioiden tyyppiin. Alkioiden tyyppi ma¨a¨ra¨ta¨a¨n vasta siina¨ vaiheessa, kun tietorakenteita varsinaisesti luo- daan. Ka¨yta¨nno¨ssa¨ ta¨ma¨ tarkoittaa, etta¨ kaikki STL:n tietorakenteet ovat luokkamalleja. Esimerkki ta¨sta¨ on vector, jossa vector<int> on kokonaislukuvektori, vector<string> merkkijonovektori ja niin edelleen. STL:n toteutus mallien avulla antaa mahdollisuuden siihen, etta¨ vaikka itse STL:ssa¨ alkioiden tyyppi on ja¨tetty avoimeksi, ka¨a¨nta¨ja¨ voi kuitenkin tehda¨ STL:a¨a¨ ka¨ytta¨va¨ssa¨ koodissa kaikki tarpeelliset tyyppitarkastukset ja antaa mahdolliset virheilmoitukset jo ka¨a¨nno¨s- aikana. Ta¨ssa¨ suhteessa STL eroaa esimerkiksi Javan tietorakenteista. Niissa¨ ka¨yteta¨a¨n geneerisyyden aikaansaamiseksi periytymista¨. Kaikki Javan tietorakenteet sisa¨lta¨va¨t luokkaa Object olevia alkioi- ta. Koska kaikki Javan luokat on periytetty ta¨sta¨ luokasta, voi tieto- rakenteeseen tallettaa mita¨ tahansa alkioita. Ta¨ma¨ kuitenkin tarkoit- taa, etta¨ Javassa yhteen tietorakenteeseen voi sekoittaa minka¨ tahansa tyyppisia¨ alkioita. Kun alkiot myo¨hemmin luetaan ulos, viitteet nii- hin ta¨ytyy erikseen tyyppimuunnoksella muuntaa alkioiden todellis- ta tyyppia¨ vastaaviksi. Jos alkion todellinen tyyppi onkin oletetusta poikkeava, tuloksena on ajoaikainen virheilmoitus. Ta¨ssa¨ mielessa¨ mallit ovat periytymista¨ turvallisempi keino ta¨ma¨n tyyppisen genee- risyyden toteuttamiseen. 10.1. STL:n perusperiaatteita 304 10.1.1 STL:n rakenne Alkioiden tyypin auki ja¨tta¨misen lisa¨ksi STL:ssa¨ on paljon muutakin geneerista¨. Tietorakenteita ka¨ytta¨va¨t algoritmit ovat tietorakenteista riippumattomia ja geneerisesti kirjoitettuja, joten niita¨ voi ka¨ytta¨a¨ myo¨s itse kirjoitettujen tietorakenteiden kanssa, kunhan vain tietyt vaatimukset ta¨yttyva¨t. Samoin tietorakenteiden muistinhallinta (se, miten ja mista¨ tarvittava muisti varataan) on geneerisesti kirjoitet- tu, joten ohjelmoija voi itse vaikuttaa siihen. STL on myo¨s hyva¨ esi- merkki tiettyjen suunnittelumallien, erityisesti iteraattoreiden (alilu- ku 9.3.2) ka¨yto¨sta¨. Ta¨ma¨n luvun ei ole tarkoitus olla kattava STL-opas vaan tar- koituksena on kertoa STL:n perusperiaatteet ja ka¨yda¨ niita¨ la¨pi sii- na¨ laajuudessa, kun ne liittyva¨t olio-ohjelmointiin ja geneerisyyteen. STL:ssa¨ on sellaisia hyo¨dyllisia¨ ominaisuuksia ja yksityiskohtia, joi- den la¨pika¨ymiseen ei ta¨ssa¨ kirjassa ole mahdollisuuksia ja jotka eiva¨t muutenkaan sovi hyvin ta¨ma¨n kirjan aihepiiriin. Tarkempaa tietoa STL:n yksityiskohdista ja sen ka¨yto¨sta¨ ohjelmointiin lo¨ytyy monista C++-oppikirjoista [Lippman ja Lajoie, 1997], [Stroustrup, 1997]. Lisa¨k- si STL:n ja muun C++:n vakiokirjaston ka¨yto¨sta¨ lo¨ytyy kirja “The C++ Standard Library” [Josuttis, 1999]. STL muodostuu seuraavista osista: • Sa¨ilio¨t (container) ovat STL:n tarjoamia tietorakenteita. Niita¨ ka¨sitella¨a¨n aliluvussa 10.2. • Iteraattorit (iterator) ovat “kirjanmerkkeja¨” sa¨ilio¨iden la¨pika¨y- miseen. Aliluku 10.3 kertoo iteraattoreista tarkemmin. • Geneeriset algoritmit (generic algorithm) ka¨sitteleva¨t sa¨ilio¨ita¨ iteraattoreiden avulla. Niista¨ kerrotaan aliluvussa 10.4. • Sa¨ilio¨sovittimet (container adaptor) ovat sa¨ilio¨malleja, jotka to- teutetaan halutun toisen sa¨ilio¨n avulla. Niilla¨ voi muuntaa sa¨i- lio¨n rajapinnan toisenlaiseksi. Sa¨ilio¨sovittimia ka¨sitella¨a¨n ly- hyesti aliluvun 10.2.3 lopussa. • Funktio-oliot (function object) ovat olioita, jotka ka¨ytta¨ytyva¨t kuten funktiot ja joita voi ka¨ytta¨a¨ muuan muassa algoritmien toiminnan sa¨a¨ta¨miseen. Niista¨ kerrotaan aliluvussa 10.5. 10.1. STL:n perusperiaatteita 305 • Varaimet (allocator) ovat olioita sa¨ilio¨iden muistinhallinnan ra¨a¨ta¨lo¨intiin. Lyhyesti selitettyna¨ varaimet ovat olioita, jotka osaavat varata ja vapauttaa muistia. Normaalisti STL:n sa¨ilio¨t varaavat muistinsa new’lla¨ ja vapauttavat deletella¨. Jos niille an- netaan ylima¨a¨ra¨isena¨ tyyppiparametrina varainluokka, ne ka¨yt- ta¨va¨t ko. luokan palveluita tarvitsemansa muistin varaamiseen ja vapauttamiseen. Na¨ma¨ osat eiva¨t muodosta irrallisia kokonaisuuksia, vaan ka¨yt- ta¨va¨t la¨hes kaikki toisiaan. Esimerkiksi sa¨ilio¨ita¨ voi tietyssa¨ ma¨a¨rin ka¨ytta¨a¨ ymma¨rta¨ma¨tta¨ muita STL:n osia, mutta algoritmien ka¨ytto¨ vaatii iteraattoreiden ymma¨rta¨misen ja iteraattoreiden ka¨ytto¨ puoles- taan sa¨ilio¨iden. Funktio-olioita taas tarvitaan joidenkin algoritmien sa¨a¨ta¨miseen. STL:n ehka¨ va¨hiten ka¨ytetty osa on varaimet, joita tar- vitaan vain jos halutaan pakottaa sa¨ilio¨t varaamaan muistia juuri tie- tylla¨ tavallisuudesta poikkeavalla tavalla. 10.1.2 Algoritmien geneerisyys Olio-ohjelmoinnissa on totuttu siihen, etta¨ luokan oliot sisa¨lta¨va¨t ha- lutut tiedot ja tietoja ka¨sitteleva¨ koodi kirjoitetaan luokan ja¨senfunk- tioihin. Ta¨sta¨ syysta¨ tuntuisi alkuun loogiselta, etta¨ STL:n tarjoamat algoritmit olisi toteutettu sa¨ilio¨iden ja¨senfunktioina. Na¨in ei kuiten- kaan ole tehty, vaan la¨hes kaikki STL:n algoritmit on kirjoitettu irral- lisina funktioina, jotka eiva¨t kuulu mihinka¨a¨n luokkaan vaan ottavat kaiken tarvittavan tiedon parametreinaan. Mika¨ on syyna¨ ta¨ha¨n olio- vastaiseen toteutukseen? Syy algoritmien toteuttamiseen tavallisina funktioina on, etta¨ STL:n algoritmien geneerisyys ei rajoitu ka¨sitelta¨vien alkioiden tyy- pin auki ja¨tta¨miseen. Kaikki algoritmit on toteutettu funktiomalleina. Samalla tavoin kuin sa¨ilio¨t ja¨tta¨va¨t alkioidensa tyypin auki, STL:n al- goritmit ja¨tta¨va¨t auki sen, minka¨ tyyppisen tietorakenteen kanssa ne toimivat. Na¨in samaa geneerista¨ find-algoritmia voidaan ka¨ytta¨a¨ etsi- ma¨a¨n halutunlaista alkiota niin listasta, vektorista kuin joukostakin. Etuna ta¨sta¨ on, etta¨ algoritmeja ei tarvitse kirjoittaa erikseen jokaiselle sa¨ilio¨lle vaan yksi geneerinen funktiomalli toimii kaikkien sa¨ilio¨iden kanssa. Ta¨ma¨ algoritmien geneerisyys tarkoittaa myo¨s, etta¨ STL:n algorit- mien ka¨ytto¨ ei rajoitu vain STL:n omiin sa¨ilio¨ihin. Ohjelmoija voi 10.1. STL:n perusperiaatteita 306 kirjoittaa omia tietorakenteitaan, ja jos ne toteuttavat STL:n algorit- mien asettamat vaatimukset, na¨ita¨ algoritmeja voi ka¨ytta¨a¨ ka¨sittele- ma¨a¨n myo¨s ohjelmoijan omia tietorakenteita. Jos ohjelmoija esimer- kiksi toteuttaa hajautustaulun tai binaaripuun, STL:n find-algoritmi pystyy etsima¨a¨n alkioita niista¨kin. Na¨in STL:n geneerisyys mahdol- listaa myo¨s STL:n “laajentamisen” omilla sa¨ilio¨tyypeilla¨. 10.1.3 Tietorakenteiden jaotteluperusteet Kun kirjoissa ja oppilaitoksissa opetetaan tietorakenteita, niissa¨ kes- kityta¨a¨n luonnollisesti erityisesti tietorakenteiden sisa¨iseen rakentee- seen (ta¨sta¨ johtuu koko nimi “tietorakenne”). Niinpa¨ tiettya¨ tietora- kennetta ajatellessaan suurimmalle osalle ohjelmoijista tulee mieleen juuri tietorakenteen sisa¨inen rakenne. Kuva 10.1 sisa¨lta¨a¨ esimerkke- ja¨ mielikuvista, joita tyypilliselle ohjelmoijalle saattaisi tulla sanoista “vektori” ja “lista”. Olio-ohjelmoinnin kannalta ta¨ma¨ tapa mielta¨a¨ tietorakenteet nii- den rakenteen avulla on ja¨rjeto¨n. Koko olio-ohjelmoinnin kantava ideahan on keskittya¨ olioiden ka¨yto¨ssa¨ rajapintoihin ja piilottaa sisa¨i- nen toteutustapa ka¨ytta¨ja¨lta¨. Niinpa¨ tietorakenteiden ka¨yto¨n kannal- ta oleellista ei saisi olla se, miten tietorakenne toteutetaan vaan miten tietorakennetta on tarkoitus ka¨ytta¨a¨. Se, onko vektori toteutettu yhte- na¨isena¨ muistialueena ja sisa¨lta¨a¨ko¨ lista linkkiosoittimet seuraavaan ja edelliseen alkioon, on osa tietorakenteen sisa¨ista¨ toteutusta, jonka ei pita¨isi na¨kya¨ tietorakenteesta ulospa¨in. 0 1 2 3 4 5 Alkio Alkio Alkio Alkio Alkio Alkio (a) “vektori” Alkio seur. edell. Alkio seur. edell. Alkio seur. edell. Alkio seur. edell. ens. viim. (b) “lista” KUVA 10.1: Tietorakenteiden hera¨tta¨mia¨ mielikuvia 10.1. STL:n perusperiaatteita 307 Olio-ohjelmoinnin kannalta olisi siis oleellista luokitella tietora- kenteet niiden rajapintojen perusteella. Kun tietorakenteiden tarjoa- mia rajapintoja tarkastelee, huomaa kuitenkin pian, etta¨ varsin mo- nilla tietorakenteilla on periaatteessa aivan samanlainen rajapinta ja samanlaiset operaatiot. Esimerkiksi vektoriin voi lisa¨ta¨ alkioita ha- luttuun paikkaan (jos oletetaan, etta¨ vektori kasvattaa kokoaan tarvit- taessa), alkioita voi poistaa halutusta paikasta ja lisa¨ksi halutun al- kion voi hakea sen ja¨rjestysnumeron perusteella. Ta¨sma¨lleen samat operaatiot ovat kuitenkin mahdollisia myo¨s listan tapauksessa, ja on helppo keksia¨ myo¨s muita tietorakenteita, joissa na¨ma¨ operaatiot ovat mahdollisia. Samanlaisista operaatioista huolimatta on kuitenkin selva¨a¨, etta¨ vektori ja lista ovat jollain lailla oleellisesti erilaisia tietorakenteita. Ta¨ma¨ ero on siina¨, etta¨ eri tietorakenteiden sina¨nsa¨ samanlaisten ope- raatioiden tehokkuus saattaa olla erilainen. Joillekin tietorakenteille uusien alkioiden lisa¨a¨minen on nopeaa, toisille hidasta. Nopeus saat- taa myo¨s riippua siita¨, mihin kohtaan uusi alkio lisa¨ta¨a¨n. Vastaavasti tietyn alkion hakemisen ja muiden operaatioiden tehokkuus riippuu tietorakenteesta. Esimerkiksi alkion lisa¨a¨minen listan tietyn alkion pera¨a¨n on no- pea operaatio, koska listan ta¨ytyy vain luoda uusi alkio ja linkitta¨a¨ se osoittimia muuttamalla mukaan listaan. Vastaava lisa¨ysoperaatio vek- torille on kuitenkin hidas, koska uudelle alkiolle ta¨ytyy tehda¨ tilaa esimerkiksi siirta¨ma¨lla¨ kaikkia lisa¨yskohdan ja¨lkeen tulevia alkioita yhdella¨ eteenpa¨in. Vastaavasti halutun ja¨rjestysnumeron omaavan al- kion haku vektorista on nopeaa, koska vektorin alkiot sijaitsevat pe- ra¨kka¨isissa¨ muistiosoitteissa. Listalle vastaava operaatio taas on hi- das, koska ainoa tapa etsia¨ alkio listasta on la¨htea¨ sen ensimma¨isesta¨ alkiosta ja seurata seuraavaan alkioon osoittavaa linkkia¨ tarvittavan monta kertaa. Edella¨ mainitut asiat on otettu STL:ssa¨ huomioon. Sen sa¨ilio¨t on jaettu kahteen pa¨a¨kategoriaan, joihin kuuluvien sa¨ilio¨iden rajapinnat ovat keskena¨a¨n la¨hestulkoon samanlaiset. Sen sijaan rajapinnan ope- raatioilta vaadittu tehokkuus vaihtelee sa¨ilio¨sta¨ toiseen. STL pyrkii myo¨s esta¨ma¨a¨n sa¨ilio¨iden tehottoman ka¨yto¨n. Jokin operaatio saattaa puuttua kokonaan tietyn tyyppisesta¨ sa¨ilio¨sta¨, jos sen toteuttaminen tehokkaasti ta¨llaiselle sa¨ilio¨lle ei olisi mahdollista. Sa¨ilio¨iden operaa- tioiden tehokkuusvaatimukset on dokumentoitu C++-standardissa var- sin tarkasti, kun taas niiden sisa¨iseen toteutukseen ei oteta lainkaan 10.1. STL:n perusperiaatteita 308 kantaa. STL:n sa¨ilio¨iden nimet on kuitenkin valittu niin, etta¨ niiden tehokkuusvaatimukset ovat nimeen na¨hden “luonnolliset” (esimer- kiksi list-tietorakenteeseen on nopeaa lisa¨ta¨ uusia alkioita). Kun ohjelmassa tulee tarve ka¨ytta¨a¨ jotain tietorakennetta, ohjel- moijan tulisi ensimma¨isena¨ miettia¨ ohjelman tehokkuusvaatimuksia. Niiden operaatioiden, joita tietorakenteelle suoritetaan usein, tulisi olla nopeita. Sen sijaan harvoin suoritettavien operaatioiden tehok- kuudella ei yleensa¨ ole mita¨a¨n va¨lia¨. Na¨iden tietojen perusteella oh- jelmoija voi sitten valita STL:sta¨ tietorakenteen, jonka tehokkuusomi- naisuudet ovat ohjelman tarpeita vastaavat. 10.1.4 Tehokkuuskategoriat Erilaisten operaatioiden tehokkuuden ma¨a¨rittely ja luokittelu ei ole yksinkertainen asia. Tehokkuuden mittaaminen milli- tai nanosekun- neissa ei ole ja¨rkeva¨a¨, koska ta¨llainen “absoluuttinen” tehokkuus tie- tysti riippuu operaation toteutuksen tehokkuuden lisa¨ksi myo¨s ka¨yte- tyn tietokoneen nopeudesta. Todellinen nopeus riippuu myo¨s esimer- kiksi tietorakenteisiin talletettujen alkioiden lukuma¨a¨ra¨sta¨ ja koosta — on hitaampaa siirta¨a¨ paljon tai suuria alkioita. Kuitenkin STL:n tapaisessa geneerisessa¨ kirjastossa on tarve vertailla operaatioiden te- hokkuuksia keskena¨a¨n. Tietojenka¨sittelytieteessa¨ yleisesti ka¨ytetty tehokkuuden mittari on se, miten tietyn operaation nopeus muuttuu, kun tietorakenteen koko kasvaa. Ta¨lla¨ tavoin ajateltuna ei ole va¨lia¨ silla¨, kuinka suuria alkiot ovat, kuinka monta niita¨ ta¨sma¨lleen on tai kuinka nopea itse tietokone on. Oleellista on se, miten operaatio hidastuu, kun tietora- kenteen alkioiden ma¨a¨ra¨ kasvaa. Esimerkkina¨ voidaan ja¨lleen pita¨a¨ vektoria ja listaa. Alkion lisa¨a¨- minen vektorin alkuun kesta¨a¨ sita¨ kauemmin, mita¨ suurempi vektori on. Tarkasti ottaen lisa¨a¨miseen kuluva aika on suoraan verrannolli- nen vektorin kokoon. Sen sijaan listan alkuun lisa¨a¨minen kesta¨a¨ ta¨s- ma¨lleen yhta¨ kauan, oli lista kuinka suuri tahansa. Vastaavasti vekto- rin alkion hakeminen ja¨rjestysnumeron perusteella on vakioaikainen operaatio kun taas listalle saman operaation aika on suoraan verran- nollinen haetun alkion ja¨rjestysnumeroon, jota taas rajoittaa listan koko. Operaation tehokkuus saattaa tietysti riippua myo¨s tietorakenteen alkioiden sisa¨llo¨sta¨, keskina¨isesta¨ ja¨rjestyksesta¨ ja muista seikoista. 10.1. STL:n perusperiaatteita 309 Operaation suoritusajalle voidaan na¨iden perusteella arvioida seka¨ yla¨- etta¨ alaraja. Ohjelman tehokkuuden varmistamisen kannalta on kuitenkin usein oleellista, kuinka operaatio ka¨ytta¨ytyy pahimmassa mahdollisessa tilanteessa. Usein tehokkuudessa kiinnostaa vain suoritusajan yleinen ka¨yt- ta¨ytyminen. Ta¨llo¨in puhutaan usein operaation tehokkuuden ker- taluokasta (order of growth). Sille voi laskea useita erilaisia tehok- kuusmittoja, joille tietojenka¨sittelytieteessa¨ on omat merkinta¨tapan- sa. Na¨ista¨ merkinta¨tavoista yleisimma¨t ovat • Θ-notaatio, joka kertoo suoritusajan kertaluokan • O-notaatio, joka kertoo suoritusajan “asymptoottisen yla¨rajan” — kertaluokan, jota suoritusaika ei varmasti ylita¨, kun alkioiden ma¨a¨ra¨ kasvaa tietyn rajan yli • Ω-notaatio, joka kertoo suoritusajan “asymptoottisen alarajan” — kertaluokan, jota suoritusaika ei varmasta alita, kun tietty al- kioma¨a¨ra¨ yliteta¨a¨n. Na¨iden tehokkuuden merkinta¨tapojen tarkka kuvaus ja¨a¨ ta¨ma¨n kirjan aihepiirin ulkopuolelle, mutta niista¨ lo¨ytyy tietoa la¨hes kaikista algo- ritmeja ka¨sittelevista¨ kirjoista ja muista la¨hteista¨ (esimerkiksi “Intro- duction to Algorithms” [Cormen ja muut, 1990]). On huomattava, etta¨ kertaluokkanotaatio kertoo vain, kuinka ope- raation suoritusaika ka¨ytta¨ytyy alkioiden ma¨a¨ra¨n kasvaessa. Se ei kuitenkaan anna mita¨a¨n tietoa varsinaisesta suoritusajasta. Esimer- kiksi kahdesta lineaarisesta operaatiosta toinen voi olla paljon toista hitaampi — lineaarisuus kertoo vain, etta¨ molemmissa suoritusaika kasvaa suoraan suhteessa alkioiden ma¨a¨ra¨a¨n. STL:n kannalta ta¨rkein tehokkuuden mittari on yla¨rajan mittari O. Se kertoo, kuinka tehokas operaatio ainakin on. Esimerkiksi vakio- aikainen tehokkuus O(1) kertoo, ettei nopeus riipu alkioiden luku- ma¨a¨ra¨sta¨. Lineaarinen tehokkuus O(n) taas kertoo, etta¨ suoritusaika on suoraan verrannollinen alkioiden lukuma¨a¨ra¨a¨n tai pienempi. Ku- vaan 10.2 seuraavalla sivulla on kera¨tty ta¨rkeimma¨t STL:ssa¨ ka¨ytetyt tehokkuusluokat selityksineen ja O-merkinto¨ineen. Toinen STL:ssa¨ jonkin verran ka¨ytetty mittari on keskima¨a¨ra¨inen tehokkuus. Sita¨ ka¨yteta¨a¨n joskus O-tehokkuuden rinnalla, jos huo- noimman mahdollisen tapauksen tehokkuus eroaa oleellisesti kes- kima¨a¨ra¨isesta¨ tehokkuudesta. Esimerkiksi ja¨rjestysoperaatio sort on 10.2. STL:n sa¨ilio¨t 310 Nimitys O-notaatio Selitys Ka¨a¨nno¨saikainen (compile-time) O(0)a Operaatio suoritetaan ka¨a¨nno¨saikana eli se ei vaikuta suoritusaikaan. Vakioaikainen (constant) O(1) Suoritusaika ei riipu alkioiden ma¨a¨ra¨s- ta¨. Amortisoidusti vakioaikainen (amortized constant) O(1) Operaatio on “ka¨yta¨nno¨ssa¨” vakioai- kainen, yksitta¨istapauksissa kenties hi- taampi. Logaritminen (logarithmic) O(log n) Suoritusaika on verrannollinen alkioi- den ma¨a¨ra¨n logaritmiin. Lineaarinen (linear) O(n) Suoritusaika on suoraan verrannollinen alkioiden ma¨a¨ra¨a¨n. O(n log n) Suoritus hidastuu enemma¨n kuin line- aarisesti, ei kuitenkaan viela¨ nelio¨llises- ti. Nelio¨llinen (quadratic) O(n2) Suoritusaika on verrannollinen alkioi- den ma¨a¨ra¨n nelio¨o¨n. aVaikka merkinta¨ O(0) onkin teoreettisesti oikea, sita¨ ei yleensa¨ ka¨yteta¨. KUVA 10.2: Erilaisia tehokkuuskategorioita keskima¨a¨ra¨iselta¨ tehokkuudeltaan n log n, vaikka se pahimmassa ta- pauksessa voi olla esimerkiksi O(n2). 10.2 STL:n sa¨ilio¨t Kuten jo aiemmin ta¨ssa¨ luvussa on todettu, STL:n sa¨ilio¨iden rajapin- nat muistuttavat suuresti toisiaan. Rajapintojensa puolesta STL:n sa¨i- lio¨t muutamaa poikkeusta lukuun ottamatta jakautuvat kahteen kate- goriaan: • Sarjat (sequence) ovat sa¨ilio¨ita¨, joiden alkioita pystyy hake- maan niiden ja¨rjestysnumeron perusteella. Samoin alkioita voi lisa¨ta¨ haluttuun paikkaan ja poistaa siita¨. Esimerkiksi taulukko- tyyppi vector on ta¨llainen sarja. 10.2. STL:n sa¨ilio¨t 311 • Assosiatiiviset sa¨ilio¨t (associative container) puolestaan perus- tuvat siihen, etta¨ alkioita haetaan sa¨ilio¨sta¨ avaimen (key) pe- rusteella. Esimerkiksi puhelinluettelo muistuttaa assosiatiivista sa¨ilio¨ta¨ — siina¨ numeron pystyy etsima¨a¨n nopeasti nimen pe- rusteella. Vaikka na¨iden kahden kategorian sa¨ilio¨t eroavatkin rajapinnoil- taan, niilla¨ on myo¨s yhteisia¨ rajapintaoperaatioita. Kaikilta sa¨ilio¨ilta¨ voi esimerkiksi kysya¨ ja¨senfunktiolla empty, ovatko ne tyhjia¨. Lisa¨ksi niiden tarkan koon voi selvitta¨a¨ ja¨senfunktiolla size. Kuten mallit yleensa¨kin, myo¨s STL:n sa¨ilio¨t asettavat joitakin vaa- timuksia tyyppiparametreilleen eli alkioidensa tyypille. Jotta tietyn- tyyppisia¨ alkioita varten voisi luoda sa¨ilio¨n, alkioiden tyypin ta¨ytyy toteuttaa kaksi ehtoa. Tyypilla¨ tulee olla • kopiorakentaja, joka luo alkupera¨isen olion kanssa samanlaisen olion • sijoitusoperaattori, jonka tuloksena sijoituksen kohteena olevas- ta oliosta tulee samanlainen sijoitetun olion kanssa. Lisa¨ksi STL:n assosiatiiviset sa¨ilio¨t vaativat, etta¨ alkioiden avai- mia voi myo¨s kopioida ja sijoittaa ja lisa¨ksi kahta avainta ta¨ytyy pys- tya¨ vertailemaan, jotta avaimet voidaan panna ja¨rjestykseen (oletusar- voisesti vertailu tehda¨a¨n operaattorilla <). Na¨ista¨ vaatimuksista seuraa se, etta¨ viitteet eiva¨t kelpaa STL:n sa¨i- lio¨iden alkioiksi tai avaimiksi. Viitteiden tapauksessahan viitteeseen sijoittaminen ei muuta viitetta¨ viittaamaan toiseen olioon, vaan sijoit- taa viitteen pa¨a¨ssa¨ olevat oliot. Riippuu ka¨ytetysta¨ ka¨a¨nta¨ja¨sta¨, osaa- ko se antaa virheilmoituksen, jos ohjelmassa yriteta¨a¨n luoda viitesa¨i- lio¨ita¨. Kaikki STL:n sa¨ilio¨t varaavat itse lisa¨a¨ muistia tarvittaessa, kun niihin lisa¨ta¨a¨n uusia alkioita. Kun sa¨ilio¨ tuhotaan, se vapauttaa kai- ken varaamansa muistin. Sen sijaan ei ole varmaa, vapauttaako sa¨ilio¨ varaamaansa muistia heti, kun alkio poistetaan sa¨ilio¨sta¨. Monet sa¨i- lio¨t nimitta¨in saattavat pita¨a¨ muistia “varastossa” ja ottaa sen uudel- leen ka¨ytto¨o¨n, kun sa¨ilio¨o¨n myo¨hemmin lisa¨ta¨a¨n uusia alkioita. 10.2. STL:n sa¨ilio¨t 312 10.2.1 Sarjat (“pera¨kka¨issa¨ilio¨t”) Sarjat (sequence) ovat sa¨ilio¨ita¨, joissa alkiot sijaitsevat “pera¨kka¨in” ja joissa jokaisella alkiolla on ja¨rjestysnumero. Alkioita voi selata ja¨rjes- tyksessa¨, ja halutun alkion voi hakea sen ja¨rjestysnumeron perusteel- la. Uusia alkioita voi lisa¨ta¨ sa¨ilio¨ssa¨ haluttuun paikkaan, ja vanhoja voi poistaa. STL tarjoaa kolme erilaista sarjasa¨ilio¨ta¨: vector, deque ja list. Kaikissa sarjoissa annetaan sarjan alkioiden tyyppi mallin tyyp- piparametrina, siis esimerkiksi vector<float>, deque<int> ja list<string>. Ta¨ma¨n lisa¨ksi ylima¨a¨ra¨isena¨ tyyppiparametrina voi antaa sarjan muistinhallintaan ka¨ytetta¨va¨n varaimen, mutta ta¨ta¨ mahdollisuutta ei ka¨sitella¨ ta¨ssa¨ enempa¨a¨. Sarjojen rajapinta on suurelta osin yhtena¨inen. Uuden alkion voi kaikissa sarjoissa lisa¨ta¨ ja¨senfunktiolla insert ja vanhoja alkioita voi poistaa ja¨senfunktiolla erase. Lisa¨ksi koko sarjan voi tyhjenta¨a¨ ja¨- senfunktiolla clear. Lisa¨ksi osasta sarjoja lo¨ytyy viela¨ “ylima¨a¨ra¨isia¨” ja¨senfunktioita sellaisia toimintoja varten, jotka kyseisessa¨ sarjassa ovat erityisen nopeita. Esimerkiksi vector-tyypista¨ lo¨ytyy ja¨senfunk- tio push back, joka lisa¨a¨ uuden alkion taulukon loppuun. Saman toi- minnon saisi tehtya¨ myo¨s insert-ja¨senfunktiolla, mutta silloin sille pita¨isi erikseen kertoa, etta¨ lisa¨ys tehda¨a¨n taulukon loppuun. Varsinaisten sarjojen lisa¨ksi useimmat STL:n algoritmit suostuvat ka¨sittelema¨a¨n myo¨s C++:n perustaulukoita (esimerkiksi int t[10]) ku- ten sarjoja, vaikka niiden ulkoinen rajapinta ei olekaan varsinaisesti sarjojen rajapintavaatimusten mukainen. Samoin merkkijonotyyppia¨ string voi ka¨sitella¨ kuten merkeista¨ muodostuvaa vektoria. Koska suurimmat erot eri sarjasa¨ilio¨iden va¨lilla¨ ovat tehokkuudes- sa, kuvaan 10.3 seuraavalla sivulla on kera¨tty tyypillisimpia¨ sarjojen tarjoamia operaatioita ja niiden tehokkuuksia. Vektori — vector Vektori (vector) on STL:n vastine taulukoille. Vektoreita voi indek- soida taulukoiden tapaan, ja ta¨ma¨ alkioiden haku ja¨rjestysnumeron perusteella on vakioaikainen. Uusien alkioiden lisa¨a¨minen vektoriin on lineaarinen operaatio, paitsi jos uusi alkio lisa¨ta¨a¨n vektorin lop- puun, jolloin lisa¨ys on amortisoidusti vakioaikainen. Samoin alkioi- 10.2. STL:n sa¨ilio¨t 313 Operaatio vector deque list 1./viim. alkio (front/back) vakio vakio vakio Mielivaltainen indeksointi ([ ], at) vakio vakio — 1. alkion lisa¨ys (push front) —a vakio vakio 1. alkion poisto (pop front) —b vakio vakio Viim. alkion lisa¨ys (push back) amort. vakio vakio vakio Viim. alkion poisto (pop back) vakio vakio vakio Mieliv. lisa¨ys/poisto (insert/erase) lineaar. lineaar. vakio Sarjan koko/tyhjyys (size/empty) vakio vakio lin./vakio Sarjan tyhjenta¨minen (clear) lineaar. lineaar. lineaar. aOperaation voi suorittaa ja¨senfunktiolla insert. bOperaation voi suorittaa ja¨senfunktiolla erase. KUVA 10.3: Sarjojen operaatioiden tehokkuuksia den poisto on lineaarinen operaatio, paitsi viimeisen alkion poisto on vakioaikainen. Vektori saadaan ka¨ytto¨o¨n komennolla #include <vector>. Sen rajapinnassa on sarjojen perusrajapinnan lisa¨ksi edella¨ mainittu in- deksointi. Indeksoinnin voi tehda¨ joko normaaleilla hakasulkeilla [ ], jolloin mahdollista yli-indeksointia ei va¨ltta¨ma¨tta¨ tarkasteta, tai ja¨- senfunktiolla at, joka heitta¨a¨ poikkeuksen, jos annettu indeksi on lii- an suuri. Vektorin loppuun lisa¨ysta¨ ja poistoa varten ovat ja¨senfunk- tiot push back ja pop back. Ta¨ma¨n lisa¨ksi vektorin ensimma¨isen ja viimeisen alkion voi lukea ja¨senfunktioiden front ja back avulla. Normaalin C++:n taulukon tapaan vektorin alkiot sijaitsevat muis- tissa pera¨kka¨in.\u0017 Kun vektoriin lisa¨ta¨a¨n uusi alkio, sen kokoa ta¨y- tyy kasvattaa. Ta¨ma¨ puolestaan tarkoittaa, etta¨ vektorin ta¨ytyy varata uusi suurempi muistialue, kopioida vanhat alkiot sinne ja vapauttaa vanha muistialue. Koska ta¨ma¨ operaatio on hidas (tarkasti ottaen li- neaarinen), vektori ei varaa uutta muistia joka lisa¨yksella¨. Sen sijaan vektori varaa aina kerralla tarvittavaa suuremman muistialueen, jon- ka alkuun se kopioi olemassa olevat alkiot. Kun uusia alkioita lisa¨ta¨a¨n . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . \u0017Tarkasti ottaen ta¨llaista vaatimusta ei lo¨ydy C++-standardista. Ta¨ma¨ on kuulemma kuiten- kin ollut standardin kirjoittajien tarkoitus, ja kaikissa ka¨a¨nta¨jissa¨ vektori on toteutettu ta¨lla¨ tavalla. Lisa¨ksi vaatimus lisa¨ta¨a¨n standardiin seuraavan pa¨ivityksen yhteydessa¨. 10.2. STL:n sa¨ilio¨t 314 vektorin loppuun, niille on jo valmiiksi muistia, joten uutta muistin- varausta ei tarvita. Kun varalla oleva muisti on kulutettu loppuun, vektorin ta¨ytyy taas varata suurempi muistialue ja niin edelleen. Ta¨lla¨ tavoin uuden alkion lisa¨a¨minen vektorin loppuun saadaan “ka¨yta¨nno¨ssa¨” vakioaikaiseksi, vaikka aina silloin ta¨llo¨in muistinva- raus ja vanhojen alkioiden kopiointi hidastavatkin operaatiota. Vek- torista lo¨ytyy myo¨s ja¨senfunktio reserve, jolla vektoria voi ka¨skea¨ va- rautumaan annettuun ma¨a¨ra¨a¨n alkioita. Ta¨llo¨in vektori varaa kerral- la riitta¨va¨sti muistia, jolloin uudelleenvarausta ei tarvita niin kauan, kuin alkioiden ma¨a¨ra¨ pysyy annetuissa rajoissa. Vektori on sopiva tietorakenne tilanteisiin, joissa tietorakennetta indeksoidaan paljon mutta uusia alkioita lisa¨ta¨a¨n korkeintaan van- hojen pera¨a¨n ja vanhoja alkioita poistetaan vain lopusta. Ka¨yta¨nno¨s- sa¨ ta¨ma¨ tarkoittaa, etta¨ vektori kelpaa korvaamaan C++:n perustaulukot la¨hes kaikkialla. Listaus 10.1 sisa¨lta¨a¨ esimerkin vektoreiden ka¨yto¨sta¨. Funktio laskeFibonacci ta¨ytta¨a¨ parametrina annetun vektorin anne- tulla ma¨a¨ra¨lla¨ Fibonaccin lukuja. (Fibonaccin luvuista kaksi ensim- ma¨ista¨ on 1 ja seuraavat alkiot saadaan aina laskemalla yhteen kaksi 1 #include <vector> 2 #include <algorithm> 3 using std::vector; 4 using std::min; 5 6 void laskeFibonacci(unsigned long lkm, vector<unsigned long>& taulukko) 7 { 8 taulukko.clear(); // Tyhjenna¨ varmuuden vuoksi 9 taulukko.reserve(lkm); // Varaudu na¨in moneen alkioon 10 11 for (unsigned long i = 0; i < min<unsigned long>(lkm, 2); ++i) 12 { 13 // Ensimma¨iset kaksi alkiota ovat 1 14 taulukko.push back(1); 15 } 16 for (unsigned long i = 2; i < lkm; ++i) 17 { 18 // Loput alkiot kahden edellisen summa 19 taulukko.push back(taulukko[i-2] + taulukko[i-1]); 20 } 21 } LISTAUS 10.1: Fibonaccin luvut vectorilla 10.2. STL:n sa¨ilio¨t 315 edellista¨ alkiota.) Pakka / kaksipa¨inen jono — deque Kaksipa¨inen jono (double-ended queue) on taulukko, jossa molem- piin pa¨ihin tehta¨va¨t lisa¨ykset ja poistot ovat vakioaikaisia. Muualle tehta¨va¨t lisa¨ykset ja poistot ovat lineaarisia. Kaksipa¨isen jonon nimi deque a¨a¨ntyy “dek” samoin kuin englannin sana “deck”, joka tarkoit- taa korttipakkaa. Koska deque muistuttaa toiminnallisuudeltaankin jonkin verran korttipakkaa (kortteja on helppo lisa¨ta¨ ja poistaa pakan molemmista pa¨ista¨), ta¨ssa¨ teoksessa ka¨yteta¨a¨n kaksipa¨isesta¨ jonosta ta¨stedes lyhempa¨a¨ nimitysta¨ pakka. Pakan rajapinta tarjoaa sarjojen perusrajapinnan lisa¨ksi viela¨ vek- torin rajapinnan eli operaatiot [ ], at, front, back, push back ja pop back. Na¨iden operaatioiden tehokkuuskin on kertaluokaltaan sa- ma kuin vektorissa. Koska myo¨s pakan alkuun lisa¨a¨minen ja siel- ta¨ poistaminen on nopeaa, pakassa on ta¨ta¨ varten viela¨ operaa- tiot push front ja pop front. Pakka saadaan ka¨ytto¨o¨n komennolla #include <deque>. Koska pakan rajapinnassa on kaikki samat operaatiot kuin vekto- rissakin, ja operaatioiden tehokkuuskin on sama, pakkaa voi ka¨ytta¨a¨ kaikkialla missa¨ vektoriakin. Pakan indeksointi on kuitenkin ka¨yta¨n- no¨ssa¨ aika lailla vektorin indeksointia hitaampaa (vaikka molemmat ovatkin vakioaikaisia), joten suoritustehoa vaativissa tilanteissa vek- tori on parempi vaihtoehto. Pakka eroaa vektorista edukseen siina¨, etta¨ pakan alkuun lisa¨ys ja siita¨ poisto ovat vakioaikaisia, kun ne vektorissa ovat lineaarisia. Pak- ka onkin ka¨teva¨ tietorakenne toteuttamaan esimerkiksi puskureita, joissa tietoa lisa¨ta¨a¨n toiseen pa¨a¨ha¨n ja luetaan toisesta. Listaus 10.2 seuraavalla sivulla sisa¨lta¨a¨ pakalla toteutetun luokan LukuPuskuri, jo- hon voi lisa¨ta¨ rajoittamattoman ma¨a¨ra¨n kokonaislukuja ja josta ne voi lukea pois samassa ja¨rjestyksessa¨. Lisa¨ksi puskurista voi lukea anne- tussa kohdassa olevan alkion arvon. Lista — list Lista (list) on viimeinen STL:n sarjoista. Nimensa¨ mukaisesti lista on tietorakenne, joka tehokkuudeltaan vastaa kahteen suuntaan linkitet- tya¨ listarakennetta. Alkioiden lisa¨a¨minen ja poistaminen ovat vakio- 10.2. STL:n sa¨ilio¨t 316 1 #include <deque> 2 using std::deque; 3 4 class LukuPuskuri 5 { 6 public: 7 // (Rakentajat ja purkaja tyhjia¨) 8 void lisaa(int luku); // Lisa¨a¨ loppuun 9 int lue(); // Lukee ja poistaa alusta 10 int katso(int paikka); // Lukee annetusta paikasta 11 bool onkoTyhja() const; 12 13 private: 14 deque<int> puskuri; 15 }; 16 17 void LukuPuskuri::lisaa(int luku) 18 { 19 puskuri.push back(luku); 20 } 21 22 int LukuPuskuri::lue() 23 { 24 int luku = puskuri.front(); 25 puskuri.pop front(); 26 return luku; 27 } 28 29 bool LukuPuskuri::onkoTyhja() const 30 { 31 return puskuri.empty(); 32 } 33 34 int LukuPuskuri::katso(int paikka) 35 { 36 return puskuri.at(paikka); // Ilman tarkistuksia: puskuri[paikka] 37 } LISTAUS 10.2: Puskurin toteutus dequella 10.2. STL:n sa¨ilio¨t 317 aikaisia riippumatta siita¨, mihin kohtaan listalla lisa¨ys tai poisto koh- distuu. Listasta voi myo¨s lukea ensimma¨isen tai viimeisen alkion va- kioajassa mutta mielivaltaisen alkion lukeminen indeksoimalla puut- tuu kokonaan, koska sita¨ ei saisi toteutetuksi tehokkaasti. Aliluvus- sa 10.3 esitelta¨va¨t iteraattorit antavat kuitenkin mahdollisuuden se- lata listaa la¨pi alkio kerrallaan. Listan esittely luetaan komennolla #include <list>. Listan raja- pinta tarjoaa sarjojen perusrajapinnan palvelut seka¨ operaatiot back, push back, pop back, front, push front ja pop front. Indeksointiope- raatiot [ ] ja at puuttuvat. Lista tarjoaa lisa¨ksi erityisoperaatioita, jotka listan linkitetty ra- kenne tekee mahdolliseksi toteuttaa tehokkaasti. Ja¨senfunktiolla splice voi vakioajassa siirta¨a¨ yhden listan alkiot haluttuun paikkaan toisessa listassa. Samoin silla¨ voi vakioajassa siirta¨a¨ osan listasta toi- seen paikkaan samassa listassa. Lisa¨ksi lista tarjoaa sita¨ varten opti- moidut erikoisversiot era¨ista¨ STL:n algoritmeista. Listan etuna vektoriin ja pakkaan verrattuna on se, etta¨ listojen yhdista¨minen ja pilkkominen seka¨ alkioiden lisa¨ys keskelle listaa ja poisto sielta¨ ovat vakioaikaisia. Vektorille ja pakalle kaikki na¨ma¨ ope- raatiot ovat lineaarisia. Sen sijaan listaa ei voi indeksoida, joten sita¨ ei voi ka¨ytta¨a¨ taulukkotyyppina¨. Lista onkin parhaimmillaan pusku- reiden ja pinojen toteutuksessa seka¨ tilanteissa, joissa tietorakenteita tulee pystya¨ yhdistelema¨a¨n. 10.2.2 Assosiatiiviset sa¨ilio¨t Assosiatiiviset sa¨ilio¨t (associative container) eroavat sarjoista siina¨, etta¨ alkioita ei lueta, lisa¨ta¨ tai poisteta niiden “sijainnin” tai ja¨rjestys- numeron perusteella vaan jokaiseen alkioon liittyy avain (key), jonka perusteella alkion voi myo¨hemmin hakea. Ta¨sta¨ tulee myo¨s ta¨ma¨n sa¨ilio¨tyypin nimi: assosiatiiviset sa¨ilio¨t ma¨a¨ra¨a¨va¨t assosiaation (“yh- teyden”) avaimen ja alkion va¨lille. Osassa assosiatiivisia sa¨ilio¨ita¨ al- kio itse toimii myo¨s avaimena, osassa avain ja alkio ovat erillisia¨. STL tarjoaa nelja¨ assosiatiivista sa¨ilio¨ta¨: set, multiset, map ja multimap. Assosiatiivisille sa¨ilio¨ille annetaan tyyppiparametreina seka¨ avai- men etta¨ alkion tyyppi. Ylima¨a¨ra¨isina¨ tyyppiparametreina on lisa¨ksi mahdollista antaa avainten suuruusvertailuun ka¨ytetta¨va¨ funktio se- ka¨ muistinhallintaan ka¨ytetta¨va¨ varain. Ja¨lleen na¨ma¨ lisa¨ominaisuu- det ja¨teta¨a¨n ta¨ssa¨ teoksessa la¨pika¨yma¨tta¨. 10.2. STL:n sa¨ilio¨t 318 Rajapinnaltaan assosiatiiviset sa¨ilio¨t muistuttavat toisiaan. Al- kioita etsita¨a¨n ja¨senfunktiolla find, lisa¨a¨ta¨a¨n ja¨senfunktiolla insert ja poistetaan ja¨senfunktiolla erase. Ta¨ma¨n lisa¨ksi jotkut sa¨ilio¨tyypit tarjoavat lisa¨operaatioita. Kaikissa assosiatiivisissa sa¨ilio¨issa¨ alkioiden lisa¨a¨minen, poista- minen ja hakeminen avaimen perusteella ovat tehokkuudeltaan loga- ritmisia operaatioita. Ta¨ma¨ onkin niiden etu sarjoihin verrattuna. Jos johonkin sarjasa¨ilio¨o¨n talletettaisiin seka¨ avain etta¨ alkio, ainoa tapa tietylla¨ avaimella varustetun alkion etsimiseen olisi ka¨yda¨ la¨pi koko sarja alusta alkaen, ja ta¨ma¨ olisi tehokkuudeltaan lineaarinen operaa- tio. Kuten kaikki muutkin sa¨ilio¨t, assosiatiivisen sa¨ilio¨n alkiot voi se- lata la¨pi yksi kerrallaan aliluvussa 10.3 esitelta¨via¨ iteraattoreita ka¨yt- ta¨en. Ta¨llo¨in alkiot ka¨yda¨a¨n la¨pi avainten suuruusja¨rjestyksessa¨. Joukko — set Joukko (set) on yksinkertaisin assosiatiivisista sa¨ilio¨ista¨. Joukossa al- kio itse toimii avaimena, mista¨ johtuen joukolle annetaan tyyppi- parametrina vain alkion tyyppi samoin kuin sarjoille. Esimerkiksi set<char> ma¨a¨rittelee joukon, johon talletetaan merkkeja¨. Joukot otetaan ka¨ytto¨o¨n komennolla #include <set>. Ka¨yta¨nno¨ssa¨ joukko vastaa aika hyvin “matemaattisen joukon” ka¨- sitetta¨. Siihen voi lisa¨ta¨ alkioita, niita¨ voi poistaa, ja joukolta voidaan kysya¨, onko annettu alkio jo joukossa. Kaikki na¨ma¨ operaatiot teh- da¨a¨n logaritmisessa ajassa. Samanarvoisia alkioita voi joukossa olla vain yksi kerrallaan kuten matematiikan joukossakin. Joukko on ka¨yta¨nno¨llinen tietorakenne silloin, kun ohjelmassa ta¨ytyy ylla¨pita¨a¨ jonkinlaista rekisteria¨, johon lisa¨ta¨a¨n alkioita seka- laisessa ja¨rjestyksessa¨, ja kun pita¨a¨ pystya¨ nopeasti testaamaan, onko annettu alkio rekisterissa¨. Listaus 10.3 seuraavalla sivulla ma¨a¨rittelee esimerkkina¨ funktion rekisterointi, jolla voidaan rekistero¨ida¨ nimia¨ (merkkijonoja). Funktio ylla¨pita¨a¨ nimijoukkoa rekisteri ja palauttaa true, jos rekistero¨ita¨va¨ nimi oli uusi eli sita¨ ei viela¨ ollut rekisterissa¨. Assosiatiivisten sa¨ilio¨iden operaatioilla on era¨ita¨ erityispiirteita¨, jotka helpottavat niiden tehokasta ka¨ytto¨a¨. STL:n optimointimahdol- lisuuksien yksityiskohtainen la¨pika¨ynti ei kylla¨ka¨a¨n ole mahdollis- ta ta¨ssa¨ teoksessa, mutta seuraava esimerkki havainnollistaa, millai- sia mahdollisuuksia STL tarjoaa. Listauksen 10.3 rekistero¨intifunktio 10.2. STL:n sa¨ilio¨t 319 1 #include <set> 2 #include <string> 3 using std::set; 4 using std::string; 5 6 // Palauttaa true, jos rekistero¨ida¨a¨n uusi nimi, muuten false 7 bool rekisterointi(string const& nimi) 8 { 9 static set<string> rekisteri; // Staattinen, sa¨ilyy kutsujen va¨lilla¨ 10 11 if (rekisteri.find(nimi) == rekisteri.end()) 12 { 13 // Ei ole ollut aiemmin 14 rekisteri.insert(nimi); // Lisa¨a¨ rekisteriin 15 return true; 16 } 17 else 18 { 19 // Nimi lo¨ytyi jo 20 return false; 21 } 22 } LISTAUS 10.3: Nimien rekistero¨inti setilla¨ toimii kylla¨ logaritmisessa ajassa mutta on siina¨ mielessa¨ tehoton, et- ta¨ ensin rivilla¨ 11 tutkitaan, lo¨ytyyko¨ annettua merkkijonoa, ja sitten rivilla¨ 14 sama merkkijono lisa¨ta¨a¨n joukkoon, jos se ei jo ollut siella¨. Na¨in alkion paikkaa joudutaan etsima¨a¨n kaksi kertaa pera¨ja¨lkeen. Tilanne, jossa alkiota ensin etsita¨a¨n ja sitten mahdollisesti lisa¨ta¨a¨n se, on eritta¨in yleinen. Niinpa¨ joukon insert-operaatio itse asiassa li- sa¨a¨ annetun alkion vain, jos alkio ei jo ollut sa¨ilio¨ssa¨. Lisa¨ksi operaa- tio palauttaa paluuarvonaan std::pair-tyyppisen struct-tietoraken- teen, jossa on myo¨s tieto siita¨, suoritettiinko lisa¨ysta¨ vai ei. Na¨in sama rekistero¨intifunktio voidaan toteuttaa tehokkaammin yhdella¨ ainoal- la insert-kutsulla. Listaus 10.4 seuraavalla sivulla na¨ytta¨a¨ ta¨llaisen tehokkaamman toteutuksen. Monijoukko — multiset Monijoukko (multiset) eroaa tavallisesta joukosta siina¨, etta¨ samanar- voisia alkioita voi olla monijoukossa useita. Monijoukolta voi alkion olemassaolon lisa¨ksi kysya¨, montako annetun arvoista alkiota moni- 10.2. STL:n sa¨ilio¨t 320 1 #include <set> 2 #include <string> 3 using std::set; 4 using std::string; 5 6 // Palauttaa true, jos rekistero¨ida¨a¨n uusi nimi, muuten false 7 bool rekisterointi optimoitu(string const& nimi) 8 { 9 static set<string> rekisteri; // Staattinen, sa¨ilyy kutsujen va¨lilla¨ 10 11 // insert palauttaa parin, jonka ja¨lkimma¨inen osa second ilmoittaa, 12 // tehtiinko¨ lisa¨ys vai lo¨ytyiko¨ alkio jo joukosta 13 return rekisteri.insert(nimi).second; 14 } LISTAUS 10.4: Tehokkaampi versio listauksesta 10.3 joukossa on. Ta¨ma¨ tehda¨a¨n ja¨senfunktiolla count (itse asiassa count on myo¨s tavallisen joukon rajapinnassa, jossa se palauttaa aina arvon 1). Monijoukon ka¨ytto¨o¨notto tehda¨a¨n komennolla #include <set> samoin kuin joukonkin. Monijoukon ka¨ytto¨tarkoitus on la¨hes sama kuin joukon, mutta monijoukko mahdollistaa samanarvoisten alkioiden lisa¨a¨misen ja las- kemisen. Listaus 10.5 sisa¨lta¨a¨ uuden rekistero¨intifunktion, joka lisa¨a¨ nimen rekisteriin ja ilmoittaa paluuarvonaan, kuinka monta kertaa nimi on lisa¨tty rekisteriin. 1 #include <set> 2 #include <string> 3 using std::multiset; 4 using std::string; 5 6 // Palauttaa nimelle suoritettujen rekistero¨intien lukuma¨a¨ra¨n 7 unsigned long int monirekisterointi(string const& nimi) 8 { 9 static multiset<string> rekisteri; // Staattinen, sa¨ilyy kutsujen va¨lilla¨ 10 rekisteri.insert(nimi); 11 return rekisteri.count(nimi); 12 } LISTAUS 10.5: Nimirekistero¨inti multisetilla¨ 10.2. STL:n sa¨ilio¨t 321 Assosiaatiotaulu — map Assosiaatiotaulu (map) on tietorakenne, jossa avain ja alkio ovat eril- lisia¨ ja jossa avaimen perusteella voidaan hakea haluttu alkio. Assosi- aatiotaulua voi myo¨s ajatella “taulukkona”, jossa indeksina¨ ka¨yteta¨a¨n halutun tyyppista¨ avainta kokonaisluvun sijaan. Yhta¨ avainta kohden voi assosiaatiotaulussa olla vain yksi alkio. Assosiaatiotaulu ottaa kaksi tyyppiparametria, jotka ma¨a¨ra¨a¨va¨t avaimen ja alkion tyypit. Esimerkiksi map<string, double> on taulu, josta voi etsia¨ liukulukuja avaimina toimivien merkkijono- jen avulla. Vastaavasti map<int,string> antaa mahdollisuuden liit- ta¨a¨ merkkijonoihin kokonaisluvun, jonka perusteella merkkijonon voi myo¨hemmin hakea. Assosiaatiotaulut saa ka¨ytto¨o¨n komennolla #include <map>. Assosiaatiotaulun rajapinnassa on kaikki assosiatiivisten sa¨ilio¨i- den operaatiot. Kuten muissakin assosiatiivisissa sa¨ilio¨issa¨, alkioiden etsiminen, lisa¨a¨minen ja poistaminen kesta¨va¨t logaritmisen ajan. Kos- ka assosiaatiotaulu muistuttaa taulukkoa, siihen on lisa¨tty myo¨s in- deksointi [ ]. Ta¨ma¨ etsii alkion avaimen perusteella kuten find, mut- ta jos alkiota ei lo¨ydy, indeksointi lisa¨a¨ automaattisesti tauluun uu- den alkion, joka luodaan alkiotyypin oletusrakentajalla. Indeksointi palauttaa sitten joko lo¨ytyneen alkion tai ta¨ma¨n uuden alkion. Na¨in assosiaatiotaulua voi ka¨ytta¨a¨ taulukkona, joka ta¨yttyy tyhjilla¨ alkioil- la samalla, kun sita¨ indeksoidaan. Listauksessa 10.6 on toteutettu nimien rekistero¨inti assosiaatio- taululla. Nyt rekisteri onkin assosiaatiotaulu, joka liitta¨a¨ avaimena toimivaan merkkijonoon kokonaisluvun, joka kertoo montako kertaa 1 #include <map> 2 #include <string> 3 using std::map; 4 using std::string; 5 6 unsigned long monirekisterointi2(string const& nimi) 7 { 8 static map<string, unsigned long> rekisteri; // Sa¨ilyy kutsujen va¨lilla¨ 9 return ++rekisteri[nimi]; // Jos uusi nimi, lisa¨ta¨a¨n autom. arvolla 0 10 } LISTAUS 10.6: Nimirekistero¨inti mapilla¨ 10.2. STL:n sa¨ilio¨t 322 merkkijono on rekistero¨ity. Funktio ka¨ytta¨a¨ hyva¨kseen assosiaatiotau- lujen indeksointia. Kun funktiolle annetaan uusi merkkijono, indek- sointi ei lo¨yda¨ sita¨ taulusta. Ta¨llo¨in tauluun lisa¨ta¨a¨n uusi alkio, joka alustetaan oletusarvoonsa eli nollaksi. Jos taas merkkijono on jo rekis- tero¨ity, indeksointi palauttaa tiedon siita¨, montako kertaa rekistero¨in- ti on jo tehty. Rekistero¨intilaskurin kasvattaminen saadaan nyt teh- dyksi helposti kasvattamalla indeksoinnilla lo¨ydettya¨ arvoa yhdella¨. Assosiaatiomonitaulu — multimap Assosiaatiomonitaulu (multimap) eroaa assosiaatiotaulusta samal- la tavoin kuin monijoukko joukosta. Assosiaatiomonitaulussa yhta¨ avainta kohden voi olla useita alkioita. Tilanne vastaa esimerkiksi puhelinluetteloa, jossa yhta¨ nimea¨ kohti voi olla useita puhelinnu- meroita. Assosiaatiomonitaulun esittely luetaan komennolla #include <map> kuten tavallisenkin assosiaatiotaulun. Rajapin- nan osalta assosiaatiomonitaulu muistuttaa assosiaatiotaulua, paitsi etta¨ indeksointia ei ole toteutettu. Syyna¨ ta¨ha¨n on, etta¨ ena¨a¨ yh- ta¨ avainta kohden ei va¨ltta¨ma¨tta¨ lo¨ydy vain yhta¨ arvoa, jonka voisi palauttaa. Ja¨senfunktio find palauttaa jonkin annettua avain- ta vastaavan alkion. Sopivista alkioista ensimma¨isen voi hakea ja¨senfunktiolla lower bound ja viimeista¨ sopivaa seuraavan ja¨sen- funktiolla upper bound, ja molemmat saa tietoonsa yhdella¨ kutsulla equal range. Na¨ita¨ ja aliluvussa 10.3 ka¨sitelta¨via¨ iteraattoreita ka¨yt- ta¨en voi monitaulusta ka¨yda¨ la¨pi kaikki tiettya¨ avainta vastaavat alkiot. Listaus 10.7 seuraavalla sivulla esittelee puhelinluetteloluokan, joka on toteutettu assosiaatiomonitaulua ka¨ytta¨en. Ja¨senmuuttuja luettelo on monitaulu, jossa merkkijonon (nimen) perusteella voi hakea kokonaisluvun (puhelinnumeron). Koska monitaulun tyyppi- ma¨a¨rittely on varsin pitka¨, listauksessa rivilla¨ 18 on ma¨a¨ritelty tyyp- pinimi LuetteloMap. Puhelinluettelon ja¨senfunktioiden toteutus lo¨y- tyy listauksesta 10.8 sivulla 324. Ja¨senfunktio tulosta ka¨ytta¨a¨ ite- raattoreita sopivien puhelinnumeroiden tulostamiseen, joten kyseis- ta¨ koodia kannattaa tutkia tarkemmin vasta aliluvun 10.3 lukemisen ja¨lkeen. 10.2. STL:n sa¨ilio¨t 323 1 #include <map> 2 #include <string> 3 #include <iostream> 4 using std::multimap; 5 using std::pair; 6 using std::string; 7 using std::cout; 8 using std::endl; 9 10 class PuhLuettelo 11 { 12 public: 13 // Tyhja¨t rakentajat ja purkaja kelpaavat 14 void lisaa(string const& nimi, unsigned long numero); 15 unsigned long poista(string const& nimi); 16 void tulosta(string const& nimi) const; 17 private: 18 typedef multimap<string, unsigned long> LuetteloMap; 19 LuetteloMap luettelo ; 20 }; LISTAUS 10.7: multimapilla¨ toteutettu puhelinluetteloluokka 10.2.3 Muita sa¨ilio¨ita¨ Sarjojen ja assosiatiivisten sa¨ilio¨iden lisa¨ksi STL sisa¨lta¨a¨ muuta- mia muita sa¨ilio¨ita¨, jotka eiva¨t tarkasti ottaen kuulu rajapinnal- taan kumpaankaan kategoriaan. Ta¨llaisia sa¨ilio¨ita¨ ovat totuusarvo- vektori vector<bool>, bittivektori bitset seka¨ sa¨ilio¨sovittimet queue, priority queue ja stack. Ta¨ma¨ aliluku esittelee lyhyesti na¨iden sa¨i- lio¨iden perusominaisuudet. Totuusarvovektori — vector<bool> Vektorimalli vector on varsin ka¨teva¨ perustaulukkotyyppina¨ ja kel- paisi periaatteessa sellaisenaan totuusarvovektoriksi eli taulukoksi jonka alkiot ovat tyyppia¨ bool. Ongelmaksi muodostuu kuitenkin, et- ta¨ C++:ssa jokainen olio vie muistia va¨hinta¨a¨n yhden tavun verran, ja ta¨ma¨ rajoitus koskee myo¨s perustyyppia¨ bool olevia alkioita. Na¨in ta- vallista vektoria ka¨ytta¨en esimerkiksi 800 totuusarvon taulukko veisi muistia va¨hinta¨a¨n 800 tavua (todenna¨ko¨isesti enemma¨n). Jokaisen to- 10.2. STL:n sa¨ilio¨t 324 22 void PuhLuettelo::lisaa(string const& nimi, unsigned long numero) 23 { 24 luettelo .insert(make pair(nimi, numero)); // make pair luo “parin” 25 } 26 27 unsigned long PuhLuettelo::poista(string const& nimi) 28 { 29 return luettelo .erase(nimi); // Palauttaa poistettujen lukuma¨a¨ra¨n 30 } 31 32 void PuhLuettelo::tulosta(string const& nimi) const 33 { 34 // Etsi nimen perusteella ala- ja yla¨raja 35 pair<LuetteloMap::const iterator, LuetteloMap::const iterator> 36 alkuJaLoppu = luettelo .equal range(nimi); 37 // Ka¨y la¨pi lo¨ytyneet alkio ja tulosta 38 cout << \"Henkilo¨ \" << nimi << \":\" << endl; 39 for (LuetteloMap::const iterator i = alkuJaLoppu.first; 40 i != alkuJaLoppu.second; ++i) 41 { 42 cout << \" \" << i->second << endl; 43 } 44 } LISTAUS 10.8: Puhelinluettelon toteutus tuusarvon esitta¨miseen riitta¨isi kuitenkin jo yksi bitti, joten teoriassa 800 totuusarvon taulukon voisi saada mahtumaan 100 tavuun. Ta¨ma¨ on esimerkki tilanteesta, jossa luokkamallien erikoistuk- sesta on hyo¨tya¨. STL:ssa¨ on ma¨a¨ritelty mallille vector erikoistus vector<bool>. Ta¨ma¨n erikoistuksen toteutus pakkaa totuusarvot muistin yksitta¨isiin bitteihin niin, etta¨ taulukko vie va¨hemma¨n muis- tia. Koska totuusarvovektori on toteutettu luokkamallin erikoistukse- na, ei ka¨ytta¨ja¨n periaatteessa tarvitse edes tieta¨a¨, etta¨ vector<bool> toteutukseltaan eroaa jotenkin tavallisista vektoreista. Todellisuus ei kuitenkaan ole aivan na¨in ruusuinen. Koska to- tuusarvovektoria ei ole toteutettu aidosti erillisina¨ alkioina, sen ra- japinnassa on pienia¨ eroavaisuuksia normaalin vektorin rajapintaan. Peruska¨yto¨ssa¨ na¨ma¨ eroavaisuudet tuskin tulevat koskaan esille, mut- ta geneerisessa¨ ohjelmoinnissa niilla¨ saattaa olla merkitysta¨. To- tuusarvovektorin eroista tavallisiin sa¨ilio¨ihin on kirjoitettu artikkeli “When Is a Container Not a Container?” [Sutter, 1999]. 10.2. STL:n sa¨ilio¨t 325 Bittivektori — bitset Bittivektori (bitset) on tarkoitettu binaaristen bittisarjojen ka¨sitte- lyyn. Rajapinnaltaan se ei itse asiassa ole edes sa¨ilio¨, mutta C++-stan- dardissa se jostain syysta¨ esitella¨a¨n luvun “assosiatiiviset sa¨ilio¨t” alla. Bittivektorin saa ka¨ytto¨o¨n komennolla #include <bitset>. Malliparametrinaan bittivektori saa yhden kokonaisluvun, joka kertoo kuinka monta bittia¨ vektori sisa¨lta¨a¨. Esimerkiksi bitset<64> ma¨a¨rittelee 64 bitin vektorin. Vektorin koko pysyy vakiona koko ajan, ja sen bitit alustetaan nollaksi. Bittivektorin rajapinnassa on tyypillisia¨ binaarisia operaatioita. Vektorin yksitta¨isia¨ bitteja¨ voi asettaa ykko¨siksi ja nolliksi tai ka¨a¨nta¨a¨ pa¨invastaisiksi. Lisa¨ksi kahdelle samankokoiselle vektorille voi teh- da¨ binaariset operaatiot &= (and), |= (or), ^= (xor) ja flip tai ~ (not). Vektorin bitteja¨ voi myo¨s siirta¨a¨ halutun ma¨a¨ra¨n oikealle tai vasem- malle operaattoreilla >>= ja <<=. Kaiken kukkuraksi bittivektori tar- joaa mahdollisuuden ykko¨sbittien laskemiseen, bittivektorin muutta- miseen kokonaisluvuksi ja takaisin seka¨ joitain muita erityisoperaa- tioita. Sa¨ilio¨sovittimet Kuten jo aiemmin on mainittu, STL tarjoaa joukon sa¨ilio¨sovittimia (container adaptor), jotka eiva¨t itsessa¨a¨n ole sa¨ilio¨ita¨, mutta joiden avulla sa¨ilio¨n rajapinnan saa “sovitetuksi toiseen muottiin”. Sa¨ilio¨so- vittimia on STL:ssa¨ seuraavat kolme: • Pino stack on luokkamalli, jonka rajapinnassa on pinon ka¨sit- telyyn tarvittavat operaatiot empty, size, top, push ja pop. Na¨ista¨ push lisa¨a¨ uuden alkion pinon pa¨a¨lle, top palauttaa pa¨a¨llimma¨i- sen alkion ja pop poistaa sen. Pinon esittely luetaan komennolla #include <stack>. Tyyppiparametrina pinolle annetaan alkioiden tyyp- pi ja sa¨ilio¨, jota ka¨ytta¨en pino toteutetaan. Esimerkiksi stack<int, list<int> > ma¨a¨rittelee pinon, joka on sisa¨isesti toteutettu listana.] Pinon oletustoteutuksena on deque, joten pelkka¨ stack<int> tuottaa pakalla toteutetun pinon. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ]Huomaa syntaksissa sanava¨li kahden loppukulmasulkeen va¨lilla¨! Ilman sita¨ ka¨a¨nta¨ja¨ luu- lee, etta¨ on kyse operaattorista >>, ja antaa usein la¨hes ka¨sitta¨ma¨tto¨ma¨n virheilmoituksen. 10.3. Iteraattorit 326 • Jono queue on samoin luokkamalli, joka tarjoaa rajapinnas- saan jonolle tyypilliset operaatiot empty, size, front, back, push ja pop. Jonossa push lisa¨a¨ uuden alkion jonon pera¨a¨n ja pop poistaa alkion jonon alusta. Operaatio front lukee alkion jo- non alusta ja back lopusta. Jonon saa ka¨ytto¨o¨nsa¨ komennolla #include <queue>. Jonon luomisen syntaksi on aivan sama kuin pinonkin tapauk- sessa, eli queue<string, list<string> > luo listana toteute- tun merkkijonojonon (,) ja queue<int> kokonaislukujonon, jo- ka on toteutettu pakan avulla. • Prioriteettijono priority queue on muuten kuin jono, mutta alkiot sijoitetaan suuruusja¨rjestykseen. Na¨in prioriteettijonosta luetaan front operaatiolla jonon pienin alkio, pop poistaa pie- nimma¨n alkion ja niin edelleeen. Prioriteettijonon syntaksi on sama kuin muidenkin sa¨ilio¨sovittimien, mutta sen toteutuksena ei voi ka¨ytta¨a¨ listaa vaan toteutuksen on tuettava mielivaltaista indeksointia. Oletustoteutuksena prioriteettijonolla on vektori, ja sen esittely luetaan samalla komennolla #include <queue> kuin jononkin. 10.3 Iteraattorit Ohjelmoinnissa on hyvin tavallista ka¨yda¨ tietorakenteen alkioita la¨- pi ja¨rjestyksessa¨ yksi kerrallaan. Vektoreiden ja pakkojen tapauksessa ta¨ma¨ onnistuu helposti indeksoinnin avulla, mutta esimerkiksi lis- toilla ja assosiatiivisilla sa¨ilio¨illa¨ ei ole nopeaa tapaa hakea annetun ja¨rjestysnumeron ma¨a¨ra¨a¨ma¨a¨ alkiota. Lisa¨ksi indeksointi on vektorei- den ja pakkojenkin tapauksessa tarpeettoman tehoton operaatio, kos- ka indeksoinnissa alkioiden laskeminen “aloitetaan aina alusta” eli indeksi ilmoittaa halutun alkion sijainnin suhteessa tietorakenteen alkuun. Tyypillinen ratkaisuyritys ongelmaan on lisa¨ta¨ la¨pika¨ymiseen tar- vittavat operaatiot itse tietorakenteeseen. Esimerkiksi listaluokasta voisi lo¨ytya¨ operaatiot annaEnsimmainen, annaSeuraava ja onkoLoppu, joiden avulla listan alkiot saisi ka¨ydyksi la¨pi. Ta¨llo¨in lista muistai- si itse, missa¨ alkiossa la¨pika¨yminen on silla¨ hetkella¨ menossa. Ta¨ssa¨ ratkaisuyrityksessa¨ on kuitenkin kaksi ongelmaa: 10.3. Iteraattorit 327 • Koska listan ta¨ytyy sisa¨lta¨a¨ tieto senhetkisesta¨ paikasta, lista- olion tila koostuu seka¨ listan alkioista etta¨ la¨pika¨yntipaikasta. Ta¨llo¨in operaatiot annaEnsimmainen ja annaSeuraava muuttavat va¨ista¨ma¨tta¨ listan tilaa eiva¨tka¨ na¨in ollen voi olla vakioja¨sen- funktioita (aliluku 4.3). Ta¨ma¨ puolestaan tarkoittaa sita¨, etta¨ va- kioviitteen pa¨a¨ssa¨ olevaa listaa ei voi selata la¨pi, koska vakio- viitteen la¨pi saa kutsua vain sellaisia ja¨senfunktioita, jotka eiva¨t muuta oliota. Vakioviitteiden ka¨ytto¨ parametreina on niin yleis- ta¨, etta¨ ta¨ma¨ rajoitus aiheuttaa suuria ongelmia. • Toinen ongelma on, etta¨ varsin usein listaa pita¨isi pystya¨ ka¨y- ma¨a¨n la¨pi kahdesta kohtaa yhta¨ aikaa. Esimerkiksi voi olla tar- peen lukea listaa samanaikaisesti alusta loppuun ja lopusta al- kuun ja vertailla alkioita keskena¨a¨n. Koska listaolio muistaa vain yhden paikan listalla, ta¨llaista “limitta¨ista¨” la¨pika¨yntia¨ ei voi tehda¨. Ratkaisu ta¨ha¨n la¨pika¨yntiongelmaan lo¨ytyy ka¨ytta¨ma¨lla¨ olio-oh- jelmoinnin perusperiaatteita. Koska listan alkioiden sa¨ilytta¨minen ja la¨pika¨yntipaikan muistaminen ovat selva¨sti erillisia¨ asioita, voidaan luoda kaksi luokkaa. Toinen on varsinainen lista, joka ei sisa¨lla¨ mi- ta¨a¨n paikkatietoa. Toinen on “kirjanmerkki”, joka vain muistaa, mis- sa¨ kohtaa listaa ollaan la¨pika¨yma¨ssa¨. Ta¨llainen rakenne on aliluvus- sa 9.3.2 esitelty suunnittelumalli Iteraattori. 10.3.1 Iteraattoreiden ka¨ytto¨kohteet STL:ssa¨ iteraattorin ka¨site on eritta¨in ta¨rkea¨. Jokaista sa¨ilio¨tyyppia¨ kohti STL tarjoaa myo¨s iteraattorityypin, jonka avulla sa¨ilio¨n alkiot voi ka¨yda¨ la¨pi. Iteraattoria voi ajatella kirjanmerkkina¨, joka muistaa tietyn paikan tietyssa¨ sa¨ilio¨ssa¨. Iteraattoria voi siirta¨a¨ sa¨ilio¨n sisa¨ssa¨, ja sen “la¨pi” voi myo¨s lukea ja muuttaa sa¨ilio¨n alkioita. Kuva 10.4 seuraavalla sivulla havainnollistaa iteraattoreiden toimintaa. Iteraattoreita ka¨yteta¨a¨n kaikkialla STL:ssa¨ ilmoittamaan tiettya¨ paikkaa sa¨ilio¨ssa¨. Esimerkiksi poisto-operaatio erase ottaa paramet- rikseen iteraattorin, joka ilmoittaa missa¨ kohdassa oleva alkio pois- tetaan. Ta¨llo¨in poistettava alkio on iteraattorin osoittaman va¨lin oi- kealla puolella. (Tarkasti ottaen “oikea” ja “vasen” ovat tietysti tieto- koneessa ja¨rjetto¨mia¨ termeja¨. Sanonnalla “oikealla puolella” tarkoite- 10.3. Iteraattorit 328 Alkio2 Alkio3 Alkio4 Alkio5 Iteraattori Iteraattori Iteraattori Alkio1 Säiliö lukeminen muuttaminen x q siirto p KUVA 10.4: Iteraattorit ja sa¨ilio¨t taan ta¨ssa¨ ja¨lkimma¨ista¨ niista¨ alkioista, joiden va¨liin iteraattori osoit- taa.) Iteraattoreita ka¨yteta¨a¨n paikan ilmaisemiseen myo¨s, kun uusia al- kioita lisa¨ta¨a¨n sa¨ilio¨o¨n. Jos sa¨ilio¨ssa¨ on jo aiemmin n alkiota, uuden alkion voi lisa¨ta¨ n+1 eri paikkaan. Na¨in ollen iteraattorilla pita¨a¨ myo¨s olla n+1 paikkaa, johon se voi osoittaa. Ta¨ma¨ saadaan aikaan, kun ite- raattori voi osoittaa minka¨ tahansa kahden alkion va¨liin ja lisa¨ksi sa¨i- lio¨n kumpaankin pa¨a¨ha¨n eli ensimma¨isen alkion vasemmalle puolel- le ja viimeisen alkion oikealle puolelle. Kuvan 10.4 oikeanpuoleisin iteraattori x havainnollistaa ta¨ta¨. Uusi alkio lisa¨ta¨a¨n aina iteraattorin osoittaman va¨lin oikealle puolelle. Na¨in sa¨ilio¨n loppuun osoittavan iteraattorin avulla lisa¨tty alkio lisa¨ta¨a¨n todella sa¨ilio¨n loppuun. Kun iteraattorin la¨pi luetaan alkion arvo tai muutetaan sita¨, ope- raatio kohdistuu aina iteraattorin oikealla puolella olevaan alkioon samoin kuin alkion poistamisessakin. Sa¨ilio¨n loppuun osoittavan ite- raattorin oikealla puolella ei kuitenkaan ole alkiota. Se onkin vain erityinen “loppumerkki”, ja ohjelmoijan on pidetta¨va¨ huoli siita¨, ettei sa¨ilio¨n loppuun osoittavan iteraattorin la¨pi yriteta¨ lukea tai kirjoittaa. Sa¨ilio¨n loppuun osoittavaa iteraattoria ka¨yteta¨a¨n myo¨s merkkina¨ siita¨, etta¨ operaatio ei onnistunut. Esimerkiksi assosiatiivisten sa¨ilio¨i- den hakuoperaatio find palauttaa paluuarvonaan iteraattorin, jonka oikealla puolella lo¨ytynyt alkio on. Mika¨li halutunlaista alkiota ei lo¨y- 10.3. Iteraattorit 329 tynyt, find palauttaa sa¨ilio¨n loppuun osoittavan iteraattorin merkki- na¨ epa¨onnistumisesta. Kahta iteraattoria voi myo¨s ka¨ytta¨a¨ ma¨a¨ra¨a¨ma¨a¨n tietyn va¨lin (range) sa¨ilio¨ssa¨. Ta¨lla¨ tarkoitetaan niita¨ alkioita, jotka ja¨a¨va¨t iteraat- toreiden va¨liin. Esimerkiksi kuvassa 10.4 iteraattorit p ja q ma¨a¨ra¨a¨va¨t va¨lin, johon kuuluvat alkiot 1, 2 ja 3. Jos iteraattorit osoittavat samaan kohtaan, niiden va¨lilla¨ ei ole alkioita ja niiden ma¨a¨ra¨a¨ma¨ va¨li on tyh- ja¨. Va¨lien avulla sa¨ilio¨sta¨ voidaan esimerkiksi poistaa useita alkioita kerrallaan. Lisa¨ksi STL:n algoritmit ottavat yleensa¨ parametreikseen nimenomaan iteraattoriva¨leja¨, jolloin algoritmin voi kohdistaa vain osaan sa¨ilio¨ta¨. 10.3.2 Iteraattorikategoriat Erilaiset sa¨ilio¨t tarjoavat erilaisia mahdollisuuksia siirta¨a¨ iteraattoria nopeasti paikasta toiseen. Esimerkiksi yhtena¨isella¨ muistialueella to- teutetun vektorin tapauksessa iteraattorin saa vakioajassa siirretyksi kuinka paljon tahansa eteen- tai taaksepa¨in. Sen sijaan linkkiketjuilla toteutetussa listassa iteraattorin saa siirretyksi vakioajassa vain yh- den askeleen verran. STL:n suunnitteluperiaatteena on ollut, etta¨ kaikkien iteraattoreil- le tehtyjen operaatioiden ta¨ytyy onnistua vakioajassa. Ta¨lla¨ tavoin voidaan varmistua siita¨, etta¨ STL:n algoritmit toimivat luvatulla te- hokkuudella siita¨ riippumatta, millaisia iteraattoreita niille annetaan parametreiksi. Iteraattorit voidaan jakaa erilaisiin kategorioihin sen mukaan, millaisia vakioaikaisia operaatioita ne pystyva¨t tarjoamaan. Kuva 10.5 seuraavalla sivulla na¨ytta¨a¨ STL:n iteraattorikategoriat. Kuvaan on merkitty myo¨s, mihin kategoriaan kuuluvia iteraattoreita eri STL:n sa¨ilio¨t tarjoavat. Vaikka kuva onkin piirretty UML-tyyliin periytymista¨ ka¨ytta¨en, C++-standardi ei vaadi, etta¨ erilaiset iteraattorit on todellisuudessa periytetty toisistaan. Riitta¨a¨, etta¨ niiden rajapinnat ovat kuvan mukaiset. STL:n iteraattorikategoriat ovat • syo¨tto¨iteraattori (input iterator). Iteraattorin la¨pi voi vain lukea alkioita mutta ei muuttaa. Lisa¨ksi iteraattoria voi siirta¨a¨ yhden askelen kerrallaan eteenpa¨in. Iteraattorin p tarjoamat operaatiot ovat – *p : Iteraattorin osoittaman alkion arvon lukeminen 10.3. Iteraattorit 330 input iterator * (luku), ==, !=, ++, =, −> syöttöiteraattori output iterator tulostusiteraattori * (kirjoitus), ==, !=, ++, =, −> forward iterator * (luku/kirjoitus) eteenpäin−iteraattori bidirectional iterator −− kaksisuuntainen iteraattori random access iterator +=, −=, +, −, [], <, >, <=, >= hajasaanti−iteraattori deque vector set multiset map multimap list T[] KUVA 10.5: Iteraattorikategoriat – p-> : Iteraattorin osoittaman alkion ja¨senfunktion kutsumi- nen tai struct-kenta¨n lukeminen (vertaa osoittimet) – ++p : Iteraattorin siirta¨minen yhdella¨ eteenpa¨in – p++ : Kuten ++p, mutta palauttaa paluuarvonaan iteraatto- rin, joka osoittaa alkupera¨iseen paikkaan^ – p=q : Iteraattorin sijoittaminen toiseen samanlaiseen – p==q, p!=q : Sen vertailu, osoittavatko iteraattorit samaan paikkaan. • tulostusiteraattori (output iterator). Iteraattori on muuten kuin lukuiteraattori, mutta sen la¨pi voi vain muuttaa alkioita, ei lu- kea. Muuttaminen tapahtuu syntaksilla *p=x. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ^Iteraattoreissa syntaksi ++p on suositeltavampi kuin p++ silloin, kun paluuarvolla ei ole merkitysta¨. Ta¨ha¨n on syyna¨ tehokkuus: p++ joutuu palauttamaan kopion iteraattorin alkupe- ra¨isesta¨ arvosta, kun taas ++p voi palauttaa vain viitteen iteraattoriin itseensa¨, jolloin kopiointia ei tarvita. Ta¨llaisia tehokkuusasioita ka¨siteltiin aiemmin aliluvuissa 7.3 ja 7.1. 10.3. Iteraattorit 331 • eteenpa¨in-iteraattori (forward iterator). Iteraattorin la¨pi voi lu- kea ja muuttaa alkioita, ja lisa¨ksi iteraattoria voi siirta¨a¨ yhdella¨ eteenpa¨in. Eteenpa¨in-iteraattorin rajapinta on yhdistelma¨ luku- ja tulostusiteraattorin rajapinnoista. • kaksisuuntainen iteraattori (bidirectional iterator). Iteraattori on muuten kuin eteenpa¨in-iteraattori, mutta se voi myo¨s siirtya¨ yhden askelen kerrallaan taaksepa¨in. Uudet operaatiot ovat – --p : Iteraattorin siirta¨minen yhdella¨ taaksepa¨in – p-- : Kuten --p, mutta palauttaa paluuarvonaan iteraatorin, joka osoittaa alkupera¨iseen paikkaan. • hajasaanti-iteraattori (random access iterator). Iteraattori on kuin kaksisuuntainen iteraattori, mutta sita¨ voi siirta¨a¨ kerralla mielivaltaisen ma¨a¨ra¨n eteen- tai taaksepa¨in. Lisa¨ksi iteraattoria indeksoimalla voi lukea ja muuttaa muitakin alkioita kuin ite- raattorin oikealla puolella olevaa. Hajasaanti-iteraattoreita voi vertailla keskena¨a¨n sen selvitta¨miseksi, kumpi on sa¨ilio¨ssa¨ en- simma¨isena¨. Lisa¨ksi kahdesta hajasaanti-iteraattorista voi las- kea, montako alkiota niiden va¨lissa¨ on. Na¨ihin tarvittavat ope- raatiot ovat – p+=n : Iteraattorin siirta¨minen n askelta eteenpa¨in (taakse- pa¨in, jos n on negatiivinen) – p-=n : Iteraattorin siirta¨minen n askelta taaksepa¨in (eteen- pa¨in, jos n on negatiivinen) – p+n : Tuottaa uuden iteraattorin, joka osoittaa p:sta¨ n askelta eteenpa¨in (taaksepa¨in, jos n on negatiivinen) – p-n : Tuottaa uuden iteraattorin, joka osoittaa p:sta¨ n askelta taaksepa¨in (eteenpa¨in, jos n on negatiivinen) – p[n] : Sen alkion lukeminen tai muuttaminen, joka on p:n osoittamasta n askelta eteenpa¨in (taaksepa¨in, jos n on ne- gatiivinen) – p-q : Kahden iteraattorin erotus ilmoittaa, kuinka monta askelta p:sta¨ eteenpa¨in q osoittaa (jos q osoittaa paikkaan ennen p:ta¨, tulos ilmoitetaan negatiivisena) 10.3. Iteraattorit 332 – p<q, p<=q, p>q, p>=q : Iteraattori on toista iteraattoria “pie- nempi”, jos sen osoittama paikka on sa¨ilio¨ssa¨ ennen toisen osoittamaa paikkaa. Iteraattoreiden operaatioiden syntaksi on tarkoituksella valittu sellaiseksi, etta¨ se vastaa C++:n ja C:n osoitinaritmetiikkaa, jota voi suo- rittaa taulukkoihin osoittavilla osoittimilla. Iteraattorit ovat itse asias- sa vain osoitinaritmetiikan yleistys mielivaltaisille tietorakenteille. Vastaavasti C++:n perustaulukoihin osoittavat osoittimet kelpaavat ite- raattoreiksi STL:n algoritmeissa. 10.3.3 Iteraattorit ja sa¨ilio¨t Iteraattorit liittyva¨t kiintea¨sti sa¨ilio¨ihin, joten STL:ssa¨ itse iteraattori- luokat ja iteraattoreiden luominen on siirretty sa¨ilio¨luokkien sisa¨lle. Jokainen STL:n sa¨ilio¨luokka ma¨a¨rittelee kaksi luokan sisa¨ista¨ tyyp- pia¨ iterator ja const iterator. Na¨ista¨ iterator ma¨a¨rittelee kyseisel- le sa¨ilio¨lle sopivan iteraattoriluokan. Tyyppi const iterator puoles- taan ma¨a¨rittelee vakio-iteraattorin, joka on muuten samanlainen kuin iterator, mutta sen la¨pi ei voi muuttaa sa¨ilio¨n sisa¨lto¨a¨. Vakio-iteraat- toreiden ka¨ytto¨ vastaa vakio-osoittimien ja vakioviitteiden ka¨ytto¨a¨. Na¨ma¨ iteraattoriluokat kuuluvat kuvan 10.5 na¨ytta¨miin iteraattorika- tegorioihin sa¨ilio¨tyypin mukaan. Sa¨ilio¨iden ma¨a¨rittelemia¨ iteraattorityyppeja¨ ka¨yteta¨a¨n kuin mita¨ tahansa aliluvussa 8.3 ka¨siteltyja¨ rajapintatyyppeja¨. Esimerkiksi ko- konaislukuvektoriin osoittava iteraattori saadaan luoduksi syntaksil- la vector<int>::iterator p; Ta¨llaisista tyyppinimista¨ tulee usein pitkia¨, joten niille kannattaa an- taa paikallisesti lyhyempi nimi typedef-ma¨a¨rittelylla¨: typedef vector<int> Taulu; typedef Taulu::iterator TauluIter; TauluIter p; Kun uusi iteraattori luodaan, se on oletusarvoisesti “tyhja¨” eika¨ osoita minneka¨a¨n. Ta¨llo¨in sita¨ ei myo¨ska¨a¨n saa siirta¨a¨ eika¨ sen la¨pi 10.3. Iteraattorit 333 saa yritta¨a¨ lukea tai kirjoittaa. Jokaisen sa¨ilio¨n rajapinnassa ovat ja¨- senfunktiot begin ja end, joista begin palauttaa sa¨ilio¨n alkuun osoitta- van “alkuiteraattorin”. Vastaavasti end palauttaa “loppuiteraattorin”, joka osoittaa sa¨ilio¨n loppuun. Na¨ma¨ paluuarvot voi sitten sijoittaa talteen iteraattorimuuttujiin. Eritta¨in tyypillinen iteraattoreiden ka¨ytto¨kohde on ka¨yda¨ sa¨ilio¨n alkioit la¨pi yksi kerrallaan. Ta¨sta¨ on esimerkki listauksessa 10.9. Riveilla¨ 3–8 nollataan vektorin alkiot while-silmukassa. Ta¨llo¨in luo- daan iteraattori ja sijoitetaan siihen beginin palauttama iteraattori sa¨i- lio¨n alkuun. Ensimma¨isen alkion voi nyt lukea, tai sita¨ voi muuttaa *-operaattorin avulla. Seuraavaan alkioon pa¨a¨see ++-operaatiolla. Jo- ka kierroksella iteraattoria verrataan ja¨senfunktion end palauttamaan loppuiteraattoriin, jotta tiedeta¨a¨n, milloin kaikki alkiot on ka¨yty la¨pi. Ta¨llo¨in on muistettava, etta¨ loppuiteraattori osoittaa paikkaan viimei- sen alkion ja¨lkeen. Riveilla¨ 13–17 puolestaan tulostetaan sa¨ilio¨n alkiot for-silmukas- sa. Koska alkioita ei ole tarkoitus muuttaa, kannattaa ta¨ssa¨ ka¨ytta¨a¨ va- kio-iteraattoria. Listauksen esimerkissa¨ on itse asiassa pakko ka¨ytta¨a¨ vakio-iteraattoria, koska funktio saa parametrinaan vakioviitteen vek- 1 void nollaaAlkiot(vector<int>& vektori) 2 { 3 vector<int>::iterator i = vektori.begin(); // Alkuun 4 while (i != vektori.end()) // Toistetaan kunnes ollaan lopussa 5 { 6 *i = 0; // Nollataan alkio 7 ++i; // Siirryta¨a¨n seuraavaan alkioon 8 } 9 } 10 11 void tulostaAlkiot(vector<int> const& vektori) 12 { 13 for (vector<int>::const iterator i = vektori.begin(); 14 i != vektori.end(); ++i) 15 { 16 cout << *i << \" \"; 17 } 18 cout << endl; 19 } LISTAUS 10.9: Sa¨ilio¨n la¨pika¨yminen iteraattoreilla 10.3. Iteraattorit 334 toriin. Ta¨ma¨n viitteen la¨pi vektoria ei saa muuttaa. Ta¨ma¨ on varmis- tettu STL:ssa¨ niin, etta¨ vakioviitteen ja -osoittimen kautta kutsuttuna begin ja end palauttavat automaattisesti vakio-iteraattorin, jonka voi sijoittaa vain toiseen vakio-iteraattoriin. Sa¨ilio¨iden omat ja¨senfunktiot ka¨ytta¨va¨t yksinomaan iteraattorei- ta paikan ilmaisemiseen (ainoana poikkeuksena on vektorin ja pakan indeksointi, jossa ka¨yteta¨a¨n ja¨rjestysnumeroa). Vaikka iteraattoreiden ka¨ytto¨ vaatii totuttelua, niiden avulla sa¨ilio¨iden ka¨ytto¨ ka¨y varsin ka¨- teva¨sti. Esimerkiksi uuden alkion 3 lisa¨a¨minen kokonaislukuvekto- rin v alkuun tehda¨a¨n syntaksilla v.insert(v.begin(), 3). Vastaavas- ti vektorin viidennen alkion poisto onnistuu iteraattoriaritmetiikkaa ka¨ytta¨en kutsulla v.erase(v.begin()+5). 10.3.4 Iteraattoreiden kelvollisuus Periaatteessa iteraattorit osoittavat tiettya¨ paikkaa sa¨ilio¨ssa¨ eiva¨tka¨ na¨in ollen ole sidoksissa itse sa¨ilio¨n alkioihin. Iteraattoreiden sisa¨i- nen toteutus riippuu kuitenkin ka¨yta¨nno¨ssa¨ sa¨ilio¨iden sisa¨isesta¨ ra- kenteesta, ja iteraattorit sisa¨lta¨va¨t todenna¨ko¨isesti osoittimia sa¨ilio¨i- den sisa¨iseen toteutukseen tai alkioihin. Jos nyt sa¨ilio¨sta¨ poistetaan alkioita tai sinne lisa¨ta¨a¨n uusia alkioi- ta, voi sa¨ilio¨sta¨ riippuen sen sisa¨inen rakenne muuttua. Esimerkiksi vektori saattaa varata lisa¨a¨ tilaa ja siirta¨a¨ alkiot sinne. Ta¨ma¨ voi puo- lestaan aiheuttaa sen, etta¨ iteraattorin sisa¨inen tieto ei ena¨a¨ olekaan ajan tasalla. STL:ssa¨ sanotaan, etta¨ iteraattori on kelvollinen (valid) niin kau- an, kun se on ka¨ytto¨kelpoinen. Muutokset sa¨ilio¨ssa¨ saattavat aiheut- taa sen, etta¨ iteraattorista tulee kelvoton (invalid). Ta¨llo¨in sanotaan, etta¨ jokin sa¨ilio¨n operaatio mita¨to¨i (invalidate) tietyt iteraattorit. Ta¨l- laiselle kelvottomalle iteraattorille ainoat sallitut operaatiot ovat tu- hoaminen ja uuden arvon sijoittaminen. Kaikki muut operaatiot ai- heuttavat sen, etta¨ ohjelman ka¨ytta¨ytyminen on ma¨a¨rittelema¨to¨n. Ite- raattoreiden lisa¨ksi kelvollisuus koskee myo¨s osoittimia ja viitteita¨ sa¨ilio¨n alkioihin. Jos muutos sa¨ilio¨ssa¨ siirta¨a¨ esimerkiksi alkion toi- seen paikkaan muistissa, myo¨s kaikki osoittimet ja viitteet kyseiseen alkioon muuttuvat kelvottomiksi. Iteraattoreiden muuttuminen kelvottomiksi riippuu sa¨ilio¨sta¨ ja siita¨, millainen muutos siihen tehda¨a¨n. Seuraavassa luettelossa on 10.3. Iteraattorit 335 lueteltu STL:n sa¨ilio¨t ja sa¨ilio¨o¨n kohdistuvien muutosten vaikutus iteraattoreihin, viitteisiin ja osoittimiin. • Vektori: Jos uuden alkion lisa¨a¨misessa¨ ei tarvita muistin uudel- leenvarausta (vektorille on varattu riitta¨va¨sti tilaa reservella¨), mita¨to¨ityva¨t kaikki iteraattorit, osoittimet ja viitteet, jotka osoit- tavat lisa¨yspaikan ja¨lkeisiin alkioihin. Jos uudelleenvaraus suo- ritetaan, mita¨to¨ityva¨t kaikki vektoriin osoittavat iteraattorit, osoittimet ja viitteet. Alkion poisto vektorista mita¨to¨i kaikki iteraattorit, osoittimet ja viitteet poistopaikasta alkaen vektorin loppuun saakka. • Pakka: Uuden alkion lisa¨ys pakan alkuun tai loppuun mita¨to¨i kaikki pakkaan osoittavat iteraattorit. Osoittimet ja viitteet sen sijaan sa¨ilyva¨t kelvollisina. Alkion poisto pakan alusta tai lo- pusta mita¨to¨i vain heti poistettavan alkion vasemmalla puolella olevan iteraattorin seka¨ tietysti osoittimet ja viitteet poistettuun alkioon. Lisa¨ys pakan keskelle tai poisto sielta¨ mita¨to¨i kaikki pakkaan osoittavat iteraattorit, osoittimet ja viitteet. • Lista: Alkion lisa¨a¨minen ei mita¨to¨i mita¨a¨n. Alkion poisto mi- ta¨to¨i heti alkion vasemmalla puolella olevan iteraattorin seka¨ osoittimet ja viitteet poistettuun alkioon. • Assosiatiiviset sa¨ilio¨t: Lisa¨ys ja poisto vaikuttavat samalla ta- voin kuin listaan. Iteraattoreiden kelvollisuuden huomioon ottaminen on a¨a¨rimma¨i- sen ta¨rkea¨a¨ ohjelmoinnissa. Koska kelvottomaan iteraattoriin kohdis- tetut operaatiot aiheuttavat ohjelman ma¨a¨rittelema¨tto¨ma¨n ka¨ytta¨y- tymisen, kelvottomista iteraattoreista aiheutuvia virheita¨ on eritta¨in vaikea lo¨yta¨a¨. Moniin C++-ympa¨risto¨ihin on saatavilla STL-toteutuk- sia, jotka osaavat “testitilassa” ollessaan antaa kelvottomien iteraat- toreiden ka¨yto¨sta¨ va¨litto¨ma¨sti ja¨rkeva¨n virheilmoituksen. Yksi ta¨llai- nen STL-toteutus on STLport [Fomitchev, 2001]. Eri sa¨ilio¨t mita¨to¨iva¨t iteraattoreitaan eri tavalla. Niinpa¨ sa¨ilio¨iden tehokkuuden lisa¨ksi myo¨s sa¨ilio¨n operaatioiden mita¨to¨imisvaikutuk- set kannattaa ottaa huomioon ohjelmaan sopivaa sa¨ilio¨tyyppia¨ valit- taessa. 10.3. Iteraattorit 336 10.3.5 Iteraattorisovittimet Tavallisten iteraattoreiden lisa¨ksi STL tarjoaa myo¨s joukon iteraatto- risovittimia (iterator adaptor). Na¨ma¨ ovat “erikoisiteraattoreita”, jot- ka ka¨ytta¨ytyva¨t jollain lailla tavallisista iteraattoreista poiketen. Ite- raattorisovittimien perinpohjainen la¨pika¨ynti ei ole mahdollista ta¨s- sa¨ teoksessa, mutta ta¨ssa¨ aliluvussa ne esitella¨a¨n kuitenkin lyhyes- ti. Iteraattorisovittimet ovat hyvia¨ esimerkkeja¨ siita¨, etta¨ iteraattorei- den ka¨ytto¨alue on paljon laajempi kuin vain sa¨ilio¨iden yksinkertai- nen la¨pika¨ynti. Iteraattorisovittimien avulla voidaan myo¨s muunnel- la STL:n algoritmien toiminnallisuutta. Iteraattorisovittimet otetaan ka¨ytto¨o¨n komennolla include <iterator>. • Ka¨a¨nteisiteraattorit (reverse iterator) ovat tavallisten iteraatto- reiden “peilikuvia”. Ka¨a¨nteisiteraattoreilla ++ siirta¨a¨ iteraattoria taaksepa¨in ja vastaavasti -- eteenpa¨in. Myo¨s muut siirto-ope- raatiot on peilattu. Lisa¨ksi luku ja kirjoitus ka¨a¨nteisiteraattorin kautta kohdistuvat iteraattorin osoittaman paikan vasemmalle puolelle. Jokaisesta STL:n sa¨ilio¨sta¨ saa ka¨a¨nteisiteraattorin ja¨- senfunktioilla rbegin, joka palauttaa ka¨a¨nteisiteraattorin sa¨ilio¨n loppuun (siis peilattuna alkuun), ja rend, joka antaa vastaavasti ka¨a¨nteisiteraattorin sa¨ilio¨n alkuun. • Lisa¨ysiteraattorit/lisa¨a¨ja¨t (insert iterator/inserter) ovat tulos- tusiteraattoreita, joiden la¨pi kirjoittaminen lisa¨a¨ sa¨ilio¨o¨n uu- den alkion vanhan alkion muuttamisen sijasta. Niiden avul- la saadaan STL:n algoritmit lisa¨a¨ma¨a¨n alkioita sa¨ilio¨ihin. Uu- den alkion lisa¨a¨minen ka¨y yksinkertaisesti syntaksilla *p=x. Ta¨ma¨n ja¨lkeen lisa¨ysiteraattoria ta¨ytyy viela¨ siirta¨a¨ eteen- pa¨in ++-operaattorilla. Sa¨ilio¨n alkuun lisa¨a¨va¨n lisa¨ysiteraat- torin saa funktiokutsulla front inserter(sailio) ja loppuun lisa¨a¨va¨n kutsulla back inserter(sailio). Annetun iteraatto- rin kohdalle alkioita lisa¨a¨va¨n iteraattorin saa aikaan kutsulla inserter(sailio, paikka). • Virtaiteraattorit (stream iterator) ovat luku- tai tulostusi- teraattoreita, jotka sa¨ilio¨iden sijaan lukevat ja kirjoittavat C++:n tiedostovirtoihin. Na¨in esimerkiksi tiedostoja voi ka¨yt- ta¨a¨ STL:n algoritmeissa sa¨ilio¨iden tapaan. Esimerkiksi cin- virrasta kokonaislukuja lukevan lukuiteraattorin saa syntak- silla istream iterator<int>(cin) ja merkkijonoja cout-vir- 10.4. STL:n algoritmit 337 taan pilkuilla erotettuina tulostava iteraattori luodaan kutsulla ostream iterator<string>(cout, ’,’). STL:n valmiina tarjoamien iteraattorisovittimien lisa¨ksi ohjelmoi- ja voi kirjoittaa myo¨s omia iteraattorityyppeja¨a¨n. Na¨in iteraattorit an- tavat varsin monipuolisen tyo¨kalun sa¨ilio¨iden ka¨sittelyyn. 10.4 STL:n algoritmit STL tarjoaa monenlaisia algoritmeja sa¨ilio¨iden ka¨sittelyyn. Jo aiem- min on todettu, etta¨ na¨ma¨ algoritmit on toteutettu irrallisina funk- tiomalleina ja¨senfunktioiden sijaan. Ta¨ma¨ tarkoittaa sita¨, etta¨ algorit- mien ta¨ytyy saada kaikki tarvitsemansa tieto parametrien avulla. Yh- delleka¨a¨n STL:n algoritmille ei kuitenkaan anneta parametreina itse sa¨ilio¨ita¨, vaan kaikki algoritmit ottavat parametreinaan iteraattorei- ta. Syita¨ ta¨ha¨n ehka¨ va¨ha¨n ylla¨tta¨va¨a¨n suunnitteluperiaatteeseen on useita: • Iteraattoreiden avulla algoritmi saadaan toimimaan vain osal- le sa¨ilio¨ta¨. Suurin osa STL:n algoritmeista ottaa parametreinaan kahden iteraattorin ma¨a¨ra¨a¨ma¨n va¨lin. Ta¨ma¨ voi olla joko koko sa¨ilio¨ (jos parametreina annetaan begin- ja end-kutsujen tuotta- mat iteraattorit) tai vain osa siita¨. • Iteraattorit mahdollistavat sen, etta¨ sama algoritmi tekee toimin- tonsa erilaisten sa¨ilio¨iden kesken, koska itse sa¨ilio¨iden tyyp- pia¨ ei tarvitse kertoa algoritmille. Esimerkiksi merge-algoritmin avulla voi yhdista¨a¨ listan ja pakan sisa¨llo¨t vektoriin. • Iteraattorisovittimien avulla voi vaikuttaa algoritmin toimin- taan. Esimerkiksi find-algoritmi etsii normaalisti ensimma¨isen halutun arvoisen alkion. Kun sille annetaan normaalien iteraat- toreiden sijaan ka¨a¨nteisiteraattoreita, se etsiikin viimeisen so- pivan alkion. Samoin normaalisti copy-algoritmi korvaa sa¨ilio¨n alkiot toisen sa¨ilio¨n alkioilla. Jos ka¨yteta¨a¨n lisa¨ysiteraattoreita, copy kuitenkin lisa¨a¨ kopioitavat alkiot korvaamisen sijasta. • Mika¨a¨n ei esta¨ ohjelmoijaa kirjoittamasta omia iteraattorityyp- peja¨a¨n. Ta¨llo¨in STL:n algoritmit toimivat myo¨s niiden kanssa. 10.4. STL:n algoritmit 338 La¨hes mihin tahansa tietorakenneluokkaan on helppo kirjoit- taa siihen sopivat iteraattoriluokat, joten STL:n algoritmit saa va¨ha¨lla¨ vaivalla sovitetuksi la¨hes mihin tahansa tietorakentee- seen. Iteraattoreiden avulla ta¨ma¨ onnistuu, vaikka itse tietora- kenteen rajapinta ei olisikaan yhtena¨inen STL:n sa¨ilio¨iden raja- pintojen kanssa. Voidaan sanoa, etta¨ iteraattorit ovat ika¨a¨n kuin liima algoritmien ja sa¨ilio¨iden va¨lissa¨. Koska algoritmit ka¨ytta¨va¨t iteraattoreita sa¨ilio¨i- den ka¨sittelyyn, niiden tehokkuus riippuu myo¨s iteraattoreiden te- hokkuudesta. Ta¨ma¨ ei kuitenkaan ole ongelma, koska iteraattorit tar- joavat vain sellaisia operaatioita, jotka ovat vakioaikaisia. Eri iteraat- torikategoriat tarjoavat na¨ita¨ operaatioita eri ma¨a¨ra¨n. Toimiakseen te- hokkaasti algoritmit saattavat vaatia, etta¨ niille annettavien iteraatto- reiden ta¨ytyy kuulua va¨hinta¨a¨n tiettyyn kategoriaan. Esimerkiksi sa¨ilio¨n alkiot ja¨rjesta¨va¨ sort-algoritmi vaatii, etta¨ sil- le annettujen iteraattoreiden ta¨ytyy olla hajasaanti-iteraattoreita. Mi- ka¨li na¨in ei ole, annetaan ka¨a¨nno¨saikainen virheilmoitus. Ta¨sta¨ ra- joituksesta seuraa, etta¨ listaa ei voi ja¨rjesta¨a¨ sort-algoritmilla, koska se tarjoaa vain kaksisuuntaiset iteraattorit (ta¨ma¨n vuoksi lista tarjoaa ja¨rjesta¨misen ja¨senfunktionaan). Sen sijaan find-algoritmille riitta¨a¨, etta¨ annetut iteraattorit ovat va¨hinta¨a¨n lukuiteraattoreita. Iteraattorikategorioiden avulla ka¨a¨nta¨ja¨ voi jo ka¨a¨nno¨saikana var- mistua, etta¨ algoritmit pystyva¨t toteuttamaan tehokkuuslupauksensa. Mika¨li algoritmille yriteta¨a¨n antaa iteraattori sellaiseen sa¨ilio¨o¨n, jon- ka operaatiot eiva¨t ole algoritmille riitta¨va¨n tehokkaita, algoritmi ei yksinkertaisesti ka¨a¨nny. Saatu virheilmoitus saattaa kylla¨kin ka¨a¨nta¨- ja¨sta¨ riippuen olla varsin kryptinen, kuten mallien yhteydessa¨ ika¨va¨ kylla¨ usein tapahtuu. Jotta STL:n algoritmeja pystyisi ka¨ytta¨ma¨a¨n, niiden esittelyt ta¨y- tyy ensin ottaa ka¨ytto¨o¨n komennolla #include <algorithm>. Seuraa- vassa listassa on esitelty lyhyesti joitain STL:n algoritmeja. Listan tarkoituksena on antaa yleiskuva siita¨, millaisia asioita algoritmeil- la pystyy tekema¨a¨n. Algoritmien tarkemman syntaksin ja kuvauksen voi lukea monista C++-kirjoista [Lippman ja Lajoie, 1997], [Stroustrup, 1997] tai erityisesti C++:n kirjastoja ka¨sittelevista¨ kirjoista [Josuttis, 1999]. • copy(alku, loppu, kohde) kopioi va¨lilla¨ alku–loppu olevat al- kiot iteraattorin kohde pa¨a¨ha¨n. Se korvaa vanhat alkiot kopioi- 10.4. STL:n algoritmit 339 duilla, joten alkioiden lisa¨a¨miseen tarvitaan lisa¨ysiteraattoreita. Operaatio on lineaarinen, alku ja loppu lukuiteraattoreita, kohde tulostusiteraattori. • find(alku, loppu, arvo) etsii va¨lilta¨ alku–loppu ensimma¨isen alkion, jonka arvo on arvo, ja palauttaa iteraattorin siihen. Jos sopivaa alkiota ei lo¨ydy, palautetaan loppu. Tehokkuudeltaan operaatio on lineaarinen, alku ja loppu ovat lukuiteraattoreita. Algoritmille voi arvon sijaan antaa parametrina totuusarvon pa- lauttavan funktion, joka kertoo onko sille annettu parametri ha- lutunlainen. • sort(alku, loppu) ja¨rjesta¨a¨ va¨lilla¨ alku–loppu olevat alkiot suuruusja¨rjestykseen. Algoritmin tehokkuus on keskima¨a¨rin n log n, ja iteraattoreiden on oltava hajasaanti-iteraattoreita. sort-algoritmille voi myo¨s antaa vertailufunktion ylima¨a¨ra¨ise- na¨ parametrina. • merge(alku1, loppu1, alku2, loppu2, kohde) edellytta¨a¨, etta¨ va¨lien alku1–loppu1 ja alku2–loppu2 alkiot ovat suuruusja¨rjes- tyksessa¨. Algoritmi yhdista¨a¨ kyseisten va¨lien alkiot ja kopioi ne suuruusja¨rjestyksessa¨ iteraattorin kohde pa¨a¨ha¨n aivan kuten copy. Operaatio on lineaarinen, alku- ja loppu-iteraattorit ovat lukuiteraattoreita, kohde on tulostusiteraattori. • for each(alku, loppu, funktio) antaa jokaisen va¨lilla¨ alku– loppu olevan alkion vuorollaan funktion parametriksi ja kutsuu funktiota. Tehokkuudeltaan for each on lineaarinen. Jos alku ja loppu ovat lukuiteraattoreita, funktio ei saa muuttaa parametri- naan saamansa alkion arvoa. Jos iteraattorit ovat eteenpa¨in-ite- raattoreita, alkioita saa muuttaa. • partition(alku, loppu, ehtofunktio) ja¨rjesta¨a¨ va¨lilla¨ alku– loppu olevat alkiot niin, etta¨ ensin tulevat ne alkiot, joilla ehtofunktio palauttaa true, ja sitten ne, joilla se palauttaa false. Tehokkuus algoritmissa on lineaarinen, iteraattoreiden tulee olla kaksisuuntaisia iteraattoreita. • random shuffle(alku, loppu) sekoittaa va¨lilla¨ alku–loppu ole- vat alkiot satunnaiseen ja¨rjestykseen. Sekoituksen tehokkuus 10.5. Funktio-oliot 340 on lineaarinen, alku ja loppu hajasaanti-iteraattoreita. Algorit- mille voi ylima¨a¨ra¨isena¨ parametrina antaa oman satunnaislu- kugeneraattorin. Listauksessa 10.10 on lyhyt esimerkki STL:n algoritmien ka¨yto¨s- ta¨. Listauksen funktio ottaa ensin muistiin vektorin ensimma¨isen ja viimeisen alkion arvot. Sen ja¨lkeen se ja¨rjesta¨a¨ vektorin suuruusja¨r- jestykseen. Se etsii viela¨ ja¨rjestetysta¨ vektorista muistiin otettuja ar- voja vastaavat alkiot. Lopuksi se poistaa na¨iden va¨liin ja¨a¨va¨t alkiot eli alkiot, jotka ovat suurempia tai yhta¨ suuria kuin alkupera¨inen en- simma¨inen alkio mutta pienempia¨ kuin alkupera¨inen viimeinen al- kio. Operaatioista find ja erase ovat lineaarisia ja sort keskima¨a¨- rin n log n, joten koko funktion keskima¨a¨ra¨iseksi tehokkuudeksi tulee n log n. 10.5 Funktio-oliot Monissa tapauksissa STL:n algoritmeille pita¨a¨ iteraattoreiden lisa¨ksi va¨litta¨a¨ myo¨s tietoa siita¨, miten algoritmin tulisi toimia. Esimerkiksi algoritmille sort voi antaa tarvittaessa tiedon siita¨, miten alkioiden 1 #include <algorithm> 2 using std::find; 3 using std::sort; 4 #include <vector> 5 using std::vector; 6 7 void jarjestaJaPoista(vector<int>& vektori) 8 { 9 int eka = vektori.front(); // Ensimma¨inen alkio talteen 10 int vika = vektori.back(); // Viimeinen alkio talteen 11 sort(vektori.begin(), vektori.end()); // Ja¨rjesta¨ 12 vector<int>::iterator ekanpaikka = 13 find(vektori.begin(), vektori.end(), eka); // Etsi eka 14 vector<int>::iterator vikanpaikka = 15 find(vektori.begin(), vektori.end(), vika); // Etsi vika 16 // Poista alkiot eka ≤alkio < vika 17 vektori.erase(ekanpaikka, vikanpaikka); 18 } LISTAUS 10.10: Esimerkki STL:n algoritmien ka¨yto¨sta¨ 10.5. Funktio-oliot 341 “suuruutta” vertaillaan keskena¨a¨n. Vastaavasti algoritmi find if etsii ensimma¨isen alkion, joka toteuttaa algoritmille va¨litetyn ehdon. STL:ssa¨ on yleensa¨ kaksi tapaa va¨litta¨a¨ algoritmien (ja assosia- tiivisten sa¨ilio¨iden) sisa¨a¨n ta¨llaista “toiminnallisuutta”. Toinen tapa, funktio-osoittimet (function pointers), on jo C-kielesta¨ pera¨isin. Nii- den lisa¨ksi STL:ssa¨ ka¨yteta¨a¨n usein funktio-olioita (function objects), joilla voidaan saada hieman yleiska¨ytto¨isempia¨ ratkaisuja. Seuraavat aliluvut esitteleva¨t na¨iden kahden tavan perusteet. 10.5.1 Toiminnallisuuden va¨litta¨minen algoritmille Oletetaan, etta¨ ohjelmassa halutaan tulostaa kaikki kokonaislukuvek- torin alkiot, jotka ovat arvoltaan alle 5. Ta¨ma¨ olisi tietysti mahdollis- ta tehda¨ suhteellisen helposti for-silmukalla, mutta toisaalta STL:sta¨ lo¨ytyy valmiina algoritmeja halutunlaisten alkioiden etsimiseen. Jos tarkoituksena olisi hakea alkiot, joiden arvo on yhta¨ suuri kuin 5, lo¨y- tyisi ensimma¨inen ta¨llainen alkio yksinkertaisesti kutsulla find(vektori.begin(), vektori.end(), 5) Sen sijaan 5:tta¨ pienempien alkioiden etsiminen on hieman vaa- tivampaa. STL:sta¨ ei lo¨ydy valmista algoritmia find less, koska eri- laisia ta¨llaisia hakualgoritmeja olisi niin monta erilaista, ettei niiden koodaaminen erikseen olisi ja¨rkeva¨a¨. Sen sijaan STL:ssa¨ on algorit- mi find if, jolle voidaan kertoa, millaista alkiota halutaan. Helpoin tapa ka¨ytta¨a¨ ta¨ta¨ algoritmia on va¨litta¨a¨ sille kolmantena parametrina funktio-osoitin (function pointer), joka osoittaa funktioon jota ka¨yte- ta¨a¨n alkioiden testaamiseen. Funktio-osoitinta voi ajatella tavallisena osoittimena, joka muut- tujaan tai olioon osoittamisen sijaan osoittaakin johonkin ohjelman funktioon. Ta¨llaisen osoittimen saa luotua normaalilla syntaksilla &funktionimi, ja funktio-osoittimen la¨pi funktiota kutsutaan ika¨a¨n kuin osoitin itse olisi kyseinen funktio. Funktio-osoittimen tyyppi ma¨a¨ra¨a¨ millaisia parametreja ottaviin funktioihin osoitin voi osoit- taa. Samoin tyyppi ma¨a¨ra¨a¨ myo¨s osoitettavan funktion paluutyypin. Ta¨ssa¨ teoksessa ei menna¨ funktio-osoittimien ka¨yto¨n yksityiskohtiin, mutta mainittakoon, etta¨ funktio-osoittimien lisa¨ksi C++ tarjoaa myo¨s mahdollisuuden ja¨senfunktio-osoittimiin (member function pointer), jotka voi laittaa osoittamaan tietyn luokan tietynlaisiin ja¨senfunktioi- 10.5. Funktio-oliot 342 hin. Sen sijaan C++ ei tunne ka¨sitetta¨ “viite (ja¨sen)funktioon”, vaikka osoittimet ovatkin mahdollisia. Listaus 10.11 na¨ytta¨a¨ esimerkin funktio-osoittimen ka¨yto¨sta¨. Siina¨ on ensin ma¨a¨ritelty funktio onkoAlle5, joka ottaa kokonaislukupara- metrin ja palauttaa totuusarvon, joka kertoo oliko parametri 5:tta¨ pie- nempi. Rivilla¨ 10 STL:n algoritmille find if va¨liteta¨a¨n osoitin ta¨ha¨n funktioon. Algoritmi ka¨y la¨pi ja¨rjestyksessa¨ vektorin alkiot ja kutsuu osoittimen pa¨a¨ssa¨ olevaa funktiota antaen kunkin alkion sille para- metrina. Algoritmi jatkaa ta¨ta¨ niin kauan, kunnes kaikki alkiot on ka¨y- ty la¨pi tai osoittimen pa¨a¨ssa¨ oleva funktio on palauttanut arvon tosi. Ta¨lla¨ tavoin find if ta¨ssa¨ tapauksessa etsii vektorista ensimma¨isen 5:tta¨ pienemma¨n alkion. Samalla tavalla monet muutkin STL:n algoritmit ottavat para- metreikseen funktio-osoittimia, joilla alkioita voi testata tai ka¨sitella¨. Na¨in samaa algoritmia voi ka¨ytta¨a¨ useaan eri tarkoitukseen antamal- la sille osoitin sopivaan funktioon. Funktio-osoittimien ka¨yto¨ssa¨ on kuitenkin rajoituksensa. Jos ohjelmassa haluttaisiin myo¨s etsia¨ vek- torista alkioita, jotka ovat arvoltaan pienempia¨ kuin 7, pita¨isi ohjel- maan kirjoittaa uusi testausfunktio onkoAlle7. Ta¨ma¨ ei tietenka¨a¨n ole ja¨rkeva¨ ratkaisu, jos testausfunktioiden ma¨a¨ra¨ kasvaisi suureksi. Viela¨ ongelmallisemmaksi tilanne tulee, jos testauksessa ka¨ytetta¨- va¨ raja ei olekaan ka¨a¨nno¨saikana tiedossa, vaan se saadaan funktioon 1 bool onkoAlle5(int i) 2 { 3 return i < 5; 4 } 5 6 void tulostaAlle5(vector<int> const& v) 7 { 8 vector<int>::const iterator i = v.begin(); 9 10 while ((i = find if(i, v.end(), &onkoAlle5)) != v.end()) 11 { 12 cout << *i << ’ ’; 13 ++i; 14 } 15 cout << endl; 16 } LISTAUS 10.11: Funktio-osoittimen va¨litta¨minen parametrina 10.5. Funktio-oliot 343 parametrina, kysyta¨a¨n ka¨ytta¨ja¨lta¨ tai vaikkapa lasketaan ajoaikana. Ongelmana on, etta¨ kaikki funktion tarvitsemat tiedot ta¨ytyy antaa sille parametreina silloin, kun funktiota kutsutaan (globaalit muuttu- jat antavat ta¨ha¨n pienen porsaanreia¨n, mutta niiden ka¨yto¨n ongelmat ovat yleensa¨ suuremmat kuin hyo¨dyt). Ta¨ssa¨ tapauksessa haluttaisiin testauksessa ka¨ytetta¨va¨ raja kiinnitta¨a¨ jo silloin, kun vertailufunktio annetaan find if-algoritmille parametrina, ja find if:n sisa¨lla¨ sitten varsinaisesti kutsuttaisiin funktiota ja annettaisiin sille testattava al- kio. C++:n funktiot ja funktio-osoittimet eiva¨t kuitenkaan taivu ta¨llai- seen ka¨ytto¨o¨n, vaan niiden sijaan ta¨ytyy ka¨ytta¨a¨ seuraavassa esitelta¨- via¨ funktio-oliota. 10.5.2 Funktio-olioiden periaate Funktio-oliot (function object) ovat olioita, joille on ma¨a¨ritelty funk- tiokutsuoperaattori (). Ta¨ma¨n avulla na¨ita¨ olioita voi “kutsua” aivan kuin ne olisivat funktioita. Verrattuna tavallisiin funktioihin funktio- olioilla on se hyva¨ puoli, etta¨ ne voivat muistaa asioita kutsukerto- jen va¨lilla¨. Funktio-olioille voi esimerkiksi antaa luomisen yhteydes- sa¨ tietoja, jotka olio panee talteen. Kun olio sitten va¨liteta¨a¨n paramet- rina jollekin algoritmille, se voi kutsujen yhteydessa¨ ka¨ytta¨a¨ na¨ita¨ si- sa¨a¨nsa¨ talletettuja tietoja hyva¨kseen. Joissain C++-teoksissa funktio-olioista ka¨yteta¨a¨n myo¨s nimitysta¨ funktori (functor). Ta¨ta¨ nimitysta¨ olisi lyhyydesta¨a¨n huolimatta eh- ka¨ syyta¨ va¨ltta¨a¨, koska matematiikassa termi funktori on jo ka¨yto¨ssa¨, ja silla¨ tarkoitetaan varsin eri asiaa. Funktio-olioilla voidaan sen si- jaan saada aikaan samanlaisia vaikutuksia kuin funktionaalisen oh- jelmoinnin sulkeumilla (closure) [Wikstro¨m, 1987]. C++:ssa funktio-olioita saadaan aikaan kirjoittamalla luokkia, jois- sa on ma¨a¨ritelty (yksi tai useampi) ja¨senfunktio nimelta¨ operator(). Listaus 10.12 seuraavalla sivulla na¨ytta¨a¨ esimerkin ta¨llaisesta. Kun luokasta luodaan olio, voi ta¨ta¨ oliota kutsua ika¨a¨n kuin se oli- si oikea funktio. Syntaksi olio(parametrit) aiheuttaa funktiokut- suoperaattorin kutsumisen ika¨a¨n kuin ohjelmaan olisi kirjoitettu olio.operator()(parametrit). Hyo¨tyna¨ funktio-olioissa on, etta¨ niilla¨ voi normaalien olioiden ta- paan olla ja¨senmuuttujia, joihin talletetaan tietoja. Tyypillisin tapaus on, etta¨ oliota luotaessa luokan rakentajalle va¨liteta¨a¨n parametreja, jotka talletetaan olion ja¨senmuuttujiin. Ta¨ma¨n ja¨lkeen oliota kutsut- 10.5. Funktio-oliot 344 1 class OnkoAlle 2 { 3 public: 4 OnkoAlle(int raja); 5 // Funktiokutsuoperaattori 6 inline bool operator()(int verrattava) const; 7 private: 8 int raja ; 9 }; ... 10 OnkoAlle::OnkoAlle(int raja) : raja (raja) 11 { 12 } 13 14 inline bool OnkoAlle::operator()(int verrattava) const 15 { 16 return verrattava < raja ; 17 } LISTAUS 10.12: Esimerkki funktio-olioluokasta taessa (sen funktiokutsuoperaattoria kutsuttaessa) olio voi kutsun yh- teydessa¨ va¨litettyjen parametrien lisa¨ksi ka¨ytta¨a¨ hyva¨kseen myo¨s ja¨- senmuuttujiin talletettuja tietoja. Periaatteessa olio voi tietysti myo¨s muuttaa ja¨senmuuttujiensa arvoja kutsujen yhteydessa¨, mutta ta¨ta¨ ei yleensa¨ suositella¨ STL:n yhteydessa¨ muutamaa poikkeusta lukuu- nottamatta. Kuva 10.6 seuraavalla sivulla havainnollistaa tavallisen funktion ja funktio-olion eroja. Ohjelmassa funktio-olioita voi ka¨ytta¨a¨ monella eri tavalla. Lis- tauksessa 10.13 seuraavalla sivulla esitella¨a¨n niista¨ muutama. Funk- tiossa kaytto1 luodaan rivilla¨ 3 funktio-olio aivan tavallisen olion ta- paan, ja sille annetaan vertailun rajaksi luku 5. Rivilla¨ 4 funktio-oliota sitten kutsutaan ja sille annetaan vertailtavaksi luvuksi 8. Ta¨ma¨ esi- merkki on tarkoitettu vain havainnollistamaan funktio-olioiden syn- taksia, silla¨ na¨in yksinkertaisessa esimerkissa¨ funktio-oliosta ei viela¨ ole mita¨a¨n varsinaista hyo¨tya¨. Listauksen 10.13 toinen esimerkki on jo ka¨ytto¨kelpoisempi. Ri- veilla¨ 7–13 ma¨a¨ritella¨a¨n funktiomalli kysyJaTestaa, joka ottaa para- metrinaan funktio-olion. Funktiomalli lukee syo¨tteesta¨ luvun ja kut- suu funktio-oliota antaen luetun luvun parametriksi. Jos funktio-olio palauttaa arvon tosi, tulostetaan teksti. Ta¨llainen funktiomalli on var- 10.5. Funktio-oliot 345 funktio param1 param2 tulos (a) Normaali funktio Funktio−olio Luominen tieto Funktio−olio tieto Kutsuminen parametri tulos operator() operator() (b) Funktio-olio KUVA 10.6: Funktio-olioiden idea 1 void kaytto1() 2 { 3 OnkoAlle fo(5); ... 4 if (fo(8)) { cout << \"8 < 5!\" << endl; } 5 } 6 7 template <typename FunktioOlio> 8 void kysyJaTestaa(FunktioOlio const& fo) 9 { 10 int i; 11 cin >> i; // Virhetarkastelu puuttu 12 if (fo(i)) { cout << \"Ehto toteutui!\" << endl; } 13 } 14 15 void kaytto2(int raja) 16 { 17 kysyJaTestaa(OnkoAlle(raja)); 18 kysyJaTestaa(OnkoAlle(2*raja)); 19 } LISTAUS 10.13: Funktio-olion ka¨ytto¨esimerkkeja¨ 10.5. Funktio-oliot 346 sin yleiska¨ytto¨inen, koska sen ei tarvitse tieta¨a¨ miten luettua lukua testataan, koska testaus tapahtuu parametrina saadussa funktio-olios- sa. Lopuksi funktiossa kaytto2 kutsutaan ta¨ta¨ funktiomallia. Kutsun yhteydessa¨ luodaan suoraan va¨liaikainen funktio-olio luokan raken- tajaa kutsumalla, ja olioon tallentuu testauksen yla¨raja. Kun kutsu on ohi, tuhotaan va¨liaikainen funktio-olio automaattisesti. Funktioiden kaytto2 ja kysyJaTestaa kuvaamaa toiminnallisuut- ta ei voisi saada aikaan esimerkiksi funktio-osoittimia ka¨ytta¨en. Lis- tauksessahan kaytto2 tallettaa oman parametrinsa va¨litetta¨va¨n funk- tio-olion sisa¨a¨n, ja funktio-oliota puolestaan kutsutaan toisessa funk- tiossa. Ta¨llaisessa ka¨yto¨ssa¨ funktio-oliot ovat la¨hes va¨ltta¨ma¨tto¨mia¨. Kaikki funktio-osoittimia hyva¨ksyva¨t STL:n algoritmit hyva¨ksy- va¨t parametreikseen myo¨s funktio-olioita (lisa¨ksi tietyissa¨ tilanteissa STL:ssa¨ funktio-oliot ovat ainoa vaihtoehto). Niinpa¨ aiemmin esite- tyn find if-esimerkin voi kirjoittaa yleiska¨ytto¨isemmin myo¨s funk- tio-olioita ka¨ytta¨ma¨lla¨. Ta¨ma¨ on tehty listauksessa 10.14, jossa tulos- tettavien arvojen yla¨raja saadaan funktioon parametrina. Funktio-olioiden yhteydessa¨ on syyta¨ huomata, etta¨ niita¨ ka¨yt- ta¨va¨t algoritmit saattavat tyypillisesti joskus kopioida ka¨ytta¨mia¨a¨n funktio-olioita. Niinpa¨ jokaisessa funktio-olioluokassa tulisi olla toi- miva kopiorakentaja (aliluku 7.1.2). Lisa¨ksi kaikki STL:n algoritmit eiva¨t takaa, etta¨ ne ka¨ytta¨isiva¨t jatkuvasta samaa kopiota funktio- oliosta. Na¨in kaikissa STL:n algoritmeissa funktio-olio ei voi tallet- taa sisa¨a¨nsa¨ kutsukertojen va¨lilla¨ muuttuvaa tietoa, koska seuraava kutsukerta voikin ka¨ytta¨a¨ eri kopiota oliosta. Kaikkein turvallisinta 1 void tulostaAlle(vector<int> const& v, int raja) 2 { 3 vector<int>::const iterator i = v.begin(); 4 5 while ((i = find if(i, v.end(), OnkoAlle(raja))) != v.end()) 6 { 7 cout << *i << ’ ’; 8 ++i; 9 } 10 cout << endl; 11 } LISTAUS 10.14: Funktio-olion ka¨ytto¨ STL:ssa¨ 10.5. Funktio-oliot 347 onkin yleensa¨ tehda¨ funktiokutsuoperaattorista vakioja¨senfunktio ja pita¨a¨ funktio-olion ja¨senmuuttujien arvot muuttumattomina. 10.5.3 STL:n valmiit funktio-oliot C++:n standardikirjasto tarjoaa valmiina joukon funktio-olioita, jotta kaikkein tavallisimmissa tapauksissa ohjelmoijan ei aina tarvitsisi kirjoittaa omia funktio-olioluokkiaan. Kaikkia kirjaston funktio-olio- tyyppeja¨ ei esitella¨ ta¨ssa¨, mutta seuraavassa pyrita¨a¨n antamaan sup- pea yleiskuva siita¨, miten kirjaston yleiska¨ytto¨isia¨ funktio-olioita on tarkoitus ka¨ytta¨a¨. Koska on varsin ka¨teva¨a¨ pystya¨ va¨litta¨ma¨a¨n kaikkia C++:n perus- operaatioita myo¨s funktio-olioparametreina, STL ma¨a¨rittelee la¨hes kaikkia ta¨llaisia operaatioita varten luokkamallit, joista funktio-olioi- ta pystyy luomaan. Ta¨llaisia malleja ovat muun muassa plus, minus, multiplies, divides, equal to, less ja greater. Kaikki na¨ma¨ ottavat tyyppiparametrinaan funktio-olion parametrien tyypin, joten esimer- kiksi kaksi liukulukua yhteenlaskeva funktio-olio olisi plus<double> ja kahden kokonaisluvun pienemmyytta¨ vertailevan funktio-olion tyyppi olisi vastaavasti less<int>. Na¨ma¨ ta¨ssa¨ mainitut funktio-oliot eiva¨t sisa¨lla¨ ja¨senmuuttujia, vaan ne matkivat aivan tavallisia funk- tioita ja niilla¨ on vain oletusrakentaja. Ta¨ma¨n vuoksi niita¨ paramet- reina va¨litetta¨essa¨ funktio-oliot luodaan oletusrakentajaa kutsumalla, siis esimerkiksi less<int>(). Tavallisia funktioita matkivien funktio-olioiden lisa¨ksi STL tarjo- aa funktio-oliosovittimia (function object adaptor), jotka muuntavat olemassa olevia funktio-olioita toisenlaisiksi. Sovittimista ta¨rkeim- ma¨t ovat bind1st ja bind2nd, joiden avulla kaksi parametria ottavista funktio-olioista kumman tahansa parametrin voi “kiinnitta¨a¨” halut- tuun arvoon funktio-olioita luotaessa. Kuva 10.7 seuraavalla sivulla na¨ytta¨a¨ sovittimen bind2nd rakenteen. Sovittimet ovat itsekin funktio-olioita, joiden funktiokutsuope- raattori ottaa vain yhden parametrin. Ja¨senmuuttujissaan sovitin pi- ta¨a¨ muistissa varsinaisen kaksiparametrisen funktio-olion ja lisa¨ksi kiinnitetyn parametrin arvon. Na¨ma¨ annetaan sovittimelle sita¨ luo- taessa. Kun sovittimen funktiokutsuoperaattoria kutsutaan yhdella¨ parametrilla, kutsuu sovitin ja¨senmuuttujassaan olevaa funktio-olio- ta ja antaa sille saamansa parametrin seka¨ ja¨senmuuttujaansa talle- 10.6. C++: Template-metaohjelmointi 348 bind2nd less<int> 3 param operator() tulos tulos operator() KUVA 10.7: Funktio-olio bind2nd(less<int>(), 3) tetun kiintea¨n parametrin. Tuloksena saadun paluuarvon sovitin pa- lauttaa itse edelleen kutsujalle. Funktio-oliosovittimien avulla pystyy muista funktio-olioista muodostamaan ka¨teva¨sti halutun testauksen tai operaation suoritta- via versioita. Listauksessa 10.15 seuraavalla sivulla na¨yteta¨a¨n, miten aiemmin esimerkkina¨ ollut annettua arvoa pienempien alkioiden tes- taus voidaan koodata STL:n omia funktio-olioita ja sovittimia ka¨yt- ta¨ma¨lla¨. Listauksen lauseke bind2nd(less<int>(), arvo) luo yhden parametrin ottavan funktio-olion, joka palauttaa toden, jos parametri on pienempi kuin arvo. Ta¨ma¨ funktio-olio va¨liteta¨a¨n sitten paramet- rina find if-algoritmille. 10.6 C++: Template-metaohjelmointi Normaalisti tietokoneohjelmia ajettaessa ohjelmat ka¨sitteleva¨t ka¨yt- to¨kohteeseensa liittyvia¨ tietoja, jotka niille ajoaikana syo¨teta¨a¨n (tai jotka on upotettu osaksi itse ohjelman rakennetta). Toisaalta voidaan myo¨s ajatella, etta¨ itse tietokoneohjelmakin on vain dataa, jota tieto- kone ka¨sittelee (suorittamalla sita¨). Ta¨ma¨ tulee selkea¨sti esille siina¨, 10.6. C++: Template-metaohjelmointi 349 1 #include <functional> 2 using std::less; 3 using std::bind2nd; 4 5 void tulostaAlle2(vector<int> const& v, int raja) 6 { 7 vector<int>::const iterator i = v.begin(); 8 9 while ((i = find if(i, v.end(), bind2nd(less<int>(),raja))) != v.end()) 10 { 11 cout << *i << ’ ’; 12 ++i; 13 } 14 cout << endl; 15 } LISTAUS 10.15: C++:n funktio-olioiden less ja bind2nd ka¨ytto¨ etta¨ jokainen C++-ohjelmahan on vain ajoaikaista tietoa C++-ka¨a¨nta¨ja¨lle, joka muuntaa C++-koodin konekieliseksi objektitiedostoksi. 10.6.1 Metaohjelmoinnin ka¨site Ohjelmia, jotka ka¨sitteleva¨t toisia ohjelmia ja kenties tuottavat lop- putuloksenaan uusia ohjelmia, sanotaan metaohjelmiksi_ (metapro- gram). Ta¨llaisia ohjelmia ovat varsinaisten ka¨a¨nta¨jien lisa¨ksi myo¨s erilaiset ohjelmageneraattorit, esika¨a¨nta¨ja¨t, ka¨ytto¨liittyma¨generaatto- rit ja niin edelleen. Vastaavasti metaohjelmien kirjoittamista kutsu- taan yleisesti metaohjelmoinniksi (metaprogramming). Kirjan “Gen- erative Programming” [Czarnecki ja Eisenecker, 2000] luvussa 10 on varsin hyva yleiskatsaus metaohjelmoinnin ka¨sitteisiin. Metaohjelmointi muuttuu huomattavasti mielenkiintoisemmak- si, jos ohjelma pystyy tutkimaan omaa rakennettaan ja kenties vai- kuttamaan omaan koodiinsa. Ta¨llaista ohjelman kykya¨ “itsetutkiske- luun” kutsutaan nimella¨ reﬂektio (reﬂection). Joissain kielissa¨ kuten Smalltalk:ssa tuki reﬂektiolle on varsin laaja, ja Smalltalk-ohjelmat pys- tyva¨t tutkimaan omaa luokkahierarkiaansa, luomaan uusia aliluokkia . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . _Kreikan sana “meta” tarkoittaa suunnilleen samaa kuin “ja¨lkeen”. Sita¨ ka¨yteta¨a¨n yleisesti etuliitteena¨, jolla kuvataan alkupera¨isen ka¨sitteen suhteen “ylemma¨lla¨ tasolla” olevaa asiaa. Niinpa¨ metaohjelmointi on ohjelmointia, jossa metaohjelma ka¨sittelee varsinaista ohjelma- koodia. 10.6. C++: Template-metaohjelmointi 350 tai jopa ajoaikana muuttamaan olion tyypin toiseksi. Aliluvun 8.2.1 metaluokat ovat yksi esimerkki Smalltalk:n reﬂektio-ominaisuuksista. C++:n ja Javan tapaisissa ka¨a¨nnetta¨vissa¨ kielissa¨ mahdollisuus reﬂektioon on yleensa¨ paljon rajoitetumpi. Javan Reflection-rajapin- nan avulla ohjelma voi jonkin verran tarkastella omaa rakennettaan ja esim. kysya¨ ajoaikana, mita¨ ja¨senfunktioita luokasta lo¨ytyy ja vaikka- pa kutsua ja¨senfunktiota, jonka nimi lo¨ytyy merkkijonomuuttujasta. Sen sijaan helppoa mahdollisuutta ohjelmankoodin muuttamiseen tai tuottamiseen ei ole. C++:n mahdollisuudet reﬂektioon ajoaikana ovat viela¨ rajoitetummat. Aliluvussa 6.5.3 esitellyt dynamic cast ja typeid ovat rajoitettuja yksinkertaisia esimerkkeja¨ tilanteista, joissa ohjelma pystyy tutkimaan omaan rakenteeseensa liittyvia¨ asioita (ta¨s- sa¨ olion sijaintia luokkahierarkiassa). Samalla tavoin sizeof-operaat- torin avulla ohjelma voi kysya¨, montako tavua muistia jonkin tyyppi vie. Vaikka C++:n ajoaikaiset metaohjelmointimahdollisuudet ovat var- sin rajoitetut, tekeva¨t kielen funktio- ja luokkamallit mahdollisek- si ka¨a¨nno¨saikaisen metaohjelmoinnin (static metaprogramming), jo- ta C++:ssa usein kutsutaan myo¨s template-metaohjelmoinniksi (tem- plate metaprogramming). Siina¨ template-mekanismin avulla voidaan kirjoittaa metaohjelmia, jotka tutkivat tyyppiparametreina annettuja tyyppeja¨ ja vakioita ja tekeva¨t niiden perusteella pa¨a¨to¨ksia¨ tuotetta- vasta koodista. Ta¨ta¨ mahdollisuutta ei missa¨a¨n vaiheessa suunniteltu tarkoituksellisesti C++-kieleen, vaan se havaittiin “vahingossa” 90-lu- vun puoliva¨lissa¨. Ta¨ma¨n vuoksi C++:n template-metaohjelmointi on ka¨ytto¨kelpoisuudestaan huolimatta usein tyo¨la¨sta¨ ja kirjoitettu koodi vaikealukuista. Ka¨a¨nno¨saikaista metaohjelmointia voi ajatella myo¨s niin, etta¨ kir- joitettu ohjelma jakautuu ika¨a¨n kuin kahteen osaan: ka¨a¨nno¨saikana suoritettavaan metakoodiin, joka voi tutkia rajoitetusti ohjelman ra- kennetta ja vaikuttaa sen perusteella ka¨a¨nnetta¨va¨a¨n C++-koodiin. Ta¨- ma¨ ka¨a¨nnetty koodi suoritetaan sitten ajoaikana, ja siina¨ kaikki “meta- tason” asiat on jo kiinnitetty eika¨ niihin ena¨a¨ voi vaikuttaa. Seuraavis- sa aliluvuissa tutustutaan lyhyesti C++:n template-metaohjelmoinnin mahdollisuuksiin. 10.6. C++: Template-metaohjelmointi 351 10.6.2 Metaohjelmointi ja geneerisyys Perinteista¨ ohjelmaa kirjoitettaessa tarvetta ohjelman “itsetutkiske- luun” ja reﬂektioon harvemmin esiintyy, mutta tilanne muuttuu no- peasti, kun aletaan suunnitella yleiska¨ytto¨isia¨ ohjelmakomponentte- ja, joissa (kuten aliluvussa 9.1 todettiin) on yleensa¨ seka¨ samanlaise- na pysyvia¨ osia etta¨ ka¨ytto¨kohteesta riippuvaa koodia. C++:n template- mekanismi antaa ta¨ha¨n erotteluun mahdollisuuden, kun itse funktio- tai luokkamalli edustaa pysyva¨a¨ yleiska¨ytto¨ista¨ koodia ja auki ja¨tetyt tyyppiparametrit ka¨ytto¨kohteen mukaan ma¨a¨ra¨tta¨via¨ asioita. Varsin helposti to¨rma¨ta¨a¨n tilanteeseen, jossa pelkka¨ auki ja¨tetyn tyypin ka¨ytta¨minen yleiska¨ytto¨isessa¨ template-koodissa ei riita¨, vaan koodin pita¨isi pystya¨ tutkimaan auki ja¨tettyjen tyyppien ominaisuuk- sia ja tehda¨ niiden perusteella pa¨a¨to¨ksia¨ esim. toisten tyyppien tai ka¨ytetta¨va¨n algoritmin valinnasta. C++:n tapauksessa metaohjelmoin- ti tapahtuu la¨hes yksinomaan template-mekanismin avulla, ja lisa¨k- si sita¨ tyypillisesti ka¨yteta¨a¨n nimenomaan funktio- ja luokkamallien avulla toteutetuissa yleiska¨ytto¨isissa¨ ohjelmakomponenteissa. C++:ssa ja muissa suoraan konekielelle ka¨a¨nnetta¨vissa¨ kielissa¨ me- taohjelmointia rajoittaa se, etta¨ lopullinen konekielinen ohjelmabi- naari on kiintea¨, eika¨ siina¨ ena¨a¨ voi olla auki ja¨tettyna¨ ohjelman ra- kenteeseen liittyvia¨ asioita. Samasta syysta¨ kaikki funktio- ja luokka- mallitkin instantioidaan jo ka¨a¨nno¨saikana. Niinpa¨ C++:n template-me- taohjelmointi rajoittuukin ka¨a¨nno¨saikaisiin pa¨a¨to¨ksiin ka¨a¨nnetta¨va¨n ohjelman rakenteesta. Tilannetta on ehka¨ helpointa ajatella niin, etta¨ metaohjelmoinnilla kirjoitettava “metakoodi” suoritetaan jo ohjelmaa ka¨a¨nnetta¨essa¨, ja se vaikuttaa ainoastaan siihen, millainen lopullises- ta ohjelmabinaarista tulee. Tyypillinen ka¨ytto¨kohde metaohjelmoinnille yleiska¨ytto¨isissa¨ oh- jelmakirjastoissa on optimointi. Metaohjelmoinnin avulla yleiska¨yt- to¨inen kirjasto voi ka¨a¨nno¨saikana valita ka¨ytto¨kohteeseen sopivan al- goritmin tai sa¨a¨ta¨a¨ ja viritta¨a¨ algoritmia kohteeseen parhaiten sopi- vaksi. Ta¨llo¨in puhutaan usein mukautuvasta ja¨rjestelma¨sta¨ (adap- tive system). Yksinkertainen esimerkki ta¨sta¨ on aliluvussa 10.2.1 vector-luokkamallin yhteydessa¨ mainittu vector<bool>, jossa STL valitsee bool-tyypin tapauksessa vektorille muistinkulutuksen kan- nalta tehokkaamman toteutuksen. Ka¨a¨nno¨saikainen metaohjelmointi tekee kuitenkin mahdolliseksi myo¨s monimutkaisemman mukautu- misen. 10.6. C++: Template-metaohjelmointi 352 10.6.3 Metafunktiot Perinteisessa¨ ohjelmoinnissa funktiot ovat ohjelman perusosia. Mah- dollisimman abstraktilla tasolla ajatellen funktiot ovat ohjelman ra- kenteita, jotka ajoaikana tuottavat niille annettujen parametrien pe- rusteella paluuarvon (paluuarvon lisa¨ksi funktioilla voi olla myo¨s si- vuvaikutuksia, mutta niista¨ ei ta¨ssa¨ va¨liteta¨). Samalla tavoin meta- funktioilla (metafunction) tarkoitetaan metaohjelman rakenteita, jot- ka tuottavat niille annetun ohjelman rakenteeseen liittyva¨n tiedon (esim. tyypin) perusteella jotain muuta ohjelman rakenteeseen liitty- va¨a¨. C++:n ka¨a¨nno¨saikaisessa metaohjelmoinnissa on monia eri tapoja saada aikaan metafunktioina toimivia rakenteita. Tyypillisesti niiden parametreina on ohjelman tyyppeja¨ tai ka¨a¨nno¨saikaisia vakioita, joi- den perusteella metafunktio tuottaa toisia tyyppeja¨, valitsee tuotetta- vaa koodia tai laskee uusia ka¨a¨nno¨saikaisia vakioita. On huomatta- va, etta¨ funktiomallit ja metafunktiot eiva¨t ole alkuunkaan sama asia, vaikka ta¨llaista va¨a¨rinka¨sitysta¨ joskus esiintyy ja vaikka funktiomal- leja voidaan metaohjelmointiin ka¨ytta¨a¨kin. Ehka¨ yksinkertaisin esimerkki C++:n metafunktiosta on kielen si- sa¨a¨nrakennettu operaattori sizeof. Sille annetaan parametrina jokin kielen tyyppi, ja sizeof laskee ka¨a¨nno¨saikana, montako tavua muis- tia kyseisen tyyppinen muuttuja tarvitsisi.\u0013 Esimerkiksi sizeof(int) palauttaa ohjelmaa ka¨a¨nnetta¨essa¨ arvon 4, jos int-tyyppi vaatii kysei- sessa¨ ympa¨risto¨ssa¨ 4 tavua muistia. Olennaista sizeof-operaattorissa on, ettei siita¨ aiheudu minka¨a¨nlaista ajoaikaista suoritettavaa koodia, vaan koko operaatio suoritetaan jo ohjelmaa ka¨a¨nnetta¨essa¨. Toinen jopa ha¨ma¨a¨va¨n yksinkertainen tapa saada aikaan meta- funktioita ovat luokkien sisa¨lla¨ ma¨a¨ritellyt vakiot ja tyypit. Ole- tetaan, etta¨ meilla¨ on ohjelmassa STL:n sa¨ilio¨ vector<int>, jo- hon ohjelmakoodissa halutaan iteraattori. Kuten aiemmin on todet- tu, lo¨ytyy iteraattorin tyyppi sa¨ilio¨n sisa¨isena¨ tyyppina¨ syntaksilla vector<int>::iterator. Tarkemmin ajatellen ta¨ssa¨kin on kyse me- tafunktiosta — sa¨ilio¨n tyypin perusteella saadaan ka¨a¨nno¨saikana sel- ville sa¨ilio¨o¨n sopivan iteraattorin tyyppi. Samalla tavoin sa¨ilio¨n al- kiotyypin saa selville metafunktiolla ::value type. Selkea¨mmin me- . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . \u0013Vaihtoehtoisesti sizeof-operaattorille voi antaa parametriksi minka¨ tahansa C++:n lausek- keen. Ta¨llo¨in sizeof kertoo, kuinka monta tavua muistia lausekkeen lopputulos vaatii. Itse lausekkeen arvoa ei kuitenkaan lasketa missa¨a¨n vaiheessa, muistinkulutuksen kun voi pa¨a¨tella¨ jo itse lausekkeen rakenteesta. 10.6. C++: Template-metaohjelmointi 353 tataso na¨kyy listauksen 10.16 funktiomallista, joka etsii annetusta sa¨i- lio¨sta¨ pienimma¨n alkion ja palauttaa sen. Seka¨ funktiomallin paluu- arvon etta¨ iteraattoreiden tyypin pa¨a¨tteleminen sa¨ilio¨n tyypin perus- teella on yksinkertaista metaohjelmointia. Luokkien sisa¨lle upotetut tyypit kuten ::value type ja ::iterator eiva¨t ole kovin yleiska¨ytto¨inen tapa metafunktioiden kirjoittamiseen, koska niiden ka¨ytto¨ vaatii, etta¨ kaikki luokat ma¨a¨ritteleva¨t sisa¨lla¨a¨n samalla tavoin nimetyt tyypit. Uuden “metafunktion” lisa¨a¨minen vaa- tii kaikkien metafunktion parametrina ka¨ytetta¨vien luokkien pa¨ivit- ta¨misen niin, etta¨ niiden sisa¨a¨n lisa¨ta¨a¨n haluttu uusi tyyppima¨a¨ritys. Ta¨ma¨ ei tietenka¨a¨n ole mahdollista, jos ka¨yto¨ssa¨ ei ole na¨iden luok- kien la¨hdekoodia. Esimerkiksi STL:n sa¨ilio¨tyyppeihin ei ohjelmoija voi itse lisa¨ta¨ omia tyyppima¨a¨rityksia¨a¨n, vaikka ta¨ha¨n tulisikin tar- vetta. C++:n luokkamallit ja niihin liittyva¨ mahdollisuus mallien eri- koistamiseen antavat kuitenkin mahdollisuuden kirjoittaa “irrallisia” metafunktioita, jotka tuottavat mallin parametrien perusteella uusia tyyppeja¨ tai ka¨a¨nno¨saikaisia vakioita. Ta¨ma¨ tapahtuu kirjoittamalla erillinen luokkamalli, jonka sisa¨lle on ma¨a¨ritelty sisa¨isia¨ tyyppeja¨, joiden arvo riippuu tyyppiparametrista. Ta¨llaisista malleista ka¨yte- ta¨a¨n englanniksi usein nimitysta¨ trait. Listaus 10.17 seuraavalla si- vulla na¨ytta¨a¨ yksinkertaisen trait-metafunktion ToistoStruct, joka laskee struct-tietorakenteita, joissa on useita annetun tyyppisia¨ kent- 1 template <typename Sailio> 2 typename Sailio::value type pienin(Sailio const& s) 3 { // Esimerkin lyhenta¨miseksi virhetarkastelu puuttuu 4 typename Sailio::const iterator iter = s.begin(); 5 typename Sailio::const iterator pieninIter = iter; 6 7 while (iter != s.end()) 8 { 9 if (*iter < *pieninIter) { pieninIter = iter; } 10 ++iter; 11 } 12 13 return *pieninIter; 14 } LISTAUS 10.16: Esimerkki yksinkertaisesta metaohjelmoinnista 10.6. C++: Template-metaohjelmointi 354 tia¨. Esimerkiksi kolme int-kentta¨a¨ sisa¨lta¨va¨ struct saataisiin syntak- silla ToistoStruct<int>::Kolmikko. Koska ta¨llaisten trait-mallien ai- noa tarkoitus on tarjota sisa¨isia¨ tyyppima¨a¨rittelyja¨, na¨kee C++:ssa usein ka¨ytetta¨va¨n struct-avainsanaa class:n sijaan, koska kyseessa¨ ei var- sinaisesti ole “luokka”, josta tehta¨isiin olioita, ja koska struct-raken- teessa oletusna¨kyvyys on public eika¨ private. A¨skeisen esimerkin mukaiset metafunktiot antavat mahdollisuu- den ainoastaan yksinkertaiseen uusien tyyppien luomiseen. Luokka- mallien erikoistus ja osittaiserikoistus antavat kuitenkin mahdolli- suuden huomattavasti monipuolisempiin ja ka¨ytto¨kelpoisempiin me- tafunktioihin. Niiden avulla on template-metaohjelmoinnissa mah- dollista saada aikaan if-lausetta vastaavia ehtorakenteita. Vastaavasti mallien rekursiivinen instantioiminen antaa mahdol- lisuuden toistorakenteisiin. Na¨iden kahden ominaisuuden avulla pe- riaatteessa minka¨ tahansa algoritmin saa koodattua ka¨a¨nno¨saikana suoritettavaksi template-metaohjelmaksi. Ka¨yta¨nno¨ssa¨ ta¨llaisten oh- jelmien koodi on kuitenkin helposti vaikealukuista, koska template- mekanismia ei koskaan ole suunniteltuun varsinaiseen metaohjel- 1 template <typename Tyyppi> 2 struct ToistoStruct 3 { 4 struct Yksikko 5 { 6 Tyyppi eka; 7 }; 8 struct Kaksikko 9 { 10 Tyyppi eka; 11 Tyyppi toka; 12 }; 13 struct Kolmikko 14 { 15 Tyyppi eka; 16 Tyyppi toka; 17 Tyyppi kolmas; 18 }; ... 19 }; LISTAUS 10.17: Yksinkertainen trait-metafunktio 10.6. C++: Template-metaohjelmointi 355 mointiin. Listaus 10.18 na¨ytta¨a¨ esimerkin erikoistamalla tehdysta¨ meta- funktiosta. Metafunktio IntKorotus ottaa parametrikseen kaksi koko- naislukutyyppia¨, ja sen sisa¨inen “paluuarvotyyppi” Tulos kertoo, mi- ka¨ kokonaislukutyyppi pystyy pita¨ma¨a¨n sisa¨lla¨a¨n kummankin anne- tun kokonaislukutyypin kaikki mahdolliset arvot. Ta¨ta¨ metafunktiota ka¨yteta¨a¨n listauksessa pa¨a¨ttelema¨a¨n ka¨a¨nno¨saikana min-funktiomal- lin paluutyyppi, kun min:n parametrit voivat olla keskena¨a¨n erityyp- pisia¨. IntKorotus on toteutettu niin, etta¨ itse “perusversio” mallista on tyhja¨, ja kaikki pa¨a¨ttely on koodattu erikoistuksiin, joita pita¨isi 1 template <typename T1, typename T2> 2 struct IntKorotus 3 { 4 // Erikoistamaton versio on tyhja¨, ei osata tehda¨ mita¨a¨n 5 }; 6 7 template <> 8 struct IntKorotus<char, int> 9 { 10 typedef int Tulos; 11 }; 12 13 template <> 14 struct IntKorotus<short int, char> 15 { 16 typedef short int Tulos; 17 }; 18 19 template <> 20 struct IntKorotus<short int, unsigned short int> 21 { 22 typedef int Tulos; // Ka¨a¨nta¨ja¨riippuvaista, riitta¨a¨ko¨ int 23 }; ... 24 template <typename T1, typename T2> 25 typename IntKorotus<T1, T2>::Tulos min(T1 p1, T2 p2) 26 { 27 if (p1 < p2) { return p1; } 28 else { return p2; } 29 } LISTAUS 10.18: Erikoistamalla tehty template-metafunktio 10.6. C++: Template-metaohjelmointi 356 kirjoittaa koodiin yksi jokaista mahdollista kokonaislukutyyppiparia varten. Metafunktio sisa¨lta¨a¨ myo¨s virhetarkastelun siina¨ mielessa¨, etta¨ jos IntKorotus-mallin parametrit eiva¨t ole kokonaislukutyyppeja¨, valit- see ka¨a¨nta¨ja¨ mallin erikoistamattoman perustoteutuksen, joka ei ma¨a¨- rittele tyyppia¨ Tulos ollenkaan. Ta¨ma¨ puolestaan aiheuttaa ka¨a¨nno¨s- virheen, kun tyyppiin Tulos viitataan. Luokkamallien osittaiserikoistus antaa mahdollisuuden hienova- raisempiinkin metafunktioihin. Listauksen 10.19 metafunktio pois- taa vakioma¨a¨reen const tyyppiparametristaan. Jos parametrina annet- tu tyyppi ei ole const, valitaan erikoistamaton versio, jossa on Tulos sama kuin tyyppiparametri. Jos sen sijaan tyyppiparametri on vakio- tyyppia¨, valitaan mallin osittaiserikoistus. Siina¨ tyyppiparametriin T ja¨a¨kin ena¨a¨ vain alkupera¨inen ei-vakiotyyppi, koska osittaiserikois- tuksen tyyppima¨a¨reena¨ oleva <T const> tekee selva¨ksi, etta¨ T viit- taa tyypin “ei-const”-osaan. Listauksessa ka¨yteta¨a¨n ta¨ta¨ metafunk- tiota pita¨ma¨a¨n huoli siita¨, etta¨ listauksen min-toteutuksen muuttuja pienin ei ole vakio, jotta siihen voi sijoittaa. 1 template <typename T> 2 struct PoistaConst // Oletus, kun tyyppi on “normaalityyppi” 3 { 4 typedef T Tulos; 5 }; 6 7 template <typename T> 8 struct PoistaConst<T const> // Osittaiserikoistus kaikille vakiotyypeille 9 { // Huomaa, etta¨ ta¨ssa¨ T on nyt ilman const-ma¨a¨retta¨ oleva tyyppi 10 typedef T Tulos; 11 }; ... 12 template <typename T> 13 T min(T* p1, T* p2) 14 { 15 typename PoistaConst<T>::Tulos pienin; 16 if (*p1 < *p2) { pienin = *p1; } // Sijoitus, joten pienin ei saa olla vakio 17 else { pienin = *p2; } 18 return pienin; 19 } LISTAUS 10.19: Osittaiserikoistuksella tehty metafunktio 10.6. C++: Template-metaohjelmointi 357 Edella¨ mainittuja tekniikoita ka¨ytta¨en on mahdollista kirjoittaa metafunktiokirjastoja, jotka helpottavat varsinkin luokka- ja funk- tiomallien avulla tehtya¨ geneerista¨ ohjelmointia. Ta¨llaisia kirjas- toja lo¨ytyy myo¨s valmiina. Esimerkiksi kirjassa “Modern C++ De- sign” [Alexandrescu, 2001] esitelty kirjasto Loki tarjoaa valmiita me- tafunktiototeutuksia joihinkin suunnittelumalleihin. Samoin C++-kir- jastokokoelma Boost [Boost, 2003] sisa¨lta¨a¨ monia ka¨ytto¨kelpoisia me- tafunktioita, joista osa saattaa pa¨a¨tya¨ seuraaviin C++-standardin ver- sioihin. 10.6.4 Esimerkki metafunktioista: numeric_limits Template-metaohjelmointi on yleistynyt C++-ohjelmoijien parissa vas- ta viime vuosina. Siita¨ huolimatta jo nykyisessa¨ C++-standardissa on jonkin verran metafunktioiksi laskettavia ominaisuuksia. Ta¨ssa¨ alilu- vussa esitella¨a¨n standardin metafunktio numeric limits, joka on hy- va¨ esimerkki siita¨, miten metafunktioiden avulla voidaan saada hyo¨- dyllista¨ tietoa ohjelman numeerisista tyypeista¨. Silloin ta¨llo¨in ohjelmoinnissa tulee tarve saada tietoa esimerkik- si siita¨, mika¨ on suurin mahdollinen int-tyyppiin mahtuva luku tai kuinka monen numeron tarkkuudella double luvut esitta¨a¨. C-kielessa¨ ja standardia edelta¨va¨ssa¨ C++:ssa ta¨ma¨ ratkaistiin niin, etta¨ na¨ma¨ tie- dot lo¨ytyiva¨t sopivan includen ja¨lkeen vakioina. Edelleenkin C++:ssa voi lukea otsikkotiedoston <climits>, jonka ja¨lkeen suurin int lo¨y- tyy nimella¨ INT MAX ja otsikkotiedoston <cfloat> lukemisen ja¨lkeen double-tyypin tarkkuus numeroina selvia¨a¨ vakiosta DBL MANT DIG. Ta¨ma¨ vakioihin perustuva ja¨rjestelma¨ ei kuitenkaan ole ka¨ytto¨kel- poinen geneerisessa¨ ohjelmoinnissa. Tyyppiparametrin T perusteella ei milla¨a¨n voi selvitta¨a¨, mika¨ kyseisen tyypin maksimiarvo on, kos- ka ta¨ssa¨ C:sta¨ periytyva¨ssa¨ menetelma¨ssa¨ jokaisen tyypin tiedot lo¨y- tyva¨t eri nimisista¨ vakioista, eika¨ tyyppiparametrin perusteella pysty milla¨a¨n ka¨a¨nno¨saikana muodostamaan vakiolle oikeaa nimea¨. Ta¨ma¨n vuoksi C++-standardiin lisa¨ttiin trait-metafunktio numeric limits, jo- ka kertoo tyyppiparametrinsa perusteella kyseiseen tyyppiin liittyvia¨ tietoja. Metafunktio saadaan ka¨ytto¨o¨n lukemalla otsikkotiedosto <limits>. Ta¨ma¨n ja¨lkeen ohjelma voi kysya¨ minka¨ tahansa numee- risen tyypin tietoja syntaksilla numeric limits<tyyppi>::tieto. Esi- merkiksi int tyypin maksimiarvo on numeric limits<int>::max(). 10.6. C++: Template-metaohjelmointi 358 Samoin tyypin double tarkkuus 10-ja¨rjestelma¨ssa¨ on numeric limits<double>::digits10 numeroa. Vaikka ta¨ma¨ uusi tapa on syntaksiltaan pidempi kuin vanha, sen etuna on, etta¨ esimerkiksi template-koodin sisa¨lla¨ tyyppiparametrin T pienin mahdollinen arvo selvia¨a¨ kutsulla numeric limits<T>::min() tyypista¨ T riippumatta. Taulukko 10.8 luettelee ta¨rkeimma¨t numeric limits-metafunktion tarjoamat tiedot. Tarkemmat seli- tykset metafunktion toiminnasta ja yksityiskohdista saa monista C++-oppaista. numeric limits<T>:: Selitys min(), max() Tyypin pienin ja suurin arvo. Liukulukutyypeilla¨ min() kertoo pienimma¨n positiivisen arvon. radix Tyypin sisa¨inen lukuja¨rjestelma¨ (yleensa¨ 2). digits, digits10 Tyypin tarkkuus numeroina radix-kantaisessa esitys- muodossa (digits) ja 10-ja¨rjestelma¨ssa¨ (digits10). is signed, is integer, is exact Totuusarvo true tai false riippuen siita¨, onko tyyp- pi etumerkillinen, kokonaislukutyyppi ja tarkka (siis pyo¨ristysvirheita¨ ei voi sattua). is bounded Onko lukualue rajoitettu. Tosi kaikille sisa¨a¨nrakenne- tuille tyypeille, mutta omille rajoittamattoman tark- kuuden tyypeille voisi olla epa¨tosi. is modulo Onko tyyppi modulo-aritmetiikkaa ka¨ytta¨va¨, eli pyo¨- ra¨hta¨a¨ko¨ lukualue “ympa¨ri”. Tosi etumerkitto¨mille kokonaisluvuille ja useimmiten muillekin kokonais- luvuille. Yleensa¨ epa¨tosi liukulukutyypeille. epsilon(), round error() Liukulukutyyppien tarkkuuteen liittyvia¨ tietoja. Tar- koilla luvuilla 0. min/max exponent, min/max exponent10 Liukulukutyyppien eksponentin minimi- ja maksi- miarvot sisa¨isen lukuja¨rjestelma¨n etta¨ 10-ja¨rjestel- ma¨n eksponenteille. Kokonaislukutyypeilla¨ aina 0. is iec559, has infinity, has denorm . . . IEC-559-standardin mukaisten liukulukujen tietoja, yksityiskohtainen selitta¨minen veisi liikaa tilaa., KUVA 10.8: Metafunktion numeric_limits palvelut 10.6. C++: Template-metaohjelmointi 359 Sen lisa¨ksi, etta¨ numeric limits kertoo yhtena¨isella¨ tavalla kaikis- ta C++:n sisa¨isista¨ tyypeista¨, se on myo¨s laajennettavissa omille luku- ja esitta¨ville luokille. Toteutukseltaan numeric limits on nimitta¨in jokaiselle perustyypille erikoistettu struct-luokkamalli. Niinpa¨ sita¨ voi edelleen erikoistaa omille lukutyypeille. Jos ohjelma esimerkik- si ma¨a¨rittelee oman murtolukutyypin, voi sita¨ varten kirjoittaa eri- koistuksen numeric limits-mallille, jolloin numeric limits:a¨ ka¨ytta¨- va¨t geneeriset laskenta-algoritmit toimivat myo¨s ta¨lle tyypille. Listaus 10.20 na¨ytta¨a¨ funktiomallin, joka vertailee annettujen lu- kujen yhta¨suuruutta. Kokonaisluvuille ja muille tarkoille luvuille se ka¨ytta¨a¨ ==-vertailua, liukuluvuille ja muille pyo¨ristysvirheista¨ ka¨rsi- ville luvuille erotuksen itseisarvon suuruusluokan vertaamista mah- dolliseen pyo¨ristysvirheeseen. 1 #include <limits> 2 using std::numeric limits; 3 #include <cmath> 4 using std::abs; // Itseisarvo 5 #include <algorithm> 6 using std::max; 7 8 template <typename T> 9 bool yhtasuuruus(T p1, T p2) 10 { 11 if (numeric limits<T>::is exact) 12 { // Pyo¨ristysvirheet eiva¨t ongelma, vertaillaan suoraan 13 return p1 == p2; 14 } 15 else 16 { // Pyo¨ristysvirhemahdollisuus, ka¨yteta¨a¨n kaavaa |p1−p2| max(|p1|,|p2|) ≤ε 17 return 18 abs(p1-p2) / max(abs(p1),abs(p2)) <= numeric limits<T>::epsilon(); 19 // Tarkasti ottaen ta¨ma¨ kaava ei riita¨ aina, mutta kelvatkoon., 20 } 21 } LISTAUS 10.20: Esimerkki numeric_limits-metafunktion ka¨yto¨sta¨ 10.6. C++: Template-metaohjelmointi 360 10.6.5 Esimerkki metaohjelmoinnista: tyypin valinta Edella¨ esitetyt tavat metafunktioiden kirjoittamiseen tekeva¨t metaoh- jelmoinnin mahdolliseksi, mutta metaohjelmien kirjoittaminen on varsin tyo¨la¨sta¨. Yksinkertaisenkin valinnan suorittaminen ka¨a¨nno¨sai- kana vaatii erillisen luokkamallin ja sen erikoistuksen kirjoittamista. Olisikin ka¨teva¨a¨, jos C++:n metakoodiin saataisiin edes eta¨isesti nor- maaleja ohjelmointikielen rakenteita muistuttavia mekanismeja. Ta¨llaisia valinta-, silmukka- ynna¨ muita rakenteita on onneksi mahdollista kirjoittaa erillisina¨ metafunktioina, joita voi kutsua me- takoodista. Ta¨ma¨ tekee metaohjelmista jonkin verran helppolukui- sempia, vaikka edelleenka¨a¨n template-metakoodi ei ylla¨ selkeydessa¨ la¨heska¨a¨n normaalin ohjelmakoodin tasolle. Ta¨ssa¨ aliluvussa esite- ta¨a¨n esimerkkina¨ yksinkertaisen ehtolauseen IF toteuttaminen me- tafunktiona. Vastaavalla tavalla voidaan toteuttaa toistolauseet kuten WHILE ja muut kontrollirakenteet, mutta niista¨ kiinnostuneen kannat- taa tutustua esimerkiksi kirjan “Generative Programming” [Czarnecki ja Eisenecker, 2000] lukuun 10 “Static Metaprogramming in C++”. Toteutettavan IF-metafunktion ideana on ottaa template-paramet- rina yksi bool-tyyppinen ka¨a¨nno¨saikainen vakio, joka edustaa testat- tavaa ehtoa, ja kaksi tyyppia¨. IF tuottaa paluuarvokseen jommankum- man na¨ista¨ tyypeista¨ riippuen siita¨ onko ehto tosi vai epa¨tosi. Itse me- tafunktion toteutus on kohtalaisen yksinkertainen ja perustuu luok- kamallin osittaiserikoistukseen. Metafunktion koodi on listauksessa 10.21 seuraavalla sivulla. Sen erikoistamaton perustoteutus vastaa tilannetta, jossa ehto on tosi, jo- ten perustoteutuksessa sisa¨inen tyyppi Tulos ma¨a¨ritella¨a¨n samaksi kuin ensimma¨inen tyyppiparametri Tosi. Lisa¨ksi mallille ma¨a¨ritel- la¨a¨n osittaiserikoistus sita¨ tapausta varten, etta¨ ehto on epa¨tosi. Ta¨l- lo¨in tyyppi Tulos ma¨a¨ritella¨a¨n samaksi kuin toinen tyyppiparametri Epatosi. Lopputuloksena on metafunktio, jossa Tulos on joko sama tyyppi kuin Tosi tai Epatosi totuusarvosta riippuen. Toteutetun metafunktion avulla voidaan nyt kirjoittaa koodia, jos- sa ka¨ytetty tyyppi valitaan ka¨a¨nno¨saikana annetun ehdon perusteel- la. Esimerkiksi halutaan valita muuttujan tyypiksi int, jos ta¨ma¨ tyyp- pi on va¨hinta¨a¨n nelja¨n tavun kokoinen, muuten valitaan long int. Ta¨ma¨ onnistuu seuraavasti: IF< (sizeof(int)>=4), int, long int >::Tulos muuttuja = 0; 10.6. C++: Template-metaohjelmointi 361 4 // Perustapaus, jos ehto on tosi 5 template <bool EHTO, typename Tosi, typename Epatosi> 6 struct IF 7 { 8 typedef Tosi Tulos; 9 }; 10 11 // Osittaiserikoistus, jos ehto on epa¨tosi 12 template <typename Tosi, typename Epatosi> 13 struct IF<false, Tosi, Epatosi> 14 { 15 typedef Epatosi Tulos; 16 }; LISTAUS 10.21: Metafunktion IF toteutus Listaus 10.22 na¨ytta¨a¨ toisen ka¨ytto¨esimerkin metafunktiolle IF. Kyseessa¨ on sama min-funktiomallin paluutyypin valinta kuin listauk- sessa 10.18, mutta nyt erillisen paluutyyppia¨ varten kirjoitetun meta- funktion sijaan valitaan kahdesta parametrityypista¨ se, joka koko nu- meroina on numeric limits:n mukaan suurempi (ta¨ma¨ versio ei ota huomioon esimerkiksi parametrityyppien etumerkkeja¨ vaan olettaa etta¨ molemmat parametrit ovat joko etumerkillisia¨ tai etumerkitto¨mia¨ kokonaislukuja). 1 // Valitaan paluutyypiksi parametreista “laajempi” 2 template <typename T1, typename T2> 3 typename IF< (std::numeric limits<T1>::digits >= 4 std::numeric limits<T2>::digits), 5 T1, T2 >::Tulos 6 min(T1 const& p1, T2 const& p2) 7 { 8 if (p1 < p2) 9 { 10 return p1; 11 } 12 else 13 { 14 return p2; 15 } 16 } LISTAUS 10.22: Metafunktion IF ka¨ytto¨esimerkki 10.6. C++: Template-metaohjelmointi 362 10.6.6 Esimerkki metaohjelmoinnista: optimointi Ta¨ha¨n mennessa¨ kaikki esitetyt metafunktiot ovat vain tuottaneet uusia tyyppeja¨ tai valinneet ka¨a¨nno¨saikana olemassa olevien tyyp- pien va¨lilla¨. Toinen ta¨rkea¨ ka¨ytto¨kohde metaohjelmoinnille on koo- din optimointi ka¨a¨nno¨saikana tehta¨vien pa¨a¨telmien perusteella. Ka¨y- ta¨nno¨ssa¨ ta¨ma¨ tarkoittaa usein, etta¨ useista eri algoritmivaihtoehdois- ta valitaan metaohjelmoinnin avulla tehokkain ka¨ytto¨kohteeseen so- piva, tai algoritmin toimintaa sa¨a¨deta¨a¨n ka¨a¨nno¨saikaisten laskelmien perusteella. Ta¨llaisesta optimoinnista on usein suuri hyo¨ty, koska se voidaan tehda¨ huomattavasti korkeammalla abstraktiotasolla kuin mihin C++-ka¨a¨nta¨jien omat optimointialgoritmit pystyva¨t. Esimerkkina¨ ta¨mantapaisesta korkean tason optimoinnista ka¨yte- ta¨a¨n ta¨ssa¨ STL:n iteraattoreita. Kuten aiemmin on todettu, iteraatto- reita pystyy liikuttelemaan sa¨ilio¨n sisa¨lla¨ eri tavoin riippuen siita¨, mihin iteraattorikategoriaan ne kuuluvat. Eteenpa¨initeraattorit pys- tyva¨t liikkumaan vain yhden alkion eteenpa¨in kerrallaan. Kaksisuun- taiset iteraattorit taas pystyva¨t liikkumaan yhden alkion verran eteen- tai taaksepa¨in. Hajasaanti-iteraattorit puolestaan pystyva¨t hyppima¨a¨n mielivaltaisen hypyn kumpaan suuntaan tahansa. Jaottelu kategorioi- hin STL:ssa¨ perustui siihen, etta¨ kaikkien iteraattoreiden liikkumi- sien taataan tapahtuvan vakioajassa. Joissain tapauksissa olisi ta¨rkea¨a¨ pystya¨ siirta¨ma¨a¨n iteraattoria ha- lutun verran eteen- tai taaksepa¨in riippumatta siita¨, kuinka tehokasta liikkuminen on. Ta¨ma¨ on tietysti mahdollista ka¨ytta¨ma¨lla¨ kaikille ite- raattoreille ka¨ytta¨ma¨lla¨ ++-operaattoria silmukassa, mutta silloin siir- ta¨minen tulee tehtya¨ turhan tehottomasti hajasaanti-iteraattoreille, jotka pystyisiva¨t hyppa¨a¨ma¨a¨n halutun verran kerrallakin. Olisi ihan- teellista, jos sama siirtymisfunktio pystyisi siirta¨ma¨a¨n hajasaanti-ite- raattoreita +=-operaatiolla ja muita iteraattoreita silmukassa ++-ope- raattorilla. Ilman metaohjelmointia ta¨ta¨ ongelmaa ei ole mahdollista ratkais- ta helposti C++:ssa. Pelkka¨ normaali if-lause ei riita¨, koska muille kuin hajasaanti-iteraattoreille +=-operaattoria ka¨ytta¨va¨ koodi aiheut- taa ka¨a¨nno¨svirheen. Ta¨ten sama C++-funktio ei pysty ajoaikana pa¨a¨t- telema¨a¨n miten iteraattoria tulisi liikuttaa tehokkaasti. Ratkaisuna ongelmaan on kirjoittaa kuormitettu funktiomalli, jol- la on eri toteutukset eri iteraattorityyppeja¨ varten. Na¨ista¨ toteutuksis- ta valitaan sitten ka¨a¨nno¨saikana kutakin kutsua varten sopiva. Ta¨ta¨ 10.6. C++: Template-metaohjelmointi 363 varten ta¨ytyy kuitenkin pystya¨ ka¨a¨nno¨saikana selvitta¨ma¨a¨n mihin ite- raattorikategoriaan tietty iteraattori kuuluu. Onneksi STL tarjoaa juu- ri ta¨ta¨ tarkoitusta varten valmiin trait-metafunktion iterator traits. Metafunktiolle iterator traits annetaan tyyppiparametri- na iteraattori (tai C++:n perustaulukkotyypin tapauksessa nor- maali osoitin, joka toimii iteraattorina taulukkoon). Ta¨ma¨n ja¨lkeen metafunktio kertoo iteraattorin ominaisuuksia. Ta¨- ta¨ esimerkkia¨ varten meita¨ kiinnostaa ominaisuuksista vain tyyppi iterator traits<T>::iterator category. Ta¨ma¨ tyyp- pi on jokin tyypeista¨ input iterator tag, output iterator tag, forward iterator tag, bidirectional iterator tag tai random access iterator tag. Na¨ma¨ tyypit on periytetty toisistaan niin, etta¨ saadaan iteraattorikategorioita vastaava luokkahierarkia, joka na¨kyi esimerkiksi kuvassa 10.5 sivulla 330. Ta¨ta¨ iteraattorikategorioihin jakamista voidaan nyt ka¨ytta¨a¨ hy- va¨ksi algoritmin valinnassa. Listauksessa 10.23 seuraavalla sivulla on kolme eri toteutusta funktiomallille siirry apu.Na¨ma¨ eroavat toi- sistaan viimeisen parametrin tyypin osalta. Ta¨ta¨ viimeista¨ paramet- ria ka¨yteta¨a¨n iteraattorikategorian valintaan.` Ensimma¨inen versio funktiomallista hyva¨ksyy kolmanneksi parametriksi lukuiteraattori- kategorian (johon kuuluvat periytta¨ma¨lla¨ kaikki iteraattorikategoriat tulostusiteraattoria lukuun ottamatta). Toisen version lisa¨parametrin tyyppi ma¨a¨ra¨a¨ kategoriaksi kaksisuuntaisen iteraattorin ja kolmas ha- jasaanti-iteraattorin. Na¨ma¨ eri versiot on toteutettu eri tavoilla niin, etta¨ kutakin iteraattorityyppia¨ liikutetaan niin tehokkaasti kuin mah- dollista. Varsinainen funktiomalli siirry on toteutettu riveilla¨ 43–48 niin, etta¨ se kutsuu apufunktiota siirry apu ja antaa sille kolmanneksi pa- rametriksi iterator traits-metafunktion avulla saadun iteraattori- kategorian. Ta¨ma¨n kategorian perusteella ka¨a¨nta¨ja¨ sitten kutsuu ky- seiselle iteraattorille sopivaa apufunktiota, joka siirta¨a¨ iteraattorin te- hokkaimmalla tavalla halutun verran eteen- tai taaksepa¨in. Kuten ta¨sta¨ esimerkista¨ huomataan, metaohjelmointi antaa C++:ssa varsin monipuoliset mahdollisuudet koodin korkealla tasolla tapah- tuvaan optimointiin. Ika¨va¨ kylla¨ metaohjelmointiin tarvittavat “kikat” ovat usein varsin vaikealukuisia ja -selkoisia, joten ta¨llainen metaoh- . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . `Funktiomalleissa saattaa oudoksuttaa se, etta¨ niiden kolmannesta parametrista on para- metrilistassa mainittu vain tyyppi, eika¨ parametrin nimea¨. Ta¨ma¨ on laillista C++:aa ja kertoo ka¨a¨nta¨ja¨lle, ettei kyseista¨ parametria ka¨yteta¨ itse koodissa lainkaan. 10.6. C++: Template-metaohjelmointi 364 1 #include <iterator> 2 using std::iterator traits; 3 using std::input iterator tag; 4 using std::bidirectional iterator tag; 5 using std::random access iterator tag; 6 7 // Kuormitetut funktiomallit eri tapauksille 8 // Perustapaus, va¨hinta¨a¨n lukuiteraattori 9 template<typename Iteraattori, typename Siirtyma> 10 void siirry apu(Iteraattori& iter, Siirtyma s, input iterator tag) 11 { 12 // Ollaan varmasti menossa eteenpa¨in, lukuiteraattorit eivat osaa muuta 13 for (Siirtyma i = 0; i != s; ++i) 14 { 15 ++iter; 16 } 17 } 18 19 // Erikoistapaus 1, va¨hinta¨a¨n kaksisuuntainen iteraattori 20 template<typename Iteraattori, typename Siirtyma> 21 void siirry apu(Iteraattori& iter, Siirtyma s, bidirectional iterator tag) 22 { 23 // Ta¨ytyy pystya¨ menema¨a¨n eteen- tai taaksepa¨in tilanteesta riippuen 24 if (s >= 0) 25 { 26 for (Siirtyma i = 0; i != s; ++i) { ++iter; } 27 } 28 else 29 { 30 for (Siirtyma i = 0; i != s; --i) { --iter; } 31 } 32 } 33 34 // Erikoistapaus 2, hajasaanti-iteraattori 35 template<typename Iteraattori, typename Siirtyma> 36 void siirry apu(Iteraattori& iter, Siirtyma s, random access iterator tag) 37 { 38 // Hypa¨ta¨a¨n suoraan oikeaan paikkaan 39 iter += s; 40 } 41 42 // Varsinainen funktio, joka valitsee apufunktion metakoodilla 43 template<typename Iteraattori, typename Siirtyma> 44 inline void siirry(Iteraattori& iter, Siirtyma s) 45 { 46 siirry apu(iter, s, 47 typename iterator traits<Iteraattori>::iterator category()); 48 } LISTAUS 10.23: Esimerkki metaohjelmoinnista optimoinnissa 10.6. C++: Template-metaohjelmointi 365 jelmointi vaatii tuekseen selkea¨n dokumentoinnin, jotta myo¨s koodia lukevat ymma¨rta¨va¨t, mita¨ koodin on tarkoitus tehda¨. Onneksi genee- risen template-metakoodia sisa¨lta¨va¨n kirjaston ka¨ytta¨minen on kui- tenkin varsin selkea¨a¨ ja usein jopa ohjelmoijalle ta¨ysin la¨pina¨kyva¨a¨, kuten esimerkiksi STL osoittaa. 366 Luku 11 Virhetilanteet ja poikkeukset Although the source of the Operand Error has been identi- ﬁed, this in itself did not cause the mission to fail. The spec- iﬁcation of the exception-handling mechanism also con- tributed to the failure. In the event of any kind of excep- tion, the system speciﬁcation stated that: the failure should be indicated on the databus, the failure context should be stored in an EEPROM memory (which was recovered and read out for Ariane 501), and ﬁnally, the SRI processor should be shut down. – ARIANE 5 Flight 501 Failure Report [Lions, 1996] Virhetilanteisiin varautuminen ja niihin reagoiminen on aina ollut yksi vaikeimpia ohjelmoinnin haasteita. Virhetilanteissa ohjelman ta¨ytyy yleensa¨ toimia normaalista poikkeavalla tavalla, ja na¨iden uusien ohjelman suoritusreittien koodaaminen tekee ohjelmakoodis- ta helposti sekavaa. Lisa¨ksi useiden erilaisten virhetilanteiden viida- kossa ohjelmoijalta ja¨a¨ helposti tekema¨tta¨ tarvittavia siivoustoimen- piteita¨, kuten muistin vapautusta. Jos viela¨ lisa¨ksi vaaditaan, etta¨ oh- jelman ta¨ytyy toipua virheista¨ eika¨ vain esimerkiksi lopettaa ohjel- 11.1. Mika¨ virhe on? 367 man suoritusta virheilmoitukseen, ohjelmakoodin ta¨ytyy pystya¨ pe- ruuttamaan virhetilanteen vuoksi kesken ja¨a¨neet operaatiot. Virhetilanteiden ka¨sittely on kokonaisuudessaan niin laaja ai- he, ettei siita¨ ta¨ma¨n teoksen puitteissa voida kertoa kovinkaan pal- jon, eika¨ se edes kovin oleellisesti liity ta¨ma¨n teoksen aiheeseen- kaan. C++ tarjoaa kuitenkin virheiden ka¨sittelyyn erityisen mekanis- min, poikkeukset (exception), joiden toiminta perustuu luokkahierar- kioihin ja na¨in ollen sivuaa myo¨s olio-ohjelmointia. Virheka¨sittelysta¨ ja C++:n poikkeuksista lo¨ytyy lisa¨tietoa esim. kirjoista “More Effective C++” [Meyers, 1996], “Exceptional C++” [Sutter, 2000] ja “More Excep- tional C++” [Sutter, 2002c]. 11.1 Mika¨ virhe on? Useimmat tietokoneen ka¨ytta¨ja¨t sanovat ohjelman toimivan va¨a¨rin, jos siina¨ ei ole heida¨n tarvitsemaansa ominaisuutta tai ohjelma an- taa ka¨ytta¨ja¨n mielesta¨ va¨a¨ria¨ vastauksia. Ohjelmiston tekija¨n kannal- ta syyt na¨ihin va¨itteisiin voivat olla ma¨a¨rittelyssa¨ (yriteta¨a¨n tehda¨ oh- jelmalla jotain mihin sita¨ alunperinka¨a¨n ei ole tarkoitettu), suunnit- telussa (toteutukseen ei ole otettu mukaan kaikkia ma¨a¨rittelyssa¨ ol- leita asioita tai niiden toteutus on suunniteltu virheelliseksi) tai oh- jelmoinnissa (ohjelmointityo¨ssa¨ on tapahtunut virhe). Tietokoneohjelmat ovat mutkikkaita ja paraskaan ohjelmisto ei luultavasti pysty varautumaan kaikenlaisiin eri tasoilla oleviin virhe- tilanteisiin etuka¨teen — va¨hinta¨a¨nkin ta¨llaisen ohjelmiston toteutus- kustannukset nousisivat sieta¨ma¨tto¨miksi. Ohjelmistoa on kuitenkin helppo pita¨a¨ kilpailijoitaan laadukkaampana, jos siita¨ lo¨ytyy muita enemma¨n virhetilanteisiin varautuvia ominaisuuksia. Ohjelmointityo¨ssa¨ ei pysty vaikuttamaan ma¨a¨rittelyn ja suunnit- telun aikaisiin virheisiin, ne paljastuvat ohjelmiston testauksessa tai huonoimmassa tapauksessa vasta tuotantoka¨yto¨ssa¨. Ohjelmoinnissa voidaan varautua etuka¨teen pohdittuihin vikatilanteisiin, jotka voi- daan karkeasti jakaa laitteiston ja ohjelmiston aiheuttamiin. Laitteistovirheet na¨kyva¨t ohjelmistolle sen ympa¨risto¨n ka¨ytta¨yty- misena¨ eri tavoin kuin on oletettu. Ohjelmia tehta¨essa¨ oletetaan esi- merkiksi, etta¨ muuttujaan kirjoitettu arvo on sa¨ilynyt samana, kun sita¨ hetken kuluttua luetaan muuttujasta — viallinen muistipiiri tie- tokoneessa saattaa kuitenkin aiheuttaa tilanteen olevan toinen. Tie- 11.1. Mika¨ virhe on? 368 dostojen ka¨sittely voi menna¨ vikaan levyn ta¨yttymisen tai vikaantu- misen vuoksi. Ohjelma itsessa¨a¨n on saattanut osittain muuttua kun se on ladattu suoritettavaksi ja na¨in ollen toimii va¨a¨rin suorittaessaan koneka¨skyja¨, joita ohjelmoija ei ole tarkoittanut suoritettavaksi. Laitteistovirheista¨ saadaan tietoa yleensa¨ ka¨ytto¨ja¨rjestelma¨n kaut- ta. Tiedostojen ka¨sittelyssa¨ tapahtuneet virheet useimmat ka¨ytto¨ja¨r- jestelma¨t osaavat ilmoittaa ohjelmalle, mutta muut “vakavammat” lai- tevirheet voivat aiheuttaa tiedon muuttumista ilman, etta¨ siita¨ erik- seen tulee ilmoitusta ohjelmalle. Ohjelmoijan on todellisuudessa aina tehta¨va¨ jonkinlainen kompromissi sen kanssa, minka¨ tyyppisia¨ vir- heita¨ ohjelmassa pyrita¨a¨n havaitsemaan ja ka¨sittelema¨a¨n. Laitteiston tapauksessa yleisin linja on varautua ka¨ytto¨ja¨rjestelma¨n ilmoittamiin vikoihin ja ja¨tta¨a¨ muut huomioimatta luottaen niiden olevan eritta¨in harvinaisia. Era¨a¨n arvion mukaan nykyaikainen henkilo¨kohtainen tietokone tekee virheen laitteiston laskutoimituksissa keskima¨a¨rin kolmen tril- joonan (18 nollaa) laskun suorituksen ja¨lkeen. Ta¨ma¨ tarkoittaa suun- nilleen sita¨, etta¨ ajettaessa ohjelmistoa ta¨llaisella koneella tuhat vuot- ta vika esiintyy kerran. Useimmat ohjelmistot ja¨tta¨va¨t na¨ma¨ vikamah- dollisuudet tarkastamatta, mutta joskus nekin muodostuvat merkit- ta¨viksi. Esimerkiksi massiivista rinnakkaiseksi hajautettua laskentaa suorittava SETI@home-projekti ka¨ytta¨a¨ edella¨ mainitun ajan proses- soriaikaa pa¨iva¨ssa¨ ja to¨rma¨a¨ kyseiseen vikaan siis keskima¨a¨rin kerran vuorokaudessa — ta¨llo¨in vikamahdollisuus on myo¨s huomioitava oh- jelmistossa. [SETI, 2001] Ohjelmistossa virheet ovat yksitta¨isen ohjelmanpa¨tka¨n (funktio, olio, moduuli) kannalta sisa¨isia¨ tai ulkoisia. Ulkoisessa virheessa¨ koo- dia pyydeta¨a¨n tekema¨a¨n jotain, mita¨ se ei osaa tai mihin se ei pysty. Esimerkiksi funktion parametrilla on va¨a¨ra¨ arvo, syo¨tetiedosto ei nou- data ma¨a¨riteltya¨ muotoa, tai ka¨ytta¨ja¨ on valinnut toimintosekvenssin jossa ei ole “ja¨rkea¨”. Sisa¨isessa¨ virheessa¨ toteutus ajautuu itse tilan- teeseen jossa jotain menee pieleen (esimerkiksi muisti loppuu tai to- teutusalgoritmissa tulee jokin a¨a¨riraja vastaan). Virhetilanteita huomioivassa ohjelmoinnissa on usein kaikista helpoin vaihe havaita virhetilanne. Ta¨ha¨n toimintaan ka¨ytto¨ja¨rjestel- ma¨t, ohjelmakirjastot ja ohjelmointikielet tarjoavat la¨hes aina keinoja. Havaitsemista paljon vaikeampaa on suunnitella ja toteuttaa se, mita¨ vikatilanteessa tehda¨a¨n. 11.2. Mita¨ tehda¨ virhetilanteessa? 369 11.2 Mita¨ tehda¨ virhetilanteessa? Varautuva ohjelmointi (defensive programming, [McConnell, 1993]) on ohjelmointityyli, jota voisi verrata autolla ajossa ennakoivaan ajo- tapaan. Vaikka oma toiminta (koodi) olisi ta¨ysin oikein ja sovittujen sa¨a¨nto¨jen mukaista, kannattaa silti varautua siihen, etta¨ muut osallis- tujat voivat toimia va¨a¨rin. Usein ajoissa tapahtunut virheiden ja on- gelmien havaitseminen mahdollistaa niihin sopeutumisen jopa siten, etta¨ ohjelmissa ka¨ytta¨ja¨n ei tarvitse huomata mita¨a¨n erityistilannetta edes syntyneen. Seuraavassa on listattu muutamia tapoja, joilla ohjelman osa voi toimia havaitessaan virhetilanteen. Sopiva suhtautuminen virhee- seen on va¨hinta¨a¨nkin ohjelmakomponentin suunnitteluun kuuluva asia. Ei ole olemassa yhta¨ ainoata oikeata tai va¨a¨ra¨a¨ tapaa — hyvin suunniteltu komponentti voi ottaa virheisiin reagoinnin omalle vas- tuulleen, mutta hyva¨na¨ ratkaisuna voidaan pita¨a¨ myo¨s sellaista, joka “ainoastaan” ilmoittaa havaitsemansa virheet komponentin ka¨ytta¨ja¨l- le. • Suorituksen keskeytys (abrupt termination) on a¨a¨rimma¨inen ta- pa toimia kun ohjelmassa kohdataan virhe. Ja¨rjestelma¨n suorit- taminen keskeyteta¨a¨n va¨litto¨ma¨sti ja usein ilman, etta¨ virheti- lannetta yriteta¨a¨n edes mitenka¨a¨n kirjata myo¨hempa¨a¨ tarkas- telua varten. Ta¨ma¨n tavan ka¨ytto¨a¨ tulisi va¨ltta¨a¨, silla¨ pysa¨hty- neesta¨ ohjelmasta ei edes aina tiedeta¨ miksi pysa¨htyminen ta- pahtui. Valitettavan useassa ka¨ytto¨ja¨rjestelma¨ssa¨ ja ohjelmointi- kielten ajoympa¨risto¨issa¨ ta¨ma¨ on oletustoiminta silloin kun jo- kin virhe on havaittu (esimerkiksi kaatuminen muistin loppues- sa). • Suorituksen hallittu lopetus (abort, exit)\u0017 on edellista¨ lievempi tapa, jossa yriteta¨a¨n siivota ohjelmiston tila vapauttamalla kaik- ki sen varaamat resurssit ja kirjaamalla virhetilanne pysyva¨a¨n talletuspaikkaan seka¨ ilmoittamalla virheesta¨ ka¨ytta¨ja¨lle ennen suorituksen lopettamista. • Jatkaminen (continuation) tarkoittaa havaitun virheen ja¨tta¨mis- ta¨ huomiotta. Ma¨a¨rittelynsa¨ mukaisesti virhe on ohjelman ei- . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . \u0017Huom: C++-kielen funktiot abort() ja exit() toteuttavat suorituksen keskeytyksen, eiva¨t hallittua lopetusta. 11.2. Mita¨ tehda¨ virhetilanteessa? 370 toivottu tila, joten sellaisen ja¨tta¨minen ka¨sittelema¨tta¨ havain- noinnin ja¨lkeen on hyvin hyvin harvinainen toimintamalli. Jos- kus esimerkiksi ka¨ytto¨liittyma¨ssa¨ tapahtumien puuttuminen tai katoaminen voi olla tilanne, jossa jatkaminen tulee kysymyk- seen — esimerkiksi yksitta¨iset hiirikohdistimen paikkatietoa si- sa¨lta¨va¨t tapahtumat voivat kadota ilman etta¨ tilanteella on mi- ta¨a¨n vaikutusta ohjelmiston toimintaan. • Peruuttaminen (rollback) tarkoittaa ja¨rjestelma¨n tilan palautta- mista siihen tilanteeseen, mika¨ se oli ennen virheen aiheutta- neen operaation ka¨ynnista¨mista¨. Ta¨ma¨ helpottaa huomattavas- ti esimerkiksi operaation yritta¨mista¨ uudelleen, koska tiedeta¨a¨n tarkasti missa¨ tilassa ohjelmisto on, vaikka virhe onkin tapahtu- nut. Valitettavasti peruuttamisen toteuttaminen on usein mut- kikasta (voi aiheuttaa itsessa¨a¨n virheita¨ ohjelmistoon) ja resurs- seja kuluttavaa. Yksi yksinkertainen toteutustapa on operaation alussa luoda kopio muutettavasta datasta ja tehda¨ muutokset ta¨ha¨n kopioon. Jos operaatio menee la¨pi ilman virheita¨, vaih- detaan kopio alkupera¨isen datan tilalle ja virheiden sattuessa tuhotaan kopioitu data (alkupera¨inen pysyy koskemattomana). • Toipuminen (recovery) on ohjelman osan paikallinen toteutus hallitusta lopetuksesta. Osanen ei pysty itse ka¨sittelema¨a¨n ha- vaittua virhetta¨, mutta se pyrkii vapauttamaan kaikki varaa- mansa resurssit ennen kuin virheesta¨ tiedotetaan ohjelmistos- sa toisaalle (usein loogisesti ylemma¨lle tasolle) jonka toivotaan pystyva¨n ka¨sittelema¨a¨n havaittu virhe paremmin. Peruuttaminen ja toipuminen antavat mahdollisuuden yritta¨a¨ kor- jata tilannetta, joka johti virhetilanteeseen. Pieleen menneen operaa- tion yritta¨minen uudelleen on virheisiin reagoinnin suunnittelussa hankalinta. Helpoimmassa tapauksessa operaatio vain toistetaan (esi- merkiksi tietoliikenteessa¨ suoritetaan uudelleenla¨hetys), mutta va- litettavan usein virhetilanne johtuu ongelmista resursseissa, joiden puuttuessa tilanteen korjaaminen on hankalaa. Yksi hyva¨ esimerkki on muistin loppuminen. Ohjelmointikielet ja -ympa¨risto¨t tarjoavat la¨hes aina tavan havaita, etta¨ ohjelman suori- tuksen aikana silta¨ on loppunut muisti (viimeisin dynaamisen muis- tin varausoperaatio on epa¨onnistunut). Tilanteen voi havaita, mutta 11.3. Virhehierarkiat 371 mita¨ sille voi tehda¨? Jos muistia ei ole, niin toipumisoperaatiot eiva¨t missa¨a¨n tapauksessa saa kuluttaa lisa¨a¨ muistia. Ja¨rjestelma¨sta¨ voitaisiin yritta¨a¨ vapauttaa ka¨yto¨ssa¨ olevaa muis- tia, mutta resursseista ja¨rkeva¨sti huolta pita¨va¨ ohjelmisto on tietysti alunperin toteutettu siten, ettei turhia muistivarauksia ole olemas- sa. Seuraavaksi voidaan etsia¨ “va¨hemma¨n ta¨rkeita¨” muistivarauksia ja vapautetaan ne, jolloin virhetilanne hyvin suurella todenna¨ko¨isyy- della¨ siirtyy toisaalle ohjelmistossa. Yksi varma tapa saada muistia lisa¨a¨ ka¨ytto¨o¨nsa¨ on varautua lop- pumiseen etuka¨teen varaamalla kasa “turhaa” muistia, joka voidaan turvallisesti vapauttaa uusioka¨ytto¨o¨n, jos joudutaan tilanteeseen, jos- sa muisti on lopussa [Meyers, 1996]. Muistia jatkuvasti syo¨va¨n vir- heen tapauksessa tietysti vain pitkiteta¨a¨n todellisen ongelman koh- taamista, mutta ohimeneviin muistiongelmiin ta¨ma¨ on ta¨yden toipu- misen toteuttava tapa. 11.3 Virhehierarkiat Ohjelmassa tapahtuvat mahdolliset virhetilanteet voidaan usein ja- kaa kategorioihin sen perusteella, mihin virhe liittyy. Ta¨llaista vir- heiden jaottelua voi kuvata luokkakaaviolla niin kuin tavallistenkin olioiden kategorioita. Kuva 11.1 seuraavalla sivulla na¨ytta¨a¨ esimerki- na¨, miten ta¨ma¨ jako on tehty C++:n standardikirjastossa. Kuvan hierar- kia ei tietenka¨a¨n ole ta¨ydellinen, mutta se kattaa C++:n itsensa¨ tuot- tamat poikkeukset. Ohjelmoija voi itse laajentaa ta¨ta¨ hierarkiaa tai kirjoittaa oman hierarkiansa alusta saakka, jos niin haluaa. Ta¨llaisten virhehierarkioiden etuna on, etta¨ ne tekeva¨t mahdolli- seksi virheka¨sittelyn jakamisen eri tasoihin. Esimerkiksi kuvan hie- rarkiassa virheet jakautuvat kahteen pa¨a¨kategoriaan: logiikkavirheet (logic error) ja ajoaikavirheet (runtime error). Logiikkavirheisiin kuuluvat kaikki sellaiset virheet, jotka aiheutuvat ohjelman toiminta- logiikassa havaituista virheista¨, “bugeista”. Ta¨llaisen virheen tapah- tuminen on merkki siita¨, etta¨ ohjelmassa on vikaa. Ajoaikavirheet puolestaan johtuvat siita¨, etta¨ ohjelman suorituksen aikana ajoym- pa¨risto¨ aiheuttaa tilanteen, jota ohjelma ei pysty hallitsemaan. Esi- merkkeja¨ ta¨sta¨ ovat ylivuodot (ohjelmalle syo¨teta¨a¨n liian suuria luku- ja), virheet lukualueissa (ka¨ytta¨ja¨ syo¨tta¨a¨ kuukauden numeroksi 13) ja vaikkapa tietoliikenneyhteyden katkeaminen. 11.3. Virhehierarkiat 372 exception const char* what() const logic_error runtime_error bad_exception bad_typeid range_error underflow_error overflow_error domain_error length_error out_of_range invalid_argument bad_alloc bad_cast KUVA 11.1: C++-standardin virhekategoriat Kun virheet on jaoteltu hierarkiaksi, jotkin ohjelman osat voivat esimerkiksi ka¨sitella¨ ylivuodot ja kenties toipua niista¨, mutta ja¨tta¨va¨t muut virheet ohjelman ylempien tasojen huoleksi. Ylempi ohjelman osa voi sitten ka¨sitella¨ yhtena¨isesti kaikkia ajoaikaisia virheita¨ va¨lit- ta¨ma¨tta¨ siita¨, mika¨ nimenomainen virhe on kyseessa¨. Koska virheiden muodostama hierarkia muistuttaa suuresti luok- kahierarkiaa, olio-ohjelmoinnissa voidaan virheita¨ mallintaa luokil- la, jotka toteutetaan ohjelmassa. Na¨ita¨ luokkia voidaan sitten ka¨yt- ta¨a¨ hyva¨ksi C++:n poikkeusten kanssa. Listaus 11.1 seuraavalla sivulla na¨ytta¨a¨ osan kuvan 11.1 virhetyypeista¨ luokkina, jotka lo¨ytyva¨t C++:n standardikirjastoista <exception> ja <stdexcept>. Periaatteessa ohjelmoija voi itse vapaasti pa¨a¨tta¨a¨, ka¨ytta¨a¨ko¨ itse alusta saakka suunnittelemaansa virhehierarkiaa vai periytta¨a¨ko¨ tar- vitsemansa poikkeukset kirjaston virhehierarkiasta. C++:n oman vir- hehierarkian laajentaminen tietysti yhtena¨ista¨a¨ virheiden ka¨sittelya¨, joten se lienee usein tarkoituksenmukaista. Hierarkiaan voi tietysti 11.3. Virhehierarkiat 373 1 // Na¨ma¨ kaikki ovat std-nimiavaruudessa 2 class exception 3 { 4 public: 5 exception() throw(); // throw() seliteta¨a¨n myo¨hemmin 6 exception(exception const& e) throw(); 7 exception& operator =(exception const& e) throw(); 8 virtual ~exception() throw(); 9 virtual char const* what() const throw(); ... 10 }; 11 12 class runtime error : public exception 13 { 14 public: 15 runtime error(std::string const& msg); 16 }; 17 18 class overflow error : public runtime error 19 { 20 public: 21 overflow error(std::string const& msg); 22 }; LISTAUS 11.1: Virhetyypit C++:n luokkina lisa¨ta¨ myo¨s uusia alihierarkioita periytta¨ma¨lla¨ ne standardin kanta- luokista. Listaus 11.2 seuraavalla sivulla na¨ytta¨a¨ esimerkin omasta virhe- luokasta LiianPieniArvo, joka kuvaa virhetta¨, jossa jokin ohjelman arvo on liian pieni sallittuun verrattuna. Ta¨ma¨ on erikoistapaus C++:n virhetyypista¨ domain error, joten oma virheluokka on periytetty siita¨. Listauksesta na¨kyy myo¨s, kuinka virheluokkaan voi upottaa tietoa it- se virheesta¨. Ta¨ssa¨ tapauksessa jokainen LiianPieniArvo-olio sisa¨lta¨a¨ tiedon siita¨, mika¨ liian pieni arvo oli ja mika¨ arvon minimiarvo olisi ollut. Na¨ma¨ tiedot annetaan oliolle rakentajan parametrina, kun vir- hetilanne havaitaan ja virheolio luodaan. Virhetta¨ ka¨sitellessa¨ arvoja voi sitten kysya¨ luokan anna-ja¨senfunktioilla. 11.4. Poikkeusten heitta¨minen ja sieppaaminen 374 1 class LiianPieniArvo : public std::domain error 2 { 3 public: 4 LiianPieniArvo(std::string const& viesti, int luku, int minimi); 5 LiianPieniArvo(LiianPieniArvo const& virhe); 6 virtual ~LiianPieniArvo() throw(); 7 int annaLuku() const; 8 int annaMinimi() const; 9 private: 10 int luku ; 11 int minimi ; 12 }; LISTAUS 11.2: Esimerkki omasta virheluokasta 11.4 Poikkeusten heitta¨minen ja sieppaaminen C++:n poikkeusten periaatteena on, etta¨ virheen sattuessa ohjelma heitta¨a¨ (throw) “ilmaan” poikkeusolion, joka kuvaa kyseista¨ virhet- ta¨. Ta¨ma¨n ja¨lkeen ohjelma alkaa “peruuttaa” funktioiden kutsuhierar- kiassa ylo¨spa¨in ja yritta¨a¨ etsia¨ la¨himma¨n poikkeuska¨sittelija¨n (excep- tion handler), joka pystyy sieppaamaan (catch) virheolion ja reagoi- maan virheeseen. Jokaisella poikkeuska¨sittelija¨lla¨ on oma koodiloh- konsa, valvontalohko (try-block), jonka sisa¨lla¨ syntyva¨t virheet ovat sen vastuulla. Virheka¨sittelyn yhteydessa¨ poikkeusoliosta tehda¨a¨n ko- pio, joten on ta¨rkea¨a¨, etta¨ poikkeusluokilla on toimiva kopiorakentaja. Poikkeuksen heitta¨minen ja poikkeuska¨sittelija¨n etsiminen on kohtalaisen raskas operaatio verrattuna useimpiin muihin C++:n ope- raatioihin. Niinpa¨ onkin ta¨rkea¨a¨, etta¨ poikkeuksia ka¨yteta¨a¨n vain poikkeuksellisten tilanteiden ka¨sittelyyn eika¨ esimerkiksi uutena muodikkaana hyppyka¨skyna¨. 11.4.1 Poikkeushierarkian hyva¨ksika¨ytto¨ Listaus 11.3 sivulla 376 sisa¨lta¨a¨ esimerkin virheka¨sittelysta¨ keskiar- von laskennassa. Keskiarvoa laskettaessa on kaksi virhemahdollisuut- ta: lukujen lukuma¨a¨ra¨ saattaa olla nolla tai niiden summa saattaa kas- vaa liian suureksi. Lukujen summaa laskeva funktio heitta¨a¨ ylivuoto- 11.4. Poikkeusten heitta¨minen ja sieppaaminen 375 tapauksessa riveilla¨ 10 ja 15 poikkeuksen komennolla throw.] Vastaa- vasti keskiarvon laskeva funktio heitta¨a¨ poikkeuksen rivilla¨ 27, jos lukujen lukuma¨a¨ra¨ on nolla. Funktio keskiarvo1 sisa¨lta¨a¨ kaksi poikkeuska¨sittelija¨a¨ riveilla¨ 40– 47. Ka¨sittelija¨ merkita¨a¨n avainsanalla catch, jonka ja¨lkeen anne- taan parametrilistan omaisesti ka¨sittelija¨n hyva¨ksyma¨n poikkeus- olion tyyppi. Poikkeuska¨sittelija¨n parametrit on syyta¨ merkita¨ vakio- viitteiksi samasta syysta¨ kuin tavallisetkin parametrit olioita va¨litet- ta¨essa¨ (aliluku 4.3.3). Poikkeuska¨sittelija¨t pystyva¨t sieppaamaan vir- hetilanteet, jotka syntyva¨t sina¨ aikana, kun ohjelman suoritus on ri- veilla¨ 34–39 olevan try-avainsanalla merkityn valvontalohkon sisa¨l- la¨. Ensimma¨inen poikkeuska¨sittelija¨ sieppaa nollallajakovirheet, ja¨l- kimma¨inen ylivuodot. Normaalissa tapauksessa ohjelman suoritus siirtyy valvontaloh- koon, suorittaa siella¨ koodin ja hyppa¨a¨ sen ja¨lkeen poikkeuska¨sitte- lijo¨iden yli riville 48. Ta¨lla¨ tavoin virheka¨sittely ei milla¨a¨n tavalla vaikuta ohjelman normaaliin toimintaan. Jos lukujen summa kasvaa liian suureksi, rivi 10 heitta¨a¨ poikkeuk- sen. Ohjelma alkaa ta¨llo¨in etsia¨ la¨hinta¨ sopivaa poikkeuska¨sittelija¨a¨. Funktiossa summaaLuvut sellaista ei ole, joten virhe “vuotaa” ulos ta¨s- ta¨ funktiosta ja ohjelma peruuttaa takaisin funktioon laskeKeskiarvo. Siella¨ka¨a¨n ei ole poikkeuska¨sittelija¨a¨, joten ohjelma palaa funktioon keskiarvo1. Siella¨ on vihdoin ylivuotovirheen hyva¨ksyva¨ poikkeus- ka¨sittelija¨, ja ohjelman suoritus jatkuu poikkeuska¨sittelija¨n koodista rivilta¨ 46. Virheka¨sittelyn pa¨a¨tyttya¨ ohjelma ei palaa virhekohtaan vaan jatkuu koko virheka¨sittelyrakenteen ja¨lkeen rivilta¨ 48. Virheka¨sittelya¨ ei siis suoraan voi ka¨ytta¨a¨ ta¨ydelliseen virheesta¨ toipumiseen, jossa ohjelman suoritus palaisi virheka¨sittelyn ja¨lkeen takaisin valvonta- lohkoon jatkamaan sen suoritusta. Listauksen 11.3 molemmat poikkeuska¨sittelija¨t sisa¨lta¨va¨t la¨hes sa- man koodin, koska molemmat virheet ovat luonteeltaan samanlaisia. Joskus onkin ja¨rkeva¨a¨ tehda¨ poikkeuska¨sittelija¨, joka sieppaa kaikki tiettyyn virhekategoriaan kuuluvat poikkeukset. Ta¨ma¨ tehda¨a¨n laitta- malla poikkeuska¨sittelija¨n parametriksi viite virhehierarkian halut- tuun kantaluokkaan. Koska jokainen virhekantaluokasta peritty vir- he on olio-ohjelmoinnin mukaan myo¨s kantaluokan olio, poikkeus- . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ]Koodissa numeric limits<double>::max() palauttaa suurimman mahdollisen liukulu- vun. numeric limits-mallia ka¨siteltiin aliluvussa 10.6.4. 11.4. Poikkeusten heitta¨minen ja sieppaaminen 376 1 void lueLuvutTaulukkoon(vector<double>& taulu); 2 3 double summaaLuvut(vector<double> const& luvut) 4 { 5 double summa = 0.0; 6 for (unsigned int i = 0; i < luvut.size(); ++i) 7 { 8 if (summa >= 0 && luvut[i] > numeric limits<double>::max()-summa) 9 { 10 throw std::overflow error(\"Summa liian suuri\"); 11 } 12 else if (summa < 0 && 13 luvut[i] < -numeric limits<double>::max()-summa) 14 { 15 throw std::overflow error(\"Summa liian pieni\"); 16 } 17 summa += luvut[i]; 18 } 19 return summa; 20 } 21 22 double laskeKeskiarvo(vector<double> const& luvut) 23 { 24 unsigned int lukumaara = luvut.size(); 25 if (lukumaara == 0) 26 { 27 throw std::range error(\"Lukuma¨a¨ra¨ keskiarvossa 0\"); 28 } 29 return summaaLuvut(luvut) / static cast<double>(lukumaara); 30 } 31 32 void keskiarvo1(vector<double>& lukutaulu) 33 { 34 try 35 { 36 lueLuvutTaulukkoon(lukutaulu); 37 double keskiarvo = laskeKeskiarvo(lukutaulu); 38 cout << \"Keskiarvo: \" << keskiarvo << endl; 39 } 40 catch (std::range error const& virhe) 41 { 42 cerr << \"Lukualuevirhe: \" << virhe.what() << endl; 43 } 44 catch (std::overflow error const& virhe) 45 { 46 cerr << \"Ylivuoto: \" << virhe.what() << endl; 47 } 48 cout << \"Loppu\" << endl; 49 } LISTAUS 11.3: Esimerkki C++:n poikkeuska¨sittelija¨sta¨ 11.4. Poikkeusten heitta¨minen ja sieppaaminen 377 ka¨sittelija¨ sieppaa myo¨s kaikki kantaluokasta periytetyt poikkeukset. Listauksessa 11.4 on uusi keskiarvofunktio, jonka poikkeuska¨sittelija¨ sieppaa kaikki ajoaikaiset virheet. 11.4.2 Poikkeukset, joita ei oteta kiinni Ohjelmassa voi tietysti tapahtua myo¨s poikkeus, jota mika¨a¨n poik- keuska¨sittelija¨ ei sieppaa. Esimerkissa¨ on mahdollista, etta¨ lukuja taulukkoon luettaessa muisti loppuu. Ta¨llo¨in vector-luokka vuotaa ulos poikkeuksen std::bad alloc (katso aliluku 3.5.1). Jos nyt funk- tio lueLuvutTaulukkoon ei itse sieppaa ta¨ta¨ virhetta¨ ja toivu siita¨, vuo- taa virhe ulos ta¨sta¨kin funktiosta. Mika¨li virhe pa¨a¨see vuotamaan ulos pa¨a¨ohjelmastakin eli jos oh- jelmassa ei yksinkertaisesti ole sopivaa poikkeuska¨sittelija¨a¨, ohjelma kutsuu funktiota terminate. Oletusarvoisesti ta¨ma¨ funktio vain lopet- taa ohjelman suorituksen (ja kenties tulostaa virheilmoituksen). Oh- jelma voi itse tarjota oman toteutuksensa ta¨lle funktiolle, mutta joka tapauksessa ohjelman suoritus loppuu ta¨ma¨n funktion suoritukseen. Koska poikkeukset, joihin ei ole varauduttu, aiheuttavat ohjelman kaatumisen, on ta¨rkea¨a¨, etta¨ ohjelmassa otetaan jollain tasolla kiinni kaikki aiheutetut poikkeukset. Joissain tapauksissa voi tietysti olla, etta¨ ohjelman kaatuminen on hyva¨ksytta¨va¨ reaktio virhetilanteeseen. 1 void keskiarvo2(vector<double>& lukutaulu) 2 { 3 try 4 { 5 lueLuvutTaulukkoon(lukutaulu); 6 double keskiarvo = laskeKeskiarvo(lukutaulu); 7 cout << \"Keskiarvo: \" << keskiarvo << endl; 8 } 9 catch (std::runtime error const& virhe) 10 { 11 // Ta¨nne tullaan minka¨ tahansa ajoaikaisen virheen seurauksena 12 cerr << \"Ajoaikainen virhe: \" << virhe.what() << endl; 13 } 14 15 cout << \"Loppu\" << endl; 16 } LISTAUS 11.4: Virhekategorioiden ka¨ytto¨ poikkeuksissa 11.4. Poikkeusten heitta¨minen ja sieppaaminen 378 Ohjelmaan voi myo¨s lisa¨ta¨ “yleispoikkeuska¨sittelijo¨ita¨”, jotka otta- vat vastaan kaikki valvontalohkossaan tapahtuvat poikkeukset. Yleis- poikkeuska¨sittelija¨n syntaksi on catch (. . .) // Todellakin . . . eli kolme pistetta¨ { // Ta¨ma¨ poikkeuska¨sittelija¨ sieppaa kaikki poikkeukset } Yleensa¨ ta¨llaisia “kaikkivoipia” yleispoikkeuska¨sittelijo¨ita¨ ei kan- nata kirjoittaa kuin korkeintaan pa¨a¨ohjelmaan, ellei sitten ole aivan varma, etta¨ poikkeuska¨sittelija¨n koodi todella pystyy reagoimaan oi- kein kaikkiin mahdollisiin poikkeuksiin, joita valvontalohkossa voi sattua. Yleispoikkeuska¨sittelija¨ha¨n sieppaa myo¨s sellaiset virheet, joihin kenties voitaisiin paremmin reagoida ylemma¨lla¨ tasolla ohjel- massa! Poikkeuksen (,) ta¨sta¨ muodostavat tilanteet, joissa virheesta¨ riip- pumatta ta¨ytyy suorittaa tietty siivouskoodi, jonka ja¨lkeen virhe heite- ta¨a¨n uudelleen komennolla throw; ylemma¨n tason poikkeuska¨sitteli- jo¨iden hoidettavaksi. Ta¨llo¨in yleispoikkeuska¨sittelija¨ on varsin ka¨yt- to¨kelpoinen. Poikkeuksista ja siivoustoimenpiteista¨ kerrotaan enem- ma¨n aliluvussa 11.5. 11.4.3 Sisa¨kka¨iset valvontalohkot Listauksien 11.3 ja 11.4 keskiarvofunktiot eiva¨t ota mita¨a¨n kantaa muistin loppumiseen, joten niissa¨ mahdollisesti syntyva¨t muistin loppumisesta aiheutuvat poikkeukset — tai mitka¨ tahansa poikkeuk- set ajoaikavirheita¨ lukuun ottamatta — vuotavat funktioista ulos ylempiin funktioihin. Niissa¨ voi puolestaan olla omia poikkeuska¨sit- telijo¨ita¨a¨n, joista jotkin voivat sitten siepata muistin loppumisesta ai- heutuneet poikkeukset. Jos poikkeus heiteta¨a¨n usean sisa¨kka¨isen val- vontalohkon sisa¨lla¨, etsita¨a¨n “la¨hin” poikkeuska¨sittelija¨, joka pystyy sieppaamaan poikkeuksen. Listaus 11.5 seuraavalla sivulla na¨ytta¨a¨ pa¨a¨ohjelman, joka kut- suu keskiarvofunktiota ja sisa¨lta¨a¨ lisa¨ksi oman poikkeuska¨sittelija¨n- sa¨. Keskiarvofunktio ka¨sittelee itse ajoaikavirheet, mutta muut vir- heet vuotavat keskiarvofunktiosta ulos pa¨a¨ohjelmaan. Na¨ista¨ pa¨a¨oh- jelma ka¨sittelee itse muistin loppumisen ja kaikki ohjelman Virhe- luokasta periytetyt poikkeukset. 11.4. Poikkeusten heitta¨minen ja sieppaaminen 379 1 int main() 2 { 3 vector<double> taulu; 4 try 5 { 6 keskiarvo2(taulu); // Lue luvut ja laske keskiarvo 7 } 8 catch (std::bad alloc&) 9 { 10 cerr << \"Muisti loppui!\" << endl; 11 return EXIT FAILURE; 12 } 13 catch (std::exception const& virhe) 14 { 15 cerr << \"Virhe pa¨a¨ohjelmassa: \" << virhe.what() << endl; 16 return EXIT FAILURE; 17 } 18 19 return EXIT SUCCESS; 20 } LISTAUS 11.5: Sisa¨kka¨iset valvontalohkot Ta¨ma¨ mahdollisuus sisa¨kka¨isiin valvontalohkoihin on eritta¨in hyo¨dyllinen ominaisuus, varsinkin kun se yhdisteta¨a¨n virheluokka- hierarkioihin. Ta¨llo¨in ohjelman alemmissa osissa voidaan ka¨sitel- la¨ yksityiskohtaisesti tietyt virheet, kuten esimerkissa¨ keskiarvosta johtuvat ylivuodot ja nollalla jakaminen. Ohjelman ylemma¨t osat taas voivat ka¨sitella¨ yleisemma¨lla¨ tasolla laajempia virhekategorioita. Na¨in jokainen ohjelman osa voi reagoida virheisiin omalla abstraktio- tasollaan. Triviaalit pikkuvirheet siepataan alemmilla tasoilla ja yla¨- tasoille vuotavat suuremmat virheet voivat puolestaan aiheuttaa dra- maattisempia toimia. Poikkeuska¨sittelija¨ voi myo¨s halutessaan heitta¨a¨ virheen edelleen, jos se ei pysty toipumaan virheesta¨. Ta¨ma¨ saadaan aikaan poik- keuska¨sittelija¨n koodissa komennolla throw; ilman mita¨a¨n paramet- ria. Ta¨llaista osittaista poikkeuska¨sittelya¨ ka¨yteta¨a¨n hyva¨ksi funktion siivoustoimenpiteissa¨ seuraavassa aliluvussa. Poikkeuska¨sittelija¨ voi myo¨s muuttaa poikkeuksen toiseksi heitta¨ma¨lla¨ omasta koodistaan uuden poikkeuksen. 11.5. Poikkeukset ja olioiden tuhoaminen 380 11.5 Poikkeukset ja olioiden tuhoaminen Poikkeuksen sattuessa ohjelma palaa takaisin koodilohkoista ja funk- tioista, kunnes se lo¨yta¨a¨ sopivan poikkeuska¨sittelija¨n. Ta¨ma¨n peruut- tamisen tuloksena saatetaan poistua usean olion ja muuttujan na¨ky- vyysalueelta. C++ pita¨a¨ huolen siita¨, etta¨ kaikki oliot ja muuttujat, joi- den na¨kyvyysalue loppuu poikkeuksen tuloksena, tuhotaan normaa- listi. Olioiden purkajia kutsutaan, joten niiden siivoustoimenpiteet suoritetaan kuten pita¨a¨kin. Na¨in poikkeukset eiva¨t aiheuta mita¨a¨n ongelmia olioille, joiden elinkaari on staattisesti ma¨a¨ra¨tty. Java-kielessa¨ ei ole purkajia samalla tavoin kuin C++:ssa, joten sii- na¨ kielen muuten C++:aa muistuttavaan poikkeuska¨sittelyyn on lisa¨tty erityinen poikkeuska¨sittelijo¨iden ja¨lkeen tuleva finally-lohko, jossa oleva koodi suoritetaan aina lopuksi, tapahtui valvontalohkossa poik- keus tai ei. Ta¨ha¨n lohkoon voi kirjoittaa siivoustoimenpiteita¨, jotka suoritetaan aina valvontalohkosta poistumisen ja¨lkeen. Joskus vastaavasta siivouslohkosta olisi hyo¨tya¨ myo¨s C++-kielessa¨, mutta sen poikkeusmekanismista ei ta¨llaista lo¨ydy. Yleisesti ka¨ytetty ratkaisu on upottaa mahdollisimman moni siivousta vaativa asia so- pivan luokan sisa¨a¨n, jolloin luokan purkaja suorittaa tarvittavan sii- vouksen. 11.5.1 Poikkeukset ja purkajat C++ pystyy ka¨sittelema¨a¨n samassa lohkossa vain yhta¨ poikkeusta ker- rallaan. Poikkeuksen heitta¨minen aiheuttaa tarvittavien staattisen elinkaaren olioiden purkajien suorittamisen ennen kuin poikkeus on ka¨sitelty. Jos jo heitetyn poikkeuksen tuloksena kutsutaan pur- kajaa, joka puolestaan vuotaa ulos oman poikkeuksensa, tulisi sa- maan aikaan voimaan kaksi poikkeusta. C++:n poikkeuska¨sittely ei pysty ta¨ha¨n, joten se kutsuu ta¨llaisesssa tapauksessa suoraan funktio- ta terminate ja lopettaa ohjelman suorituksen. Poikkeuksia purkajien yhteydessa¨ ka¨sitella¨a¨n tarkemmin aliluvussa 11.8.3. 11.5.2 Poikkeukset ja dynaamisesti luodut oliot Dynaamisen elinkaaren oliot ovat ongelmallisia. Kuten jo luvussa 3 kerrottiin, new’lla¨ varattuja olioita ei koskaan tuhota automaattises- ti, ja ta¨ma¨ pa¨tee myo¨s poikkeuksen sattuessa. Tilanteen tekee eritta¨in 11.5. Poikkeukset ja olioiden tuhoaminen 381 vaikeaksi se, etta¨ dynaamisesti luotuihin olioihin osoittavat osoittimet ovat todenna¨ko¨isesti normaaleja paikallisia muuttujia, joten ne tuho- taan poikkeuksen seurauksena. Na¨in muistiin ja¨a¨ helposti tuhoamat- tomia olioita, joita on mahdoton tuhota, koska niihin ei ena¨a¨ pa¨a¨sta¨ ka¨siksi. Ainoa tapa va¨ltta¨a¨ edella¨ mainitun kaltaisia muistivuotoja on ym- pa¨ro¨ida¨ kaikki tarvittavat dynaamisia olioita ka¨sitteleva¨t koodiloh- kot omalla poikkeuska¨sittelija¨lla¨a¨n. Poikkeuska¨sittelija¨ tuhoaa olion deletella¨ ja sen ja¨lkeen heitta¨a¨ viela¨ tarvittaessa virheen edelleen ylemma¨lla¨ tasolla ka¨sitelta¨va¨ksi. Listauksessa 11.6 on funktio, joka luo olion dynaamisesti ja tuhoaa sen virhetilanteessa. Huomaa, etta¨ koodissa ei riita¨ varautuminen pelka¨sta¨a¨n muistin loppumiseen vaan olio ta¨ytyy tuhota minka¨ tahansa muunkin virheen sattuessa. Jos funktiossa luodaan dynaamisesti useita olioita pera¨kka¨in, on ta¨rkea¨a¨, etta¨ koodissa varaudutaan siihen, etta¨ muisti loppuu, kun vas- ta osa olioista on saatu luoduksi. Jos olioiden luomisen va¨lissa¨ vie- la¨ suoritetaan koodia, jossa voi tapahtua virheita¨, kannattaa koodiin yleensa¨ kirjoittaa useita sisa¨kka¨isia¨ valvontalohkoja. Listaus 11.7 seu- raavalla sivulla sisa¨lta¨a¨ esimerkin ta¨llaisesta funktiosta. 1 void siivousfunktio1() 2 { 3 vector<double>* taulup = new vector<double>(); 4 5 try 6 { // Jos ta¨a¨lla¨ sattuu virhe, vektori pita¨a¨ tuhota 7 keskiarvo2(*taulup); 8 } 9 catch (. . .) 10 { // Otetaan kiinni kaikki virheet ja tuhotaan vektori 11 delete taulup; taulup = 0; 12 throw; // Heiteta¨a¨n poikkeus edelleen ka¨sitelta¨va¨ksi 13 } 14 15 delete taulup; taulup = 0; // Ta¨nne pa¨a¨sta¨a¨n, jos virheita¨ ei satu 16 } LISTAUS 11.6: Esimerkki dynaamisen olion siivoamisesta 11.6. Poikkeusma¨a¨reet 382 1 void siivousfunktio2() 2 { 3 vector<double>* taulup = new vector<double>(); 4 try 5 { 6 // Jos ta¨a¨lla¨ sattuu virhe, vektori pita¨a¨ tuhota 7 keskiarvo2(*taulup); 8 vector<double>* taulu2p = new vector<double>(); 9 try 10 { // Jos ta¨a¨lla¨ sattuu virhe, myo¨s uusi vektori pita¨a¨ tuhota 11 for (unsigned int i = 0; i < taulup->size(); ++i) 12 { // Lasketaan taulukon nelio¨t 13 taulu2p->push back((*taulup)[i] * (*taulup)[i]); 14 } 15 cout << \"Nelio¨iden k.a.=\" << laskeKeskiarvo(*taulu2p) << endl; 16 } 17 catch (. . .) 18 { // Otetaan kiinni kaikki virheet ja tuhotaan vektori 19 delete taulu2p; taulu2p = 0; 20 throw; // Heiteta¨a¨n virhe ylemma¨lle tasolle 21 } 22 delete taulu2p; taulu2p = 0; // Ta¨nne tullaan, jos virheita¨ ei satu 23 } 24 catch (. . .) 25 { // Otetaan kiinni kaikki virheet ja tuhotaan vektori 26 delete taulup; taulup = 0; 27 throw; // Heiteta¨a¨n poikkeus edelleen ka¨sitelta¨va¨ksi 28 } 29 delete taulup; taulup = 0; // Ta¨nne pa¨a¨sta¨a¨n, jos virheita¨ ei satu 30 } LISTAUS 11.7: Virheisiin varautuminen ja monta dynaamista oliota 11.6 Poikkeusma¨a¨reet Funktioista ulos vuotavat poikkeukset ovat olennainen osa funktion dokumentaatiota. Ilman niita¨ funktion kutsuja ei tieda¨, mihin kaik- kiin poikkeuksiin tulee varautua. C++ antaa mahdollisuuden merki- ta¨ funktioon, minka¨ tyyppiset poikkeukset saavat vuotaa funktios- ta ulos. Ta¨ma¨ tapahtuu poikkeusma¨a¨reiden (exception speciﬁcation) avulla. Ika¨va¨ kylla¨ ka¨yta¨nto¨ on standardoinnin ja¨lkeen osoittanut, et- ta¨ hyva¨sta¨ tarkoituksesta huolimatta poikkeusma¨a¨reet eiva¨t C++:ssa ole kovinkaan ka¨ytto¨kelpoinen mekanismi, koska ne suureksi osak- 11.7. Muistivuotojen va¨ltta¨minen: auto_ptr 383 si pohjautuvat ajoaikaisiin tarkastuksiin siita¨, millaisia poikkeuksia funktioista saa vuotaa ulos. Niinpa¨ nykyisin monet C++-asiantuntijat suosittelevatkin, ettei poikkeusma¨a¨reita¨ ka¨ytetta¨isi, vaan funktioista mahdollisesti vuotavat poikkeukset dokumentoitaisiin muuten raja- pintadokumentaatioon. Lisa¨a¨ tietoa poikkeusma¨a¨reista¨ ja syista¨ niiden va¨ltta¨ma¨iseen lo¨y- tyy esimerkiksi Herb Sutterin artikkeleista “A Pragmatic Look at Ex- ception Speciﬁcations — The C++ feature that wasn’t” [Sutter, 2002a] ja “Exception Safety and Exception Speciﬁcations: Are They Worth It?” [Sutter, 2003]. Olio-ohjelmoinnissa on lisa¨ksi useita tilanteita, joissa funktio ei tieda¨, mita¨ kaikkia poikkeuksia voi vuotaa ulos. Ta¨llainen tilanne voi sattua erityisesti polymorﬁsmia ja malleja (geneerisyytta¨) ka¨ytetta¨es- sa¨, jolloin funktio ei va¨ltta¨ma¨tta¨ tarkalleen tieda¨, millaisia olioita se ka¨sittelee, joten se ei myo¨ska¨a¨n tieda¨ mahdollisia poikkeuksia. On kuitenkin vaikeaa ohjelmoida niin, etta¨ varautuu kaikkiin mahdol- lisiin — jopa tuntemattomiin — poikkeuksiin, joten ohjelmoinnissa tulisi aina pyrkia¨ dokumentoimaan mahdolliset ulos vuotavat poik- keukset. Jos funktiosta voi vuotaa ulos jotkin tunnetut poikkeukset ja lisa¨k- si mahdollisesti muitakin, poikkeusma¨a¨reita¨ ei voi ka¨ytta¨a¨, koska ne edellytta¨va¨t, etta¨ poikkeuslistaan merkita¨a¨n kaikki mahdolliset poik- keukset. Ta¨llaisissa tapauksissa kannattaakin vain dokumentoida so- pivaan ohjelmakommenttiin tunnetut poikkeukset ja lisa¨ksi mainita, etta¨ muitakin poikkeuksia saattaa vuotaa ulos. 11.7 Muistivuotojen va¨ltta¨minen: auto_ptr Dynaamisen elinkaaren oliot tuottavat poikkeuska¨sittelyssa¨ paljon ongelmia. Niita¨ tarvitaan va¨ltta¨ma¨tta¨, jos olion elinkaari ei osu yksiin minka¨a¨n koodilohkon na¨kyvyysalueen kanssa. Toisaalta dynaamises- ti luotujen olioiden tuhoaminen kaikissa mahdollisissa virhetilan- teissa lisa¨a¨ tarvittavan koodin ma¨a¨ra¨a¨ ja tekee koodista vaikealukui- semman. Ta¨ma¨n vuoksi ISOC++:aan lisa¨ttiin luokka nimelta¨ auto ptr, joka ratkaisee osan dynaamisten olioiden ongelmista. Ta¨ta¨ auto ptr- luokkaa ei ika¨va¨ kylla¨ aina ole vanhahkojen ka¨a¨nta¨ja¨versioiden kir- jastoissa. Sen sijaan uudemmista ka¨a¨nta¨jista¨ sen pita¨isi lo¨ytya¨. 11.7. Muistivuotojen va¨ltta¨minen: auto_ptr 384 11.7.1 Automaattiosoittimet ja muistinhallinta Automaattiosoittimen saa ka¨ytto¨o¨nsa¨ komennolla #include <memory>. Se ka¨ytta¨ytyy ulkoisesti la¨hes ta¨sma¨lleen samoin kuin tavallinen osoitinkin: sen saa alustaa osoittamaan dy- naamisesti luotuun olioon, siihen voi sijoittaa uuden olion, ja sen pa¨a¨ssa¨ olevaan olioon pa¨a¨see ka¨siksi normaalisti operaattoreilla * ja ->. Erona automaattiosoittimien ja tavallisten osoittimien va¨lilla¨ on, etta¨ automaattiosoitin omistaa pa¨a¨ssa¨a¨n olevan olion. Lisa¨ksi automaattiosoittimilla ei voi tehda¨ osoitinaritmetiikkaa eika¨ niita¨ voi ka¨ytta¨a¨ osoittamaan taulukoihin. Huomaa, etta¨ automaattio- soittimen pa¨a¨ha¨n saa sijoittaa vain dynaamisesti new’lla¨ luotuja olioita! Omistaminen tarkoittaa sita¨, etta¨ kun automaattiosoitin tuhou- tuu, se suorittaa automaattisesti delete-operaation pa¨a¨ssa¨a¨n olevalle oliolle. Automaattiosoittimien hyo¨dyllisyys piilee juuri omistamises- sa. Sen ansiosta ohjelmoijan ei tarvitse va¨litta¨a¨ dynaamisesti luotujen olioiden tuhoamisesta. Jos ne on alunperin pantu automaattiosoit- timen pa¨a¨ha¨n, olion tuhoamisvastuu siirtyy osoittimelle. Koska itse automaattiosoittimen elinkaari on staattinen, ka¨a¨nta¨ja¨ pita¨a¨ huolen olion tuhoamisesta. Listaus 11.8 seuraavalla sivulla sisa¨lta¨a¨ esimerkin automaattio- soittimien ka¨yto¨sta¨. Siita¨ ka¨y ilmi automaattiosoittimien samankal- taisuus tavallisten osoittimien kanssa ja se, miten automaattiosoit- timet helpottavat virheka¨sittelya¨ verrattuna listauksen 11.7 koodiin. Koska automaattiosoitin pita¨a¨ itse huolen pa¨a¨ssa¨a¨n olevan olion tu- hoamisesta, virhetilanteissa ei tarvitse ryhtya¨ mihinka¨a¨n erikoistoi- menpiteisiin — poikkeuksen sattuessa paikallisena muuttujana oleva automaattiosoitin tuhotaan automaattisesti, ja se puolestaan tuhoaa dynaamisesti luodun olion. Automaattiosoittimesta saa halutessaan ulos tavallisen “ei- omistavan” osoittimen ja¨senfunktiokutsulla osoitin.get(). Ta¨ta¨ voi ka¨ytta¨a¨, jos ohjelmassa tulee joskus tarve viitata olioon myo¨s taval- lisen osoittimen la¨pi. Ta¨llo¨in on kuitenkin syyta¨ muistaa, etta¨ olion omistus sa¨ilyy automaattiosoittimella, joten olio tuhoutuu edelleen automaattisesti automaattiosoittimen tuhoutuessa. 11.7. Muistivuotojen va¨ltta¨minen: auto_ptr 385 1 #include <memory> 2 using std::auto ptr; ... 3 void siivousfunktio2() 4 { 5 auto ptr< vector<double> > taulup(new vector<double>()); 6 keskiarvo2(*taulup); 7 8 auto ptr < vector<double> > taulu2p(new vector<double>()); 9 for (unsigned int i = 0; i < taulup->size(); ++i) 10 { // Lasketaan taulukon nelio¨t 11 taulu2p->push back((*taulup)[i] * (*taulup)[i]); 12 } 13 cout << \"Nelio¨iden keskiarvo: \" << laskeKeskiarvo(*taulu2p) << endl; 14 } LISTAUS 11.8: Esimerkki automaattiosoittimen auto_ptr ka¨yto¨sta¨ 11.7.2 Automaattiosoittimien sijoitus ja kopiointi Ohjelmoinnissa tulee usein vastaan tilanne, jossa monta osoitinta osoittaa samaan olioon. Automaattiosoittimien tapauksessa ta¨ma¨ ei kuitenkaan ole mahdollista, koska olion voi omistaa vain yksi au- tomaattiosoitin kerrallaan. Jos automaattiosoittimia voisi osoittaa sa- maan olioon useita, ta¨ma¨ olio tulisi tuhotuksi useaan kertaan. Ta¨ma¨n vuoksi automaattiosoittimien kopiointi ja sijoittaminen on ma¨a¨ritelty niin, etta¨ operaatiot siirta¨va¨t olion osoittimesta toiseen. Siirta¨minen tarkoittaa sita¨, etta¨ kopioinnin ja¨lkeen uusi osoitin osoittaa olioon ja omistaa sen kun taas vanha osoitin on “tyhjenty- nyt” eika¨ ena¨a¨ osoita minneka¨a¨n. Vastaavasti sijoituksen yhteydes- sa¨ alkupera¨inen osoitin menetta¨a¨ olion eika¨ osoita ena¨a¨ minneka¨a¨n kun taas sijoituksen kohteena oleva osoitin tuhoaa vanhan olionsa ja siirtyy omistamaan uutta oliota. Ta¨llainen sijoituksen ja kopioinnin semantiikka, jossa myo¨s alkupera¨inen osoitin muuttuu, eroaa oleelli- sesti normaalista sijoituksesta ja kopioinnista, joten sen kanssa kan- nattaa olla tarkkana. Omistuksen siirtyminen on kuitenkin myo¨s eritta¨in hyo¨dyllinen piirre. Sen avulla funktiossa voi luoda automaattiosoittimen pa¨a¨ha¨n dynaamisesti olion ja palauttaa funktiosta auto ptr-tyyppisen paluu- arvon. Ta¨lla¨ tavoin olion omistus siirtyy paluuarvon mukana auto- maattisesti funktiosta kutsujan puolelle eika¨ kummankaan tarvitse 11.7. Muistivuotojen va¨ltta¨minen: auto_ptr 386 va¨litta¨a¨ olion tuhoamisesta edes virhetilanteissa. Listaus 11.9 na¨yt- ta¨a¨, kuinka automaattiosoitinta voi ka¨ytta¨a¨ hyva¨ksi olion omistuksen siirta¨miseen paikasta toiseen. Olion omistuksen ja sen siirtymisen vuoksi automaattiosoitinta voi ka¨ytta¨a¨ myo¨s dokumentoimaan omistusta. Ohjelman voi kirjoit- taa niin, etta¨ kaikki dynaamisesti luodut oliot ovat aina jonkin au- tomaattiosoittimen pa¨a¨ssa¨. Jos funktiolle ta¨ytyy va¨litta¨a¨ tieto oliosta, mutta ei omistusvastuuta, ka¨yteta¨a¨n joko viitetta¨ tai tavallista osoi- tinta. Jos taas funktiolle halutaan siirta¨a¨ myo¨s omistusvastuu, va¨lite- ta¨a¨n automaattiosoitin. Ta¨llaisessa ohjelmoinnissa dynaamisesti luo- tuja olioita ei tarvitse koskaan tuhota ja ainakin periaatteessa muis- tivuodot ovat mahdottomia. Ka¨yta¨nno¨ssa¨ ohjelmoijan ta¨ytyy kuiten- kin varmistua siita¨, etteiva¨t tavalliset osoittimet tai viitteet ja¨a¨ viittaa- maan jo tuhottuun olioon ja ettei jo siirrettyyn olioon yriteta¨ pa¨a¨sta¨ 12 typedef auto ptr< vector<double> > AutoTauluPtr; 13 14 AutoTauluPtr tuotaTaulukko() 15 { 16 AutoTauluPtr taulup(new vector<double>()); 17 lueLuvutTaulukkoon(*taulup); 18 return taulup; 19 } 20 AutoTauluPtr tuotaNeliotaulukko(vector<double> const* taulup) 21 { // Parempi vaihtoehto: tuotaNeliotaulukko(vector<double> const& taulu) 22 AutoTauluPtr neliop(new vector<double>()); 23 for (unsigned int i = 0; i < taulup->size(); ++i) 24 { // Lasketaan taulukon nelio¨t 25 neliop->push back((*taulup)[i] * (*taulup)[i]); 26 } 27 return neliop; 28 } 29 30 void siivousfunktio3() 31 { 32 AutoTauluPtr taulu1p(tuotaTaulukko()); // Taulukon omistus siirtyy ta¨nne 33 AutoTauluPtr taulu2p(tuotaNeliotaulukko(taulu1p.get())); // Samoin ta¨ssa¨ 34 35 cout << \"Keskiarvo: \" << laskeKeskiarvo(*taulu1p) << endl; 36 cout << \"Nelio¨iden keskiarvo: \" << laskeKeskiarvo(*taulu2p) << endl; 37 } LISTAUS 11.9: Automaattiosoitin ja omistuksen siirto 11.7. Muistivuotojen va¨ltta¨minen: auto_ptr 387 ka¨siksi vanhan automaattiosoittimen kautta. 11.7.3 Automaattiosoittimien ka¨yto¨n rajoitukset Aiemmin ta¨ssa¨ luvussa on jo tullut esille joitain automaattiosoittimia koskevia rajoituksia. Seuraavaan luetteloon on kera¨tty ta¨rkeimma¨t ra- joitukset. • Automaattiosoittimen pa¨a¨ha¨n saa laittaa vain dynaamisesti new’lla¨ luotuja olioita. • Vain yksi automaattiosoitin voi osoittaa samaan olioon kerral- laan. Automaattiosoittimien sijoitus ja kopiointi siirta¨va¨t olion omistuksen automaattiosoittimelta toiselle. • Sijoituksen ja kopioinnin ja¨lkeen olioon ei pa¨a¨se ka¨siksi vanhan automaattiosoittimen kautta. • Automaattiosoittimelle ei voi tehda¨ osoitinaritmetiikkaa (++, --, indeksointi ynna¨ muut). • Automaattiosoittimen pa¨a¨ha¨n ei voi panna tavallisia taulukoita (sen sijaan vector ei tuota ongelmia). • Automaattiosoittimia ei voi laittaa STL:n tietora- kenteiden sisa¨lle. Ta¨ma¨ tarkoittaa, etta¨ esimerkiksi vector< auto ptr<int> > ei toimi. Na¨ma¨ rajoitukset huomioon ottamalla automaattiosoittimet hel- pottavat huomattavasti dynaamisten olioiden hallintaa. Rajoituksis- ta ehka¨ ika¨vin on se, ettei automaattiosoittimia voi panna suoraan STL:n tietorakenteisiin. Syy ta¨ha¨n on se, etta¨ STL vaatii etta¨ tieto- rakenteiden alkioita voi kopioida ja sijoittaa normaalisti. Automaat- tiosoittimien omistuksen siirto aiheuttaa sen, etta¨ sijoituksessa ja ko- pioinnissa alkupera¨inen olio muuttuu, joten automaattiosoittimet ei- va¨t ole yhteensopivia STL:n kanssa. Vaihtoehtoja automaattiosoittimille ovat “a¨lykka¨a¨t osoittimet” (smart pointer), jotka ka¨ytta¨va¨t viitelaskureita olion elinkaaren ma¨a¨- ra¨a¨miseen. Niiden idea on, etta¨ a¨lykka¨a¨t osoittimet pita¨va¨t ylla¨ las- kuria siita¨, kuinka monta osoitinta olioon osoittaa. Siina¨ vaihees- sa, kun viimeinen a¨lyka¨s osoitin tuhoutuu tai lakkaa muuten osoit- tamasta olioon, se tuhoaa olion. A¨lykka¨a¨t osoittimet eiva¨t kuu- lu ISOC++ -kieleen, mutta verkosta saa useita toimivia toteutuksia. 11.8. Olio-ohjelmointi ja poikkeusturvallisuus 388 Ta¨llaisia ovat esim. Boost-kirjaston shared ptr [Boost, 2003] tai Andrei Alexandrescun Loki-kirjaston uskomattoman monipuolinen SmartPtr [Alexandrescu, 2001] [Alexandrescu, 2003]. 11.8 Olio-ohjelmointi ja poikkeusturvallisuus Ta¨ha¨n mennessa¨ ta¨ssa¨ luvussa on ka¨sitelty C++:n poikkeusmekanis- min perusteet. Vaikka itse mekanismi ei olekaan kovin monimutkai- nen, on vikasietoisen ja poikkeuksiin varautuvan ohjelman kirjoitta- minen kuitenkin yleensa¨ eritta¨in monimutkaista ja tarkkuutta vaati- vaa tyo¨ta¨. Syyna¨ ta¨ha¨n on, etta¨ virhetilanteita – ja na¨in ollen myo¨s poikkeuksia – voi tapahtua la¨hes missa¨ tahansa kohdassa ohjelmaa. Olio-ohjelmoinnin kapselointi piilottaa luokkien sisa¨isen toteu- tuksen, joten luokan ka¨ytta¨ja¨ ei voi na¨hda¨, miten luokka on toteutettu ja millaisia virheita¨ koodissa voi syntya¨. Kapselointi tekee myo¨s mah- dolliseksi sen, etta¨ luokan toteutusta muutetaan myo¨hemmin, jolloin uuden toteutuksen reagointi virheisiin voi poiketa aiemmasta. Kai- ken kukkuraksi periytyminen ja polymorﬁsmi aiheuttavat sen, ettei luokkahierarkian ka¨ytta¨ja¨ edes va¨ltta¨ma¨tta¨ tarkasti tieda¨, minka¨ luo- kan oliota ka¨ytta¨a¨ (kun olio on kantaluokkaosoittimen pa¨a¨ssa¨). Kaikki ta¨ma¨ tekee entista¨ ta¨rkea¨mma¨ksi sen, etta¨ kaikki mahdolli- set luokasta tai moduulista ulos vuotavat virhetilanteet ja poikkeuk- set dokumentoidaan rajapinnan dokumentaatiossa. Na¨in luokan ka¨yt- ta¨ja¨ voi varautua kaikkiin tarpeellisiin poikkeustilanteisiin ilman, et- ta¨ ha¨n tieta¨a¨ luokan tai moduulin sisa¨ista¨ toteutusta. Samoin periytymishierarkiassa on ta¨rkea¨a¨, etta¨ aliluokan uudel- leen ma¨a¨rittelema¨t virtuaalifunktiot eiva¨t aiheuta sellaisia virhetilan- teita ja poikkeuksia, joita ei ole dokumentoitu jo kantaluokan rajapin- tadokumentaatiossa. Luokan rajapinnasta vuotavat poikkeustilanteet ja luokan reagoiminen niihin kuuluvat suoraan periytymisen “aliluo- kan olio on myo¨s kantaluokan olio” -suhteeseen, joten aliluokan tu- lee noudattaa kantaluokan ka¨ytta¨ytymista¨ myo¨s poikkeustilanteissa. Toisaalta ta¨ma¨ tarkoittaa myo¨s sita¨, etta¨ kantaluokka ei saa omassa ra- japintadokumentaatiossaan tarjota liian suuria lupauksia poikkeusti- lanteissa, koska ta¨llo¨in saattaa pahimmassa tapauksessa ka¨yda¨ niin, etta¨ on mahdotonta kirjoittaa aliluokkaa, jonka laajennettu ja muutet- tu toiminnallisuus edelleen pita¨isi kaikista kantaluokan lupauksista kiinni. 11.8. Olio-ohjelmointi ja poikkeusturvallisuus 389 Virhetilanteissa ja¨rkeva¨sti toimivien ja vikasietoisten luokkien kir- joittaminen tulee helpommaksi, jos ensin ma¨a¨ritella¨a¨n joukko peli- sa¨a¨nto¨ja¨, joita olioiden tulee virhetilanteissa noudattaa. Lisa¨ksi olioi- den ka¨ytta¨ytyminen virhetilanteissa voidaan jakaa selkeisiin katego- rioihin, jolloin rajapintadokumentaation kirjoittaminen ja ymma¨rta¨- minen tulee helpommaksi. Ta¨ssa¨ aliluvussa esitella¨a¨n C++:ssa usein ka¨ytetta¨va¨t poikkeusturvallisuuden tasot seka¨ muutamia yleisia¨ poik- keuksiin ja C++:n luokkiin liittyvia¨ mekanismeja. 11.8.1 Poikkeustakuut C++:n poikkeuksista ja niiden ka¨yto¨sta¨ on jo ehtinyt kertya¨ ka¨yta¨nno¨n kokemusta, vaikka poikkeusmekanismi tulikin kieleen varsin myo¨- ha¨isessa¨ vaiheessa. Lisa¨ksi vikasietoisuudesta on tietysti paljon tieta¨- mysta¨ myo¨s ajalta ennen C++:aa. Mista¨a¨n loppuunkalutusta aiheesta ei kuitenkaan ole kysymys, vaan uusia poikkeuksiin liittyvia¨ mekanis- meja ja koodaustapoja kehiteta¨a¨n jatkuvasti. Periaatteessa luokan pita¨isi erikseen dokumentoida jokaisesta pal- velustaan, mita¨ virhetilanteita palvelussa voi sattua, ja miten palve- lu reagoi niihin. Ta¨ma¨ ei kuitenkaan ole aina ja¨rkeva¨a¨, koska ta¨llo¨in rajapintadokumentti saattaa helposti kasvaa niin suureksi, etta¨ sen ka¨ytto¨kelpoisuus vaarantuu. Lisa¨ksi luokka ei aina voi edes tarkasti ma¨a¨ritella¨ kaikia mahdollisia virhetilanteita. Ta¨ta¨ esiintyy sita¨ enem- ma¨n, mita¨ geneerisempi ja yleiska¨ytto¨isempi luokka on. Esimerkiksi C++:n vector ei voi milla¨a¨n luetella push back-operaa- tiosta ulos vuotavia poikkeuksia. Kyseinen operaatiohan aiheuttaa uuden alkion luomisen ja lisa¨a¨misen vektoriin, ja vektorilla ei ole mi- ta¨a¨n ka¨sitysta¨ siita¨, millaisia virhetilanteita uuden alkion luomiseen voi liittya¨. Ta¨llaisten ongelmien ratkaisemiseksi voidaan antaa yksinkertai- nen jaottelu siita¨, miten luokka voi tyypillisesti virhetilanteisiin suh- tautua. Ta¨ssa¨ esitella¨a¨n ta¨ma¨n jaottelun perusteet, tarkemmin asiaan voi tutustua esim. kirjoista “Exceptional C++” [Sutter, 2000] ja “More Exceptional C++” [Sutter, 2002c]. Alla olevan jaottelun poikkeuksiin suhtautumisesta on ilmeisesti ensimma¨isena¨ julkaissut David Abrahams. Ha¨nen mukaansa luokka voi tarjota operaatioilleen erilaisia poikkeustakuita (exception guar- antee). [Abrahams, 2003] 11.8. Olio-ohjelmointi ja poikkeusturvallisuus 390 Minimitakuu (minimal guarantee) Va¨hin, mita¨ luokka voi tehda¨, on taata, etta¨ mika¨li olion palvelu kes- keytyy virhetilanteen vuoksi (ja poikkeus vuotaa ulos), niin olio ei hukkaa resursseja ja on edelleen sellaisessa tilassa, etta¨ sen voi tuho- ta. Ta¨ma¨ tarkoittaa, etta¨ virhetilanteenkaan sattuessa olio ei aiheuta muistivuotoja eika¨ muitakaan resurssivuotoja. Olion ei tarvitse sisa¨i- sesti olla “ja¨rkeva¨ssa¨” tilassa (luokkainvariantin ei tarvitse olla voi- massa), mutta sen purkajan tulee pystyy hoitamaan tarvittavat sii- voustoimenpiteet. Minimitakuu takaa siis vain, etta¨ olion voi tuhota ilman ongelmia. Mahdollisesti lisa¨ksi olion “resetoiminen” sopivalla ja¨senfunktiolla voi olla mahdollista, tai uuden arvon sijoittaminen olioon. On varsin selva¨a¨, etta¨ minimitakuuta lo¨yhempa¨a¨ lupausta ei ole ka¨yta¨nno¨llista¨ antaa. Jos oliota ei voi edes turvallisesti tuhota virheen ja¨lkeen, ja se voi vuotaa muistia ja resursseja, ei olion ka¨ytta¨mista¨ saa turvalliseksi milla¨a¨n keinoin. Perustakuu (basic guarantee) Perustakuu takaa kaiken minka¨ minimitakuukin, mutta lisa¨ksi se ta- kaa, etta¨ olion luokkainvarianttia ei ole rikottu. Olio on siis poik- keuksen ja¨lkeenkin ka¨ytto¨kelpoisessa tilassa, ja sen ja¨senfunktioita voi kutsua. On kuitenkin huomattava, ettei perustakuu tarkoita sita¨, etta¨ olion ta¨ytyisi olla ennustettavassa tilassa virhetilanteen ja¨lkeen. Perustakuu takaa vain, etta¨ olio ei ole mennyt rikki poikkeuksen joh- dosta. Olion tila voi olla ennallaan, puoliva¨lissa¨ kohti onnistunutta suoritusta tai olio voi olla muuttunut johonkin aivan toiseen lailli- seen tilaan. Jos esimerkiksi vektorin insert-operaatiolla vektoriin lisa¨ta¨a¨n useita alkioita, ja operaation aikana tapahtuu virhe (esim. alkion ko- pioiminen vuotaa poikkeuksen), ei vektorin alkioista ena¨a¨ ole var- muutta. Saattaa olla, etta¨ osa alkioista ja¨a¨ va¨a¨ra¨a¨n paikkaan vekto- rissa tai jotkin alkiot saattavat jopa olla vektorissa kahteen kertaan tai puuttua kokonaan. Siita¨ huolimatta tiedeta¨a¨n, etta¨ vektorin voi edelleen tuhota normaalisti, sen voi tyhjenta¨a¨, ja sen alkiot voi edel- leen ka¨yda¨ la¨pi, vaikka alkioiden arvoista ei olekaan varmuutta. C++:n standardikirjaston luokat antavat perustakuun la¨hes kaikista operaa- 11.8. Olio-ohjelmointi ja poikkeusturvallisuus 391 tioistaan. Joistain operaatioista luvataan lisa¨ksi viela¨ enemma¨n kuin perustakuu vaatii. Vahva takuu (strong guarantee) Vahva takuu takaa, etta¨ kun luokan oliolle suoritetaan jokin operaa- tio, niin operaatio saadaan joko suoritettua loppuun ilman virheita¨, tai poikkeuksen sattuessa olion tila pysyy alkupera¨isena¨. Ta¨ma¨ tar- koittaa sita¨, etta¨ jos operaatiossa tapahtuu virhe, niin poikkeuksen vuotamisen ja¨lkeen olio on ta¨sma¨lleen samassa tilassa kuin ennen ko- ko operaatiotakin. Ta¨sta¨ ka¨yteta¨a¨n myo¨s usein englanninkielista¨ ter- mia¨ “commit or rollback” – operaatio saa joko toimintansa loppuun tai “kiera¨hta¨a¨ takaisin” tilaansa ennen operaatiota. Vahvan takuun antavat operaatiot ovat luokan ka¨ytta¨ja¨n kannal- ta varsin helppoja, koska virheen sattuessa olion tila on sa¨ilynyt en- nallaan. Sen sijaan luokan toteuttajalle vahva takuu tuottaa yleen- sa¨ jonkin verran lisa¨vaivaa. Vahvan takuun voi toteuttaa esimerkiksi niin, etta¨ operaation yhteydessa¨ olion tilan tarvittavat osat kopioi- daan muualle, ja itse operaatio suoritetaankin ta¨lle kopiolle. Jos ope- raatio onnistui, voidaan lopputulos sitten siirta¨a¨ takaisin olioon. Vir- heen sattuessa varsinaista oliota ei taas olekaan viela¨ muutettu, joten operaatio voi yksinkertaisesti tuhota tyo¨kopion ja heitta¨a¨ poikkeuk- sen. Aliluvussa 11.9 on esimerkki vahvan takuu tarjoavasta sijoitus- operaattorista. Vahvan takuun toteuttaminen saattaa usein vaatia luokan koodaa- mista siten, etta¨ se kuluttaa hieman enemma¨n muistia ja toimii hie- man hitaammin kuin ilman takuuta. Sen vuoksi vahva takuu ei ole- kaan mika¨a¨n “ihanne”, johon tulisi aina pyrkia¨, mutta joissain tilan- teissa se tekee luokan ka¨ytta¨ja¨n ela¨ma¨n paljon helpommaksi. Aivan kaikkia operaatioita ei lisa¨ksi edes voi kirjoittaa niin, etta¨ ne tarjoai- sivat vahvan takuun. C++:n standardikirjasto pyrkii tarjoamaan vahvan poikkeustakuun sellaisille operaatioille, joissa takuu on mahdollista toteuttaa ja¨rke- va¨lla¨ vaivalla ja joissa vahvasta takuusta on ka¨ytta¨ja¨lle eniten hyo¨tya¨. Esimerkiksi kaikkien STL:n sa¨ilio¨iden push back-operaatiot antavat vahvan takuun – jos alkion lisa¨a¨minen epa¨onnistuu, sisa¨lta¨a¨ sa¨ilio¨ poikkeuksen vuotaessa samat alkiot kuin ennen operaatiota. 11.8. Olio-ohjelmointi ja poikkeusturvallisuus 392 Nothrow-takuu (nothrow guarantee) Kaikkein vahvin poikkeustakuu “nothrow” on varsin yksinkertainen. Se takaa, etta¨ operaation suorituksessa ei voi sattua virheita¨. Mita¨a¨n poikkeuksia ei siis voi vuotaa operaatiosta ulos, ja operaatio onnistuu aina. Nothrow-takuu on ka¨ytta¨ja¨n kannalta ideaalinen, koska virhei- siin ei tarvitse varautua lainkaan. Sen sijaan on tietysti selva¨a¨, etta¨ suuri osa operaatioista ei milla¨a¨n voi tarjota nothrow-takuuta, koska niihin sisa¨ltyy aina jokin virhemahdollisuus. Nothrow-takuu on kuitenkin hyo¨dyllinen virheturvallisen ohjel- moinnin kannalta. Joissain tapauksissa nimitta¨in ohjelmaa ei voi kir- joittaa virheturvalliseksi, elleiva¨t jotkin tietyt operaatiot tarjoa no- throw-takuuta. Esimerkiksi vahvan takuun tarjoaminen vaatii, et- ta¨ onnistuneen operaation lopussa tyo¨kopion kopioiminen takai- sin itse olioon ei voi epa¨onnistua – kopioimisen ta¨ytyy siis tarjota nothrow-takuu. Samoin aliluvussa 11.7 ka¨sitelty automaattiosoitin auto ptr tarjoaa nothrow-takuun suurimmalle osalle operaatioistaan. Lisa¨ksi nothrow-takuun antavat STL:n sa¨ilio¨iden erase, pop back ja pop front. C++:n poikkeusmekanismin kannalta nothrow-takuu vastaa poik- keusma¨a¨reen throw() ka¨ytto¨a¨. Kyseisen poikkeusma¨a¨reen ka¨ytta¨mi- nen ei kuitenkaan ole mitenka¨a¨n va¨ltta¨ma¨to¨nta¨, vaan nothrow-ta- kuun voi tarjota myo¨s perinteisesti dokumentoimalla. Poikkeusneutraalius (exception neutrality) Poikkeusneutraalius ei ole vaihtoehtoinen poikkeustakuu perusta- kuun, vahvan takuun ja nothrow-takuun rinnalla, mutta se liittyy kui- tenkin olennaisesti samaan aiheeseen “toisella akselilla”. Poikkeus- neutraaliutta on, etta¨ yleiska¨ytto¨isen luokan operaatiot vuotavat sisa¨l- la¨a¨n olevien komponenttien poikkeukset ulos muuttumattomina. Ta¨- ma¨ tarkoittaa la¨hinna¨ sita¨, etta¨ luokka ei itse muuta poikkeuksia jon- kin toisen tyyppisiksi, vaan pa¨a¨sta¨a¨ alkupera¨isen poikkeuksen ka¨ytta¨- ja¨lle saakka. Poikkeusneutraaliuden lisa¨ksi luokan pita¨isi tietysti tar- jota myo¨s jokin muista poikkeustakuista, la¨hinna¨ joko perustakuu tai vahva takuu. Siten luokka voi tietysti ottaa poikkeuksen va¨liaikaises- ti kiinni ja reagoida siihen (esim. toteuttaakseen vahvan poikkeusta- kuun). 11.8. Olio-ohjelmointi ja poikkeusturvallisuus 393 Hyva¨ esimerkki poikkeusneutraaliudesta on vector. Vekto- rin poikkeusneutraalius tarkoittaa, etta¨ jos esimerkiksi vektorin push back-operaation yhteydessa¨ lisa¨tta¨va¨n alkion kopioiminen ai- heuttaa poikkeuksen (eli alkiotyypin kopiorakentaja vuotaa poik- keuksen), niin vektori tarvittavan siivouskoodin suorittamisen ja¨l- keen vuotaa ta¨ma¨n saman poikkeuksen edelleen ulos ka¨ytta¨ja¨lleen. (Siis ka¨yta¨nno¨ssa¨ ta¨ma¨ tapahtuu suorittamalla siivouskoodin lopussa komento throw;.) Poikkeusneutraalius on vektorin tapaisten yleiska¨ytto¨isten luok- kamallien tapauksessa varsin toivottavaa. Vektorin koodihan ei voi tieta¨a¨ mita¨a¨n vektorin alkioiden ka¨ytta¨ytymisesta¨ ja niissa¨ mahdolli- sesti sattuvista virhetilanteista, koska vektorin koodi on ta¨ysin alkio- tyypista¨ riippumatonta. Sen sijaan vektorin ka¨ytta¨ja¨lla¨ on tavallisesti tarkka tieto siita¨, miten vektorin alkioissa tapahtuviin virheisiin tuli- si reagoida. Ta¨ma¨n vuoksi on ta¨rkea¨a¨, etta¨ vektori va¨litta¨a¨ alkioiden poikkeukset ka¨ytta¨ja¨lleen saakka, jotta poikkeus saadaan ka¨siteltya¨ asianmukaisesti. 11.8.2 Poikkeukset ja rakentajat Olion luominen on sen elinkaaren kannalta erikoinen tapahtuma, koska luomisen onnistumisesta riippuu, onko koko olio olemassa vai ei. Ta¨ma¨n vuoksi luomiseen liittyy poikkeuksien ja virhetilanteiden kannalta joitain hieman erikoisia piirteita¨, jotka on syyta¨ ka¨yda¨ la¨pi. Olion luomisen vaiheet Yksi oleellinen kysymys on, milloin olio varsinaisesti syntyy, eli mil- loin alustustoimet ovat niin pitka¨lla¨, etta¨ voidaan puhua jo “uudesta oliosta”. C++:n oliomalli ma¨a¨rittelee ta¨ma¨n niin, etta¨ olion katsotaan lopullisesti syntyneen, kun kaikki olion luomiseen kuuluvat rakenta- jat on suoritettu onnistuneesti loppuun. Ta¨ha¨n kuuluvat niin kanta- luokkien rakentajat kuin myo¨s olion ja¨senmuuttujien rakentajat, jos ja¨senmuuttujat ovat olioita. Oleelliseksi ta¨ma¨ “syntymishetki” tulee silloin, kun olion luomi- sen aikana tapahtuu virhe. Jos nimitta¨in olion luomisen jossain osa- vaiheessa tapahtuu poikkeus, jonka kyseinen osa pa¨a¨sta¨a¨ vuotamaan ulos, ei oliota ole saatu luotua onnistuneesti. Osa siita¨ on kuitenkin 11.8. Olio-ohjelmointi ja poikkeusturvallisuus 394 todenna¨ko¨isesti saatu luotua loppuun saakka, kuten esimerkiksi jot- kin ja¨senmuuttujat ja kenties osa olion kantaluokkaosista. Jos virhe on kuitenkin tapahtunut ennen kuin kaikki olioon liitty- va¨t rakentajat on saatu suoritettua, ei oliota ole C++:n kannalta saatu luoduksi. Ta¨llo¨in C++ pita¨a¨ automaattisesti huolen siita¨, etta¨ kaikki ne olion osat, jotka on jo ehditty luoda, tuhotaan automaattisesti osana poikkeuska¨sittelya¨. Ta¨ha¨n kuuluu niin tarvittavien purkajien kutsu- minen kuin muistin vapauttaminenkin. Poikkeuksen tekeva¨t tuttuun tapaan dynaamisesti luodut oliot, joita ei ta¨ssa¨ka¨a¨n tapauksessa tuho- ta automaattisesti. Lopputulos olion luomisen kannalta joka tapauk- sessa on, etta¨ olio ei koskaan ehtinyt syntya¨. Listaus 11.10 na¨ytta¨a¨ esimerkin oliosta, jossa on ja¨senmuuttujina useita toisia olioita. Jos nyt henkilo¨oliota luotaessa syntyma¨pa¨iva¨n luominen onnistuu, mutta nimen luomisessa tapahtuu virhe (esim. muisti loppuu), niin henkilo¨olion luominen keskeyteta¨a¨n. Lisa¨ksi koska syntyma¨pa¨iva¨ on jo saatu luotua, niin se tuhotaan automaat- tisesti. Dynaamisesti luodut osaoliot Koska ja¨senmuuttujien ja muiden osaolioiden tuhoaminen poikkeuk- sen sattuessa tapahtuu automaattisesti, ei siita¨ yleensa¨ aiheudu vai- vaa ohjelmoijalle. Sen sijaan dynaamisesti luotujen olioiden kanssa tulee olla eritta¨in huolellinen. Niita¨ ei tuhota automaattisesti, joten on eritta¨in ta¨rkea¨a¨, etta¨ olioita ei luoda dynaamisesti rakentajan alus- 1 class Henkilo 2 { 3 public: 4 Henkilo(int p, int k, int v, std::string const& nimi, 5 std::string const& hetu); 6 ~Henkilo(); ... 7 private: 8 Paivays syntymapvm ; 9 std::string nimi ; 10 std::string* hetup ; 11 }; LISTAUS 11.10: Esimerkki luokasta, jossa on useita osaolioita 11.8. Olio-ohjelmointi ja poikkeusturvallisuus 395 tuslistassa. Jos na¨in nimitta¨in tehda¨a¨n, dynaamisesti luodut oliot ja¨a¨- va¨t tuhoamatta, jos jonkin niiden ja¨lkeen alustettavan ja¨senmuuttu- jan luominen epa¨onnistuu. Ta¨llaiseen virheeseen ei voi reagoida edes rakentajan rungon koodissa olevalla virheka¨sittelija¨lla¨, koska raken- tajan koodiin siirryta¨a¨n vasta, kun kaikki ja¨senmuuttujat on onnistu- neesti luotu. Ja¨senmuuttujan luomisvirheessa¨ rakentajan runkoon ei siis pa¨a¨sta¨ ollenkaan. Ta¨ma¨n vuoksi kannattaa noudattaa periaatetta, jossa ja¨senmuut- tujaosoittimet alustetaan rakentajan alustuslistassa nolliksi ja oliot luodaan dynaamisesti niiden pa¨a¨ha¨n vasta rakentajan rungossa. Na¨in dynaamisesti luotavat oliot luodaan vasta, kun kaikki normaalit ja¨- senmuuttujat on saatu onnistuneesti luotua. Jos rakentajan rungos- sa luodaan dynaamisesti useita olioita, ta¨ytyy koodissa olla tietys- ti tarvittava virheenka¨sittely, joka varmistaa etta¨ virheen sattuessa jo luodut oliot tuhotaan. Listaus 11.11 na¨ytta¨a¨ esimerkkina¨ listauk- sen 11.10 rakentajan toteutuksen. Automaattiosoittimia ka¨ytetta¨essa¨ ta¨ta¨ ongelmaa ei pa¨a¨se synty- ma¨a¨n, koska virheenkin sattuessa automaattiosoittimen purkaja tu- hoaa dynaamisesti luodun olion. Ta¨ma¨n vuoksi automaattiosoittimen pa¨a¨ha¨n voi alustaa dynaamisesti luodun olion jo alustuslistassa. 1 Henkilo::Henkilo(int p, int k, int v, std::string const& nimi, 2 std::string const& hetu) 4 : syntymapvm (p, k, v), nimi (nimi), hetup (0) 5 { 6 try 7 { 8 hetup = new std::string(hetu); 9 } 10 catch (. . .) 11 { // Ta¨nne pa¨a¨sta¨a¨n, jos hetun luominen epa¨onnistuu 12 // Siivotaan tarvittaessa, olion luominen epa¨onnistui 13 throw; // Heiteta¨a¨n virhe edelleen ka¨sitelta¨va¨ksi 14 } 15 } LISTAUS 11.11: Olioiden dynaaminen luominen rakentajassa 11.8. Olio-ohjelmointi ja poikkeusturvallisuus 396 Luomisvirheisiin reagoiminen Jos olion luomisen yhteydessa¨ tapahtuu virhe esimerkiksi ja¨senmuut- tujaa luotaessa, ei luotava olio voi milla¨a¨n toipua ta¨sta¨ virheesta¨, vaan koko olion luominen epa¨onnistuu. Ta¨llo¨in jo luotujen osaolioiden purkajia kutsutaan ja poikkeus vuotaa koodiin, jossa olio luotiin. Jos luokassa kuitenkin on tarve virheen sattuessa yritta¨a¨ toipua virhees- ta¨ ja kenties yritta¨a¨ epa¨onnistuneen osaolion luomista uudelleen, on ta¨ha¨n yksi keino. Sellaiset osaoliot, joiden luominen voi epa¨onnis- tua, voi nimitta¨in luoda tavallisen ja¨senmuuttujan sijaan dynaami- sesti osoittimen pa¨a¨ha¨n. Ta¨llo¨in osaolion luomisen new’lla¨ voi tehda¨ rakentajan rungossa, jonka virheenka¨sittelykoodi sitten yritta¨a¨ luo- mista tarvittaessa uudelleen. Vaikka tavallisten ja¨semuuttujien ja kantaluokkaosien luomisvir- heista¨ ei voikaan toipua, saattaa luokalla olla kuitenkin tarve reagoida ta¨llaisiin virheisiin. Kenties luokan tulee kirjoittaa tieto epa¨onnistu- misesta virhelokiin tai antaa ka¨ytta¨ja¨lle virheilmoitus. Joskus osao- lion luomisesta aiheutunut poikkeus voi myo¨s olla va¨a¨ra¨a¨ tyyppia¨, ja luokka haluaisi itse vuotaa ulos toisentyyppisen poikkeuksen. Standardoinnin myo¨ta¨ C++:aan lisa¨ttiin na¨ita¨ tarpeita varten erityi- nen funktion valvontalohko (function try block). Sita¨ voi ka¨ytta¨a¨ ra- kentajissa (ja purkajissa), ja sen virheka¨sittelija¨t ottavat kiinni poik- keukset, jotka tapahtuvat ja¨senmuuttujien tai kantaluokkaosien luo- misen (purkajan tapauksessa tuhoamisen) yhteydessa¨. Listaus 11.12 seuraavalla sivulla na¨ytta¨a¨ ta¨ma¨n valvontalohkon syntaksin. Avain- sana try tulee jo ennen rakentajan alustuslistaa, eika¨ sen ja¨lkeen tule aaltosulkuja. Valvontalohko kattaa automaattisesti virheet, jotka syn- tyva¨t osaolioita luotaessa tai itse rakentajan rungossa. Valvontaloh- koon liittyva¨t virheka¨sittelija¨t tulevat aivan rakentajan loppuun sen rungon ja¨lkeen. Funktion valvontalohkosta on huomattava, etta¨ sen virheka¨sitteli- jo¨ihin pa¨a¨sta¨essa¨ olion ennen virhetta¨ luodut ja¨senmuuttujat ja kan- taluokkaosat on jo tuhottu. Virheka¨sittelijo¨issa¨ ei siis ena¨a¨ voi viita- ta ja¨senmuuttujiin tai kantaluokan palveluihin eika¨ na¨in ollen voi suorittaa mita¨a¨n varsinaisia siivousoperaatioita (kuten dynaamisen muistin vapauttamista). Samoin virheka¨sittelija¨ ei voi ka¨sitella¨ vir- hetta¨ loppuun ja na¨in toipua siita¨, vaan jokaisen virheka¨sittelija¨n on lopuksi joko heitetta¨va¨ sama virhe uudelleen (komennolla throw;) tai sitten heitetta¨va¨ kokonaan uusi erityyppinen virhe. Jos virheka¨sitteli- 11.8. Olio-ohjelmointi ja poikkeusturvallisuus 397 1 Henkilo::Henkilo(int p, int k, int v, std::string const& nimi, 2 std::string const& hetu) 3 try // Huomaa try-sanan paikka! 4 : syntymapvm (p, k, v), nimi (nimi), hetup (0) 5 { ... 15 } 16 catch (. . .) 17 { // Ta¨nne pa¨a¨sta¨a¨n, jos ja¨senmuuttujien (tai kantaluokan) 18 // luominen epa¨onnistuu. Tehda¨a¨n tarvittaessa toimenpiteita¨. 19 throw; // Tai heiteta¨a¨n jokin toinen poikkeus 20 } LISTAUS 11.12: Funktion valvontalohko rakentajassa ja¨ ei tee na¨ista¨ kumpaakaan, heiteta¨a¨n alkupera¨inen virhe automaat- tisesti uudelleen. 11.8.3 Poikkeukset ja purkajat Kuten aliluvussa 11.5 todettiin, poikkeuksen sattuessa kutsutaan au- tomaattisesti kaikkien sellaisten olioiden purkajia, joiden elinkaari loppuu virheka¨sittelija¨n etsimisen yhteydessa¨. Na¨in olion purkajaa saatetaan kutsua sellaisessa tilanteessa, jossa vireilla¨ on jo yksi poik- keustilanne. Jos purkaja viela¨ vuotaa ulos toisen poikkeuksen, ei C++:n poikkeusmekanismi pysty selviytyma¨a¨n tilanteesta, vaan ohjelman suoritus keskeyteta¨a¨n kutsumalla funktiota terminate. Ta¨ma¨n vuoksi on eritta¨in ta¨rkea¨a¨, etta¨ olioiden purkajista ei vuo- da poikkeuksia ulos. Yleensa¨ ta¨ma¨ ei ole ongelma, koska suurin osa siivoustoimenpiteista¨ — kuten esimerkiksi muistin vapauttaminen — on luonteeltaan sellaisia, etta¨ ne eiva¨t voi epa¨onnistua. Mika¨li purka- jissa joudutaan kuitenkin tekema¨a¨n toimenpiteita¨, jotka voivat epa¨on- nistua, niissa¨ mahdollisesti tapahtuvat poikkeukset tulisi ottaa kiinni ja ka¨sitella¨ jo purkajassa itsessa¨a¨n. Toinen mahdollisuus on kirjoittaa luokalle erillinen siivoa-ja¨sen- funktio, joka suorittaa kaikki sellaiset siivoustoimenpiteet, jotka voi- vat epa¨onnistua. Luokan ka¨ytta¨jien tulee sitten kutsua ta¨ta¨ ja¨senfunk- tiota aina ennen olion tuhoamista, jolloin mahdolliset virheet voi- daan ottaa kiinni. Ta¨llainen erillinen siivousfunktio on kuitenkin ko¨mpelo¨, eika¨ se edelleenka¨a¨n ratkaise kysymysta¨ siita¨, mita¨ pita¨isi 11.8. Olio-ohjelmointi ja poikkeusturvallisuus 398 tehda¨, jos yhden virhetilanteen jo vireilla¨ ollessa kutsutaan siivous- funktiota ja havaitaan toinen virhe. Mainittakoon, etta¨ Java-kielessa¨ ta¨llaiset erilliset siivousfunktiot ovat C++:aa tavallisempia, koska kie- lessa¨ ei ole C++:n purkajaa vastaavaa mekanismia. On huomattava, etteiva¨t edella¨ olleet asiat tarkoita sita¨, etteiko¨ luokan purkajassa saisi sattua poikkeusta. Jos na¨in tapahtuu, ta¨ytyy purkajan vain itse kyeta¨ sieppaamaan syntynyt poikkeus ja toipu- maan siita¨. Oleellista on, ettei poikkeus vuoda purkajasta ulos. Pe- riaatteessa siis kaikki purkajat tulisi kirjoittaa niin, etta¨ niille voisi antaa poikkeusma¨a¨reen throw(). Se, kirjoitetaanko tuo poikkeusma¨a¨- re todella na¨kyville purkajaan, on sitten makuasia. Kun oliota tuhotaan, myo¨s sen kaikkien ja¨senmuuttujien ja kanta- luokkaosien purkajia kutsutaan. Periaatteessa C++ antaa mahdollisuu- den ottaa kiinni na¨issa¨ osaolioiden purkajissa sattuvat poikkeukset itse “emo-olion” purkajassa. Ta¨ha¨n tarkoitukseen purkajaan voi kir- joittaa samanlaisen funktion valvontalohkon kuin rakentajaan alilu- vussa 11.8.2 sivulla 396. Ta¨llo¨in ta¨ma¨n valvontalohkon virheka¨sitte- lija¨a¨n siirryta¨a¨n, jos itse purkajasta tai jonkin ja¨senmuuttujan tai kan- taluokkaosan purkajasta vuotaa poikkeus ulos. Ta¨lle mekanismille ei kuitenkaan ole ka¨yta¨nno¨ssa¨ mita¨a¨n ka¨ytto¨a¨, koska purkajista ei kos- kaan tulisi vuotaa poikkeuksia, joten normaali purkajan sisa¨lla¨ oleva virheka¨sittely on ka¨yta¨nno¨ssa¨ aina riitta¨va¨. Samoin C++ tarjoaa funktion uncaught exception, joka palauttaa ar- von true, jos ohjelmassa on vireilla¨ jokin poikkeus. Joskus ta¨ta¨ na¨kee ka¨ytetta¨va¨n siihen, etta¨ purkaja vuotaa poikkeuksen vain, jos vireilla¨ ei ole toista poikkeusta. Ta¨ma¨ka¨a¨n tapa ei ole suotava. Se ei nimit- ta¨in vastaa siihen kysymykseen, mita¨ tehda¨a¨n jos vireilla¨ tosiaan on toinen virhetilanne. Purkajassa tapahtuneen toisen virheen huomiot- ta ja¨tta¨minen tuskin on kovin turvallista, eika¨ ole kovin ja¨rkeva¨a¨, et- ta¨ purkaja toimii muutenkaan kahdella eri tavalla riippuen siita¨, on- ko muualla havaittu virheita¨. Yleinen mielipide onkin nykyisin, etta¨ uncaught exception-funktio on C++:ssa ka¨ytto¨kelvoton, eika¨ funktion valvontalohkojakaan pita¨isi ka¨ytta¨a¨ muualla kuin korkeintaan raken- tajissa. [Sutter, 2002c] 11.9. Esimerkki: poikkeusturvallinen sijoitus 399 11.9 Esimerkki: poikkeusturvallinen sijoitus Esimerkkina¨ poikkeusturvallisuuden huomioon ottamisesta ka¨sitel- la¨a¨n seuraavaksi listauksessa 11.13 esitetyn luokan Kirja sijoitusope- raattorin kirjoittamista. Listaus na¨ytta¨a¨ myo¨s yhden mahdollisen si- joitusoperaattorin toteutuksen. Ta¨ma¨ sijoitusoperaattori on kirjoitet- tu aliluvun 7.2 ohjeiden mukaan, joten ena¨a¨ ta¨ytyy miettia¨ sen poik- keusturvallisuutta. 11.9.1 Ensimma¨inen versio Ensimma¨inen vaihe on miettia¨, millaisia virhemahdollisuuksia sijoi- tuksessa voi sattua ja mita¨ niista¨ seuraa. Viitteiden ja osoittimien ka¨- sittelyissa¨ ei virheita¨ voi sattua (ne antavat nothrow-takuun). Sen si- jaan merkkijonojen sijoituksessa voi sattua virhe. Na¨in ja¨ljelle ja¨a¨va¨t seuraavat mahdolliset tapahtumasarjat: 1. Mita¨a¨n virheita¨ ei satu, sijoitus saadaan suoritettua onnistu- neesti. Ta¨ma¨ on tietysti toivottava lopputulos. 1 class Kirja 2 { 3 public: ... 4 Kirja& operator =(Kirja const& kirja); 5 private: 6 std::string nimi ; 7 std::string tekija ; 8 }; . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1 Kirja& Kirja::operator =(Kirja const& kirja) 2 { 3 if (this != &kirja) 4 { 5 nimi = kirja.nimi ; 6 tekija = kirja.tekija ; 7 } 8 return *this; 9 } LISTAUS 11.13: Yksinkertainen luokka, jolla on sijoitusoperaattori 11.9. Esimerkki: poikkeusturvallinen sijoitus 400 2. On mahdollista, etta¨ heti ensimma¨ista¨ merkkijonoa nimi sijoi- tettaessa tulee virhe. Ta¨llo¨in sijoitus keskeytyy ja poikkeus vuo- taa ulos sijoitusoperaattorista, jolloin toista merkkijonoa ei eh- dita¨ ka¨sitella¨ lainkaan. 3. Viimeinen mahdollisuus on, etta¨ ensimma¨inen sijoitus onnis- tuu, mutta toisessa tapahtuu virhe ja poikkeus vuotaa ulos si- joituksesta. Vaihtoehto 1 ei luonnollisesti vaadi miettimista¨ poikkeusturval- lisuuden kannalta. Sen sijaan tilanne vaihtoehdossa 2 riippuu sii- ta¨, millaisen poikkeustakuun string antaa omalle sijoitukselleen. C++- standardista (ja esim. teoksesta “The C++ Standard Library” [Josuttis, 1999]) ka¨y ilmi, etta¨ suurin osa C++:n vakiokirjaston luokista, string mukaanlukien, tarjoaa ka¨ytta¨ja¨lleen perustakuun. Ta¨ma¨ tarkoittaa siis sita¨, etta¨ virheen ja¨lkeen olio on jossain ka¨ytto¨kelpoisessa, muttei va¨ltta¨ma¨tta¨ ennustettavassa tilassa. Ka¨yta¨nno¨ssa¨ ta¨ma¨ tarkoittaa, etta¨ virheen sattuessa string sisa¨lta¨a¨ jonkin ja¨rkeva¨n merkkijonoarvon, mutta meilla¨ ei ole tietoa siita¨, mika¨ se on. Olion voi kuitenkin esim. tuhota onnistuneesti tai siihen voi yritta¨a¨ sijoittaa uuden arvon. Kirja-luokan kannalta ta¨ma¨ tarkoittaa, etta¨ vaihtoehdon 2 ja¨lkeen kirjaolio itse on myo¨s jossain ka¨ytto¨kelpoisessa muttei ennustettavas- sa tilassa. Vaikka kirjan tekija¨ on sa¨ilynytkin ennallaan, ei kirjan ni- mesta¨ voida sanoa mita¨a¨n! Jos sen sijaan string olisi tarjonnut vah- van poikkeustakuun ja luvannut, etta¨ virheen sattuessa sen arvo sa¨i- lyy muuttumattomana, olisi tilanne ollut toinen. Silloin voitaisiin ol- la varmoja, etta¨ vaihtoehdon 2 sattuessa myo¨s kirjaolion sisa¨lto¨ on sa¨ilynyt ennallaan, koska epa¨onnistuneen merkkijonosijoituksen li- sa¨ksi ei ehditty tehda¨ mita¨a¨n muuta. Vaihtoehto 3 on hieman ongelmallisempi. Siina¨ kirjan nimen si- joittaminen onnistuu, mutta tekija¨n sijoituksessa tulee virhe. Koska string tarjoaa ka¨ytta¨ja¨lleen perustakuun, on tuloksena tilanne, jossa kirjan nimi on muutettu, mutta merkkijono tekija on jossain ka¨yt- to¨kelpoisessa mutta ei ennustettavassa tilassa. Kirjan sijoitus on siis osaksi tapahtunut, osaksi epa¨onnistunut. Ta¨ssa¨ tapauksessa tilannetta ei muuttuisi, vaikka string olisikin tarjonnut vahvan poikkeustakuun. Lopputuloksena olisi ta¨llo¨in kir- ja, jonka nimi olisi muutettu mutta tekija¨ olisi alkupera¨inen. Kirjan tila ei siis olisi alkupera¨inen eika¨ myo¨ska¨a¨n haluttu lopullinen, jo- 11.9. Esimerkki: poikkeusturvallinen sijoitus 401 ten poikkeusturvallisuuden kannalta ta¨ssa¨kin tapauksessa kirjan tila olisi “ka¨ytto¨kelpoinen muttei ja¨rkeva¨”. Kun kaikki vaihtoehdot otetaan huomioon, saadaan tulokseksi, et- ta¨ luokan Kirja sijoitus voi tarjota ka¨ytta¨ja¨lleen perustakuun. Pahin mahdollinen tilanne on, etta¨ virheen sattuessa kirjan tila on tuntema- ton, mutta kirja on kylla¨ muuten ka¨ytto¨kelpoinen, ja sen voi esimer- kiksi tuhota tai siihen voi yritta¨a¨ sijoittaa uudelleen. Kuten ta¨sta¨ esimerkista¨ na¨kyy, kaikkien mahdollisten virhetilan- teiden analysoiminen on varsin mutkikasta jo na¨inkin yksinkertai- sessa luokassa. Tilanne muuttuu tietysti helposti eritta¨in hankalaksi, kun luokan rakenne monimutkaistuu. 11.9.2 Tavoitteena vahva takuu Kuten esimerkki na¨ytta¨a¨, perustakuun tarjoaminen on yleensa¨ koh- talaisen helppoa, jos ka¨yto¨ssa¨ on muita perustakuun tarjoavia ope- raatioita. Perustakuu ei kuitenkaan ole ka¨ytta¨ja¨n kannalta paras mah- dollinen, koska operaation epa¨onnistuessa ollaan hukattu olion alku- pera¨inen tila, mika¨ tekee esimerkiksi virheesta¨ toipumisen vaikeak- si. Ta¨ma¨n vuoksi onkin hyva¨ miettia¨, miten Kirja-luokan sijoituksen saisi tarjoamaan vahvan poikkeustakuun — virheen sattuessa kirjan tila olisi sama kuin ennen koko sijoitusta. Ensin kannattaa miettia¨, auttaisiko tilannetta, jos C++ tarjoaisi string-luokan, joka tarjoaisi vahvan poikkeustakuun sijoitukselle. Ta¨llo¨in a¨skeisen listan vaihtoehdot 1 ja 2 olisivat vahvan takuun mu- kaisia. Nimen sijoituksen epa¨onnistuminen sa¨ilytta¨a¨ nimen alkupe- ra¨isena¨, joten joko kirjan sijoitus onnistuu tai sitten se epa¨onnistuu ja kirjan tila sa¨ilyy alkupera¨isena¨. Sen sijaan vaihtoehto 3 tuottaa edel- leen ongelmia. Sen tapahtuessa kirjan nimi on saatu sijoitettua, mutta tekija¨n arvo ja¨a¨ ennalleen sijoituksen epa¨onnistuttua. Kirjan tila ta¨ssa¨ tapauksessa olisi “ka¨ytto¨kelpoinen mutta ei toi- vottu”, mika¨ ei poikkeusturvallisuuden kannalta eroa olennaises- ti perustakuun lupauksesta “ka¨ytto¨kelpoinen muttei ennustettava”. Kummassakin tapauksessa kirja on menetta¨nyt alkupera¨isen arvonsa, mutta ei ole saanut haluttua uutta arvoa. Siis kirja voisi ta¨lla¨ sijoi- tusoperaattorilla tarjota vain perustakuun, vaikka string tarjoaisikin vahvan takuun. Listaus 11.14 seuraavalla sivulla na¨ytta¨a¨ seuraavan version sijoi- tusoperaattorista. Siina¨ yriteta¨a¨n kierta¨a¨ a¨skeinen ongelma ottamal- 11.9. Esimerkki: poikkeusturvallinen sijoitus 402 la talteen kirjan alkupera¨inen nimi ja tekija¨. Jos jumpikumpi sijoitus epa¨onnistuu, palautetaan nimi ja tekija¨ ennalleen. Ta¨ssa¨ tapauksessa virhemahdollisuuksia tulee heti kaksi lisa¨a¨, koska uusien merkkijo- nomuuttujien luominen voi epa¨onnistua. Na¨issa¨ tapauksissa kirjan tilaan ei kuitenkaan ole ehditty koskea, joten na¨ma¨ virheet eiva¨t ole ristiriidassa vahvan takuun kanssa. Ika¨va¨ kylla¨, ta¨ma¨ yritelma¨ ei toimi. Mika¨a¨n ei nimitta¨in takaa, etta¨ virheen ja¨lkeen alkupera¨isten arvojen palauttaminen onnistuu ilman virheita¨! Arvojen palauttaminen virheka¨sittelija¨ssa¨ riveilla¨ 14–15 on samanlainen merkkijonojen sijoitus kuin muutkin, ja se voi epa¨on- nistua, jolloin alkupera¨isia¨ arvoja ei saadakaan palautettua niin kuin piti. Lopputuloksena on, etta¨ Kirja-luokan sijoitus voi edelleen tarjo- ta vain perustakuun, koska virheen ja¨lkeen on mahdollista, etta¨ kir- jan tila on ka¨ytto¨kelpoinen muttei ennustettava. Ta¨llainen tilanne on ka¨yta¨nno¨ssa¨ va¨ista¨ma¨to¨n, jos operaatio on suoritettava useassa osas- sa, ja virhe voi syntya¨ niin, etta¨ vain jotkin osista saadaan suoritettua. 1 Kirja& Kirja::operator =(Kirja const& kirja) 2 { 3 if (this != &kirja) 4 { 5 std::string vanhanimi(nimi ); 6 std::string vanhatekija(tekija ); 7 try 8 { 9 nimi = kirja.nimi ; 10 tekija = kirja.tekija ; 11 } 12 catch (. . .) 13 { 14 nimi = vanhanimi; 15 tekija = vanhatekija; 16 throw; 17 } 18 } 19 return *this; 20 } LISTAUS 11.14: Sijoitus, joka pyrkii tarjoamaan vahvan takuun (ei toimi) 11.9. Esimerkki: poikkeusturvallinen sijoitus 403 11.9.3 Lisa¨ta¨a¨n epa¨suoruutta Ohjelmoinnin vanha sananlasku sanoo, etta¨ la¨hes minka¨ tahansa ongelman voi ratkaista lisa¨a¨ma¨lla¨ ohjelmaan epa¨suoruutta (yleensa¨ osoittimia). Ta¨ma¨ viisaus pa¨tee ta¨ssa¨kin tapauksessa. Edellisen yri- tyksen ongelmaksi muodostui, etta¨ sijoituksen epa¨onnistuessa van- hoja arvoja ei saatu palautettua. Yksi ratkaisukeino on havaita, etta¨ vaikka merkkijonon sijoitta- minen voi epa¨onnistua, niin merkkijono-osoittimen sijoittaminen on- nistuu aina. Kirjan nimi ja tekija¨ laitetaankin dynaamisesti luotuina osoittimien pa¨a¨ha¨n. Ta¨llo¨in sijoituksessa uudesta nimesta¨ ja tekija¨sta¨ voidaan ensin luoda dynaamisesti erilliset kopiot. Jos kopioinnissa ei tapahdu virheita¨, voidaan vanha nimi ja tekija¨ korvata uusilla ja tu- hota vanhat ilman, etta¨ virheita¨ voi ena¨a¨ sattua. Listaus 11.15 na¨ytta¨a¨ ta¨llaisen luokan ja listaus 11.16 seuraavalla sivulla sen toteutuksen. Ta¨ma¨ toteutus tarjoaa vihdoin ka¨ytta¨ja¨lleen vahvan poikkeusta- kuun. Kirjan sijoitus joko onnistuu tai sitten tapahtuu virhe ja kirja sa¨ilyy alkupera¨isessa¨ tilassa. Ta¨sta¨ on varsin helppo varmistua, koska koodissa kirjan varsinaiseen tilaan kosketaan vasta, kun kaikki mah- dolliset virhepaikat on jo ohitettu. Ta¨ma¨ on mahdollista, koska osoit- timien sijoitus ja merkkijonon tuhoaminen eiva¨t voi aiheuttaa poik- keuksia, joten kirjan vanhan tilan korvaaminen uudella ei voi keskey- tya¨, vaan se saadaan aina suoritettua onnistuneesti loppuun saakka. Samasta syysta¨ sijoitusoperaattorissa normaalisti va¨ltta¨ma¨to¨n itseen sijoituksen testaus ei ole ena¨a¨ va¨ltta¨ma¨to¨n, koska olion vanha arvo 1 class Kirja 2 { 3 public: 4 Kirja(std::string const& nimi, std::string const& tekija); 5 // Tarvitaan myo¨s oma kopiorakentaja (dynaaminen muistinhallinta)! 6 ~Kirja(); ... 7 Kirja& operator =(Kirja const& kirja); 8 private: 9 std::string* nimip ; 10 std::string* tekijap ; 11 }; LISTAUS 11.15: Kirjaluokka epa¨suoruuksilla 11.9. Esimerkki: poikkeusturvallinen sijoitus 404 1 Kirja::Kirja(std::string const& nimi, std::string const& tekija) 2 : nimip (0), tekijap (0) 3 { 4 try 5 { 6 nimip = new std::string(nimi); 7 tekijap = new std::string(tekija); 8 } 9 catch (. . .) 10 { 11 delete nimip ; nimip = 0; 12 delete tekijap ; tekijap = 0; 13 throw; 14 } 15 } 16 17 Kirja::~Kirja() 18 { 19 delete nimip ; nimip = 0; 20 delete tekijap ; tekijap = 0; 21 } 22 23 Kirja& Kirja::operator =(Kirja const& kirja) 24 { 25 if (this != &kirja) // Periaatteessa tarpeeton! 26 { 27 std::string* uusinimip = 0; 28 std::string* uusitekijap = 0; 29 try 30 { 31 uusinimip = new std::string(*kirja.nimip ); 32 uusitekijap = new std::string(*kirja.tekijap ); 33 // Jos pa¨a¨stiin ta¨nne, ei virheita¨ tullut 34 delete nimip ; nimip = uusinimip; // Onnistuvat aina 35 delete tekijap ; tekijap = uusitekijap; // Samoin na¨ma¨ 36 } 37 catch (. . .) 38 { 39 delete uusinimip; uusinimip = 0; 40 delete uusitekijap; uusitekijap = 0; 41 throw; 42 } 43 } 44 return *this; 45 } LISTAUS 11.16: Uuden kirjaluokan rakentaja, purkaja ja sijoitus 11.9. Esimerkki: poikkeusturvallinen sijoitus 405 tuhotaan vasta uuden arvon luomisen ja¨lkeen. Tehokkuusmielessa¨ it- seen sijoittamisen testaus saattaa silti olla paikallaan. Esimerkista¨ huomaa selva¨sti, etta¨ ta¨ssa¨ versiossa vahva poikkeus- takuu on tehnyt luokasta selva¨sti ko¨mpelo¨mma¨n ja vaikeammin hal- littavan. Lisa¨ksi jokaisen kirjaolion luominen vaatii kaksi uutta dy- naamista olion luomista, mika¨ tuhlaa hieman muistia ja hidastaa oh- jelmaa va¨ha¨isessa¨ ma¨a¨rin. Jos merkkijonoja olisi useampi kuin kak- si, muuttuisi tilanne aina vain pahemmaksi. Ratkaisua kannattaa siis viela¨ jalostaa. 11.9.4 Tilan eriytta¨minen (“pimpl”-idiomi) Vahvan poikkeustakuun antava toteutus saadaan tyylikka¨a¨mma¨ksi ja tehokkaammaksi, jos olion tila (sen ja¨senmuuttujat) ja itse olio (sen “identiteetti”) erotetaan toisistaan. Helpoimmin ta¨ma¨ tapahtuu laitta- malla ja¨senmuuttujat omaan struct-tietorakenteeseensa, johon olios- sa on sitten osoitin. Ta¨llaisella ja¨rjestelylla¨ voidaan olion tila korva- ta toisella yksinkertaisesti sijoittamalla osoittimen pa¨a¨ha¨n uusi tila- tietorakenne. Ta¨ssa¨kin tapauksessa tilan sisa¨lta¨va¨ struct ta¨ytyy luo- da dynaamisesti, mutta luomisia tapahtuu vain yksi ja¨senmuuttujien ma¨a¨ra¨sta¨ riippumatta. Lisa¨ksi dynaamista muistinhallintaa voidaan helpottaa korvaamalla osoittimet aliluvun 11.7 automaattiosoittimil- la. Listaukset 11.17 seuraavalla sivulla ja 11.18 seuraavalla sivul- la na¨ytta¨va¨t esimerkin erillisen tilarakenteen ka¨yto¨sta¨. Siina¨ Kirja- luokan sisa¨lla¨ on ma¨a¨ritelty erillinen struct-tietorakenne Tila. Luo- kan esittelyn yhteydessa¨ ta¨sta¨ tietorakenteesta riitta¨a¨ vain ennak- koesittely (aliluku 4.4), koska luokan esittelyssa¨ tarvitaan vain (au- tomaatti)osoitin tietorakenteeseen. Itse tietorakenteen ma¨a¨rittely voi olla vasta luokan toteuttavassa kooditiedostossa. Ta¨lla¨ tavalla rat- kaisu myo¨s lisa¨a¨ luokan kapselointia, koska ja¨senmuuttujat eiva¨t ena¨a¨ na¨y otsikkotiedostossa. Ta¨llaisesta erillisesta¨ tilatietorakenteesta ka¨yteta¨a¨n englanninkielisessa¨ C++-kirjallisuudessa yleisesti nimitysta¨ “pimpl” (private implementation)^ ja se on joskus varsin ka¨ytto¨kel- poinen muutenkin kuin poikkeuksien yhteydessa¨ [Sutter, 2000, koh- dat 26–30]. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ^Lisa¨ksi pimpl on ah, niin puujalka-hauskasti la¨hella¨ sanaa “pimple” = ﬁnni. 11.9. Esimerkki: poikkeusturvallinen sijoitus 406 1 class Kirja 2 { 3 public: 4 Kirja(std::string const& nimi, std::string const& tekija); 5 // Tarvitaan myo¨s oma kopiorakentaja! 6 // Oma purkaja tarvitaan, jotta auto ptr ennakkoesittelylle toimii 7 ~Kirja(); ... 8 Kirja& operator =(Kirja const& kirja); 9 private: 10 struct Tila; 11 std::auto ptr<Tila> tilap ; 12 }; LISTAUS 11.17: Kirja eriytetylla¨ tilalla ja automaattiosoittimella 1 struct Kirja::Tila 2 { 3 std::string nimi ; 4 std::string tekija ; 5 Tila(std::string const& nimi, std::string const& tekija) 6 : nimi (nimi), tekija (tekija) {} 7 }; 8 9 Kirja::Kirja(std::string const& nimi, std::string const& tekija) 10 : tilap (new Tila(nimi, tekija)) 11 { 12 } 13 14 Kirja::~Kirja() 15 { // Automaattiosoitin tuhoaa tilan automaattisesti 16 } 17 18 Kirja& Kirja::operator =(Kirja const& kirja) 19 { 20 std::auto ptr<Tila> uusitilap(new Tila(*kirja.tilap )); 21 tilap = uusitilap; // Ei voi epa¨onnistua ja tuhoaa vanhan tilan 22 return *this; 23 } LISTAUS 11.18: Eriytetyn tilan rakentajat, purkaja ja sijoitus 11.9. Esimerkki: poikkeusturvallinen sijoitus 407 Automaattiosoittimen ansiosta luokan rakentajat ja purkajat na¨yt- ta¨va¨t jo paljon siistimmilta¨ kuin aiemmin. Samoin sijoitusoperaatto- rissa ei ena¨a¨ tarvitse reagoida virheisiin, koska automaattiosoitin pi- ta¨a¨ huolen dynaamisesti luotujen tilatietorakenteiden tuhoamisesta. Sijoituksesta on myo¨s ja¨tetty tilasyista¨ pois itseen sijoituksen testaus, koska se ei ena¨a¨ ole va¨ltta¨ma¨to¨n. Ta¨ma¨ ratkaisu on jo varsin ylla¨pi- detta¨va¨ ja tehokkuudeltaankin todenna¨ko¨isesti siedetta¨va¨. 11.9.5 Tilan vaihtaminen pa¨ikseen Nyt kun vahvan poikkeustakuun antavalle sijoitukselle on lo¨ytynyt yleispa¨teva¨ ratkaisu, voidaan viela¨ tutkia, eiko¨ nimenomaan esimer- kin merkkijonojen tapauksessa voitaisi pa¨a¨sta¨ tehokkaampaa ratkai- suun. Va¨ha¨n string-luokan rajapintaa tutkimalla ta¨llainen lo¨ytyy- kin. Luokka nimitta¨in tarjoaa ja¨senfunktion swap, joka vaihtaa kahden merkkijonon arvot keskena¨a¨n nothrow-poikkeustakuulla. Sama ope- raatio lo¨ytyy myo¨s kaikista STL:n sa¨ilio¨ista¨. Vaihto-operaatio antaa mahdollisuuden pita¨a¨ merkkijonot Kirja-luokan normaaleina ja¨sen- muuttujina, mutta silti saavuttaa vahva poikkeustakuu. Listaus 11.19 seuraavalla sivulla na¨ytta¨a¨ esimerkin ta¨sta¨. Listauksessa luokkaan on lisa¨tty johdonmukaisuuden vuoksi oma ja¨senfunktio vaihda, joka vaihtaa kahden kirjan tilat keskena¨a¨n ka¨yt- ta¨ma¨lla¨ string-luokan swap-operaatiota. Koska swap onnistuu aina, ei myo¨ska¨a¨n vaihda-ja¨senfunktiossa voi tapahtua virhetta¨. Ta¨ta¨ ka¨yte- ta¨a¨n hyva¨ksi sijoitusoperaattorissa. Siella¨ sijoitettavasta oliosta luo- daan ensin kopio uuteen paikalliseen Kirja-olioon. Ta¨ma¨ olio edustaa sita¨, mihin sijoituksella halutaan pa¨a¨sta¨. Jos kopion luominen epa¨on- nistuu, ei alkupera¨iselle oliolle ole tehty mita¨a¨n ja vahva poikkeus- takuu pa¨tee. Jos kopiointi onnistuu, vaihdetaan sijoituksessa yksin- kertaisesti kopion ja vanhan kirjaolion tilat keskena¨a¨n. Na¨in sijoitus tulee tehtya¨. Sijoitusoperaattorista palattaessa vanhan tilan sisa¨lta¨va¨ paikallinen muuttuja lopuksi tuhoutuu. Erillinen vaihda-ja¨senfunktio on ka¨teva¨, koska sita¨ ka¨ytta¨ma¨lla¨ myo¨s Kirja-olioita ja¨senmuuttujinaan pita¨va¨t luokat voivat tarjota si- joitukselle vahvan poikkeustakuun samaa mekanismia ka¨ytta¨ma¨lla¨. Lisa¨ksi ohjelmassa saattaa muulloinkin olla ka¨teva¨a¨ pystya¨ vaihta- maan kahden olion tilat keskena¨a¨n ilman virhemahdollisuutta. Vihoviimeisena¨ esimerkkina¨ listaus 11.20 sivulla 409 yhdista¨a¨ keskena¨a¨n tilan eriytta¨misesta¨ saatavan lisa¨kapseloinnin ja tilan vaih- 11.9. Esimerkki: poikkeusturvallinen sijoitus 408 1 class Kirja 2 { 3 public: ... 4 Kirja& operator =(Kirja const& kirja); 5 void vaihda(Kirja& kirja) throw(); 6 private: 7 std::string nimi ; 8 std::string tekija ; 9 }; . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1 void Kirja::vaihda(Kirja& kirja) throw() 2 { 3 nimi .swap(kirja.nimi ); // Ei voi epa¨onnistua 4 tekija .swap(kirja.tekija ); // Eika¨ ta¨ma¨ka¨a¨n 5 } 6 7 Kirja& Kirja::operator =(Kirja const& kirja) 8 { 9 Kirja kirjakopio(kirja); // Kopio sijoitettavasta 10 vaihda(kirjakopio); // Vaihdetaan itsemme siihen, ei epa¨onnistu 11 return *this; // Vanha tila tuhoutuu kirjakopion myo¨ta¨ 12 } LISTAUS 11.19: Kirja, jossa on nothrow-vaihto tamisen. Se tarjoaa ehka¨ kaikkein tyylikka¨imma¨n yleiska¨ytto¨isen rat- kaisun, jolla vahva poikkeustakuu saadaan toteutettua la¨hes luokassa kuin luokassa. 11.9. Esimerkki: poikkeusturvallinen sijoitus 409 1 class Kirja 2 { 3 public: 4 Kirja(std::string const& nimi, std::string const& tekija); 5 Kirja(Kirja const& kirja); 6 // Oma purkaja tarvitaan, jotta auto ptr ennakkoesittelylle toimii 7 ~Kirja(); ... 8 Kirja& operator =(Kirja const& kirja); 9 void vaihda(Kirja& kirja) throw(); 10 private: 11 struct Tila; 12 std::auto ptr<Tila> tilap ; 13 }; . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1 struct Kirja::Tila 2 { 3 std::string nimi ; 4 std::string tekija ; 5 Tila(std::string const& nimi, std::string const& tekija) 6 : nimi (nimi), tekija (tekija) {} 7 }; 8 9 Kirja::Kirja(std::string const& nimi, std::string const& tekija) 10 : tilap (new Tila(nimi, tekija)) 11 { 12 } 13 14 Kirja::Kirja(Kirja const& kirja) 15 : tilap (new Tila(*kirja.tilap )) // Tilan kopiorakentaja 16 { 17 } 18 19 Kirja::~Kirja() 20 { // Automaattiosoitin tuhoaa tilan automaattisesti 21 } 22 23 void Kirja::vaihda(Kirja& kirja) throw() 24 { 25 std::auto ptr<Tila> p(tilap ); 26 tilap = kirja.tilap ; 27 kirja.tilap = p; 28 } 29 30 Kirja& Kirja::operator =(Kirja const& kirja) 31 { 32 Kirja kirjakopio(kirja); // Kopio sijoitettavasta 33 vaihda(kirjakopio); // Vaihdetaan itsemme siihen, ei epa¨onnistu 34 return *this; // Vanha tila tuhoutuu kirjakopion myo¨ta¨ 35 } LISTAUS 11.20: Yhdistelma¨ tilan eriytta¨misesta¨ ja vaihdosta 410 Liite A C++: Ei-olio-ominaisuuksia Vaikka sana “ei-olio-ominaisuuksia” onkin melkoinen hirvitys, se ku- vaa hyvin sita¨, miksi na¨ma¨ C++-ohjelmointikielen rakenteet on esitelty lyhyesti erillisessa¨ liitteessa¨. Kyseessa¨ on joukko kielen ominaisuuk- sia (la¨hinna¨ parannuksia C-kieleen), jotka C++:n ka¨ytta¨ja¨n on hyva¨ tun- tea, mutta joilla ei ole suoraan mita¨a¨n tekemista¨ kirjan aiheen eli olio- ohjelmoinnin kanssa. Esitelta¨vista¨ asioista viitteet ovat uusi “osoitustyyppi”, inline on puhtaasti optimointiominaisuus ja vector seka¨ string ovat uusia helppoka¨ytto¨isempia¨ versioita C:ssa¨ olevista ominaisuuksista. Ta¨ma¨n liitteen tarkoituksena on antaa na¨ista¨ C++:n ominaisuuksis- ta sen verran tietoa, etta¨ muun kirjan lukeminen ja ymma¨rta¨minen onnistuvat ilman suuria ongelmia. Kattavan kuvauksen na¨ista¨ omi- naisuuksista saa parhaiten jostain C++-oppikirjasta [Lippman ja Lajoie, 1997], [Stroustrup, 1997]. A.1 Viitteet C-kieli ka¨ytta¨a¨ kahdenlaisia tapoja tiedon ka¨sittelyyn ja va¨litta¨miseen esimerkiksi funktioiden parametreina: muuttujat ja osoittimet. Muut- tujat ovat varsinainen datan esitysmuoto, ja osoittimien avulla voi- daan viitata olemassa olevaan dataan (muuttuja). Osoittimet ovat to- teutukseltaan muistiosoitteita, ja niita¨ pystyy ka¨sittelema¨a¨n hyvin va- paasti: vaihtamaan arvoa mielivaltaiseksi kokonaisluvuksi ja siirta¨- A.1. Viitteet 411 ma¨a¨n osoittamaan esimerkiksi “seuraavaan” alkioon (++-operaattori). Na¨ista¨ vapauksista johtuen osoittimet ovat myo¨s usein pahojen oh- jelmointivirheiden la¨hde. Ohjelmointivirheen seurauksena va¨a¨ra¨a¨n paikkaan osoittava osoitin voi aiheuttaa muutoksia ohjelman datassa va¨a¨ra¨ssa¨ paikassa tai jossain vaiheessa ja¨rkeva¨ osoitinarvo osoittaakin muistin vapauttamisen ja¨lkeen kielletylle alueelle, jota silti yriteta¨a¨n ka¨ytta¨a¨ osoittimen la¨pi. Na¨ma¨ virheet paljastuvat usein vasta hyvin pitkien aikojen kuluttua (ohjelman aivan muut kuin virheen aiheut- taneet osat alkavat ka¨ytta¨ytya¨ virheellisesti). Osoittimien “vaarallisuuden” takia C++ ma¨a¨rittelee uuden tietotyy- pin viite (reference), joka osoittimien tapaan viittaa olemassa olevaan dataan mutta johon liittyen ka¨a¨nta¨ja¨ tekee enemma¨n ka¨ytto¨tarkistuk- sia kuin osoittimien yhteydessa¨. Viite on tavallaan synonyymi ole- massa olevalle data-alkiolle, ja jokainen viite viittaa aina johonkin dataan (se ei voi olla nolla kuten osoitin).\u0017 Aina kun viite luodaan, se ta¨ytyy samalla alustaa osoittamaan ai- kaisemmin luotuun dataan. C++ uudelleenka¨ytta¨a¨ viitteiden yhteydes- sa¨ hieman ha¨ma¨a¨va¨sti osoittimiin liittyva¨n syntaktisen merkin “&”. Kun muuttujan tietotyypin pera¨a¨n on lisa¨tty &, kyseessa¨ on viitetyyp- pi kyseista¨ tyyppia¨ olevaan alkioon: int a = 0; int b = 0; // normaali kokonaislukumuuttujien esittely int& a r = a; // kokonaislukuviite muuttujaan a Kun viitetyyppista¨ muuttujaa ka¨yteta¨a¨n, se ka¨yttytyy samoin kuin kohdassa esiintyisi alkupera¨inen muuttuja (viite on toinen muoto eli synonyymi alkupera¨iselle muuttujalle). b = a r + 1; // lukee muuttujan a arvon ⇒b == 1 a r = 2 - b; // kirjoittaa muuttujaan a ⇒a == 1 Osoittimeen voi tehda¨ viitteen, mutta tyyppia¨ “osoitin viitteeseen” ei ole olemassa(!) (katso kuva A.1 seuraavalla sivulla). int* a p = &a r; // osoittaa muuttujaan a (ei viitteeseen!) int*& a rp = a p; // synonyymi osoittimelle *a rp = 7; // muuttujan a arvo on sijoituksen ja¨lkeen 7 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . \u0017Ta¨ssa¨ on suuri ero Javan viitetyyppeihin, joilla on olemassa tyhja¨ viitearvo null. A.1. Viitteet 412 a a_r a_p a_rp KUVA A.1: Esimerkkien viittaukset Viitteiden normaali ka¨ytto¨tarkoitus ei ole luoda turhia uusia nimi- synonyymeja¨ jo olemassa oleville muuttujille vaan va¨litta¨a¨ viittauk- sia dataan funktioiden parametreina tai paluuarvoina. Viiteparamet- ri sitoutuu synonyymiksi funktiolle annettuun dataan, kun funktiota kutsutaan: void tuplaa( int& i ) { i = i * 2; } ... int x = 2; tuplaa( x ); // x on nyt 4 Viiteparametrin avulla on helpompi kirjoittaa funktioita, jotka muuttavat parametriensa arvoja. Haittapuoli taas on se, etta¨ funktion kutsusta tuplaa( x ) ei pysty pa¨a¨ttelema¨a¨n mitenka¨a¨n, muuttaako funktio parametrina annettua arvoa. Ta¨ma¨ tieto on tarkistettava funk- tion esittelysta¨, jossa funktio voi luvata vakioviiteparametrilla, etta¨ se ei muuta parametrin arvoa: struct SuuriData { . . . }; void arvonValitys( SuuriData d ); void vakioViiteValitys( SuuriData const& d ); Kumpikaan esitellyista¨ funktioista ei muuta parametrina annettua da- taa. ArvonValitys kopioi tietorakenteen datasta itselleen oman ko- pion, jolloin kutsujan antama data ei muutu. VakioViiteValitys ei myo¨ska¨a¨n muuta dataa, koska se on merkitty ka¨a¨nta¨ja¨lle vakioda- A.2. inline 413 taksi. Mahdollisesti raskasta datan kopiointioperaatiota ei kuitenkaan tehda¨, vaan funktiolle va¨littyy ka¨ytto¨o¨n viite alkupera¨iseen dataan. Funktio voi palauttaa viitteen olemassa olevaan dataan myo¨s pa- luuarvonaan. Ta¨ma¨n ominaisuuden avulla voidaan tehda¨ funktio, jo- ka valitsee palautettavan arvon, joka on jatkoka¨sitelta¨vissa¨ aivan kuin ka¨yto¨ssa¨ olisi koko ajan ollut alkupera¨inen arvo: 1 int data[10]; 2 int& kohdistin( int i ) { return data[i]; } ... 3 kohdistin(0) = 7; // sijoittaa taulukon ensimma¨iseen alkioon Funktio kohdistin palauttaa viitteen, joka on sitoutunut siihen taulukon data alkioon, jonka funktion parametri i ma¨a¨ra¨a¨. Koska pa- luuarvo ka¨ytta¨ytyy kuten alkupera¨inen alkio, siihen voidaan tehda¨ myo¨s rivin 3 mukainen sijoitus. Viitepaluuarvoihin liittyy myo¨s tilanne, jossa saadaan aikaiseksi virheellinen viite. Koska funktion paikallinen data tuhoutuu funktion suorituksen pa¨a¨tyttya¨, ta¨llaiseen dataan palautettava viite ei viittaa olemassa olevaan dataan, ja viitteen ka¨ytto¨ aiheuttaa ma¨a¨rittelema¨t- to¨ma¨n toiminnon: int& virheellinen( int a ) { int b = 2 * a; return b; // VIRHE! paikallinen muuttuja viitepaluuarvona } A.2 inline Useimmissa prosessoriarkkitehtuureissa funktiokutsu on eniten ka¨y- tetyista¨ operaatioista raskain (parametrien sijoitus esimerkiksi pi- noon ja arvokopioinnit funktion alussa ja lopussa). C++-ohjelmoija voi merkita¨ haluamansa funktiot avainsanalla inline, joka antaa ka¨a¨nta¨- ja¨lle luvan optimoida funktion ka¨ytto¨a¨. Optimoinnissa ka¨a¨nta¨ja¨ yritta¨a¨ korvata funktion kutsun sen sisa¨l- ta¨ma¨lla¨ toiminnallisuudella. Ta¨ma¨ tapahtuu siten, etta¨ funktion al- kupera¨inen toiminnallisuus ei saa muuttua mitenka¨a¨n. Jos funktio A.3. vector 414 on liian mutkikas ta¨llaiseen korvaukseen, ka¨a¨nta¨ja¨ suorittaa normaa- lin funktiokutsun. Ohjelmoijan ei tarvitse va¨litta¨a¨ siita¨, mita¨ mahdol- lisia optimointimuutoksia inlinen ka¨ytto¨ aiheuttaa syntyva¨a¨n kone- koodiin — toiminnallisuus on edelleen sama, kuin jos mita¨a¨n opti- mointia ei olisi tehty. C++-standardi ei vaadi ka¨a¨nta¨jilta¨ optimointien suorittamista, joten C++-ka¨a¨nta¨ja¨, joka ja¨tta¨a¨ inline-avainsanat otta- matta huomioon, on standardin mukaisesti toimiva. (Tietysti juuri ta¨llaiset kohdat standardissa antavat ka¨a¨nta¨ja¨valmistajille kilpailu- varaa, joten ka¨yta¨nno¨ssa¨ kaikki C++-ka¨a¨nta¨ja¨t suorittavat inline-opti- mointeja.) Esimerkiksi ohjelmakoodissa 1 inline int max( int x, int y ) 2 { 3 if( x > y ) { return x; } 4 else { return y; } 5 } ... 6 return max( a, max( b, c ) ); rivi 6 voi inline-funktiokutsujen takia muuttua muotoon: { int r1 =(b>c)?b:c; int r2 =(a>r1 )?a:r1 ; return r2 ;} Ka¨yta¨nno¨ssa¨ optimoinnit suoritetaan ka¨a¨nta¨ja¨n tuottaman konekoo- din tasolla, mutta esimerkista¨ na¨kyy inlinen idea funktiokutsujen poistamisessa. A.3 vector Taulukko on tietorakenne, joka sisa¨lta¨a¨ sarjan samaa tietotyyppia¨ ole- via alkioita. Taulukoiden perusominaisuus on indeksointi, jossa voi- daan viitata (yhta¨ nopeasti) taulukon mihin tahansa alkioon (T[i]). [Sethi, 1996, luku 4.4] C-kielessa¨ taulukot ja osoittimet ovat “la¨hisukulaisia”. Taulukon nimi toimii osoittimena taulukon ensimma¨iseen alkioon. Erityisesti A.3. vector 415 ta¨ma¨ piirre korostuu funktioiden parametreissa, joissa va¨liteta¨a¨n tau- lukoita osoittimina, vaikka syntaksi saattaisikin viitata taulukon va¨- litta¨miseen arvona: [Kerninghan ja Ritchie, 1988, luku 5.3] void poke( char s[ ] ) { s[1] = ’u’; } ... char j[ ] = \"Jyke\"; poke( j ); // j on nyt “Juke”; C++ tarjoaa C-kielen tyyppisten taulukoiden lisa¨ksi uuden tauluk- kotyypin “vektori”, joka ei ole taulukoiden tavoin sitoutunut osoitti- miin. #include <vector> // taulukkotyypin esittely ka¨ytto¨o¨n int main() { int vt[10]; // “vanha” taulukko kokonaislukuja vector<int> ut(10); // vektoritaulukko kokonaislukuja } C-taulukoihin verrattuna vektori on turvallista va¨litta¨a¨ arvopara- metrina (se kopioituu), ja vektoriin voidaan sijoittaa toinen vektori. Vektorin koko kasvaa tarvittaessa (automaattinen tilan kasvatus), mi- ka¨ va¨henta¨a¨ muistinhallinan ongelmia. Koska vektorin tilanvarausta ei ole C-taulukoiden tapaan pakko ilmoittaa luonnin yhteydessa¨, se voidaan luoda tyhja¨na¨ ja ta¨ytta¨a¨ tarvittavilla alkioilla. Taulukon raja- pinnassa oleva operaatio push back lisa¨a¨ uuden alkion taulukon lop- puun: vector<unsigned int> taulukko; // tyhja¨ vektori for( unsigned int i=1; i<=10; ++i ) // lisa¨ta¨a¨n alkiot 1. .10 { taulukko.push back(i); } Vektoritaulukoita indeksoidaan samalla syntaksilla kuin C-taulu- koitakin (ut[i]), ja samoin kuin aikaisemmin, ta¨ssa¨ indeksointita- vassa ei ole tarkastuksia indeksoinnin “onnistumisesta” eli osumi- sesta taulukon alueelle. Seka¨ vt[11] etta¨ ut[11] ovat C++-standardin A.4. string 416 kannalta ma¨a¨rittelema¨tto¨mia¨ operaatioita (viittaus tietorakenteen ul- kopuolelle). Vektori tarjoaa rajapinnassaan operaation at, joka suo- rittaa ajoaikaisen indeksoinnin oikeellisuuden tarkastuksen. Lause ut.at(11) tuottaa siis virheen (poikkeuksen avulla, katso luku 11). Kuvassa A.2 on vertailtu C-taulukon ja vektorin toimintoja. A.4 string C-kielen yhteydessa¨ merkkijonoista puhuttaessa tarkoitetaan merk- kitaulukkoa (char[ ] tai unsigned char[ ]), jonka viimeisena¨ (pa¨a¨tty- mismerkkina¨) on sovittu olevan merkkikoodi nolla (’\\0’). C++ tarjo- aa uuden merkkijonotietotyypin string, joka on “ta¨ysiverinen” tieto- tyyppi eika¨ ena¨a¨ kielen alimmalla tasolla oleva yhdistetty taulukko- ja osoitin-rakenne C:n tapaan. string-tyyppiset muuttujat muun muassa kopioituvat funktioi- Toiminta C-taulukko vektori Luominen int t[10] vector<int> t(10) Luominen, tyhja¨ (ei ja¨rkeva¨) vector<int> t Sijoitus (silmukassa) t1 = t2 Indeksointi t[i] t[i] tai t.at(i) Lisa¨ys loppuun (mahdoton) t.push back(a) Koko (tiedetta¨va¨ muualta) t.size() Tyhjennys (mahdoton) t.clear() Vertailu (silmukassa) t1 == t2 (myo¨s t1 < t2, yms.) Lisa¨ys keskelle (mahdoton) t.insert(paikka-iteraattori, a) Etsiminen (silmukassa) find(t.begin(), t.end(), a) Arvojen vaihto (silmukassa) t1.swap( t2 ) KUVA A.2: C-taulukon ja vektorin vertailu A.4. string 417 Toiminta merkkijonotaulukko string Alustus char* s = \"jaa\"; string s(\"ma¨a¨\"); Sijoitus strcpy(mihin, mista) mihin = mista Indeksointi s[i] s[i] tai s.at(i) Yhdista¨minen strcat(mihin, mika) mihin += mika Alimerkkijono strncpy(t,&s[alku],koko) t=s.substr(alku,alku+koko) Koko strlen(s) s.length() Tyhjennys s[0] = ’\\0’ s.clear() Vertailu strcmp(s, t) s == t (myo¨s s < t yms.) Rivin luku (eri tapoja) getline(cin, s) Lisa¨a¨minen (todella hankalaa) s.insert(paikka-iter, t) Etsiminen strstr(mista, mita) mista.find(mita) Arvojen vaihto char* tt=s; s=t; t=tt; s.swap(t) KUVA A.3: Merkkijonotaulukon ja C++:n stringin vertailu 1 #include <cstdlib> 2 #include <string> 3 using std::string; 4 #include <iostream> 5 using std::cin; 6 using std::cout; 7 using std::endl; 8 9 int main() 10 { 11 string sana, kokoelma(\"ALKU\"); 12 13 while( cin >> sana ) { 14 if( sana.find(\"ja\") != string::npos ) { 15 // lo¨ytyi 16 kokoelma = kokoelma + \",\" + sana; 17 } 18 } 19 cout << kokoelma << endl; 20 return EXIT SUCCESS; 21 } LISTAUS A.1: Osamerkkijonon “ja” etsinta¨ (C++) A.4. string 418 1 #include <string.h> 2 #include <stdlib.h> 3 #include <stdio.h> 4 5 int main() 6 { 7 char sana[1024]; /* ei toimi ta¨ta¨ pidemmilla¨ sanoilla */ 8 char *kokoelma, *tmp; 9 unsigned int pituus, kaytossa; 10 pituus = kaytossa = 5; /* ALKU + lopetusmerkki */ 11 kokoelma = (char*)malloc( pituus ); strcpy( kokoelma, \"ALKU\" ); 12 while( scanf(\"%s\", sana) == 1) { 13 if( strstr(sana, \"ja\") != 0 ) { 14 if( kaytossa + strlen(sana) + 2 > pituus ) { 15 /* muisti ei riita¨ -> lisa¨a¨ */ 16 tmp = kokoelma; 17 kokoelma = (char*)malloc( pituus + 2*strlen(sana) ); 18 strcpy( kokoelma, tmp ); 19 free( tmp ); 20 pituus += 2*strlen(sana); 21 } 22 strcat( kokoelma, \",\" ); 23 strcat( kokoelma, sana ); 24 kaytossa = kaytossa + strlen(sana) + 2; 25 } 26 } 27 printf(\"%s\\n\", kokoelma); 28 return EXIT SUCCESS; 29 } LISTAUS A.2: Osamerkkijonon “ja” etsinta¨ (C) den parametreina: #include <string> // merkkijonotyypin esittely ka¨ytto¨o¨n void spoke( string s ) { s[1] = ’u’; } ... string j = \"Jyke\"; spoke( j ); // j:n arvo on edelleen “Jyke” C-kielen kirjasto ma¨a¨rittelee merkkijonotaulukoiden ka¨sittelyyn joukon str-alkuisia funktioita, joiden yhteydessa¨ ohjelmoijan on jat- A.4. string 419 kuvasti itse huolehdittava siita¨, etta¨ operaatioiden kohdemerkkijo- noissa on tarpeeksi tilaa (merkkitaulukko on tarpeeksi suuri operaa- tion tuloksen sa¨ilytta¨miseen). C++:n stringin rajapinnassa ovat kaikki vastaavat operaatiot (vertailu kuvassa A.3 sivulla 417), ja niiden yh- teydessa¨ mahdolliset tulosmerkkijonon koon muutokset tapahtuvat automaattisesti. Listauksessa A.1 sivulla 417 on esimerkki string-tietorakenteen ka¨yto¨sta¨. Ohjelma lukee oletussyo¨tteesta¨a¨n sanoja ja muodostaa pil- kulla erotellun luettelon (merkkijonoon) kaikista niista¨ sanoista, jotka sisa¨lta¨va¨t sanan “ja”. Lopuksi ohjelma tulostaa muodostamansa luet- telon. Listauksessa A.2 edellisella¨ sivulla sama toiminnallisuus on toteutettu C-kielella¨ merkkijonotaulukoiden avulla, jolloin ohjelmoi- jan on itse huolehdittava muistinhallinnasta: etuka¨teen ei ole tiedos- sa syo¨tteen yksitta¨isen sanan tai syntyva¨n luettelon pituutta. Ka¨sitel- ta¨va¨n sanan maksimipituuden ongelma on ratkaistu “taikavakiolla” 1024, ja tuotettavan listan muistinhallinta on ohjelmoitu itse malloc- ja free-rutiineilla. 420 Liite B C++-tyyliopas Opettele sa¨a¨nno¨t, jotta tieda¨t miten rikkoa niita¨ oikein. – Tuntematon Koska nykyaikaiset ohjelmointikielet ovat mutkikkaita ja monita- hoisia tyo¨kaluja, tyylioppaalla pyrita¨a¨n rajaamaan projektin tai orga- nisaation tarpeisiin sopivat tavat ka¨ytta¨a¨ ta¨ta¨ tyo¨kalua. Yhtena¨isen ohjelmointityylin noudattaminen lisa¨a¨ la¨hdekoodin ymma¨rretta¨vyyt- ta¨, ylla¨pidetta¨vyytta¨, siirretta¨vyytta¨, luettavuutta ja uudelleenka¨ytet- ta¨vyytta¨. Mutkikkaissa ohjelmointikielissa¨ (kuten C++) tyylioppaan oh- jeet voivat myo¨s lisa¨ta¨ koodin tehokkuutta ja auttaa va¨ltta¨ma¨a¨n ohjel- mointivirheita¨ — tai ainakin auttaa lo¨yta¨ma¨a¨n virheet nopeammin. Hyva¨ tyyliopas pystyy parantamaan kirjoittamamme ohjelmakoodin laatua. Ta¨ha¨n tyylioppaaseen on koottu sa¨a¨nto¨ja¨, joita noudattamalla pa¨a¨see kohti tuota pa¨a¨ma¨a¨ra¨a¨ ka¨ytetta¨essa¨ C++-ohjelmointikielta¨. Opas on kokoelma sa¨a¨nto¨ja¨ ja suosituksia. Sa¨a¨nno¨t ovat pe- rusteltavissa ISOC++ -standardilla tai muilla erityisen painavilla syil- la¨. Ohjelmakoodin ulkoasuun, muokkaamiseen ja taltiointiin liitty- va¨t suositukset on tarkoitettu yhdenmukaisiksi sopimuksiksi, joita tarvittaessa muokataan tyyliohjetta sovellettaessa. Suositus “tiedos- tonimi pa¨a¨ttyy aina pa¨a¨tteeseen .cc” voidaan tarvittaessa muokata tarkoittamaan esimerkiksi tiedostopa¨a¨tetta¨ .C, .cxx tai .cpp ka¨ytety ka¨a¨nno¨sympa¨risto¨n vaatimusten mukaisesti. 1. Yleista¨ B.2. Ohjelmiston tiedostot 421 1.1. Kaikkia tyylioppaan sa¨a¨nto¨ja¨ on noudatettava. 1.2. Tyylioppaan sa¨a¨nno¨sta¨ saa aina poiketa hyva¨lla¨ perusteel- la, joka tulee kirjata ohjelmakoodin kommentteihin. 1.3. Sa¨a¨nno¨t eiva¨t koske ulkopuolista la¨hdekoodia (muualta hankittu tai koodigeneraattorin tuottama). 1.4. Vanhoja ohjelmia ei muuteta ta¨ma¨n tyylioppaan mukaisik- si, vaan niita¨ ylla¨pideta¨a¨n niita¨ tehta¨essa¨ noudatetun ka¨y- ta¨nno¨n mukaisesti. 2. Ohjelmiston tiedostot 2.1. Tiedostot ovat ma¨a¨ra¨muotoisia: alussa on koko tiedostoa koskeva kommentti. 2.2. Tiedoston kommentit kirjoitetaan C++-muodossa (// ...). Monen rivin kommentit voidaan sijoittaa C-kielen kom- menttien (/* ... */) sisa¨a¨n. 2.3. Otsikkotiedostot 2.3.1. Otsikkotiedostojen nimen pa¨a¨te on .hh Yhtena¨iseen nimea¨miseen kuuluvat ma¨a¨ra¨muotoiset tie- dostonimet. Pa¨a¨tteen valintaan vaikuttaa ensisijaisesti se, mita¨ tiedostoja ka¨ytetty ka¨a¨nno¨sympa¨risto¨ pita¨a¨ oletusar- voisesti C++-tiedostoina. Pa¨a¨tetta¨ .h ei kannata ka¨ytta¨a¨, kos- ka se on jo ka¨yto¨ssa¨ C-kielisissa¨ ohjelmissa. 2.3.2. Yhdessa¨ otsikkotiedostossa esitella¨a¨n yksi julkinen ra- japinta (luokka tai nimiavaruus). Hyvin dokumentoitu julkinen rajapinta on usein sopivan kokoinen kokonaisuus sijoitettavaksi yhteen ohjelmakoodi- tiedostoon. (Katso myo¨s kohta 5.1.) 2.3.3. Otsikkotiedosto on selkea¨sti kommentoitu, jos sen pe- rusteella pystyta¨a¨n suunnittelemaan ja toteuttamaan rajapintaa koskevat testitapaukset. 2.3.4. Otsikkotiedostoissa on ainoastaan esittelyita¨. (ISOC++:n ominaisuuksien takia otsikkotiedostossa voi olla myo¨s kokonaislukuvakioita, inline-rakenteita [aliluku A.2 sivulla 413] ja template-malleja [aliluku 9.5 sivul- la 283].) Vaikka inline- ja template-rakenteet sisa¨lta¨va¨t toteutuksen ohjelmakoodia, niin nykyisten C++-ka¨a¨nta¨jien on na¨hta¨va¨ niiden ta¨ysi ma¨a¨rittely, ennen kuin kyseisia¨ rakenteita voi- daan ka¨ytta¨a¨ (instantioida) muussa ohjelmakoodissa. B.2. Ohjelmiston tiedostot 422 2.3.5. Laajat mallit ja inline-koodit tulee kirjoittaa erilli- seen tiedostoon (jolla on yhtena¨inen pa¨a¨te esimerkiksi .icc). Ta¨ma¨ tiedosto luetaan otsikkotiedoston lopussa #include-ka¨skylla¨. Toteutustapa jolla sa¨a¨nto¨a¨ 2.3.4 voidaan noudattaa sel- kea¨sti. 2.3.6. Otsikkotiedostossa otetaan ka¨ytto¨o¨n (#include-ka¨s- kyilla¨) ainoastaan ne muut esittelyt, jotka ovat tarpeen tiedoston sisa¨lta¨ma¨n esittelyn takia. Ylima¨a¨ra¨iset #include-ka¨skyt aiheuttavat turhia riippu- vuuksia la¨hdekooditiedostojen va¨lille vaikeuttaen ylla¨pidet- ta¨vyytta¨. 2.3.7. Otsikkotiedostoissa ei saa ka¨ytta¨a¨ using-lauseita. [ali- luku 1.5.7 sivulla 47] (Katso myo¨s kohta 3.3.) Esittelyiden yhteydessa¨ on suurin vaara nimikonﬂikteista eri otsikkotiedostojen sisa¨lta¨mien nimien va¨lilla¨. Eksplisiit- tinen nimien ka¨ytto¨ na¨kyvyystarkentimella ei sotke ohjel- man nimiavaruuksia tai tuota ohjelmakoodia, jonka toi- mivuus riippuu ka¨ytettyjen otsikkotiedostojen keskina¨isesta¨ ka¨ytto¨o¨nottoja¨rjestyksesta¨. 2.3.8. Jos esittelyssa¨ (yleensa¨ otsikkotiedostossa) viitataan jo- honkin luokkatyyppiin vain osoittimella tai viitteella¨, ka¨yteta¨a¨n ennakkoesittelya¨ [aliluku 4.4 sivulla 112]. Ennakkoesittelyn saa tehda¨ vain itse tekemilleen luokille. Vain itse tehdysta¨ ohjelmakoodista voi varmasti tieta¨a¨ en- nakkoesittelyn olevan sallittua. Esimerkiksi vaikka standar- dikirjaston std::string [aliluku A.4 sivulla 416] vaikuttaa luokkatyypilta¨, siihen ei saa tehda¨ ennakkoesittelya¨, koska toteutus on tyyppialias (typedef). 2.3.9. Otsikkotiedosto on suojattava moninkertaiselta ka¨yt- to¨o¨notolta [aliluku 1.4 sivulla 39]. #include-ketjuissa moninkertainen otsikkotiedoston ka¨yt- to¨o¨notto on mahdollista ja aiheuttaa ka¨a¨nno¨svirheen (C++- rakenteet voidaan esitella¨ ka¨a¨nta¨ja¨lle vain kerran saman ka¨a¨nno¨ksen yhteydessa¨). Suojaus toteutetaan ehdollisen ka¨a¨nta¨misen esiprosessorika¨skyilla¨ otsikkotiedoston alussa ja lopussa: #ifndef LUOKKA A HH #define LUOKKA A HH ... #endif // LUOKKA A HH B.3. Nimea¨minen 423 2.3.10. #include-ka¨skyissa¨ ei saa esiintya¨ viittauksia tietyn ko- neen tiettyyn hakemistoon. Kaikki yhteen ka¨a¨nno¨sympa¨risto¨o¨n sitovat viittaukset vai- keuttavat la¨hdekoodin ylla¨pitoa ja siirretta¨vyytta¨. 2.4. Toteutustiedostot 2.4.1. Toteutukset sisa¨lta¨va¨n tiedoston pa¨a¨te on .cc 2.4.2. Toteutustiedostossa otetaan ka¨ytto¨o¨n vain kyseisen ka¨a¨nno¨syksiko¨n toteuttamisen kannalta tarpeelliset ot- sikkotiedostot. 2.4.3. Toteutustiedostossa otetaan ensin ka¨ytto¨o¨n ohjelman paikalliset esittelyt (otsikkotiedostot) ja vasta sitten ka¨a¨nno¨sympa¨risto¨n ja ISOC++-kielen ma¨a¨rittelema¨t kir- jastot [aliluku 1.5.7 sivulla 47] (katso myo¨s koh- ta 3.3.5). Ulkopuolisten esittelyiden keskina¨isesta¨ ja¨rjestyksesta¨ riip- puvat otsikkotiedostot ovat vaarallista ohjelmakoodia ja ta¨l- la¨ ka¨yta¨nno¨lla¨ ne huomataan helpommin ka¨a¨nno¨ksen yh- teydessa¨. 2.4.4. Rajapinnan toteutukset esiteta¨a¨n tiedostossa samassa ja¨rjestyksessa¨ kuin ne ovat vastaavassa otsikkotiedos- tossa. Na¨in ma¨a¨ra¨tyn rajapintaoperaation toteutuksen lo¨yta¨mi- nen helpottuu. 3. Nimea¨minen 3.1. Ohjelmakoodin kommentit ja symboliset nimet kirjoite- taan yhtena¨isella¨ kielella¨ ja nimea¨mistavalla. Esimerkiksi TTY:n opetuskieli on suomi, joten on loogista ka¨yt- ta¨a¨ samaa kielta¨ myo¨s ohjelmoidessa. Vastaavasti jos yrityksen virallinen kieli on englanti, se lienee parempi vaihtoehto ohjel- makoodin nimien ja kommentoinnin kieleksi. 3.2. Alaviivalla alkavia nimia¨ ei saa ka¨ytta¨a¨. Samoin kielletty- ja¨ ovat nimet, joissa on kaksi pera¨kka¨ista¨ alaviivaa. [alilu- ku 2.3.2 sivulla 61]. 3.3. Nimiavaruudet 3.3.1. Moduulin ka¨ytta¨ma¨t nimet pideta¨a¨n erilla¨a¨n muus- ta ohjelmistosta sijoittamalla ne nimiavaruuden (namespace) sisa¨a¨n [aliluku 1.5.1 sivulla 42]. B.3. Nimea¨minen 424 3.3.2. ISOC++:n uusia otsikkotiedostoja ja niiden kautta std- nimiavaruutta on ka¨ytetta¨va¨. [aliluku 1.5.4 sivulla 45] ja [aliluku 1.5.5 sivulla 45]. 3.3.3. Jos nimiavaruuden sisa¨lta¨ nostetaan na¨kyville nimia¨, niin ka¨ytto¨o¨n otetaan vain todella tarvittavat nimet (lauseen using namespace X ka¨ytto¨ on kiellettya¨). [ali- luku 1.5.7 sivulla 47] 3.3.4. using-lauseella nostetaan nimet ka¨ytto¨o¨n la¨helle ka¨yt- to¨paikkaa (koodilohko tai funktio). 3.3.5. Erityisen usein tarvittavat nimet voidaan nostaa na¨ky- ville koko ka¨a¨nno¨syksikko¨o¨n ja niita¨ koskevat using- lauseet tulee sijoittaa tiedoston kaikkien #include-ka¨s- kyjen ja¨lkeen, jotta “nostetut” nimet eiva¨t pa¨a¨se sotke- maan toisia esittelyita¨. ISOC++:n otsikkotiedostoja voi- daan kuitenkin pita¨a¨ hyvin muotoiltuina ja niiden yhteyteen liitetyt kyseista¨ esittelya¨ koskevat using- lauseet usein parantavat ohjelmakoodin luettavuutta. [aliluku 1.5.7 sivulla 47] #include \"prjlib.hh\" #include <iostream> using std::cout; using std::endl; #include <sstream> using std::ostringstream; using Prjlib::Puskuri; using Prjlib::Paivays; 3.3.6. Ka¨a¨nno¨syksiko¨n (tiedosto) sisa¨isessa¨ ka¨yto¨ssa¨ olevat ma¨a¨rittelyt sijoitetaan nimea¨ma¨tto¨ma¨n nimiavaruu- den sisa¨a¨n. [aliluku 1.5.8 sivulla 49] Oletuksena ka¨a¨nno¨syksiko¨n nimet voivat aiheuttaa nimi- konﬂikteja muun ohjelmiston osien kanssa linkitysvaihees- sa. Nimea¨ma¨to¨n nimiavaruus varmistaa ma¨a¨rittelyjen ole- van uniikkeja (ka¨a¨nno¨syksiko¨n sisa¨isia¨). namespace { // nimea¨ma¨to¨n nimiavaruus unsigned long int viiteLaskuri = 0; void lisaaViiteLaskuria() { ... } } B.4. Muuttujat ja vakiot 425 3.4. Luokka- (class) ja tietuetyyppien (struct), nimiavaruuk- sien (namespace) seka¨ tyyppialiaksien (typedef) nimet al- kavat isolla alkukirjaimella. 3.5. Muuttujien, funktioiden ja ja¨senfunktioiden nimet alka- vat pienella¨ alkukirjaimella. Useammasta sanasta koostu- vat nimet kirjoitetaan yhteen ja loppupa¨a¨n sanat aloitetaan isolla alkukirjaimella (esimerkiksi: haeRaportinPaivays()). Poikkeus sa¨a¨nto¨o¨n on luokan rakentaja, jonka nimen on oltava ta¨sma¨lleen sama kuin luokan nimi. 3.6. Ja¨senmuuttujien nimen viimeinen merkki on alaviiva (tai ne erotellaan muista nimista¨ jollain muulla yhtena¨isella¨ nimea¨mistavalla) [aliluku 2.3.2 sivulla 61]. 3.7. Luokkamuuttujien nimen viimeinen merkki on alaviiva (tai ne erotellaan muista nimista¨ ja mahdollisesti myo¨s ja¨- senmuuttujista jollain muulla yhtena¨isella¨ tavalla) [alilu- ku 8.2.2 sivulla 250]. Kohdan 3.6 mukainen nimea¨minen, joka helpottaa luokka- muuttujien tunnistamista ohjelmakoodissa. 3.8. Vakiot kirjoitetaan kokonaan isoilla kirjaimilla ja sanojen erottimena ka¨yteta¨a¨n tarvittaessa alaviivaa. (“Vakion” ar- vo on aina sama. Jos esim. funktion paikallisen const- muuttujan arvo lasketaan ajoaikana ja voi vaihdella funk- tion kutsukerroilla, kannattaa se varmaan nimeta¨ tavalli- sen muuttujan tapaan.) unsigned int const PUSKURIN SUURIN KOKO = 1024; 4. Muuttujat ja vakiot 4.1. Muuttujien na¨kyvyysalue tulee suunnitella mahdollisim- man pieneksi (koodilohko, funktio, luokka tai nimiava- ruus). La¨hella¨ ka¨ytto¨paikkaa ma¨a¨ritellyt ja alustetut muuttujat lisa¨a¨- va¨t ohjelmakoodin selkeytta¨. 4.2. Jokainen muuttuja on ma¨a¨ritelta¨va¨ eri lauseessa. Muuttujan tyyppi, nimi, alustus ja sita¨ koskeva kommentti ovat usein yhdelle riville sopiva kokonaisuus. Sa¨a¨nto¨ myo¨s varmis- taa, etta¨ kohdan 5.4 mukaiset osoittimien ja viitteiden ma¨a¨ritte- lyt tulevat aina oikein. “char* p1, p2;” olisi luultavasti virhe, silla¨ nyt p1 on osoitin ja p2 merkkimuuttuja. B.4. Muuttujat ja vakiot 426 4.3. Ohjelmassa ei saa olla alustamattomia muuttujia. Jos muuttujalla ei ole ja¨rkeva¨a¨ alkuarvoa, sille annetaan jo- kin “laiton” alkuarvo (nolla, miinus yksi, tms.). Alustamatto- mat muuttujat ovat yksi suurimmista (satunnaisten) virhetoi- mintojen aiheuttajista ohjelmistoissa. Laittomaan arvoon alus- tetut muuttujat eiva¨t korjaa virheita¨, mutta tuovat ne testeissa¨ helpommin esille. 4.4. Luokan ja¨senmuuttujat alustetaan aina rakentajan alustus- listassa, jossa ja¨senmuuttujat luetellaan niiden esittelyja¨r- jestyksessa¨ [aliluku 3.4.1 sivulla 80]. 4.5. #define-ka¨skya¨ ei saa ka¨ytta¨a¨ ohjelman vakioiden ja “mak- rofunktioiden” ma¨a¨rittelyyn [aliluku 4.3 sivulla 105]. Nimetyt vakiot toteutetaan C++:ssa const-muuttujina ja makrot korvataan inline-funktioilla. 4.6. Ohjelmakoodin toimintaa ohjaavat raja-arvot ja muut va- kiot ma¨a¨ritella¨a¨n keskitetysti yhdessa¨ paikassa vakiomuut- tujiksi (numeeristen vakioiden ka¨ytto¨ ohjelmassa on kiel- lettya¨). Lyhyet merkitykselta¨a¨n varmasti pysyva¨t vakiot ku- ten esimerkiksi nollaosoitin (0) tai “kymmenen alkion sil- mukka” (-5 . . . 5) on kuitenkin usein selkea¨mpa¨a¨ kirjoit- taa suoraan ka¨ytto¨paikassa na¨kyviin. Kaikki numeeriset arvot tulisi ma¨a¨ritella¨ yhdessa¨ keskitetyssa¨ paikassa (otsikkotiedosto) selkea¨sti nimetyiksi vakiomuuttujik- si, joita ka¨yteta¨a¨n muualla ohjelmakoodissa. Vakioiden arvojen muuttaminen myo¨hemmin on ta¨llo¨in helppoa. Na¨in tulisi teh- da¨ kaikille arvoille, joita mahdollisesti halutaan muuttaa ohjel- man jatkokehityksessa¨. Jos tiedeta¨a¨n hyvin varmaksi, ettei jokin arvo muutu (nolla tai “ta¨ssa¨ laiteohjaimessa on kymmenen al- kion alustus”), niin arvon kirjoittaminen ilman vakiomuuttujan tuomaa epa¨suoruutta on perusteltua. 4.7. Totuusarvot ilmaistaan ISOC++:n tietotyypilla¨ bool, jonka mahdolliset arvot ovat false ja true. C-kielen “totuusarvot” nolla ja nollasta poikkeavat kokonaislu- vut ovat edelleen toimivia, mutta niiden ka¨ytto¨a¨ ei suositella ISOC++:ssa. 4.8. Kun tyyppi ma¨a¨ritella¨a¨n vakioksi, const-sanan paikka tyy- pin ma¨a¨rittelyssa¨ on itse tyyppinimen ja¨lkeen. Ta¨ma¨ suo- siteltava tyyli on yleistyma¨ssa¨ uusissa ohjelmissa. Aiem- min koodissa on ollut tapana kirjoittaa const ennen tyyp- pinimea¨. Ta¨rkeinta¨ on, etta¨ ka¨yta¨nto¨ on yhtena¨inen koko ohjelmassa. [aliluku 4.3 sivulla 105] B.5. Asemointi ja tyyli 427 int const VAKIO = 3; // Kokonaislukuvakio int const* ptr = &VAKIO; // vakio-osoitin eli osoitin vakioon int* const PTR2 = 0; // Osoitinvakio jota ei saa muuttaa 5. Asemointi ja tyyli 5.1. Yli tuhannen rivin tiedostojen ka¨sittely on hankalaa, joten niita¨ tulisi va¨ltta¨a¨. 5.2. Yhden (ja¨sen)funktion toteutuksen pituuden ei tulisi ylit- ta¨a¨ sataa rivia¨. 5.3. Pa¨a¨telaitteiden ka¨yta¨nno¨ksi on muodostunut 80 merkkia¨ levea¨ na¨ytto¨. Ta¨sta¨ johtuen yli 79 merkin pituiset rivit kan- nattaa jakaa useammalle riville. std::string const esimerkkijono = \"Katenointi \" \"on ominaisuus jolla ka¨a¨nta¨ja¨\" \" yhdista¨a¨ pera¨kka¨in olevat\" \" merkkijonoliteraalit yhdeksi.\\n\"; std::vector<unsigned long int> laskeTaulukkoVastaus( unsigned int pituus, std::map<unsigned long int, std::string> const& tiedot ); 5.4. Osoittimen ja viitteen merkki kuuluvat tyyppinimen yh- teyteen. Paivays* ptr = NULL; void tulostaPaivays( Paivays& paivaysOlio ); 5.5. Primitiiveja¨ if, else, while, for ja do seuraa aina koodiloh- ko, joka on sisennetty johdonmukaisesti va¨lilyo¨ntimerkeil- la¨. Sarkaimen (tabulator) ka¨ytto¨ ei ole suositeltavaa, koska erilaiset tekstieditorit ja katseluohjelmat na¨ytta¨va¨t sen sisennyksen eri tavoin ja voivat tuottaa lukukelvottoman lopputuloksen koodis- ta, joka alkupera¨isessa¨ kirjoitusympa¨risto¨ssa¨ on na¨ytta¨nyt sel- kea¨lta¨. 5.6. Koodilohkon alku- ({) ja loppumerkki (}) sijoitetaan omalle rivilleen samalle sarakkeelle (tai jollakin muulla yhtena¨i- sella¨ ja selkea¨lla¨ tavalla eroteltuna muusta koodista). 5.7. Jokaisessa switch-lauseessa on default-ma¨a¨re. B.6. C-kielen rakenteet C++:ssa 428 5.8. Jokainen switch-lauseen case-ma¨a¨reen toteutus on oma koodilohkonsa ja se pa¨a¨ttyy lauseeseen break (samalla to- teutuksella voi olla useita case-ma¨a¨reita¨). switch( merkki ) { case ’a’: case ’A’: { ... break; } default: { break; } } 6. C-kielen rakenteet C++:ssa 6.1. Ohjelmasta ei saa poistua funktiolla exit() tai abort(). Na¨ma¨ funktiot lopettavat koko ohjelman suorituksen ilman, etta¨ ohjelman kaikki oliot tuhoutuvat hallitusti (niiden purkajat ja¨a¨- va¨t suorittamatta). Oikea tapa on aina poistua funktiosta main() normaalisti (return-lauseella). 6.2. main()-funktion paluuarvo on aina int. ISOC++ -standardin ma¨a¨rittelema¨ ainoa mahdollinen paluuar- vo. 6.3. main()-funktiota ei saa kutsua, siita¨ ei saa ottaa funktio- osoitinta, sita¨ (sen nimea¨) ei saa kuormittaa eika¨ se saa olla inline eika¨ staattinen funktio — toisin sanoen funktioni- mea¨ main ei saa ka¨ytta¨a¨ kuten tavallista funktiota. ISOC++ -standardi ka¨sittelee main()-funktion erikoistapauksena (se ei siis ole tavallinen funktio), ja kielta¨a¨ na¨ma¨ ka¨ytto¨tavat. 6.4. goto-lausetta ei saa ka¨ytta¨a¨. Ohjelman normaalin toiminnan hallintaan on olemassa pa- rempia kontrollirakenteita ja virhetilanteissa “hyppa¨a¨minen muualle” toteutetaan poikkeusten avulla. 6.5. Ka¨yta¨ tietovirtoja (cin, cout, cerr ja clog) C-kielen IO-funk- tioiden sijaan. B.7. Tyyppimuunnokset 429 Tietovirtojen operaatiot tekeva¨t tyyppitarkistuksia, joita esimer- kiksi C-funktio printf() ei tee. 6.6. Funktioita malloc(), realloc() ja free() ei saa ka¨ytta¨a¨, vaan niiden sijasta ka¨yteta¨a¨n operaattoreita new ja delete [aliluku 3.2 sivulla 71] [aliluku 3.5 sivulla 85]. Uudet operaattorit kutsuvat olioiden dynaamisen ka¨sittelyn yh- teydessa¨ rakentajia ja purkajia, joista vanhat muistinvarausru- tiinit eiva¨t tieda¨ mita¨a¨n. 6.7. C:n kirjastoja ka¨ytetta¨essa¨ niiden esittelyt tulee ottaa ka¨yt- to¨o¨n ma¨a¨reella¨ extern \"C\". Vain ta¨lla¨ merkinna¨lla¨ varmistetaan yhteensopivuus C-kielisen kirjaston kutsumekanismin kanssa. extern \"C\" { #include <gtk/gtk.h> #include <curses.h> } 7. Tyyppimuunnokset 7.1. Mika¨a¨n osa ohjelmakoodista ei saa luottaa implisiittiseen tyyppimuunnokseen. 7.2. Ohjelmassa tulee ka¨ytta¨a¨ kielen uusia tyyppimuun- nosoperaattoreita (static cast, dynamic cast ja reinterpret cast) vanhempien tyyppimuunnosten si- jaan [aliluku 7.4.1 sivulla 228]. 7.3. Vakio-ominaisuutta ei saa poistaa tyyppimuunnoksella (Operaattoria const cast ei saa ka¨ytta¨a¨) [aliluku 7.4.1 si- vulla 228]. Vakio on suunnitteluvaiheessa ma¨a¨ra¨tty ominaisuus, jota ei pi- ta¨isi olla tarvetta poistaa ohjelmointivaiheessa. 8. Funktiot ja ja¨senfunktiot 8.1. Funktion parametrien nimet on annettava seka¨ esittelyssa¨ etta¨ ma¨a¨rittelyssa¨ ja niiden on oltava molemmissa samat. 8.2. Funktion olioparametrin va¨litykseen ka¨yteta¨a¨n (va- kio)viitetta¨ aina kun se on mahdollista [aliluku 4.3 sivulla 105] [aliluku 7.3 sivulla 224]. 8.3. Funktiolla ei saa olla ma¨a¨rittelema¨to¨nta¨ parametrilistaa (el- lipsis notation). B.9. Luokat ja oliot 430 Ma¨a¨rittelema¨to¨n parametrilista poistaa ka¨yto¨sta¨ kaikki ka¨a¨nta¨- ja¨n tekema¨t tarkistukset parametrinva¨lityksessa¨. 8.4. Funktion mahdolliset oletusparametrit ovat na¨kyvissa¨ esit- telyssa¨ eika¨ niita¨ saa lisa¨ta¨ ma¨a¨rittelyssa¨. 8.5. Funktion paluuarvo on aina ma¨a¨ritelta¨va¨. 8.6. Funktio ei saa palauttaa osoitinta tai viitetta¨ sen omaan pai- kalliseen dataan. Paikallinen data ei ole kielen ma¨a¨rittelyn mukaan ena¨a¨ ka¨ytet- ta¨vissa¨ kun funktiosta on palattu. 8.7. Julkisen rajapinnan ja¨senfunktio ei saa palauttaa olion ja¨- senmuuttujaan viitetta¨ eika¨ osoitinta (vakioviite ja vakio- osoitin ka¨y) [aliluku 4.2 sivulla 101]. 8.8. Ja¨senfunktiosta tehda¨a¨n vakioja¨senfunktio aina kun se on mahdollista [aliluku 4.3 sivulla 105]. 9. Luokat ja oliot 9.1. Oliot toteutetaan C++:n rakenteella class ja tietueet (esimer- kiksi linkitetyn listan yksi alkio) toteutetaan rakenteella struct. Na¨in erotellaan selkea¨sti tietueet (rakenteinen tietorakenne) olioista. 9.2. Tietueella saa olla rakentajia ja virtuaalinen toteutuksel- taan tyhja¨ purkaja, muttei muita ja¨senfunktioita. Tietueissa¨ sa¨ilyteta¨a¨n yhteenkuuluvaa tietoa, jolloin niiden ai- noa sallittu toiminnallisuus liittyy uuden tietueen alustukseen ja tietueen tuhoutumiseen (virtuaalipurkaja tarvitaan periyte- tyn tietueen oikean tuhoutumisen varmistamiseksi). 9.3. Luokan osat public, protected ja private esitella¨a¨n ta¨ssa¨ ja¨rjestyksessa¨ [aliluku 4.2 sivulla 101]. Luokan ka¨ytta¨ja¨a¨ kiinnostaa ainoastaan julkisessa rajapinnas- sa olevat palvelut, joten niille selkein paikka on heti luokkaesit- telyn alussa. 9.4. Luokan ja¨senmuuttujat ovat aina na¨kyvyydelta¨a¨n private [aliluku 4.2 sivulla 101]. 9.5. Luokan esittelyssa¨ ei koskaan ole toteutusta (toteutus mahdollisista inline-ja¨senfunktioista on otsikkotiedostos- sa luokkaesittelyn ja¨lkeen) [aliluku 2.3.3 sivulla 64]. B.10. Elinkaari 431 Esittelyssa¨ on nimensa¨ mukaisesti tarkoitus ainoastaan esitella¨ jokin rakenne. Sen toteutusyksityiskohdat ovat muualla eika¨ ra- kenteen ka¨ytta¨ja¨n tarvitse niita¨ na¨hda¨. Katso myo¨s kohdat 2.3.4 ja 2.3.5. 9.6. Luokalla ei pa¨a¨sa¨a¨nto¨isesti saa olla ysta¨va¨funktioita ei- ka¨ ysta¨va¨luokkia (friend-ominaisuus) [aliluku 8.4 sivul- la 258]. “Ysta¨vyys” rikkoo olioiden kapselointiperiaatetta vastaan. Ylei- nen poikkeus sa¨a¨nno¨lle ovat kiintea¨sti yhteenkuuluvan luok- karyppa¨a¨n keskina¨iset erioikeudet esimerkiksi saman ohjelma- komponentin sisa¨lla¨. C++:ssa tulostusoperaattori operator << on pakko toteuttaa luokasta erillisena¨ funktiona, jolloin siita¨ usein tehda¨a¨n ysta¨va¨funktio (jos suunnittelussa tulostusta pide- ta¨a¨n luokan julkisen rajapinnan operaationa). 9.7. Luokalle kuormitettavien operaattoreiden semantiikka on sa¨ilytetta¨va¨ (esimerkiksi operator + tekee aina yhteenlas- kua “vastaavan” toiminnon). 10. Elinkaari 10.1. Muistinhallinta 10.1.1. Dynaamisesti varatun muistin vapauttaminen on pa¨a¨- sa¨a¨nto¨isesti sen komponentin vastuulla (olio tai mo- duuli), joka varasi muistin [aliluku 3.5.4 sivulla 90]. 10.1.2. Operaattorilla delete saa vapauttaa ainoastaan muis- tin, joka on varattu operaattorilla new (deleten para- metri saa olla vain new:n palauttama osoitin tai arvo nolla) [aliluku 3.5.2 sivulla 89]. 10.1.3. Operaattorilla delete[ ] saa vapauttaa ainoastaan muistin, joka on varattu operaattorilla new[ ] [alilu- ku 3.5.3 sivulla 90]. 10.1.4. Jos delete-lauseen parametri on osoitinmuuttuja, jo- hon voi sijoittaa, sen arvoksi sijoitetaan nolla deletea¨ seuraavassa lauseessa [aliluku 3.5.4 sivulla 90]. 10.2. Olion luonti ja tuhoaminen 10.2.1. Dynaamisesti (operaattorilla new) luodun olion tuhou- tuminen varmistetaan auto ptr-rakenteella (tai muilla a¨lykka¨illa¨ osoitintoteutuksilla) aina kun sen ka¨ytto¨ on B.10. Elinkaari 432 mahdollista [aliluku 3.5 sivulla 85] [aliluku 11.7 sivul- la 383]. (Mutta esimerkiksi kohdan 12.3.4 sa¨a¨nto¨ esta¨a¨ auto ptr:n taltioinnin STL:n sa¨ilio¨ihin.) 10.2.2. Jokaiselle luokalle on ma¨a¨ritelta¨va¨ va¨hinta¨a¨n yksi ra- kentaja [aliluku 3.4.1 sivulla 80]. 10.2.3. Rakentajissa ja purkajissa ei saa kutsua virtuaalifunk- tioita [aliluku 6.5.7 sivulla 170]. 10.2.4. Rakentajissa ja purkajissa ei saa ka¨ytta¨a¨ globaaleja eika¨ staattisia olioita. Olio voi itse olla staattinen tai globaali eiva¨tka¨ sen ka¨ytta¨- ma¨t muut vastaavan tason oliot ole viela¨ va¨ltta¨ma¨tta¨ ole- massa alustettuna kun rakentajan suoritus alkaa. Vastaa- vasti purkajassa voidaan vahingossa viitata jo aiemmin tu- hottuun olioon. 10.2.5. Jos rakentajan toteutuksessa (toteutustiedostossa) tu- lostetaan, ennen luokan esittelya¨ (otsikkotiedosto) pi- ta¨a¨ ottaa ka¨ytto¨o¨n otsikkotiedosto <iostream>. Ainoastaan ta¨lla¨ tavalla voidaan varmistua siita¨, etta¨ tu- lostusolio (esim. cout) on olemassa ennen luokasta tehtya¨ oliota. 10.2.6. Yksiparametrinen rakentaja on merkitta¨va¨ explicit- ma¨a¨reella¨ [aliluku 7.4.2 sivulla 232], ellei se erityi- sesti ole tarkoitettu implisiittiseksi tyyppikonversiok- si. Poikkeuksena kopiorakentaja, jolle ta¨ta¨ ma¨a¨ret- ta¨ ei saa laittaa. 10.2.7. Itse tehty kantaluokan purkaja on aina public ja virtu- aalinen tai protected ja ei-virtuaalinen. [aliluku 6.5.5 sivulla 168]. 10.2.8. Purkajan tulee vapauttaa kaikki tuhoutumishetkella¨ olion vastuulla olevat resurssit [aliluku 3.4.2 sivul- la 83]. 10.3. Olion kopiointi 10.3.1. Luokalle esitella¨a¨n aina kopiorakentaja [aliluku 7.1.2 sivulla 206]. 10.3.2. Tarpeettoman kopiorakentajan ka¨ytto¨ esteta¨a¨n esittele- ma¨lla¨ se ilman toteutusta luokan private-rajapintaan [aliluku 7.1.2 sivulla 206]. B.11. Periytyminen 433 10.3.3. Periytetyn luokan kopiorakentajan tulee kutsua kanta- luokan kopiorakentajaa alustuslistassaan [aliluku 7.1.2 sivulla 206]. 10.4. Olion sijoitus 10.4.1. Jokaisessa luokassa esitella¨a¨n oma sijoitusoperaattori [aliluku 7.2.2 sivulla 216]. 10.4.2. Tarpeettoman sijoitusoperaattorin ka¨ytto¨ esteta¨a¨n esit- telema¨lla¨ se ilman toteutusta luokan private-osassa [aliluku 7.2.2 sivulla 216]. 10.4.3. Periytetyn luokan sijoitusoperaattorin tulee kutsua kantaluokan sijoitusoperaattoria [aliluku 7.2.2 sivul- la 216]. 10.4.4. Sijoitusoperaattorin tulee aina varautua “itseen sijoita- mista” (x = x) vastaan [aliluku 7.2.2 sivulla 216]. 10.4.5. Sijoitusoperaattorin paluuarvo on *this [aliluku 7.2.2 sivulla 216]. 11. Periytyminen 11.1. Ainoastaan public-periytymisen ka¨ytto¨ on sallittua mallin- tamaan olio-ohjelmoinnin periytymista¨ [aliluku 6.3 sivul- la 149]. 11.2. Periytyminen on staattinen “is-a”-suhde. “has-a” toteute- taan ja¨senmuuttujilla ja tyyppiriippumaton koodi malleilla [aliluku 6.3.4 sivulla 155]. 11.3. Moniperiytymista¨ saa ka¨ytta¨a¨ ainoastaan rajapinnan tai toiminnallisen yksiko¨n periytta¨miseen (interface ja mix- ing) [aliluku 6.3 sivulla 149]. Moniperiytyminen on monimutkainen ominaisuus, jota harvoin ka¨yteta¨a¨n oliosuunnittelussa. Rajapintojen periytta¨minen taas on paljon ka¨ytetty osa oliosuunnittelua. 11.4. Rajapintaluokan kaikki ja¨senfunktiot ovat puhtaita virtu- aalifunktioita (pure virtual) [aliluku 6.6 sivulla 172]. 11.5. Periytetyn luokan rakentajan alustuslistassa ta¨ytyy kutsua kantaluokan rakentajaa [aliluku 6.3.2 sivulla 152]. 11.6. Kantaluokan ma¨a¨rittelemiin virtuaalifunktioihin, jotka pe- ritetty luokka ma¨a¨rittelee uudelleen, liiteta¨a¨n avainsana B.12. Mallit 434 virtual myo¨s periytetyssa¨ luokassa [aliluku 6.5.5 sivul- la 168]. 11.7. Periytynytta¨ ei-virtuaalista ja¨senfunktiota ei saa ma¨a¨ritella¨ uudelleen [aliluku 6.5 sivulla 159]. 11.8. Periytynytta¨ virtuaalisen ja¨senfunktion oletusparametria ei saa ma¨a¨ritella¨ uudelleen. 12. Mallit 12.1. Mallin tyyppiparametreilleen asettamat vaatimukset tulee dokumentoida mallin esittelyn yhteydessa¨ [aliluku 9.5.4 sivulla 290]. 12.2. Mallin ka¨ytta¨jille tulisi tarvittaessa tarjota valmiiksi op- timoituja toteutuksia mallin erikoistusten avulla [alilu- ku 9.5.6 sivulla 295]. 12.3. STL 12.3.1. C-kielen vanhan taulukkotyypin sijaan tulee ka¨ytta¨a¨ STL:n sarjasa¨ilio¨ita¨ vector tai deque [aliluku 10.2 si- vulla 310] [aliluku A.3 sivulla 414]. Uudet rakenteet ovat helppoka¨ytto¨isempia¨. Jos jokin (van- han ohjelmakoodin) funktio tarvitsee esimerkiksi paramet- rikseen taulukon, sille voidaan antaa osoitin vektorin al- kuun (&v[0]). 12.3.2. STL:n sisa¨isesta¨ toiminnasta ei saa tehda¨ oletuksia. Ainoastaan ISOC++:n ma¨a¨rittelemia¨ ominaisuuksia saa hyo¨dynta¨a¨. 12.3.3. Ka¨ytetyille STL-sa¨ilio¨ille annetaan kuvaavat nimet tyyppialiaksen avulla. typedef std::map<std::string,unsigned int> Puhelinluettelo; 12.3.4. STL:n sa¨ilio¨o¨n saa tallettaa ainoastaan olioita, joilla on kopiorakentaja ja sijoitusoperaattori, jotka tuotta- vat uuden tilaltaan identtisen olion [aliluku 10.2 sivul- la 310]. 12.3.5. Ka¨ytetta¨essa¨ STL:n sa¨ilio¨ita¨ on varmistuttava siita¨, etta¨ ka¨ytettyjen operaatioiden pahimman tapauksen ajan- ka¨ytto¨ on riitta¨va¨n tehokasta ohjelmiston toiminnan kannalta [aliluku 10.1 sivulla 303]. B.13. Poikkeukset 435 12.3.6. STL:n sa¨ilio¨iden yhteydessa¨ tulee ka¨ytta¨a¨ STL:n algo- ritmeja omien toteutusten sijaan [aliluku 10.4 sivul- la 337]. STL:n algoritmit ovat luultavasti omia toteutuksia tehok- kaampia ja ne pystyva¨t tekema¨a¨n sisa¨isia¨ optimointeja. 12.3.7. STL:n sa¨ilio¨n alkioden la¨pika¨ynti toteutetaan iteraat- toreiden avulla (begin()–end() -va¨li) [aliluku 10.3 si- vulla 326]. Esimerkki funktiosta, joka ka¨y la¨pi kaikki sa¨ilio¨n alkiot riip- pumatta siita¨ mita¨ STL:n sa¨ilio¨ista¨ on ka¨ytetty parametrin toteutukseen: void tulostaPuhelinluettelo(Puhelinluettelo& tk) { Puhelinluettelo::const iterator alkio; for (alkio = tk.begin(); alkio != tk.end(); ++alkio) // Vertailu “alkio < tk.end()” ei toimi // kaikkilla sa¨ilio¨illa¨ (niiden iteraattoreilla) { std::cout << *alkio << std::endl; } } 12.3.8. Iteraattorin lisa¨a¨minen ja va¨henta¨minen suoritetaan etuliiteoperaattorilla (preincrement ja predecrement) [aliluku 10.3 sivulla 326]. Lausekkeen evaluoinnin ja¨lkeen suoritettava operaatio (postincrement) on tehottomampi, koska se joutuu palaut- tamaan kopion iteraattorin vanhasta arvosta. 12.3.9. Mita¨to¨itynytta¨ iteraattoria ei saa ka¨ytta¨a¨ (ainoastaan tuhoaminen ja uuden arvon sijoittaminen ovat sallit- tuja operaatioita) [aliluku 10.3 sivulla 326]. 13. Poikkeukset 13.1. Poikkeuksia saa ka¨ytta¨a¨ ainoastaan virhetilanteiden ilmai- semiseen [aliluku 11.4 sivulla 374]. Poikkeusten ka¨sittely on muita kontrollirakenteita raskaampaa, joten niita¨ ei kannata ka¨ytta¨a¨ ohjelmiston normaalin kontrollin ohjaamiseen. 13.2. Jos poikkeuska¨sittelija¨ ei pysty ta¨ysin toipumaan sieppaa- mastaan poikkeuksesta, se “heiteta¨a¨n” uudelleen ylemma¨l- le tasolle [aliluku 11.4.2 sivulla 377]. B.13. Poikkeukset 436 13.3. Pa¨a¨ohjelmasta ei saa vuotaa poikkeuksia ulos [alilu- ku 11.4.2 sivulla 377]. Useat ajoympa¨risto¨t antavat eritta¨in epa¨selvia¨ ilmoituksia oh- jelmasta ulos vuotavien poikkeusten yhteydessa¨. 13.4. Luokan purkajasta ei saa vuotaa poikkeuksia [aliluku 11.5 sivulla 380]. 13.5. Pa¨a¨sa¨a¨nto¨isesti funktioiden poikkeusma¨a¨relistaa ei saa ka¨ytta¨a¨. Vain jos funktion kaikissa ka¨ytto¨tilanteissa var- masti tiedeta¨a¨n siita¨ mahdollisesti vuotavat poikkeukset, voi poikkeukset luetella funktioesittelyn poikkeuslistassa. [aliluku 11.6 sivulla 382]. Kirjallisuutta 437 Kirjallisuutta [Abadi ja Cardelli, 1996] Martı´n Abadi ja Luca Cardelli. A Theory of Objects. Monographs in Computer Science. Springer-Verlag, 1996. ISBN 0-387-94775-2. [Abrahams, 2003] David Abrahams. Exception-Safety in Generic Components. http://www.boost.org/more/generic_exception_ safety.html, toukokuu 2003. [Adams, 1991] James Adams. Insino¨o¨rin maailma. Art House Oy, 1991. ISBN 951-884-163-2. Suomentanut Kimmo Pietila¨inen. [Aikio ja Vornanen, 1992] Annukka Aikio ja Rauni Vornanen, toim. Uusi sivistyssanakirja. Otava, 11. painos, 1992. ISBN 951-1-11365- 8. [Alexandrescu, 2001] Andrei Alexandrescu. Modern C++ Design. C++ In-Depth Series. Addison-Wesley, 2001. ISBN 0-201-70431-5. http://www.moderncppdesign.com/. [Alexandrescu, 2003] Andrei Alexandrescu. Modern C++ Design, Chapter 7 — Smart Pointers. http://www.aw.com/samplechapter/ 0201704315.pdf, toukokuu 2003. Kirjan [Alexandrescu, 2001] lu- ku 7 verkossa julkaistuna. [Arnold ja Gosling, 1996] Ken Arnold ja James Gosling. The Java Pro- gramming Language. Addison-Wesley, 1996. ISBN 0-201-63455-4. [Beck ja Cunningham, 1989] Kent Beck ja Ward Cunningham. A Lab- oratory for Teaching Object-Oriented Thinking. Teoksessa OOP- SLA’89 – Conference on Object-Oriented Programming Systems, Languages and Applications. ACM SIGPLAN, 1989. Kirjallisuutta 438 [Binder, 1999] Robert V. Binder. The Percolation Pattern – Tech- niques for Implementing Design by Contract in C++. C++ Report, sivut 38–44, toukokuu 1999. [Booch ja muut, 1999] Grady Booch, James Rumbaugh ja Ivar Jacob- son. The Uniﬁed Modeling Language User Guide. Addison-Wesley, 1999. ISBN 0-201-57168-4. [Booch, 1987] Grady Booch. Software Engineering with Ada, 2nd edi- tion. The Benjamin/Cummings Publishing Company, 1987. ISBN 0-8053-0604-8. [Booch, 1991] Grady Booch. Object-Oriented Design with Applica- tions. The Benjamin/Cummings Publishing Company, 1991. ISBN 0-8053-0091-0. [Boost, 2003] C++ Boost library. http://www.boost.org/, helmikuu 2003. [Bo¨szo¨rme´nyi ja Weich, 1996] La´szlo´ Bo¨szo¨rme´nyi ja Carsten Weich. Programming in Modula-3 →An Introduction in Programming with Style. Springer-Verlag, 1996. ISBN 3-540-57912-5. [Budd, 2002] Timothy Budd. An Introduction to Object-oriented Pro- gramming, 3rd edition. Addison-Wesley, 2002. ISBN 0-201-76031- 2. [Carroll, 1865] Lewis Carroll. Alice’s Adventures in Wonderland. Kirjassa The Complete Works of Lewis Carroll [1988]. [Carroll, 1988] Lewis Carroll. The Complete Works of Lewis Carroll. The Penguin Group, 1988. ISBN 0-14-010542-5. [Clements ja Parnas, 1986] Paul Clements ja David Parnas. A Ratio- nal Design Process: How and Why to Fake It. IEEE Transactions on Software Engineering, 12(2):251–257, helmikuu 1986. [Coplien, 1992] James O. Coplien. Advanced C++ Programming Styles and Idioms. Addison-Wesley, 1992. ISBN 0-201-54855-0. [Coplien, 1999] James O. Coplien. Multi-Paradigm Design for C++. Addison-Wesley, 1999. ISBN 0-201-82467-1. Kirjallisuutta 439 [Coplien, 2000] James Coplien. C++ idioms patterns. Teoksessa Neil Harrison, Brian Foote ja Hans Rohnert, toim., Pattern Languages of Program Design 4, luku 10, sivut 167–197. Addison-Wesley, 2000. ISBN 0-201-43304-4. [Cormen ja muut, 1990] Thomas H. Cormen, Charles E. Leiserson ja Ronald L. Rivest. Introduction to Algorithms. The MIT Press, 1990. ISBN 0-262-53091-0. [Czarnecki ja Eisenecker, 2000] Krysztof Czarnecki ja Ulrich Eise- necker. Generative Programming: Methods, Tools, and Applications. Addison-Wesley, 2000. ISBN 0-201-30977-7. [Dijkstra, 1972] Edsger W. Dijkstra. The Humble Programmer (1972 Turing Award Lecture). Communications of the ACM (CACM), 15(10):859–866, lokakuu 1972. [Doxygen, 2005] Doxygen homepage. http://www.doxygen.org/, huhtikuu 2005. [Driesen ja Ho¨lzle, 1996] Karel Driesen ja Urz Ho¨lzle. The Direct Cost of Virtual Function Calls in C++. Teoksessa Carrie Wilpolt, toim., OOPSLA’96 – Conference on Object-Oriented Programming Systems, Languages and Applications. ACM SIGPLAN, Addison- Wesley, lokakuu 1996. [Egan, 1995] Greg Egan. Axiomatic. Millennium, 1995. ISBN 1- 85798-416-1. [Fomitchev, 2001] Boris Fomitchev. STLport. STLport home page http://www.stlport.org/, elokuu 2001. [Gaiman ja muut, 1994] Neil Gaiman, Jill Thompson ja Vince Locke. Brief Lives. Sandman-albumi. DC Comics, 1994. ISBN 1-56389- 137-9. [Gamma ja muut, 1995] Erich Gamma, Richard Helm, Ralph Johnson ja John Vlissides. Design Patterns – Elements of Reusable Object- Oriented Software. Addison-Wesley, 1995. ISBN 0-201-63361-2. [Gillam, 1997] Richard Gillam. The Anatomy of the Assignment Op- erator. C++ Report, sivut 15–23, marraskuu–joulukuu 1997. Kirjallisuutta 440 [Haikala ja Ma¨rija¨rvi, 2002] Ilkka Haikala ja Jukka Ma¨rija¨rvi. Ohjel- mistotuotanto. Suomen ATK-kustannus Oy, 2002. ISBN 952-14- 0486-8. [Heller, 1961] Joseph Heller. CATCH-22. Gummerus, 1961. ISBN 951-20-4558-3. Suomentanut Markku Lahtela. [ISO, 1998] ISO/IEC. International Standard 14882 – Programming Languages – C++, 1. painos, syyskuu 1998. [Jacobson ja muut, 1994] I. Jacobson, M. Christerson, P. Johnsson ja G. O¨vergaard. Object Oriented Software Engineering, A Use Case Driven Approach (Revised Printing). Addison-Wesley, 1994. ISBN 0-201-54435-0. [JavaBeans, 2001] JavaBeans Component Architecture. http://java. sun.com/beans/, elokuu 2001. [Josuttis, 1999] Nicolai M. Josuttis. The C++ Standard Library. Addison-Wesley, 1999. ISBN 0-201-37926-0. [Kerninghan ja Ritchie, 1988] Brian W. Kerninghan ja Dennis M. Ritchie. The C Programming Language, 2nd edition. Prentice Hall Software Series, 1988. ISBN 0-13-110370-9. [Koenig ja Moo, 2000] Andrew Koenig ja Barbara Moo. Accelerated C++. Addison-Wesley, 2000. ISBN 0-201-70353-X. [Koskimies, 2000] Kai Koskimies. Oliokirja. Suomen ATK-kustannus Oy, 2000. ISBN 951-762-720-3. [Krohn, 1992] Leena Krohn. Matemaattisia olioita. WSOY, 1992. ISBN 951-0-18407-1. [Lem, 1965] Stanislaw Lem. Kyberias, luku Ensimma¨inen matka (A) eli Trurlin elektrubaduuri. Kirjayhtyma¨, 1965. ISBN 951-26-2955- 0. Suomentanut Matti Kannosto. [Liberty, 1998] Jesse Liberty. Beginning Object-Oriented Analysis and Design with C++. Wrox Press Ltd, 1998. ISBN 1-861001-33-9. [Linnaeus, 1748] Carl Linnaeus. Systema Naturae. Holmiae, 1748. Kirjallisuutta 441 [Lions, 1996] Prof. J. L. Lions. ARIANE 5 ﬂight 501 failure. Raport- ti, Inquiry Board, heina¨kuu 1996. http://www.cafm.sbu.ac.uk/cs/ people/jpb/teaching/ethics/ariane5anot.html. [Lippman ja Lajoie, 1997] Stanley B. Lippman ja Jose´e Lajoie. C++ Primer 3rd edition. Addison-Wesley, 1997. ISBN 0-201-82470-1. [Lippman, 1996] Stanley B. Lippman. Inside the C++ Object Model. Addison-Wesley, 1996. ISBN 0-201-83454-5. [McConnell, 1993] Steve McConnell. Code Complete. Microsoft Press, 1993. ISBN 1-55615-484-4. [Meyer, 1997] Bertrand Meyer. Object-Oriented Software Construc- tion, 2nd edition. Prentice Hall, 1997. ISBN 0-13-629155-4. [Meyers, 1996] Scott Meyers. More Effective C++. Addison-Wesley, 1996. ISBN 0-201-63371-X. Saatavilla elektronisessa muodossa osana teosta [Meyers, 1999]. [Meyers, 1998] Scott Meyers. Effective C++ 2nd edition. Addison- Wesley, 1998. ISBN 0-201-92488-9. Saatavilla elektronisessa muo- dossa osana teosta [Meyers, 1999]. [Meyers, 1999] Scott Meyers. Effective C++ CD. Addison-Wesley, 1999. ISBN 0-201-31015-5. CD:lla¨ julkaistu elektroninen kirja. [OMG, 2002a] OMG. UML Resource Page. http://www.omg.org/ technology/uml/, syyskuu 2002. [OMG, 2002b] OMG. What Is OMG-UML and Why Is It Impor- tant? OMG Press Release, http://www.omg.org/gettingstarted/ what_is_uml.htm, elokuu 2002. [Pirsig, 1974] Robert M. Pirsig. ZEN ja moottoripyo¨ra¨n kunnossapito. WSOY, 1974. ISBN 951-0-19300-3. Suomentanut Leena Tammi- nen. [Pratchett, 1987] Terry Pratchett. Equal Rites. Corgi, 1987. ISBN 0- 552-13105-9. [Roszak, 1992] Theodore Roszak. Konetiedon kritiikki. Art House Oy, 1992. ISBN 951-884-085-7. Suomentanut Maarit Tillman. Kirjallisuutta 442 [Rumbaugh ja muut, 1991] James Rumbaugh, Michael Blaha, Wil- liam Premerlani, Frederick Eddy ja William Lorensen. Object- Oriented Modeling and Design. Prentice-Hall, 1991. ISBN 0-13- 630054-5. [Rumbaugh ja muut, 1999] James Rumbaugh, Ivar Jacobson ja Gra- dy Booch. The Uniﬁed Modeling Language Reference Manual. Addison-Wesley, 1999. ISBN 0-201-30998-X. [Sethi, 1996] Ravi Sethi. Programming Languages, Concepts & Con- structs, 2nd edition. Addison-Wesley, 1996. ISBN 0-201-59065-4. [SETI, 2001] SETI Newsletter: Result veriﬁcation. http: //setiathome.ssl.berkeley.edu/newsletters/newsletter8.html, heina¨kuu 2001. [Shakespeare, 1593] William Shakespeare. The Tragedy of Titus An- dronicus. Kirjassa The Complete Works of William Shakespeare [1994]. Vuosiluku summittainen. [Shakespeare, 1994] William Shakespeare. The Complete Works of William Shakespeare. Wordsworth Editions Ltd, 1994. ISBN 1- 85326-810-0. [Stroustrup, 1994] Bjarne Stroustrup. The Design and Evolution of C++. Addison-Wesley, 1994. ISBN 0-201-54330-3. [Stroustrup, 1997] Bjarne Stroustrup. The C++ Programming Language 3rd edition. Addison-Wesley, 1997. ISBN 0-201-88954-4. Teoksesta on myo¨s suomennos [Stroustrup, 2000]. [Stroustrup, 2000] Bjarne Stroustrup. C++-ohjelmointi. Teknolit, 2000. ISBN 951-846-026-4. Suomennos teoksesta [Stroustrup, 1997]. [Sudo, 1999] Philip Toshio Sudo. ZEN Computer. Simon & Schuster New York, 1999. ISBN 0-684-85410-4. [Sun Microsystems, 2005] Sun Microsystems. Javadoc tool home pa- ge. http://java.sun.com/j2se/javadoc/, huhtikuu 2005. [Sutter, 1999] Herb Sutter. When Is a Container Not a Container? C++ Report, 11(5):60–64, toukokuu 1999. Kirjallisuutta 443 [Sutter, 2000] Herb Sutter. Exceptional C++. C++ In-Depth Series. Addison-Wesley, 2000. ISBN 0-201-61562-2. [Sutter, 2002a] Herb Sutter. A Pragmatic Look at Exception Speciﬁca- tions – The C++ feature that wasn’t. C/C++ Users Journal, 20(7):59–64, heina¨kuu 2002. [Sutter, 2002b] Herb Sutter. “Export” Restrictions, Part 1. C/C++ Users Journal, 20(9):50–55, syyskuu 2002. [Sutter, 2002c] Herb Sutter. More Exceptional C++. C++ In-Depth Series. Addison-Wesley, 2002. ISBN 0-201-70434-X. [Sutter, 2003] Herb Sutter. Exception Safety and Exception Speci- ﬁcations: Are They Worth It? http://www.gotw.ca/gotw/082.htm, maaliskuu 2003. [The Doors FAQ, 2002] THE DOORS Frequently Asked Questions (FAQ). http://classicrock.about.com/library/artists/bldoors_ faq.htm, marraskuu 2002. [Vandevoorde ja Josuttis, 2003] David Vandevoorde ja Nicolai M. Jo- suttis. C++ Templates — The Complete Guide. Addison-Wesley, 2003. ISBN 0-201-73484-2. [Wikstro¨m, 1987] ˚Ake Wikstro¨m. Functional Programming Using Standard ML. Prentice Hall, 1987. ISBN 0-13-331661-0. [Wilkinson, 1995] Nancy M. Wilkinson. Using CRC Cards, An In- formal Approach to Object-Oriented Development. SIGS BOOKS, 1995. ISBN 0-13-374679-8. Englanninkieliset termit 444 Englanninkieliset termit abstract base class . . . . . . . . abstrakti kantaluokka, s. 144 actor . . . . . . . . . . . . . . . . . . . . . . ka¨ytta¨ja¨rooli, s. 123 adaptive system . . . . . . . . . . . mukautuva ja¨rjestelma¨, s. 351 aggregate . . . . . . . . . . . . . . . . . kooste, s. 134 allocator . . . . . . . . . . . . . . . . . . varain, s. 305 ambiguous . . . . . . . . . . . . . . . . moniselitteinen, s. 181 amortized constant com- plexity . . . . . . . . . . . . . . . . . . . . amortisoidusti vakioaikainen te- hokkuus, s. 310 ancestor . . . . . . . . . . . . . . . . . . esi-isa¨, s. 144 assignment operator . . . . . . sijoitusoperaattori, s. 216 associative container . . . . . . assosiatiivinen sa¨ilio¨, s. 317 attribute . . . . . . . . . . . . . . . . . . attribuutti, s. 122 base class . . . . . . . . . . . . . . . . . kantaluokka, s. 144 basic (exception) guarantee perustakuu, s. 390 bidirectional iterator . . . . . . kaksisuuntainen iteraattori, s. 331 bitset . . . . . . . . . . . . . . . . . . . . . . bittivektori, s. 325 bottom-up . . . . . . . . . . . . . . . . . kokoava jaottelu, s. 118 bridge . . . . . . . . . . . . . . . . . . . . . silta, s. 274 call by value . . . . . . . . . . . . . . arvonva¨litys, s. 224 call-through function . . . . . . la¨pikutsufunktio, s. 180 catch (exceptions) . . . . . . . . . siepata, s. 374 class . . . . . . . . . . . . . . . . . . . . . . luokka, s. 60 Englanninkieliset termit 445 class deﬁnition . . . . . . . . . . . . luokan esittely, s. 61 class hierarchy . . . . . . . . . . . . luokkahierarkia, s. 144 class invariant . . . . . . . . . . . . luokkainvariantti, s. 246 class object . . . . . . . . . . . . . . . luokkaolio, s. 249 class template . . . . . . . . . . . . . luokkamalli, s. 287 class template partial spe- cialization . . . . . . . . . . . . . . . . luokkamallin osittaiserikoistus, s. 297 closure . . . . . . . . . . . . . . . . . . . . sulkeuma, s. 343 commonality and variabil- ity analysis . . . . . . . . . . . . . . . . pysyvyys- ja vaihtelevuusanalyy- si, s. 264 compile-time complexity . . ka¨a¨nno¨saikainen tehokkuus, s. 310 component . . . . . . . . . . . . . . . . komponentti, s. 38 composite . . . . . . . . . . . . . . . . . kokoelma, s. 271 composite aggregate . . . . . . . muodostuminen (UML), s. 134 const . . . . . . . . . . . . . . . . . . . . . . vakio, s. 105 constant complexity . . . . . . . vakioaikainen tehokkuus, s. 310 constructor . . . . . . . . . . . . . . . . rakentaja, s. 80 container . . . . . . . . . . . . . . . . . sa¨ilio¨, s. 304 container adaptor . . . . . . . . . sa¨ilio¨sovitin, s. 325 conversion member func- tion . . . . . . . . . . . . . . . . . . . . . . . muunnosja¨senfunktio, s. 236 copy constructor . . . . . . . . . . kopiorakentaja, s. 206 cursor . . . . . . . . . . . . . . . . . . . . . kohdistin, s. 273 data member . . . . . . . . . . . . . . ja¨senmuuttuja, s. 61 deep copy . . . . . . . . . . . . . . . . . syva¨kopiointi, s. 205 default constructor . . . . . . . . oletusrakentaja, s. 82 derived class . . . . . . . . . . . . . . aliluokka, s. 144 descendant . . . . . . . . . . . . . . . ja¨lkela¨inen, s. 144 Design By Contract . . . . . . . . sopimussuunnittelu, s. 240 design pattern . . . . . . . . . . . . . suunnittelumalli, s. 267 destructor . . . . . . . . . . . . . . . . . purkaja, s. 83 double-ended queue . . . . . . . pakka, s. 315 Englanninkieliset termit 446 dynamic binding . . . . . . . . . . dynaaminen sitominen, s. 161 encapsulation . . . . . . . . . . . . . kapselointi, s. 33 exception . . . . . . . . . . . . . . . . . poikkeus, s. 367 exception guarantee . . . . . . . poikkeustakuu, s. 389 exception handler . . . . . . . . . poikkeuska¨sittelija¨, s. 374 exception neutrality . . . . . . . poikkeusneutraalius, s. 392 exception speciﬁcation . . . . poikkeusma¨a¨re, s. 382 forward declaration . . . . . . . ennakkoesittely, s. 112 forward iterator . . . . . . . . . . . eteenpa¨in-iteraattori, s. 331 framework . . . . . . . . . . . . . . . . sovelluskehys, s. 200 function object . . . . . . . . . . . . funktio-olio, s. 340 function object adaptor . . . . funktio-oliosovitin, s. 347 function pointer . . . . . . . . . . . funktio-osoitin, s. 341 function template . . . . . . . . . funktiomalli, s. 286 function try block . . . . . . . . . funktion valvontalohko, s. 396 functor . . . . . . . . . . . . . . . . . . . . funktio-olio, s. 340 garbage collection . . . . . . . . . roskienkeruu, s. 71 generalization . . . . . . . . . . . . . yleista¨minen, s. 147 generic algorithm . . . . . . . . . geneerinen algoritmi, s. 304 generic programming . . . . . geneerinen ohjelmointi, s. 291 genericity . . . . . . . . . . . . . . . . . yleiska¨ytto¨isyys, s. 264 getter . . . . . . . . . . . . . . . . . . . . . anna-ja¨senfunktio, s. 103 header . . . . . . . . . . . . . . . . . . . . otsikkotiedosto, s. 39 hide . . . . . . . . . . . . . . . . . . . . . . . peitta¨a¨, s. 168 identity . . . . . . . . . . . . . . . . . . . identiteetti, s. 58 inheritance . . . . . . . . . . . . . . . periytyminen, s. 144 inheritance hierarchy . . . . . periytymishierarkia, s. 144 initialization list . . . . . . . . . . alustuslista, s. 81 input iterator . . . . . . . . . . . . . . syo¨tto¨iteraattori, s. 329 Englanninkieliset termit 447 insert iterator . . . . . . . . . . . . . lisa¨ys-iteraattori, s. 336 inserter . . . . . . . . . . . . . . . . . . . lisa¨ys-iteraattori, s. 336 instantiation . . . . . . . . . . . . . . instantiointi, s. 285 interface class . . . . . . . . . . . . . rajapintaluokka, s. 190 invalid iterator . . . . . . . . . . . . kelvoton iteraattori, s. 334 invalidate (iterator) . . . . . . . mita¨to¨ida¨, s. 334 invariant . . . . . . . . . . . . . . . . . . invariantti, s. 243 iterator . . . . . . . . . . . . . . . . . . . . iteraattori, s. 304 iterator adaptor . . . . . . . . . . . iteraattorisovitin, s. 336 key . . . . . . . . . . . . . . . . . . . . . . . . avain, s. 317 linear complexity . . . . . . . . . lineaarinen tehokkuus, s. 310 list . . . . . . . . . . . . . . . . . . . . . . . . lista, s. 315 logarithmic complexity . . . . logaritminen tehokkuus, s. 310 map . . . . . . . . . . . . . . . . . . . . . . assosiaatiotaulu, s. 321 member function . . . . . . . . . . ja¨senfunktio, s. 64 member function template ja¨senfunktiomalli, s. 289 metaclass . . . . . . . . . . . . . . . . . metaluokka, s. 249 metafunction . . . . . . . . . . . . . . metafunktio, s. 352 metaprogram . . . . . . . . . . . . . metaohjelma, s. 349 metaprogramming . . . . . . . . metaohjelmointi, s. 349 method . . . . . . . . . . . . . . . . . . . metodi, s. 64 minimal (exception) guar- antee . . . . . . . . . . . . . . . . . . . . . minimitakuu, s. 390 module . . . . . . . . . . . . . . . . . . . moduuli, s. 32 multimap . . . . . . . . . . . . . . . . . assosiaatiomonitaulu, s. 322 multiple inheritance . . . . . . moniperiytyminen, s. 175 multiset . . . . . . . . . . . . . . . . . . . monijoukko, s. 319 namespace . . . . . . . . . . . . . . . . nimiavaruus, s. 41 nothrow (exception) guar- antee . . . . . . . . . . . . . . . . . . . . . nothrow-takuu, s. 392 Englanninkieliset termit 448 order of growth . . . . . . . . . . . . kertaluokka, s. 309 output iterator . . . . . . . . . . . . tulostus-iteraattori, s. 330 overloading . . . . . . . . . . . . . . . kuormittaminen, s. 81 parent class . . . . . . . . . . . . . . . kantaluokka, s. 144 pattern language . . . . . . . . . . mallikieli, s. 268 postcondition . . . . . . . . . . . . . ja¨lkiehto, s. 241 precondition . . . . . . . . . . . . . . esiehto, s. 241 pure virtual function . . . . . . puhdas virtuaalifunktio, s. 172 quadratic complexity . . . . . . nelio¨llinen tehokkuus, s. 310 random access iterator . . . . hajasaanti-iteraattori, s. 331 range . . . . . . . . . . . . . . . . . . . . . va¨li, s. 329 re-usability . . . . . . . . . . . . . . . . uudelleenka¨ytetta¨vyys, s. 264 reference . . . . . . . . . . . . . . . . . . viite, s. 411 reference copy . . . . . . . . . . . . viitekopiointi, s. 203 reﬂection . . . . . . . . . . . . . . . . . . reﬂektio, s. 349 repeated multiple inheri- tance . . . . . . . . . . . . . . . . . . . . . toistuva moniperiytyminen, s. 186 replicated multiple inheri- tance . . . . . . . . . . . . . . . . . . . . . erotteleva moniperiytyminen, s. 186 responsibility . . . . . . . . . . . . . vastuualue, s. 38 reverse iterator . . . . . . . . . . . . ka¨a¨nteis-iteraattori, s. 336 scope resolution operator . . na¨kyvyystarkenninoperaattori, s. 44 sequence . . . . . . . . . . . . . . . . . . sarja, s. 312 sequence diagram . . . . . . . . . tapahtumasekvenssi, s. 138 set . . . . . . . . . . . . . . . . . . . . . . . . joukko, s. 318 setter . . . . . . . . . . . . . . . . . . . . . . aseta-ja¨senfunktio, s. 103 shallow copy . . . . . . . . . . . . . . matalakopiointi, s. 204 shared aggregate . . . . . . . . . . jaettu kooste (UML), s. 134 Englanninkieliset termit 449 shared multiple inheritance yhdista¨va¨ moniperiytyminen, s. 187 slicing . . . . . . . . . . . . . . . . . . . . viipaloituminen, s. 210 smart pointer . . . . . . . . . . . . . a¨lyka¨s osoitin, s. 387 software crisis . . . . . . . . . . . . . ohjelmistokriisi, s. 27 specialization . . . . . . . . . . . . . erikoistaminen, s. 147 state machine . . . . . . . . . . . . . tilakone, s. 139 static data member . . . . . . . . luokkamuuttuja, s. 250 static member function . . . . luokkafunktio, s. 252 static metaprogramming . . ka¨a¨nno¨saikainen metaohjelmoin- ti, s. 350 stream iterator . . . . . . . . . . . . virtaiteraattori, s. 336 strong (exception) guaran- tee . . . . . . . . . . . . . . . . . . . . . . . . vahva takuu, s. 391 subclass . . . . . . . . . . . . . . . . . . . aliluokka, s. 144 superclass . . . . . . . . . . . . . . . . kantaluokka, s. 144 template . . . . . . . . . . . . . . . . . . malli, s. 283 template metaprogramming template-metaohjelmointi, s. 350 template specialization . . . . mallin erikoistus, s. 295 temporary object . . . . . . . . . . va¨liaikaisolio, s. 225 throw (exceptions) . . . . . . . . . heitta¨a¨, s. 374 top-down . . . . . . . . . . . . . . . . . . osittava jaottelu, s. 118 try-block . . . . . . . . . . . . . . . . . . valvontalohko, s. 374 type cast . . . . . . . . . . . . . . . . . . tyyppimuunnos, s. 227 unnamed namespace . . . . . nimea¨ma¨to¨n nimiavaruus, s. 49 use case . . . . . . . . . . . . . . . . . . . ka¨ytto¨tapaus, s. 123 valid iterator . . . . . . . . . . . . . . kelvollinen iteraattori, s. 334 vector . . . . . . . . . . . . . . . . . . . . . vektori, s. 312 virtual function . . . . . . . . . . . virtuaalifunktio, s. 159 Hakemisto 450 Hakemisto :: 42 ::Pari 289 ::annaEka 289, 296, 297 ::summaa 290 ˜Liikkuva 195 abort 369 { abstract } (UML) 144 abstrahoida 29 abstrahointi 63 abstrakti 29 abstrakti kantaluokka 144, 172–175, 193 abstraktio 29 abstraktiotaso 30, 202, 379 Adams, James 116 aikaaJaljella 104 ajoaikainen indeksointitarkastus 416 ja¨senfunktion valinta katso “dynaaminen sitominen” toteutuksen valinta 276 tyyppitarkastus 164–167, 231, 284 virheilmoitus 146, 303 <algorithm> 338 aliluokka 144 alkuiteraattori 333 alustaminen ja¨senmuuttujan 81 olion katso “rakentaja” Alustus 73 alustuslista 81 aliluokan rakentajan 153 amortisoidusti vakioaikainen tehokkuus 310, 314 analysis paralysis 141 ankh 69 anna-ja¨senfunktio 103 annaArvo 294 annaEka 288, 293, 296, 297 annaLuku 374 annaNimi 157 annaPaiva 107, 255, 257 annaPalautusPvm 262 arkkitehtuurimalli 267 arvonva¨litys katso “arvoparametri”, 224, 291 arvoparametri 224–227 aseta-ja¨senfunktio 103 Assert 246 assert 245 Assert_toteutus 247 assosiaatio 132–133 assosiaatiomonitaulu (STL) 322 Hakemisto 451 assosiaatiotaulu (STL) 321–322 assosiatiivinen sa¨ilio¨ (STL) 317–318 at 243, 416 at 313, 313, 315 attribuutti 55, 61, 122, 126, 128 auto_ptr katso “automaattiosoitin” automaattiosoitin 383–388 ja STL 387 ja taulukot 387 kopiointi 385–387 omistaminen 384 paluuarvona 385 rajoitukset 387–388 sijoitus 385–387 avain (STL) 317 back 313, 313, 315, 317, 326 back_inserter 336 bad_alloc 87 bad_cast 165 begin 435 begin 333 <bitset> 325 bitset 325 bittivektori (STL) 325 Carroll, Lewis 275 <cassert> 245 catch 87, 395, 397 catch 375 catch (...) 378 CATCH-22 201 Cheshire cat 275 class 60, 285 clear 312, 313 clone 205 commit or rollback 391 const 105 -sanan paikka 106 const_cast 230–231 const_iterator 332 copy 338 copy (Smalltalk) 205 correctness formula 241 count 320 CRC-kortti 124–126 <cstdlib> 46 <cstring> 46 Death 69 deck 315 deepCopy 206 delegointi 277 delete 89–90 delete[] 90 <deque> 315 deque 315 Design By Contract 240 Dijkstra, Edsger W. 28 Dijkstra, Edsger Wybe 20 Doors 240 double 237 Dreaded Diamond of Death 190 dualismi 58 dynaaminen elinkaari 79 automaattiosoitin 384 ja poikkeukset 380 ja taulukot 90 olion luominen 86–89 olion luominen ja tuhoaminen 85–86 olion tuhoaminen 89–90 omistusvastuu 91 Hakemisto 452 vaarat 86 virheiden va¨ltta¨minen 90–94 dynaaminen muistinhallinta 85–86 dynaaminen sitominen 147, 160–164 hinta 169–170 dynamic_cast 164, 231 ekskursio periytyminen 136 poikkeukset 87 Elain::liiku 174 elektrubaduuri 26 elinkaari C++ 76–79 dynaaminen 79 Java 74–75 Modula-3 72 olion 71–72 Smalltalk 74 staattinen 77–79 empty 313, 325, 326 end 333 ennakkoesittely 112–113 envelope/letter 275 equal_range 322 erase 312, 313, 318 erikoisiteraattori 336 erikoistaminen 147 erikoistus funktiomallin 296 luokkamallin 296 luokkamallin osittais- 297 vektorin 324 erotteleva moniperiytyminen 186 esi-isa¨ 144 esiehto 241 esim 92 esim2 93 esittely ennakko- 112 ja¨senfunktio 64 luokka 60 estetiikka 116 eteenpa¨in-iteraattori 331 exit 428 explicit 235 export 299 f 293 finalize (Java) 74 finally (Java) 380 find 318, 322, 339 ﬂavours 178 flip 325 for 314, 415 for_each 339 friend 260 front 313, 313, 315, 317, 326 front_inserter 336 funktio-olio 304, 343, 340–348 kopiointi 346 funktio-oliosovitin 347 funktio-osoitin 341 funktiomalli 286–287 erikoistus 296 funktion valvontalohko 396 funktori katso “funktio-olio” Gaiman, Neil 69 Gang of Four 268 geneerinen algoritmi 304–306, 337–340 geneerinen ohjelmointi 291 Hakemisto 453 geneerisyys 163, 263–348 GoF-kirja 268 haeEnsimmainen 167 haeRaportinPaivays 425 Haikala, Ilkka 20, 28 hajasaanti-iteraattori 331 hajotin katso “purkaja” hamekangas 110 handle/body 275 Heller, Joseph 201 Henkilo 82, 394 Henkilo::Henkilo 82, 395, 397 Henkilotiedot 239 homesieni 144 ha¨vitin katso “purkaja” identiteetti 57–58 idiomi 267 if 217, 220, 223, 297, 399, 402, 404 imeta 192 indeksointi (STL) 313, 315, 321 inline 413–414 insert 312, 313, 318 inserter 336 instanssimuuttuja katso “ja¨senmuuttuja” instantiointi 56 instantiointi, mallin 285 interface Java 51 Modula-3 51 ma¨a¨rittely 31 UML 128 « interface » (UML) 128 invariantti 243 <iostream> 45 <iostream.h> 45 is-a 137, 155, 161 istream_iterator 336 iteraattori 304, 326–329 -kategoriat 329–332 -sovitin 336–337, 337 alku- 333 eteenpa¨in- 331 hajasaanti- 331 ja algoritmit 337 ja osoittimet 332 ja sa¨ilio¨t 332–334 kaksisuuntainen 331 kelvollinen 334 kelvoton 334 ka¨a¨nteis- 336 lisa¨ys- 336 loppu- 328, 333 mita¨to¨ida¨ 334 suunnittelumalli 273–274 syo¨tto¨- 329 tulostus- 330 tyhja¨ 332 vakio- 332 virta- 336 va¨li 329, 337 <iterator> 336 iterator 332 jaettu kooste 134 jarjestaJaPoista 340 JarjestettyTaulukko::EtsiJaMuutaAlkio 248 JarjestettyTaulukko::Invariantti 248 Java 51–53, 103, 204, 205, 249, 303, 411 jfunktio 66 Jim 240, 275 Hakemisto 454 johdettu luokka katso “aliluokka” johtaminen katso “periytyminen” jono (STL) 326 joukko (STL) 318–319 julkinen rajapinta 32 ja¨lkela¨inen 144 ja¨lkiehto 241 ja¨senfunktio 64–66 anna- 103 aseta- 103 esittely 64 ka¨tkeminen 103 muunnos- 236 toteutus 64–66 vakio- 107 ja¨senfunktiomalli 289 ja¨senmuuttuja 61–64 alustaminen 81 elinkaari 63 ka¨tkeminen 103 nimea¨minen 62 olio 63 osoitin 63 rajapinnassa 102 viite 63 kaksipa¨inen jono (STL) katso “pakka” kaksisuuntainen iteraattori 331 Kana::liiku 174 kantaluokka 144 abstrakti 144, 172–175, 193 kantaluokkaosa 152 kantaluokkaosoitin 155 kantaluokkaviite 155 kapselointi 33, 63, 113, 152, 256 luokkien ysta¨vyysominaisuus 260 karkauspa¨iva¨ 110 kategorisointi 142 kauankoJouluun 68 kauankoPalautukseen 230 kayta 295 kaytakopiota 214 kaytto 114, 300 kaytto1 345 kaytto2 345 kelvollinen iteraattori 334 kelvoton iteraattori 334 kentta¨ katso “ja¨senmuuttuja” kerroPaivays 48 kertaluokka 309 keskiarvo1 376 keskiarvo2 377 ketjusijoitus 217 Kirja::˜Kirja 157, 404, 406, 409 Kirja::annaNimi 157 Kirja::annaPalautusPvm 104 Kirja::Kirja 157, 404, 406, 409 Kirja::sopiikoHakusana 161 Kirja::tulostaTiedot 161 Kirja::tulostaVirhe 161 Kirja::vaihda 408, 409 KirjaApu::tulostaTiedot 185 kirjanmerkki katso “iteraattori”, 327 KirjastonKirja 158 KirjastonKirja::˜KirjastonKirja 158 KirjastonKirja::KirjastonKirja 158 Hakemisto 455 KirjastonKirja::onkoMyohassa 158 KirjastonKirja::tulostaKirjatiedot 184, 185 KirjastonKirja::tulostaKTeostiedot 184, 185 KirjastonKirja::tulostaTiedot 162, 182 KirjastonTeosApu::tulostaTiedot 185 kloonaa 214 kloonaa-ja¨senfunktio 213 kohdistin 273, 413 kokoava jaottelu 118 kokoelma 271–273 kokoonpanollinen kooste 134 komponentti 38, 120, 121 kontravarianssi 160 kooste 134–136 C++ 135 koostuminen 135 muodostuminen 134 vs. periytyminen 198–199 koostuminen 135 kopioinnin esta¨minen 209–210 kopiointi 202–215 automaattiosoitin 385–387 funktio-olio 346 kloonaa-ja¨senfunktio 213 matala- 204–205 olion 202 perusteet 202 syva¨- 205–206 viipaloituminen 210–215 viite- 203–204 kopiorakentaja 83, 206–210, 311 esta¨minen 209–210 ja periytyminen 207–208 ka¨a¨nta¨ja¨n luoma 208–209 poikkeusolion 374 kovarianssi 160 kuormittaminen 81 kuukaudenAlkuun 225 kysyJaTestaa 345 ka¨ytta¨ja¨rooli 123 ka¨ytto¨tapaus 123, 125, 127, 140 ka¨a¨nno¨saikainen tyypitys 284 ka¨a¨nno¨saikainen metaohjelmointi 350 tehokkuus 310 tyypitys 303 ka¨a¨nno¨syksikko¨ 39 ka¨a¨nteis-iteraattori 336 ka¨a¨nta¨ja¨n luoma kopiorakentaja 208–209 oletusrakentaja 83 oletusrakentajan kutsu 153 sijoitusoperaattori 219–220 laatu 27, 120, katso [Pirsig, 1974] LainausJarjestelma::LainausJarjestelma 134 laskeFibonacci 314 laskeKeskiarvo 376 laskeTaulukkoVastaus 427 Laskija::˜Laskija 254 Laskija::Laskija 254 Laskija::tulosta_tilasto 254 laula 173 Lem, Stanislaw 26 Hakemisto 456 liiku 173, 174, 192, 193, 195 limerick 263 lineaarinen tehokkuus 310 Linne´, Carl von 142 lisaanny 192 lisaaViiteLaskuria 50, 424 <list> 317 list 315 lista (STL) 315 lisa¨ys-iteraattori 336 lisa¨a¨ja¨ katso “lisa¨ys-iteraattori” logaritminen tehokkuus 310 lokaalisuusperiaate 119, 133 Lokiviesti::Lokiviesti 154 loppuiteraattori 333 lower_bound 322 lukuma¨a¨ra¨suhde 132 LukuPuskuri::katso 316 LukuPuskuri::lisaa 316 LukuPuskuri::lue 316 LukuPuskuri::onkoTyhja 316 Luo 34, 52, 114 luo 43 Luokannimi::Luokannimi 81 luoKayttoliittyma 233 luokka -funktio 252–253 -invariantti 243, 246 -muuttuja 250–252 -olio 249 -vakio 251 :: 42 ajoaikainen kysyminen 165–167 ali- 144 attribuutti 122 dualismi 58 ennakkoesittely 112 esittely 60, 60, 61 julkinen rajapinta 101, 102 kanta- 144 kuvaaminen (UML) 128 ka¨ytto¨ 66–67 lo¨yta¨minen 123 meta- 249 moduulina 58 rajapinnan na¨kyvyys 101 rajapinta- 177, 190–198 rajapintatyyppi 253 rooli 132 sisa¨lto¨ 60 tarjotut palvelut 122 tietotyyppina¨ 59 vastuualue 122 ysta¨va¨funktio 259 ysta¨va¨luokka 260 luokkahierarkia 144 luokkainvariantti 246 luokkakaavio 127 luokkamalli 287–290 erikoistus 296 osittaiserikoistus 297 luoLukko 261 luoPaivaysOlio 50 la¨pikutsufunktio 180 maailmankaikkeus 26 main 44, 46, 47, 68, 88, 163, 167, 222, 225, 299, 379, 415, 417, 418 malli 283–301 erikoistus 295–297, 324 export 299 funktio- 286–287 instantiointi 285 ja¨senfunktio- 289 Hakemisto 457 koodin sijoittelu 298–299 luokka- 287–290 malliparametri 294–295 oletusparametri 292–293 syntaksi 285 typename 285, 299–301 tyyppi vai arvo 299–301 tyyppiparametri 285 tyyppiparametrien vaatimukset 290 vakioparametri 294 mallikieli 268 malliparametri katso “tyyppiparametri” <map> 321, 322 map 321–322 matalakopiointi 204–205 max 299, 414 <memory> 384 merge 339 metafunktio 352 metaluokka 249–250 metaohjelma 349 metaohjelmointi 349 ka¨a¨nno¨saikainen 350 metodi 64 min 286, 292, 355, 356, 361 minimitakuu 390 mita¨to¨ida¨ (iteraattori) 334 mixin 178 Mjono::kloonaa 214 Mjono::Mjono 207 Modula-3 34, 51 modulaarisuus 118 moduuli 31–34 C 39–41 C++ 41–50 elinkaari 37 jako ka¨a¨nno¨syksiko¨illa¨ 39 ka¨ytta¨ja¨ 32 lokaalisuusperiaate 119 riippuvuus 118 toteuttaja 32 vastuualue 122 monijoukko (STL) 319–320 moniperiytyminen 149, 175–190, 194 erotteleva 186 salmiakki- 190 toistuva 186 toistuva kantaluokka 190 virtuaalinen 187 yhdista¨va¨ 187 monirekisterointi 320 monirekisterointi2 321 moniselitteisyys 181–186, 187 muistin loppuminen 87, 370 muistivirheet 370 muistivuotojen va¨ltta¨minen 91–92, katso “automaattiosoitin” mukautuva ja¨rjestelma¨ 351 multimap 322 multiset 319–320 muna-kana -ongelma 112, 119 muni 192, 193 muodostin katso “rakentaja” muodostuminen 134 Murtoluku::Murtoluku 234 mutable 108 muunnosja¨senfunktio 236 muutanKuitenkin 109 myohassako 165 namespace 41 nappulaaPainettu 233 NaytaIlmoitus 278, 279 naytaViesti 73, 76 Hakemisto 458 naytaViesti1 78 naytaViesti2 78 nelio¨llinen tehokkuus 310 new 89 <new> 87, 89 new 86–89 new(nothrow) 89 new[] 90 nimea¨miska¨yta¨nto¨ 62 nimea¨ma¨to¨n nimiavaruus 49 nimiavaruus 41–50 :: 42 alias 46 hyo¨dyt 44 nimea¨ma¨to¨n 49 na¨kyvyystarkenninope- raattori 42 std 45 synonyymi 46 syntaksi 42 using 47 nollaaAlkiot 333 nothrow 89 nothrow-takuu 392 na¨kyvyysma¨a¨re 101–104 ja periytyminen 150–152 oletus 101 na¨kyvyystarkenninoperaattori 44 O-notaatio 309 odotaOKta 76 ohjelmistokriisi 27 ohjelmistosuunnittelu 117 OkDialogi 73, 76 OkDialogi::˜OkDialogi 78 OkDialogi::OkDialogi 78 OKodotus 73 oletusrakentaja 82–83 ka¨a¨nta¨ja¨n luoma 83 olio alustamaton 218 alustaminen katso “rakentaja” arvo 216 arvoparametrina 224–227 dynaaminen luominen ja tuhoaminen 85–86 elinkaari 71–72 identiteetti 57, 58 instantiointi 56 kahteen kertaan tuhoaminen 92–94 kerrosrakenne 149 kommunikointi 36 kopiointi 202–215 kuvaaminen (UML) 128 ka¨ytto¨ tuhoamisen ja¨lkeen 92–94 luokka 56, 57 luomistoimenpiteet 70–71 omistusvastuu 91 paluuarvona 224 poikkeus- 374 rajapinta 95 sijoitus 215–222 simulointi 54 tietotyyppi 57 tila 37, 55, 202 tilan eriytta¨minen 405–407 tilan vaihtaminen 407–408 todellinen vs. ohjelmallinen 55 tuhoamistoimenpiteet 71 Hakemisto 459 tuhoutuminen katso “purkaja” vakio- 106–108 va¨liaikais- 225 olio-ohjelmointi 36 olioma¨a¨rittely 35 oliosuunnittelu 35, 116–141 oliot funktio- 343, 340–348 Ω-notaatio 309 omistussuhde 134 omistusvastuu 91 onkoAlle5 342 OnkoAlle::OnkoAlle 344 OnkoAlle::operator 344 onkoKarkauspaivaa 68 onkoTyhja 316 operaatio katso “ja¨senfunktio” operator 344 optimointi inline 413 ja mallit 295 ja STL 318 osittava jaottelu 118 osoitin -aritmetiikka 332 automaatti- katso “automaattiosoitin” funktio- 341 ja ennakkoesittely 112 ja iteraattorit 332 ja¨senmuuttuja 133 kantaluokka- 155 kelvollisuus 335 this 66 vaarat 411 vakio- 109 viite osoittimeen 411 a¨lyka¨s 387 osoitinvakio 111 ostream_iterator 337 otsikkotiedosto 39, 60 suojaus useaan kertaan ka¨yto¨lta¨ 40 otsikkotiedostot ISOC++ 45 pa¨a¨te 46 package (JAVA) 259 paint 52 PaivattyLokiviesti::PaivattyLokiviesti 154 PaivattyMjono::kloonaa 214 PaivattyMjono::PaivattyMjono 208 Paivays::˜Paivays 84 Paivays::annaPaiva 107, 258 Paivays::Paivays 80 pakka (STL) 315 pakkaus (JAVA) 259 paluuarvo ja automaattiosoitin 385 palvelu katso “ja¨senfunktio” parametri arvonva¨litys 224 viite- 412 Parnas’s Principles 96 Parnas, David 96, 121 Parnasin periaatteet 96 partition 339 peitta¨minen 168 periytetty luokka katso “aliluokka” periytyminen 143, 144 dynaaminen sitominen 160–164 erikoistaminen 147 Hakemisto 460 hinta 169–170 ja kopiorakentaja 207–208 ja laajentaminen 156–159 ja luokkainvariantti 247 ja na¨kyvyys 150–152 ja purkajat 154–155 ja sijoitusoperaattori 219 ja tyyppimuunnos 164–165 ja vastuu 153, 160 luokan kysyminen 165–167 moni- katso “moniperiytyminen” peitta¨minen 168 perusteet 149–150 rajapintaluokka 190–198 syntaksi (C++) 150 toistuva 186 uudelleenka¨ytto¨ 147–148 viipaloituminen 156, 210–215, 221–222 vs. kooste 198–199 yleista¨minen 147 periytymishierarkia 143–147 perustakuu 390 pienin 353 PieniPaivays::annaPaiva 65 PieniPaivays::asetaPaiva 65 PieniPaivays::asetaPaivays 65 PieniPaivays::sijoitaPaivays 105 pimpl 405–407 pino (STL) 325 poikkeus 367 -kategoriat 371–373, 375 -ka¨sittelija¨ 374 -ma¨a¨reet 382–383 -neutraalius 392 -olio 374 -takuu 389 -turvallinen sijoitus 399–408 dynaamisesti luodut oliot 380 heitta¨minen 374 ja olioiden tuhoaminen 380–381 ja purkajat 380, 397 ja rakentajat 393 ka¨ytto¨ 374–377 minimitakuu 390 muistin loppuminen 87 nothrow-takuu 392 perustakuu 390 sieppaamaton 377–378 sieppaaminen 374 sisa¨kka¨iset 378–379 uudelleenheitta¨minen 379 vahva takuu 391 valvontalohko 374 vuotaminen 375 yleiska¨sittelija¨ 378 poistaLukko 261 poke 415 polymorﬁsmi 146 pop 325, 326 pop_back 313, 313, 315, 317 pop_front 313, 315, 317 Pratchett, Terry 142 prioriteettijono (STL) 326 priority_queue 326 private 101, 103–104, 151 protected 101, 152 public 101–103, 151 puhdas virtuaalifunktio 172 puhelinluettelo 322 Hakemisto 461 PuhLuettelo::lisaa 324 PuhLuettelo::poista 324 PuhLuettelo::tulosta 324 purkaja 76, 83–85 ja periytyminen 154–155 ja poikkeukset 380 ja virtuaalifunktiot 170–171 kutsuhetki 84 kutsumatta ja¨tta¨minen 86, 246 virtuaali- 168–169 push 325, 326 push_back 312, 313, 313, 315, 317 push_front 313, 315, 317 PVM::PVM 114 pysyvyys- ja vaihtelevuusanalyysi 264 pysyva¨isva¨itta¨ma¨ katso “invariantti” <queue> 326 queue 326 rajapinta 31, 95–99, 240 -funktio 31 -tyyppi 253–258 hyva¨n rajapinnan tunnusmerkkeja¨ 97–98 julkinen 32 ja¨senmuuttuja 102 kuvaaminen (UML) 128 luettelo erilaisista 98 luokan ja olion 248 luokan julkinen 101 luokkavakio 251 lupaus 147 sisa¨inen 258 sopimus 33, 240 suunnittelu 33, 96 tiedon ka¨tkenta¨ 33 tietotyypit 253 toteuttaminen (UML) 137 rajapintaluokka 177, 190–198 rakentaja 74, 76, 80–83 alustuslista 81 ja periytyminen 152–154 ja virtuaalifunktiot 170–171 kopio- 83, 206–210, 311 oletus- 82–83 tyyppimuunnoksena 233–236 random_shuffle 339 rbegin 336 reﬂektio 349 reinterpret_cast 231–232 rekisterointi 319 rekisterointi_optimoitu 320 rend 336 riippuvuus 131 robustness 120 role 132 rooli luokka (UML) 132 suunnittelumalli 270 roskienkeruu 71, 72, 74 RTTI 164 Run-Time Type Information 164 rutiini katso “ja¨senfunktio” saakoHameenKuukaudessa 111 Sandman 69 Hakemisto 462 sarja (STL) 312 <set> 318, 320 set 318–319 Shakespeare, William 54 siirry 364 siirry_apu 364 siivoa 76 Siivous 73 siivousfunktio1 381 siivousfunktio2 382, 385 siivousfunktio3 386 siivoustoimenpiteet 71, 378 sijoita 222 sijoituksen esta¨minen 220 sijoitus 215–222 automaattiosoitin 385–387 itseen 218–219 poikkeusturvallinen 399–408 viipaloituminen 221–222 sijoitusoperaattori 216–220 esta¨minen 220 ja periytyminen 219 ka¨a¨nta¨ja¨n luoma 219–220 silmukkaviittaus 119 silta 274–276 Simula-67 54 sisa¨inen rajapinta 258 sisa¨inen tyyppi katso “rajapintatyypit” sisa¨kka¨iset valvontalohkot 378–379 size 313, 325, 326 Smalltalk 204–206, 249 metaluokka 249 sopimus rajapinnasta 241 sopimussuunnittelu 240–245 sort 339 sovelluskehys 199–200 sovitin iteraattori- 336–337, 337 sa¨ilio¨- 304, 325–326 spagettikoodi 29 splice 317 staattinen elinkaari 77–79 <stack> 325 stack 325 Standard Template Library katso “STL” static 40, 49 static_cast 229 std 45 STL 302–348 algoritmit 337–340 assosiaatiomonitaulu 322 assosiaatiotaulu 321–322 assosiatiivinen sa¨ilio¨ 317–318 avain 317 bittivektori 325 funktio-olio 343, 340–348 geneerisyys 302 indeksointi 313, 315, 321 iteraattori katso “iteraattori” ja automaattiosoittimet 387 ja perustaulukot 312 ja string 312 ja viitteet 311 jono 326 joukko 318–319 kaksipa¨inen jono katso “pakka” kertaluokka 309 kirjallisuus 304 lista 315 Hakemisto 463 monijoukko 319–320 muistinhallinta 311 optimointi 318 pakka 315 perusteet 303–305 pino 325 prioriteettijono 326 sarja 312 sa¨ilio¨ 310–311 sa¨ilio¨sovitin 325–326 totuusvektori 323–324 vector<bool> 323–324 vektori 312–315 viipaloituminen 212 va¨li 329 STLport 335 string 238 <string> 418 string 416–419 substantiivihaku 123 sulkeuma 343 summaa 295, 300 summaaLuvut 376 suoritusaika 309 suunnittelumalli 267–270 iteraattori 273–274 kohdistin 273 kokoelma 271–273 silta 274–276 suurten kokonaisuuksien hallinta 29 switch 428 syklinen viittaus 119 synonyymi katso “viite” Systema Naturae 142 syva¨kopiointi 205–206 syo¨tto¨iteraattori 329 sa¨ilio¨ 304, 310–311 -sovitin 304 assosiatiivinen 317–318 ja iteraattorit 332–334 ja viitteet 311 la¨pika¨yminen 326, 333 sarja 312 viipaloituminen 212 sa¨ilio¨sovitin 325–326 taikuri 27 tapahtumasekvenssi 138–139 taulukko-delete 90 taulukko-new 90 tehokkuuskategoriat 308–310 tehokkuusvaatimus 307 template 285 template-metaohjelmointi 350 terminate 377 Θ-notaatio 309 this 66 throw 375 tiedon ka¨tkenta¨ 33, 118 tietoja¨sen katso “ja¨senmuuttuja” tietorakenne 30 jaotteluperusteet 306–308 kertaluokka 309 luokka 60, 66 la¨pika¨yminen 326 rajapinnat 307 tehokkuuskategoriat 308–310 Tila 406, 409 tilakone 139–140 tilan eriytta¨minen 405–407 tilan vaihtaminen 407–408 top 325 toteutusmalli 267 totuusvektori (STL) 323–324 trait 353 Hakemisto 464 Trurl 26 try 375 Tuhoa 114 tuhoutuminen olion katso “purkaja” Tulosta 34, 52 tulosta 43, 323 TulostaAikaisempi 36 tulostaAlkiot 333 tulostaAlle 346 tulostaAlle2 349 tulostaAlle5 342 tulostaKirjat 163 tulostaKTeostiedot 184, 185 tulostaPalautusaika 230 tulostaTiedot 160, 182, 185 tulostus-iteraattori 330 tuotaNeliotaulukko 386 tuotaTaulukko 386 tuplaa 412 type_info 166 typedef 332 typeid 166 <typeinfo> 166 typename 285, 299–301 tyyppimuunnos 227–236 ajoaikainen 231 C 228 const_cast 230–231 dynamic_cast 164–165, 231 implisiittinen 256, 287, 291 ja rakentaja 233 ka¨a¨nno¨saikana 229 ohjelmoijan ma¨a¨rittelema¨ 232 olio-ohjelmointi 228 reinterpret_cast 231–232 static_cast 229 uusi C++ syntaksi 228 Tyyppinimi 86 tyyppiparametri 285 vaatimukset 290 UML assosiaatio 132 interface 128 kooste 134 koostuminen 135 lukuma¨a¨ra¨suhde 132 luokka 128 luokkakaavio 127 luokkien va¨liset yhteydet 130 muodostuminen 134–135 olio 128 periytyminen 137 rajapinta 128 rakennekuvaus 140 riippuvuus 131 tapahtumasekvenssi 138 tilakone 139 toteuttaminen 137 uncaught_exception 398 Uniﬁed Modelling Language 126 upper_bound 322 using 47 using namespace 48 uusiTyyppi 228 UusiVirheIkkuna 279 vahva takuu 391 vakio 105–111 -iteraattori 332 Hakemisto 465 -ja¨senfunktio 107 -muuttuja 105 -olio 106 -osoitin 109 -viite 109, 224, 412 osoitin- 111 perustietotyyppi 105 suunnitteluvirhe 231 tyyppimuunnos 230 vakioaikainen tehokkuus 310 valvontalohko 374 vanilla 178 vappu 51 varain 305 varmistusehto 246 varmistusrutiini 245 vastuualue 38, 122 elinkaari 69 ja periytyminen 153, 160 luokka 122–123 moduuli 32 moduulin toteuttaja 32 rajapinnan sopimus 240 rajapinta 95–99 <vector> 313, 415 vector 312–315, 414–416 vector<bool> 323–324 vektori (STL) 312–315 viipaloituminen 156, 210–215, 221–222 viite 411, 410–413 -parametri 412 ja ennakkoesittely 112 ja sa¨ilio¨t 311 kantaluokka- 155 kelvollisuus 335 osoittimeen 411 paluuarvona 413 syntaksi 411 vakio- 109 vakioparametri 412 virheellinen 413 viitekopiointi 203–204 viitelaskuri 387 virheet hallittu lopetus 369 jatkaminen 370 peruuttaminen 370, 391 toipuminen 370 virheet ohjelmissa 367–368 virhehierarkia 371–373 VirheIkkuna::˜VirheIkkuna 278 VirheIkkuna::NaytaIlmoitus 278 virtaiteraattori 336 virtuaalifunktio 159–160 hinta 169–170 puhdas 172 rakentajissa ja purkajissa 170–171 virtuaalinen moniperiytyminen 187 virtuaalipurkaja 168–169 virtuaalitaulu 170 virtuaalitauluosoitin 170 virtual 159, 168 unohtaminen 168, 169 vuosi 2000 -ongelma 31, 33, 256 va¨li (STL) 329 va¨liaikaisolio 225 yhdista¨va¨ moniperiytyminen 187 yhtasuuruus 359 yleiska¨ytto¨isyys 264 Hakemisto 466 yleispoikkeuska¨sittelija¨ 378 yleista¨minen 147 yli-indeksointi 313 yliluokka katso “kantaluokka” ys::tulosta 42 ysta¨va¨funktio 259–260 ysta¨va¨luokka 260–261 zombie-olio 94 a¨lyka¨s osoitin 387"
            },
            {
                "role": "user",
                "content": "Can you summarize the key topics covered in this lecture?"
            },
            {
                "role": "assistant",
                "content": "This lecture covers foundational algorithmic concepts including problem formulation, correctness proof using induction, efficiency analysis using asymptotic notation, data structures (arrays, linked lists, BSTs, heaps), and sorting methods (selection sort, merge sort, counting sort). Additionally, it introduces the Word-RAM computational model, providing a theoretical framework for measuring algorithm efficiency."
            }
        ]
    },
    {
        "system": "Hello! I am Kiran, an AI assistant from Tampere University, dedicated to providing you with accurate and insightful assistance. My goal is to help you understand concepts, summarize lectures, and answer questions effectively.",
        "conversations": [
            {
                "role": "host",
                "content": "From pseudocode to implementation COMP.CS.300 Data structures and algorithms 1 Matti Rintala (matti.rintala@tuni.fi) Implementing pseudocode •Adapting to the application •Sanity check of inputs etc. •Error handling •Limitations from programming language •Speed and practicality issues arising from hardware and language •Maintainability modularity etc. ⇒ Pseudocode Insertion-Sort(A) 1 for next_elem := 2 to A.length do 2 key := A[next_elem] 3 place := next_elem − 1 4 while place > 0 and A[place] > key do 5 A[place + 1] := A[place] 6 place := place − 1 7 A[place + 1] := key Effects of the programming language •Indexing starts from 0 (in pseudocode often from 1) •Is indexing even used (or arrays, or...) •Is data copied, or referred to indirectly •If outside data is referred to indirectly, does it happen with: pointer, smart pointer (shared_ptr...), iterator (if data in a data structure), index (if data in vector etc.), search key (if data in a structure with fast search) •Are algorithm's \"parameters\" real parameters, or just variables etc. name: \"Matti\" age: 23 height: 1.82 name: \"Elisa\" age: 32 height: 1.87 name: \"Lumi\" age: 20 height: 1.65 name: \"Siri\" age: 45 height: 1.50 name: \"Bob\" age: 30 height: 1.97 Implementation 1 #include <vector> 2 struct Data { string name; int age; float height; }; 3 using Taulukko = std::vector<Data*>; 4 // Täytetään taulukko osoittimilla dataan 5 void insertion_sort(Taulukko& A) { 6 Data* keyp = nullptr; int place = 0; 7 for (Taulukko::size_type next_elem = 1; next_elem < A.size(); ++next_elem) { 8 keyp = Taulukko.at(next_elem); 9 place = next_elem-1; 10 while (place >= 0) { 11 elemp = Taulukko.at(place); 12 assert(elem != nullptr); 13 if (keyp->name < elemp->name) { break; } 14 Taulukko.at(place+1) = elemp; --place; 15 } 16 Taulukko.at(place+1) = keyp; 17 } 18 } Insertion-Sort(A) 1 for next_elem := 2 to A.length do 2 key := A[next_elem] 3 place := next_elem − 1 4 while place > 0 and A[place] > key do 5 A[place + 1] := A[place] 6 place := place − 1 7 A[place + 1] := key"
            },
            {
                "role": "user",
                "content": "Can you summarize the key topics covered in this lecture?"
            },
            {
                "role": "assistant",
                "content": "This lecture covers foundational algorithmic concepts including problem formulation, correctness proof using induction, efficiency analysis using asymptotic notation, data structures (arrays, linked lists, BSTs, heaps), and sorting methods (selection sort, merge sort, counting sort). Additionally, it introduces the Word-RAM computational model, providing a theoretical framework for measuring algorithm efficiency."
            }
        ]
    },
    {
        "system": "Hello! I am Kiran, an AI assistant from Tampere University, dedicated to providing you with accurate and insightful assistance. My goal is to help you understand concepts, summarize lectures, and answer questions effectively.",
        "conversations": [
            {
                "role": "host",
                "content": "Quicksort: a divide and conquer sorting algorithm 1. Introduction 2. Partitioning 3. Quicksort computation 1. Introduction Q: What does Quicksort do? A: Quicksort will place elements in an array in order (smallest to largest or largest to smallest). Quicksort properties: • does not require extra sorting space (in place sorting) • uses divide-and-conquer • almost always presented as recursive • uses partitioning Q: What is partitioning? A: Splitting array elements into (at least) two groups. Before partitioning: array has no particular order After partitioning (2 groups): elements A[1..p] have a propery and elements A[(p + 1)..n] lack property Example starting array: A = (1) Partitioning according to whether number is odd or not after partitioning: A = elements A[1..3] are odd and elements A[4..8] are even (2) Partitioning with respect to pivot 10 after partitioning: A = elements A[1..5] are less than or equal to 10 and elements A[6..8] are greater than Pivot: a number a used for partitioning. After partitioning: numbers at left end of A are at most a and numbers at right end of A are at least a. Partitioning in quicksort is done using a pivot a that is an element of the array. Consider partitioning using final element A[n] as pivot: starting array: after partitioning when pivot a is at its correct location: Insight: α is in correct position if we want to sort entire array from smallest to largest. 2. Partitioning Pseudocode cut = the current dividing line between the small elements and the large elements Example starting array: A = compute PARTITION(A, 1, 8) step code line(s) computation array A Comments on PARTITION: • choice of pivot element (α) is often randomized • one goal: keep number of element swaps small • PARTITION uses one approach for swapping; there are others • to put pivot into correct location only requires one swap (line 15) • similar approach can be used for other partitioning problems e.g. ﬁnding median 3. Quicksort computation After partitioning when pivot a is at its correct location: Insight: a is in correct position if we want to sort entire array from smallest to largest. Consequence: we can sort the subarrays to left a and the right of a Divide and conquer! But ... Are A[1..k] and A[(k+1)..n] (roughly) the same size? Depends on choice of pivot. Quicksort: description Quicksort: pseudocode Example starting array: A = compute QUICKSORT(A, 1, 8) step code line(s) computation array A recursion level k = PARTITION(A,1,8) QUICKSORT(A,1,3) k = PARTITION(A,1,3) QUICKSORT(A,1,1) QUICKSORT(A,3,3) QUICKSORT(A,5,8) k = PARTITION(A,5,8) QUICKSORT(A,5,4) QUICKSORT(A,6,8) k = PARTITION(A,6,8) Comments on QUICKSORT: • amount of pseudocode deceptive since PARTITION does all the work • interpretation: recursively ﬁnd ﬁnal locations of each element • variation: call to QUICKSORT replaced by call to simpler sorting algorithm (often insertion sort) when array size ’small’ Tämä teos on lisensoitu Creative Commons Nimeä-EiKaupallinen- EiMuutoksia 4.0 Kansainvälinen -lisenssillä. Tarkastele lisenssiä osoitteessa http://creativecommons.org/licenses/by-nc-nd/4.0/. tekijä: Frank Cameron This work is licensed under the Creative Commons Attribution- NonCommercial-NoDerivatives 4.0 International License. To view a copy of this license, visit http://creativecommons.org/licenses/by-nc-nd/4.0/. made by Frank Cameron"
            },
            {
                "role": "user",
                "content": "Can you summarize the key topics covered in this lecture?"
            },
            {
                "role": "assistant",
                "content": "This lecture covers foundational algorithmic concepts including problem formulation, correctness proof using induction, efficiency analysis using asymptotic notation, data structures (arrays, linked lists, BSTs, heaps), and sorting methods (selection sort, merge sort, counting sort). Additionally, it introduces the Word-RAM computational model, providing a theoretical framework for measuring algorithm efficiency."
            }
        ]
    },
    {
        "system": "Hello! I am Kiran, an AI assistant from Tampere University, dedicated to providing you with accurate and insightful assistance. My goal is to help you understand concepts, summarize lectures, and answer questions effectively.",
        "conversations": [
            {
                "role": "host",
                "content": "Algorithm design techniques: randomization 1. Introduction to randomization 2. Quicksort 3. Linear search 1. Introduction to randomization Categories of design techniques Category I: techniques to handle entire computation problem - decrease and conquer (incremental) - divide and conquer - dynamic programming - tranform and conquer Category II: techniques to be used in conjunction with other techniques - randomization Deterministic algorithm: no randomization Randomized algorithm: randomization intentionally added Even when the input data are fixed, a randomized algorithm will not behave the same way each time it is run. Q: Why use a randomized algorithm? A 1: It improves the algorithm's behaviour. A 2: A deterministic algorithm may have a worst case that significantly degrades the algorithm behaviour. A randomized algorithm may avoid the worst case. Successful applications of randomized algorithms: - searching and sorting: quicksort and hash tables - optimization: simulated annealing, genetic algorithms Randomization: pluses and minuses + shown to work in practice + worst case should be no more probable than any other case - adding randomization also takes time - algorithm becomes more complicated - algorithm analysis requires probability theory - need a random number generator 2. Quicksort Pseudocode for standard QUICKSORT starting array: after partitioning when pivot a is at its correct location: PARTITION(A, 1, n) Worst case input Q: What is the most number of times we will need to call QUICKSORT for array A[1..n]? Consider runnig QUICKSORT when n = 7 for two situations: (i) PARTITION always results in two subarrays that are as close as possible to being same size (ii) PARTITION always results in two subarrays that are as far as possible from being same size (i) PARTITION always results in two subarrays that are as close as possible to being same size (ii) PARTITION always results in two subarrays that are as far as possible from being same size Q: What is the most number of times we will need to call QUICKSORT for input array A[1..n]? A: 2n-1 Q: Under what conditions do we need to call QUICKSORT 2n-1 times for input array A[1..n]? A: When k = L or k = R for every call to k = PARTITION(A, L, R). Let m(L,R) be the number of iterations in PARTITION(A, L, R): m(L,R) = R - L Consider QUICKSORT(A, 1, n) when k = R for every call to k = PARTITION(A, L, R). recursion level calls k m(L,R) 1 k = PARTITION(A, 1, n), QUICKSORT(A, 1, n-1), QUICKSORT(A, n+1, n) n n-1 2 k = PARTITION(A, 1, n-1), QUICKSORT(A, 1, n-2), QUICKSORT(A, n, n-1) n-1 n-2 3 k = PARTITION(A, 1, n-2), QUICKSORT(A, 1, n-3), QUICKSORT(A, n-1, n-2) n-2 n-3 n-1 k = PARTITION(A, 1, 2), QUICKSORT(A, 1, 1), QUICKSORT(A, 3, 2) 2 1 . . Standard QUICKSORT's running time is O(n^2) for following input arrays: [1, 2, 3, ... n ] [n, n-1, n-2, ... 1 ] [n/2, ... 2, n-1, 1, n] (n even) Upper bound on running time of QUICKSORT: O(n^2) Randomization Assume procedure RANDOM(a, b) exists. Assume RANDOM(a, b) generates each value between a and b with equal probability. I Strategy for avoiding worst case: random choice of pivot Use procedure RANDOMIZED-PARTITION and RANDOMIZED-QUICKSORT. II Strategy for avoiding worst case: random permutation of starting array Random permutation also called shuffling. Run SHUFFLE(A, 1, n) before calling standard QUICKSORT(A, 1, n). 3. Linear search Pseudocode for searching number array L for x. Running times: - lower bound: for-loop is executed only once: W(1) - upper bound: for-loop is executed n times: O(n) Someone tries to 'improve' SEARCH with randomization. For array L[0 .. n-1 ] running time of SHUFFLE is Q(n). So running time of RANDOMIZED-SEARCH is W(n). Hence: running time of RANDOMIZED-SEARCH is worse than running time of SEARCH. Conclusion: use randomization wisely Tämä teos on lisensoitu Creative Commons Nimeä-EiKaupallinen- EiMuutoksia 4.0 Kansainvälinen -lisenssillä. Tarkastele lisenssiä osoitteessa http://creativecommons.org/licenses/by-nc-nd/4.0/. tekijä: Frank Cameron This work is licensed under the Creative Commons Attribution- NonCommercial-NoDerivatives 4.0 International License. To view a copy of this license, visit http://creativecommons.org/licenses/by-nc-nd/4.0/. made by Frank Cameron"
            },
            {
                "role": "user",
                "content": "Can you summarize the key topics covered in this lecture?"
            },
            {
                "role": "assistant",
                "content": "This lecture covers foundational algorithmic concepts including problem formulation, correctness proof using induction, efficiency analysis using asymptotic notation, data structures (arrays, linked lists, BSTs, heaps), and sorting methods (selection sort, merge sort, counting sort). Additionally, it introduces the Word-RAM computational model, providing a theoretical framework for measuring algorithm efficiency."
            }
        ]
    },
    {
        "system": "Hello! I am Kiran, an AI assistant from Tampere University, dedicated to providing you with accurate and insightful assistance. My goal is to help you understand concepts, summarize lectures, and answer questions effectively.",
        "conversations": [
            {
                "role": "host",
                "content": "Recursive procedures 1. What is a recursive procedure? 2. Recursive versus iterative 3. An application: binary search 1. What is a recursive procedure? A recursive procedure calls itself. A recurrence function is deﬁned using itself. General form of simple recurrence function f(): Example: computing power of a number Q: When can we use a recursive procedure? A: When the problem we want to solve can be expressed in terms of a smaller version of itself. Example: computing power of a number (contd) Features of recursive procedure: • base case(s) or trivial case(s): procedure does not call itself • recursion case(s): procedure calls itself • recursion level • each call has own parameters, own local variables • call stack (what calls are still incomplete) Example: computing power of a number (contd) Compute POWR(2,3). step recursion level code line(s) computation stack 2. Recursive versus iterative Alternative to a recursive procedure: iterative (looping) procedure: does not call itself Usually a recursive procedure can be converted to an equivalent iterative procedure. Correspondences: Example: computing power of a number (contd) Why prefer recursive procedure over iterative procedure? • often recursive is more compact • no need to handle stack • often easier to write/understand Why prefer iterative procedure over recursive procedure? • often has better performance for some programming languages • allows programmer better control over data structures Ongoing debate: try searching 'why recursive is better than iterative'. 3. An application: binary search Problem: find a value key in an sorted array A[1..n] Idea: split array in half Example Compute BINSEARCH( A,1, 7, 12) where step recursion level code line(s) computation array A Tämä teos on lisensoitu Creative Commons Nimeä-EiKaupallinen- EiMuutoksia 4.0 Kansainvälinen -lisenssillä. Tarkastele lisenssiä osoitteessa http://creativecommons.org/licenses/by-nc-nd/4.0/. tekijä: Frank Cameron This work is licensed under the Creative Commons Attribution- NonCommercial-NoDerivatives 4.0 International License. To view a copy of this license, visit http://creativecommons.org/licenses/by-nc-nd/4.0/. made by Frank Cameron"
            },
            {
                "role": "user",
                "content": "Can you summarize the key topics covered in this lecture?"
            },
            {
                "role": "assistant",
                "content": "This lecture covers foundational algorithmic concepts including problem formulation, correctness proof using induction, efficiency analysis using asymptotic notation, data structures (arrays, linked lists, BSTs, heaps), and sorting methods (selection sort, merge sort, counting sort). Additionally, it introduces the Word-RAM computational model, providing a theoretical framework for measuring algorithm efficiency."
            }
        ]
    },
    {
        "system": "Hello! I am Kiran, an AI assistant from Tampere University, dedicated to providing you with accurate and insightful assistance. My goal is to help you understand concepts, summarize lectures, and answer questions effectively.",
        "conversations": [
            {
                "role": "host",
                "content": "STL algorithms COMP.CS.300 Data structures and algorithms 1 Matti Rintala (matti.rintala@tuni.fi) STL algorithms •Lots of ready-made algorithms •Have a look at the algorithm list (like cppreference.com) •Algorithms take iterators as params •Algorithms never add/remove elements! (invalidation) •(There are also parallel versions of algorithms) STL algorithms - examples COMP.CS.300 Data structures and algorithms 1 Matti Rintala (matti.rintala@tuni.fi) Selection-Sort(A) 1 for next_elem := 1 to A.length-1 do 2 smallest := next_elem 3 for place := next_elem+1 to A.length do 4 if A[place] < A[smallest] then 5 smallest := place 6 A[next_elem] ⇄ A[smallest] (move the border between parts) (possible location of smallest = first elem) (if an even smaller elem is found...) (...take not of its location) (swap the smallest to the beginning) Mergesort(A, left, right) 1 if left < right then (nothing to be done in trivial case) 2 mid := (⌊left + right) / 2⌋ (calculate midpoint) 3 Mergesort(A, left, mid ) (sort left half) 4 Mergesort(A, mid+1, right) (sort right half) 5 Merge(A, left, mid, right) (merge sorted halfs together) Insertion-Sort(A) 1 for next_elem := 2 to A.length do 2 key := A[next_elem] 3 place := next_elem − 1 4 while place > 0 and A[place] > key do 5 A[place + 1] := A[place] 6 place := place − 1 7 A[place + 1] := key (move the border between parts) (the next element to be handled) (find the right place for next element) (make space for the element) (put the element into the correct place) Tuning STL algorithms and containers COMP.CS.300 Tietorakenteet ja algoritmit 1 Matti Rintala (matti.rintala@tuni.fi) Tuning STL •Many algorithms takes as a parameter a lambda/function, which the algorithm uses (=calls) •For example sort & comparing elems, find_if what kind of elem to look for... •The behaviour of some containers can also be tuned •For example map and order of search keys Quicksort(A, left, right) 1 if left < right then (nothing to be done in trivial case) 2 pivot := Partition(A, left, right) (partition to small & large, pivot marks split) 3 Quicksort(A, left, pivot−1 ) (sort smaller than pivot) 4 Quicksort(A, pivot+1, right) (sort larger than pivot)"
            },
            {
                "role": "user",
                "content": "Can you summarize the key topics covered in this lecture?"
            },
            {
                "role": "assistant",
                "content": "This lecture covers foundational algorithmic concepts including problem formulation, correctness proof using induction, efficiency analysis using asymptotic notation, data structures (arrays, linked lists, BSTs, heaps), and sorting methods (selection sort, merge sort, counting sort). Additionally, it introduces the Word-RAM computational model, providing a theoretical framework for measuring algorithm efficiency."
            }
        ]
    },
    {
        "system": "Hello! I am Kiran, an AI assistant from Tampere University, dedicated to providing you with accurate and insightful assistance. My goal is to help you understand concepts, summarize lectures, and answer questions effectively.",
        "conversations": [
            {
                "role": "host",
                "content": "Invalidation in containers COMP.CS.300 Data structures and algorithms 1 Matti Rintala (matti.rintala@tuni.fi) Invalidation • Invalidated iterator no longer refers to a (correct) position after insert/remove • Do not use an invalidated iterator (assigning a new position is ok) • If used, result is undefined (crash/messed up data/???) Invalidation vector<int> v={ 1,2,3,4}; auto i = v.begin(); auto j = i+1; // next after i v.erase(i); *j = 3; // !!! j invalidated! Invalidation and choosing a container •Different containers have different rules for invalidation •Another selection criteria in addition to performance (often a compromise) •vector and deque: rules complicated •unordered_map/set: safe for erasing, insertion invalidates •map/set and (forward_)list almost safe Invalidation and choosing a container Container After insertion After erase Note Invalidated! - Capacity changed vector Ok Ok * Before insertion position Invalidated! Invalidated! After insertion position deque Invalidated! Ok * Insert/erase of 1./last Invalidated! Invalidated! Insert/erase of rest (forward_)list Ok Ok * (multi)map/set Ok Ok * unordered_(multi) map/set Invalidated! - Rehash occurred Ok Ok * How to notice invalidation •Careful planning! •Some compilers have STL-debug features Gcc: -D_GLIBCXX_DEBUG -D_GLIBCXX_DEBUG_PEDANTIC •Program crashes: debugger tells where? •Program gets messed up: debugger/printouts Invalidation, pointers and indices •Any indicator of position may get invalidated! •Pointer to element: element gets moved in memory •Index to element: Insertion or removal before element •cppreference.com has a more comprihensive table"
            },
            {
                "role": "user",
                "content": "Can you summarize the key topics covered in this lecture?"
            },
            {
                "role": "assistant",
                "content": "This lecture covers foundational algorithmic concepts including problem formulation, correctness proof using induction, efficiency analysis using asymptotic notation, data structures (arrays, linked lists, BSTs, heaps), and sorting methods (selection sort, merge sort, counting sort). Additionally, it introduces the Word-RAM computational model, providing a theoretical framework for measuring algorithm efficiency."
            }
        ]
    },
    {
        "system": "Hello! I am Kiran, an AI assistant from Tampere University, dedicated to providing you with accurate and insightful assistance. My goal is to help you understand concepts, summarize lectures, and answer questions effectively.",
        "conversations": [
            {
                "role": "host",
                "content": "C++ Standard Template Library (STL) COMP.CS.300 Data structures and algoritms 1 Matti Rintala (matti.rintala@tuni.fi) Provided library vs own implementation •Self-implemented data structure/algorithm: – How operations are implemented? •Ready-made library (STL): – Implementation hidden – How to use operations (interfaces) – How to choose suitable data structure/algorithm? – How to choose an efficient data structure/algorithm? – How to combine provided data structures/algorithms? – How to tune/customize provided functionality? Parts of STL Containers Generic algorithms Iterators Parts of STL Basic operations: • [] at • push_back • erase • size • clear • ... Parts of STL Generic algorithms: • for_each • find • binary_search • set_symmetric_ difference • transform_reduce • ... Parts of STL Basic operations: • [] at • push_back • erase • size • clear • ... Containers Generic algorithms Iterators Generic algorithms: • for_each • find • binary_search • set_symmetric_ difference • transform_reduce • ... STL and asymptotic performance •STL provides asymptotic performance guarantees – Often O-notation used (\"not slower than\") – Sometimes also average performance or Θ-notation STL and asymptotic performance •STL provides asymptotic performance guarantees – Often O-notation used (\"not slower than\") – Sometimes also average performance or Θ-notation •But what is \"n\"? Depends on the situation: – The number of elements – The size of a subset of elements – Sometimes several variables (O(m*n)) STL containers COMP.CS.300 Data structures and algorithms 1 Matti Rintala (matti.rintala@tuni.fi) STL containers •Sequences – User decides the order of elements – Elements are found based on their position •Associative containers – The container decided the element order (can also be undefined) – Elements are used based on a search key •(Container adaptors stack, queue, priority_queue) STL sequences •vector, deque, list •(array, forward_list) •User decided the element order •Elements found by indexing (position number) or iterating elements in order •Insertion/removal at given position (iterator) STL associative containers •Ordered map, set – Order based on the search key •unordered_map/_set – Order undefined, can change at any time! • Many elements per search key: (unordered_)multimap/set •Removal at given position (iterator) (found by searching) STL associative containers key = data? many elems/key? order based on key? N: ...map Y: ...set N Y: ...multi... Y N: unordered_... STL associative containers •Ordered map, set – Order based on the search key •unordered_map/_set – Order undefined, can change at any time! • Many elements per search key: (unordered_)multimap/set •Removal at given position (iterator) (found by searching) key = data? many elems/key? order based on key? N: ...map Y: ...set N Y: ...multi... Y N: unordered_... STL container performance COMP.CS.300 Data structures and algorithms 1 Matti Rintala (matti.rintala@tuni.fi) (Asymptotic) container performance •Usually asymptotic performance specified (often upper limit O) •Many containers similar in interface, but performance differs. •If some operation would be \"inefficient\", it may be omitted from container •Choosing a container: – Category (sequence/associative) – Frequent operations should be fast – (Invalidation, other smaller differences) Choosing container based on performance Container General add/remove Add to start/end Remove from start/end Search (position) Search (value/key) Largest/ smallest etc. vector O(n) - / O(1)* - / O(1)* O(1) ( O(n) ) ( O(n) ) deque O(n) O(1) / O(1) O(1) / O(1) O(1) ( O(n) ) ( O(n) ) list O(1) O(1) / O(1) O(1) / O(1) ( O(n) ) ( O(n) ) ( O(n) ) (array) - - - O(1) ( O(n) ) ( O(n) ) (forward_list) O(1) - / O(n)* - / O(n)* O(n) ( O(n) ) ( O(n) ) unordered_ map/set O(n) ≈ Θ(1) - - - O(n) ≈ Θ(1) ( O(n) ) map/set O(log n) - - ( O(n) ) O(log n) O(1) (stack) - - / O(1) - / O(1) - - - (queue) - O(1) / - - / O(1) - - - (priority_queue) O(log n) - - - - O(1) STL iterators COMP.CS.300 Data structures and algorithms 1 Matti Rintala (matti.rintala@tuni.fi) Idea of iterators •A bookmark into a container •Iterating through a container •A sub-range: 2 iterators (C++20 also: ranges) 10 4 2 6 9 3 14 8 5 i1 i2 b e Role of iterators in STL •Containers – begin(), end() – Inserting into given position – Erasing an element (or range) at given position – Operation results in a position (or range) •Algorithms – Expressing position and container – Expressing operation range – Operation results in a position (or range) •Reverse iterators rbegin(), rend() Iterator performance & iterator categories COMP.CS.300 Data structures and algorithms 1 Matti Rintala (matti.rintala@tuni.fi) Iterator performance and categories •Iterator operations (++, *, ...) constant time O(1) •Depends on container what operations iterators support •Iterator categories: what operations are provided •Algorithms may require iterators of certain category Iterator performance and categories Input iterator *(read), ++, ==, !=,=, -> Output iterator *(write), ++, ==, !=, =, -> Forward iterator * (read & write) Bidirectional iterator -- Random access iterator +=, -=, +, -, <, >, <=, =>, [] forward_list list (unordered_) (multi) map/set vector deque array Iterator performance and categories •Iterator operations (++, *, ...) constant time O(1) •Depends on container what operations iterators support •Iterator categories: what operations are provided •Algorithms may require iterators of certain category Input iterator *(read), ++, ==, !=,=, -> Output iterator *(write), ++, ==, !=, =, -> Forward iterator * (read & write) Bidirectional iterator -- Random access iterator +=, -=, +, -, <, >, <=, =>, [] forward_list list (unordered_) (multi) map/set vector deque array Invalidation in containers COMP.CS.300 Data structures and algorithms 1 Matti Rintala (matti.rintala@tuni.fi) Invalidation • Invalidated iterator no longer refers to a (correct) position after insert/remove • Do not use an invalidated iterator (assigning a new position is ok) • If used, result is undefined (crash/messed up data/???) Invalidation vector<int> v={ 1,2,3,4}; auto i = v.begin(); auto j = i+1; // next after i v.erase(i); *j = 3; // !!! j invalidated! Invalidation and choosing a container •Different containers have different rules for invalidation •Another selection criteria in addition to performance (often a compromise) •vector and deque: rules complicated •unordered_map/set: safe for erasing, insertion invalidates •map/set and (forward_)list almost safe Invalidation and choosing a container Container After insertion After erase Note Invalidated! - Capacity changed vector Ok Ok * Before insertion position Invalidated! Invalidated! After insertion position deque Invalidated! Ok * Insert/erase of 1./last Invalidated! Invalidated! Insert/erase of rest (forward_)list Ok Ok * (multi)map/set Ok Ok * unordered_(multi) map/set Invalidated! - Rehash occurred Ok Ok * How to notice invalidation •Careful planning! •Some compilers have STL-debug features Gcc: -D_GLIBCXX_DEBUG -D_GLIBCXX_DEBUG_PEDANTIC •Program crashes: debugger tells where? •Program gets messed up: debugger/printouts Invalidation, pointers and indices •Any indicator of position may get invalidated! •Pointer to element: element gets moved in memory •Index to element: Insertion or removal before element •cppreference.com has a more comprihensive table"
            },
            {
                "role": "user",
                "content": "Can you summarize the key topics covered in this lecture?"
            },
            {
                "role": "assistant",
                "content": "This lecture covers foundational algorithmic concepts including problem formulation, correctness proof using induction, efficiency analysis using asymptotic notation, data structures (arrays, linked lists, BSTs, heaps), and sorting methods (selection sort, merge sort, counting sort). Additionally, it introduces the Word-RAM computational model, providing a theoretical framework for measuring algorithm efficiency."
            }
        ]
    },
    {
        "system": "Hello! I am Kiran, an AI assistant from Tampere University, dedicated to providing you with accurate and insightful assistance. My goal is to help you understand concepts, summarize lectures, and answer questions effectively.",
        "conversations": [
            {
                "role": "host",
                "content": "Säiliön valinta tehokkuuden perusteella Säiliö Yleinen lisäys/poisto Erik. lisäys alku/loppu Erik. poisto alku/loppu Haku (järjestysnro) Haku (arvo/avain) Suurin/pienin yms. vector O(n) - / O(1)* - / O(1)* O(1) ( O(n) ) ( O(n) ) deque O(n) O(1) / O(1) O(1) / O(1) O(1) ( O(n) ) ( O(n) ) list O(1) O(1) / O(1) O(1) / O(1) ( O(n) ) ( O(n) ) ( O(n) ) (array) - - - O(1) ( O(n) ) ( O(n) ) (forward_list) O(1) O(1) / ( O(n) ) O(1) / ( O(n) ) O(n) ( O(n) ) ( O(n) ) unordered_ map/set O(n) ≈ Θ(1) - - - O(n) ≈ Θ(1) ( O(n) ) map/set O(log n) - - ( O(n) ) O(log n) O(1) (stack) - - / O(1) - / O(1) - - - (queue) - O(1) / - - / O(1) - - - (priority_queue) O(log n) - - - - O(1)"
            },
            {
                "role": "user",
                "content": "Can you summarize the key topics covered in this lecture?"
            },
            {
                "role": "assistant",
                "content": "This lecture covers foundational algorithmic concepts including problem formulation, correctness proof using induction, efficiency analysis using asymptotic notation, data structures (arrays, linked lists, BSTs, heaps), and sorting methods (selection sort, merge sort, counting sort). Additionally, it introduces the Word-RAM computational model, providing a theoretical framework for measuring algorithm efficiency."
            }
        ]
    },
    {
        "system": "Hello! I am Kiran, an AI assistant from Tampere University, dedicated to providing you with accurate and insightful assistance. My goal is to help you understand concepts, summarize lectures, and answer questions effectively.",
        "conversations": [
            {
                "role": "host",
                "content": "COMP.CS.300 103 6 C++ standard library This chapter covers the data structures and algorithms in the C++ standard library. The emphasis is on things that make using the library purposefull and efﬁcient. Topics that are important from the library implementation point of view are not discussed here. COMP.CS.300 104 6.1 General information on the C++ standard library The standard library, commonly called STL, was standardized together with the C++-language autumn 1998, and it has been somewhat extended in later versions. The latest version of the standard C++17 is from 2017. It contains the most important basic data structures and algorithms. • most of the data structures and algorithms covered earlier in this material in one shape or another It also contains a lot more such as • input / output: cin, cout, . . . • processing character strings • minimum, maximum • search and modifying operations of queues • support for functional programming • complex numbers COMP.CS.300 105 • arithmetic functions (e.g. sin, log10), • vector arithmetics and support for matrix operations The interfaces are carefully thought out, ﬂexible, generic and type safe The efﬁciency of the operations the interfaces provide has been given with the O-notation. The compile time C++ template mechanism has been used to implement the genericity. Interesting elements in the standard library from the point of view of a data structures course are the containers, i.e. the data structures provided by the library and the generic algorithms together with the iterators, with which the elements of the containers are manipulated. Lambdas were introduced with C++11 and play a key role as well. COMP.CS.300 106 6.2 Iterators We see all standard library data structures as black boxes with a lot of common characteris- tics. The only thing we know is that they contain the elements we’ve stored in them and that they implement a certain set of interface functions. We can only handle the con- tents of the containers through the interface functions and with iterators. Iterator Iterator Container reading changing x q move p Elem1 Elem2 Elem3 Elem4 Elem5 Iterator COMP.CS.300 107 Iterators are handles or “bookmarks” to the elements in the data structure. • each iterator points to the beginning or the end of the data structure or between two elements. • the element on the right side of the iterator can be accessed through it, except if the iterator in question is a reverse iterator which is used to access the element on the left side. • the operations of moving the reverse iterator work in reverse, for example ++ moves the iterator one step to the left • the interface of the containers usually contains the functions begin() and end(), that return the iterators pointing to the beginning and the end of the container • functions rbegin() and rend() return the equivalent reverse iterators • an iterator can be used to iterate over the elements of the container, as the name suggests • an iterator can be used for reading and writing COMP.CS.300 108 • the location of the elements added to or removed from a container is usually indicated with iterators Each container has its own iterator type. • different containers provide different possibilities of moving the iterator from one place to another efﬁciently (cmp. reading an arbitrary element from a list/array) • the design principle has been that all iterator operations must be performed in constant time to quarantee that the generic algorithms work with the promised efﬁciency regardless of the iterator given to them • iterators can be divided into categories based on which constant time operations they are able to provide COMP.CS.300 109 An input iterator can only read element values but not change them • the value of the element the iterator points to can be read (*p) • a ﬁeld of the element the iterator points to can be read or its member func- tion can be called (p->) • the iterator can be moved one step forwards (++p or p++) • iterators can be assigned to and compared with each other (p=q, p==q, p!=) deque vector set multiset map multimap list T[] * (read), ==, !=, ++, =, −> * (write), ==, !=, ++, =, −> * (read/write) −− +=, −=, +, −, [], <, >, <=, >= input iterator output iterator forward iterator bidirectional iterator random access iterator An output iterator is like an input iterator but it can be used only to change the elements (*p=x) COMP.CS.300 110 A forward iterator is a combination of the interfaces of the input- and output iterators. A bidirectional iterator is able to move one step at a time backwards(--p or p--) A random access iterator is like a bidirectional iterator but it can be moved an arbitrary amount of steps forwards or backwards. • the iterator can be moved n steps forwards or backwards (p+=n, p-=n, q=p+n, q=p-n) • an element n steps from the iterator can be read and it can be modiﬁed (p[n]) • the distance between two iterators can be determined (p-q) • the difference of two iterators can be compared, an iterator is “smaller” than the other if its location is earlier than the other in the container (p<q, p<=q, p>q, p>=q) COMP.CS.300 111 The syntax of the iterator operations is obviously similar to the pointer aritmetic of C++. Iterators can be used with #include <iterator> An iterator of a correct type can be created with the following syntax for example. container<type stored>::iterator p; The key word auto is useful with iterators: auto p = begin( cont ); // →std::vector<std::string>::iterator The additions and removals made to the containers may invalidate the iterators already pointing to the container. • this feature is container speciﬁc and the details of it are covered with the containers COMP.CS.300 112 In addition to the ordinary iterators STL provides a set of iterator adapters. • they can be used to modify the functionality of the generic algorithms • the reverse iterators mentioned earlier are iterator adapters • insert iterators/inserters are one of the most important iterator adapters. – they are output iterators that insert elements to the desired location instead of copying – an iterator that adds to the beginning of the container is given by the function call front_inserter(container) – an iterator that adds to the end is given by back_inserter(container) – an iterator that adds to the given location is given by inserter(container, location) COMP.CS.300 113 • stream iterators are input and output iterators that use the C++ streams instead of containers – the syntax for an iterator that reads from a stream with the type cin is istream_iterator<type> (cin) – the syntax for an iterator that prints the given data to the cout stream data separated with a comma is ostream_iterator<type> (cout, ’,’) • move iterators replace the copying of an element with a move. COMP.CS.300 114 6.3 Containers The standard library containers are mainly divided into two categories based on their interfaces: • sequences – elements can be searched based on their number in the container – elements can be added and removed at the desired location – elements can be scanned based on their order in the container • associative containers – elements are placed in to the container to the location determined by their key – by default the operator < can be used to compare the key values of the elements in ordered containers • the interface’s member functions indice how the container is supposed to be used. COMP.CS.300 115 The containers: Container type Library Sequences array vector deque list (forward_list) Assosiative map containers set unordered unordered_map assosiative unordered_set Adapters queue stack COMP.CS.300 116 Containers are passed by value. • the container takes a copy of the data stored in it • the container returns copies of the data it contains ⇒changes made outside the container do not effect the data stored in the container • all elements stored into the containers must implement a copy constructor and an assignment operator. – basic types have one automatically • elements of a type deﬁned by the user should be stored with a pointer pointing to them – this is sensible from the efﬁciency point of view anyway COMP.CS.300 117 • the shared_pointer is available in C++11 for easier management of memory is situations where several objects share a resource – contains an inbuild reference counter and deletes the element once the counter becomes zero – there is no need to call delete. Furthermore is must not be called. – deﬁnition: auto pi = std::make_shared<Olio>(params); – handy especially if a data structure ordered based on two different keys is needed * store the satellite data with a shared_pointer * store the pointers in two different containers based on two different keys COMP.CS.300 118 Sequence containers: Array array<type> is a constant sized array. • Initialization std::array<type, size> a = {val, val,...}; • Can be indexed with .at() or with []. Functions front() and back() access the ﬁrst and the last element. • Provides iterators and reverse iterators. • empty(), size() and max_size() • The function data() accesses the underlying array. Operations are constant time except fill() and swap() which are O(n) COMP.CS.300 119 Vector vector<type> is an ﬂexible sized array where additions and removals are efﬁcient at the end • Initialization vector<int> v {val, val, ...}; • Constant time indexing with .at(), [] and a (amortized) constant time addition with push_back() and removal with pop_back() at the end of the vector. • Inserting an element elsewhere with insert() and removal with erase is linear, O(n) • emplace_back(args); builds the element directly into the vector • The size can be increased with .resize(size, initial_val); – the initial value is voluntary – if needed, the vecor can allocate more memory automatically – memory can also be reserved in advance: .reserve(size), .capacity() • iterators are invalidated in the following situations COMP.CS.300 120 – if the vector originally didn’t have enough space reserved for it any addition can cause the invalidation of all iterators – the removals only invalidate those iterators that point to the elements after the removal point – additions to the middle always invalidate the iterators after the addition point • a special implementation has been given to vector<bool> which is different from the general implementation the templates would create in order to save memory – the goal: makes 1 bit / element possible where the ordinary implementation would probably use 1 byte / element i.e. 8 bits / element COMP.CS.300 121 Deque deque<type> is an array open at both ends • initialization: deque<type> d {val, val, val...}; • provides similar services and almost the same interface as vector • in addition, provides an efﬁcient (O(1) amortized running-time) of addition and removal at both ends .push_front(val), .emplace_front(args), .pop_front() • iterators are invalidated in the following situations – all additions can invalidate the iterators – removals at the middle invalidate all iterators – all addition and removal operations elsewhere except at the ends can invalidate references and pointers COMP.CS.300 122 List is a container that support bidirectional iteration • initialization: list<type> l {val, val, val }; • addition and removal is everywhere constant time, there is no indexing operation • addition and removal don’t invalidate the iterators or references (except naturally to elements removed) • list provides several special services – .splice(location, list2) makes the other list a part of the second list infront of the location – .splice(location, list, val) moves an element from another list or the same list infront of the location – .splice(location, list, beg, end) – .merge(list2) and .sort(), stable, O(nlogn) on average – .reverse(), linear COMP.CS.300 123 Amortized running time Vector is a ﬂexible sized array, i.e. the size increases when needed • when a new element no longer ﬁts into the array, a new, larger one is allocated and all the elements are moved there • the array never gets smaller ⇒the memory allocation is only reduced when a new contents are copied on top of the old one ⇒Adding an element to the end of the vector was mentioned as amortized constant time • the performance is analyzed as a unit, the efﬁciency of a sequence of operations is investigated instead of single operations • each addition operation that requires expensive memory allocation is preceeded by an amount of inexpensive additions relative to the price of the expensive operation COMP.CS.300 124 • the cost of the expensive operation can be equally divided to the inexpensive operations • now the inexpensive operations are still constant time, although they are a constant coefﬁcient more inefﬁcient than in reality • the savings can be used to pay for the expensive operation ⇒all addition operations at the end of a vector can be done in amortized constant time COMP.CS.300 125 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 This can be proven with the accounting method: • charge three credits for each addition • one credit is used for the real costs of the addition • one credit is saved with the element added to i • one credit is saved at the element at i −1 2 · vector.capacity() • when the size of the array needs to be increased, each element has one credit saved and the expensive copying can be payed for with them COMP.CS.300 126 Associative containers: set<type> and multiset<type> is a dynamic set where • elements can be searched, added and deleted in logarithmic time • elements can be scanned in ascending order in amortized constant time so that the scan from the beginning to the end is always a linear time operation • an order of size must be deﬁned for the elements “<” – can be implemented separately as a part of the type or as a parameter of the constructor • determines the equality with ¬(x<y ∨y<x) ⇒a sensible and efﬁcient deﬁnition to “<” needs to be given • the same element can be in the multiset several times, in the set the elements are unique • changing the value of the element directly has been prohibited COMP.CS.300 127 – the old element needs to be removed and a new one added instead • interesting operations: – .find(val) ﬁnds the element (ﬁrst of many in the multiset) or returns .end() if it isn’t found – .lower_bound(val) ﬁnds the ﬁrst element ≥element – .upper_bound(val) ﬁnds the ﬁrst > element – .equal_range(val)returns make_pair( .lower_bound(val), .upper_bound(val) ) but needs only one search (the size of the range is 0 or 1 for the set) – for sets insert returns a pair (location, added), since elements already in the set may not be added • the standard quarantees that the iterators are not invalidated by the addition or removal (except of course to the elements removed) COMP.CS.300 128 map<key_type, val_type> and multimap<key_type, val_type> • store ( key, satellite data ) pairs – the type of the pair is pair<type1, type2> – a pair can be created with the function make_pair – the ﬁelds of the pair are returned by .first(), .second() • initialization std::map<key_type, val_type> m {{key1, val1}, {key2, val2}, key3, val3},...}; e.g. std::map<std::string,int> anim { {\"bear\",4}, {\"giraffe\",2}, {\"tiger\",7} }; • map can be exceptionally indexed with the key O(nlogn) – If the key is not found, a new valuepair key-type is added to the container • the iterators are not invalidated by the addition or removal COMP.CS.300 129 Hash tables Unordered set/multiset is a structure that contains a set of elements and unodered map/multimap contains a set of key-value pairs. • the interfaces of unordered-map/set resemble map and set • the most signiﬁcant differences: – the elements are not ordered (unordered) – addition, removal and searching is on average constant time and in the worst-case linear – a set of member function valuable for hashing, such as rehash(size), load_factor(), hash_function() ja bucket_size(). • the size of the hash table is automatically increased in order to keep the load factor of the buckets under a certain limit – changing the size of the hash table (rehashing) is on average linear, worst-case quadratic – rehashing invalidates all iterators but not pointers or references COMP.CS.300 130 Additionally other containers are found in the Standard library: bitset<bit_amount> • #include<bitset> • for handling ﬁxed sized binary bitsets • provides typical operations (AND, OR, XOR, NOT) Strings string • #include <string> • the character string has been optimized for other purposes in C++ and they are usually not percieved as containers. However, they are, in addition to other purposes, also containers. • store characters but can be made to store other things also • they provide, among others, iterators, [. . . ], .at(. . . ), .size(), .capacity() and swap • string can get very large and automatically allocate new memory when necessary COMP.CS.300 131 • Care should be taken with the modifying operations of strings (catenation, removal) since they allocate memory and copy elements, which makes them heavy operations for long strings • it is often sensible anyway to store the strings with a pointer when for example storing them into the containers to avoid needless copying • for the same reason strings should be passed by reference In addition to containers, STL provides a group of container adapters, that are not containers themselves but that can be “adapted into a new form” through the interface of the container: COMP.CS.300 132 Stack stack<element_type, container_type> • provides in addition to the normal class-operations only – stack-operations, .push(. . . ), .top(), .pop() – size queries .size() and .empty() – comparisons “==”, “<” etc. • .pop() doesn’t return anything, the topmost element is evaluated with .top() • the topmost element of the stack can be changed in place: stack.top() = 35; • what’s interesting from the course’s point of view is that the user can choose an implementation based on different containers – any container that provides back(), push_back() and pop_back() is usable. Especially vector, list and deque. – stack<type> basic_stack; (deque) – stack<type, list<type> > list_stack; COMP.CS.300 133 Queue queue<element_type, container_type> • queue operations .push(. . . ), .pop(), .front(), .back()(!) • otherwise more or less like the stack Priority queue priority_queue<element_type, container\\_type> • has an almost identical interface as the queue • implemented with a heap • any container that provides front(), push_back() and pop_back() and random access iteration can be used. Especially vector (default) and deque • elements have a different order: .top() returns the largest • returns any of the equal elements • the topmost element cannot be changed with top in place • like with associative containers, the sorting criterion can be given as a parameter to <> or as the constructor parameter COMP.CS.300 134 data- add add remove remove nth elem. search remove structure to end elsewhere 1st elem. elem. (index) elem. largest array O(1) O(n)[2] vector O(1) O(n) O(n) O(n)[1] O(1) O(n)[2] O(n)[3] list O(1) O(1) O(1) O(1) O(n) O(n) O(n)[3] deque O(1) O(n)[4] O(1) O(n)[1] O(1) O(n)[2] O(n)[3] stack[9] O(1) O(1)[5] queue[9] O(1)[6] O(1)[7] priority O(log n) O(log n)[8] queue[9] [10] set O(log n) O(log n) O(log n) O(n) O(log n) O(log n) (multiset) map O(log n) O(log n) O(log n) O(n) O(log n) O(log n) (multimap) unordered_ O(n) O(n) O(n) O(n) (multi)set ≈Θ(1) ≈Θ(1) ≈Θ(1) O(n) unordered_ O(n) O(n) O(n) O(n) (multi)map ≈Θ(1) ≈Θ(1) ≈Θ(1) O(n) COMP.CS.300 135 [1] constant-time with the last element, linear otherwise [2] logarithmic if the container is sorted, else linear [3] constant time if the data structure is sorted, else linear [4] constant time with the ﬁrst element, else linear [5] possible only with the last element [6] addition possible only at the beginning [7] removal possible only at the end [8] query constant time, removal logarithmic [9] container adapter [10] addition is done automatically based on the heap order COMP.CS.300 136 6.4 Generic algorithms The standard library contains most of the algorithms covered so far. All algorithms have been implemented with function templates that get all the necessary information about the containers through their parameters. The containers are not however passed directly as a parameter to the algorithms, iterators to the containers are used instead. • parts of the container can be handled instead of the complete container • the algorithm can get an iterator to containers of different type which makes it possible to combine the contents of a vector and a list and store the result into a set • the functionality of the algorithms can be changes with iterator adapters COMP.CS.300 137 • the programmer can implement iterators for his/her own data structures which makes using algorithms possible also with them Not all algorithms can be applied to all data structures efﬁciently. ⇒part of the algorithms accept only iterators from one iterator category as parameters. • this ensures the efﬁciency of the algorithms since all operations the iterator provides are constant time • if the iterator is of an incorrect type, a compile-time error is given ⇒if the algorithm is given a data structure for which it cannot be implemented efﬁciently, the program won’t even compile COMP.CS.300 138 The standard library algorithms are in algorithm. In addition the standard deﬁnes the C-language library cstdlib. Algorithms are divided into three main groups: Non-modifying sequence operations, modifying sequence operations and sorting and related operations. A short description on some of the algorithms that are interesting from the course’s point of view (in addition there are plenty of straightforward scanning algorithms and such): COMP.CS.300 139 Binary search • binary_search(ﬁrst, end, value) tells if the value is in the sorted sequence – ﬁrst and end are iterators that indicate the beginning and the end of the search area, which is not necessarily the same as the beginning and the end of the data structure • there can be several successive elements with the same value ⇒lower_bound and upper_bound return the limits of the area where the value is found – the lower bound is and the upper bound isn’t in the area • the limits can be combined into a pair with one search: equal_range • cmp. BIN-SEARCH COMP.CS.300 140 Sorting algorithms • sort(beg, end) and stable_sort(beg, end) • the running-time of sort is O(nlogn) and stable_sort is O(nlogn) is enough memory is available and O(nlog2n) otherwise • the sorting algorithms require random access iterators as parameter ⇒cannot be used with lists, but list provides a sort of its own (and an non-copying merge) as a member function • there is also a sort that ends once a desired amount of the ﬁrst elements are sorted: partial_sort(beg, middle, end) • in addition is_sorted(beg, end) and is_sorted_until(beg, end) nth_element( ﬁrst, nth, end ) • ﬁnds the element that would be at index nth in a sorted container • resembles RANDOMIZED-SELECT • iterators must be random-access COMP.CS.300 141 Partition • partition(ﬁrst, end, condition) unstable • stable_partition(ﬁrst, end, condition) stable but slower and/or reserves more memory • sorts the elements in the range ﬁrst - end so that the elements for which the condition-function returns true come ﬁrst and then the ones for which condition is false. • cmp. QUICK-SORT’s PARTITION • the efﬁciency of partition is linear • the iterators must be bidirectional • in addition is_partitioned and partition_point COMP.CS.300 142 merge( beg1, end1, beg2, end2, target) • The algorithm merges the elements in the ranges beg1 - end1 and beg2 - end2 and copies them in an ascending order to the end of the iterator target • the algorithm requires that the elements in the two ranges are sorted • cmp. MERGE • the algorithm is linear • beg- and end-iterators are input iterators and target is an output iterator Heaps • Heap algorithms equivalent to those described in chapter 3.1. can be found in STL • push_heap( ﬁrst, end) HEAP-INSERT COMP.CS.300 143 • pop_heap( ﬁrst, end ) makes the topmost element the last (i.e. to the location end −1) and executes HEAPIFY to the range ﬁrst . . . end −1 – cmp. HEAP-EXTRACT-MAX • make_heap( ﬁrst, end) BUILD-HEAP • sort_heap( ﬁrst, end ) HEAPSORT • the iterators must be random-access • in addition is_heap and is_heap_until COMP.CS.300 144 Set operations • The C++ standard library contains functions that support this • includes( ﬁrst1, end1, ﬁrst2, end2 ) subset ⊆ • set_union( ﬁrst1, end1, ﬁrst2, end2, result ) union ∪ • set_intersection(. . . ) intersection ∩ • set_difference(. . . ) difference - • set_symmetric_difference(. . . ) • ﬁrst- and end-iterators are input iterators and result is an output iterator find_first_of( ﬁrst1, end1, ﬁrst2, end2 ) • there is a condition in the end that limits the elements investigated • ﬁnds the ﬁrst element from the ﬁrst queue that is also in the second queue • the queue can be an array, a list, a set , . . . COMP.CS.300 145 • a simple implementation is Θ(nm) in the worst case where n and m are the lengths of the queues • the second queue is scanned for each element in the ﬁrst queue – the slowest case is when nothing is found ⇒slow if both queues are long • the implementation could be made simple, efﬁcient and memorysaving by requiring that the queues are sorted NOTE! None of the STL algorithms automatically make additions or removals to the containers but only modify the elements in them • for example merge doesn’t work if it is given an output iterator into the beginning of an empty container • if the output iterator is expected to make additions instead of copying the iterator adapter insert iterator must be used (chapter 6.2) COMP.CS.300 146 6.5 Lambdas: [](){} Situations where a need to pass functionality on to the functions arise often with the algorithm library • e.g. find_if, for_each, sort Lambdas are nameless functions of an undeﬁned type. They take parameters, return a value and can refer to the creation environment and change it. Syntax: [environment](parameters)->returntype {body} • environment is empty if the lamba does not refer to its environment • parameter can be left out • if there is no ->returntype it is void. It can also be deducted from a simple return statement. • e.g. [](int x, int y){ return x+y;} for_each( v.begin(), v.end(),[] (int val) {cout<<val<<endl;}); std::cin >> lim; //local var std:find_if(v.begin(), v.end(),[lim](int a){return a<lim;}); COMP.CS.300 147 STL algorithms can be seen as named special loops whose body is given in the lambda. bool all = true; for (auto i : v) { if (i%10 != 0) { all = false; break; } } if (all) {...} if (std::all_of(v.begin(), v.end(), [](int i){return i%10==0;}){...}"
            },
            {
                "role": "user",
                "content": "Can you summarize the key topics covered in this lecture?"
            },
            {
                "role": "assistant",
                "content": "This lecture covers foundational algorithmic concepts including problem formulation, correctness proof using induction, efficiency analysis using asymptotic notation, data structures (arrays, linked lists, BSTs, heaps), and sorting methods (selection sort, merge sort, counting sort). Additionally, it introduces the Word-RAM computational model, providing a theoretical framework for measuring algorithm efficiency."
            }
        ]
    },
    {
        "system": "Hello! I am Kiran, an AI assistant from Tampere University, dedicated to providing you with accurate and insightful assistance. My goal is to help you understand concepts, summarize lectures, and answer questions effectively.",
        "conversations": [
            {
                "role": "host",
                "content": "Tips for increasing the actual performance COMP.CS.300 Data structures and algorithms 1 Matti Rintala (matti.rintala@tuni.fi) Design/implementation •Pseudocode/designing – Estimate of asymptotic performance – Asymptotically best algorithm choices •Implementation/coding – As efficient implementation as possible – Optimization often affects only the constant coefficient (still important!) Improving asymptotic performance •Choose the asymptotically best algorithm •Avoid doing unnecessary work – Choose an algorithm that only does what is needed •Make sure that used containers etc. provide the asymptotic performance that the algorithms expect Tips for optimizing the actual performance •Avoid doing the same thing again and again (when not necessary) – Store (intermediate) results – Use pointers etc. •Remember often used (small) results – They don't have to be calculated again •Sharing data vs. copying •Doing a \"small\" thing on the side of a larger thing"
            },
            {
                "role": "user",
                "content": "Can you summarize the key topics covered in this lecture?"
            },
            {
                "role": "assistant",
                "content": "This lecture covers foundational algorithmic concepts including problem formulation, correctness proof using induction, efficiency analysis using asymptotic notation, data structures (arrays, linked lists, BSTs, heaps), and sorting methods (selection sort, merge sort, counting sort). Additionally, it introduces the Word-RAM computational model, providing a theoretical framework for measuring algorithm efficiency."
            }
        ]
    },
    {
        "system": "Hello! I am Kiran, an AI assistant from Tampere University, dedicated to providing you with accurate and insightful assistance. My goal is to help you understand concepts, summarize lectures, and answer questions effectively.",
        "conversations": [
            {
                "role": "host",
                "content": "COMP.CS.300 Tietorakenteet ja algoritmit 1 1 COMP.CS.300 Tietorakenteet ja algoritmit 1 COMP.CS.300 Tietorakenteet ja algoritmit 1 2 Lähteet Luentomoniste pohjautuu vahvasti prof. Antti Valmarin vanhaan luentomonisteeseen Tietorakenteet ja algoritmit. Useimmat algoritmit ovat peräisin kirjasta Introduction to Algorithms; Thomas H. Cormen, Charles E. Leiserson, Ronald L. Rivest, Clifford Stein. Lisäksi luentomonistetta koottaessa on käytetty seuraavia kirjoja: • Introduction to The Design & Analysis of Algorithms; Anany Levitin • Olioiden ohjelmointi C++:lla; Matti Rintala, Jyke Jokinen • Tietorakenteet ja Algoritmit; Ilkka Kokkarinen, Kirsti Ala-Mutka • The C++ Standard Library; Nicolai M. Josuttis COMP.CS.300 Tietorakenteet ja algoritmit 1 3 1 Johdanto Mietitään ensin hiukan syitä tietorakenteiden ja algoritmien opiskelulle Algorithms in the world COMP.CS.300 Tietorakenteet ja algoritmit 1 4 1.1 Miksi? Mitkä ovat sinun elämääsi eniten vaikuttavat algoritmit? Picture: Chris Watt COMP.CS.300 Tietorakenteet ja algoritmit 1 5 Tietokoneohjelmia ei ole olemassa ilman algoritmeja • algoritmeihin törmäät esimerkiksi seuraavissa sovelluksissa: COMP.CS.300 Tietorakenteet ja algoritmit 1 6 COMP.CS.300 Tietorakenteet ja algoritmit 1 7 aina, kun käytät tietokonetta, käytät myös algoritmeja. COMP.CS.300 Tietorakenteet ja algoritmit 1 8 1.2 Kaikki pennillä? Via: The Washington Post COMP.CS.300 Tietorakenteet ja algoritmit 1 9 Tietorakenteita tarvitaan ohjelmissa käsiteltävän tiedon tallettamiseen ja sen käsittelyn mahdollistamiseen ja helpottamiseen • tietorakenteita on monia eivätkä ne kaikki sovi kaikkiin tilanteisiin ⇒ohjelmoijan pitää osata valita tilanteeseen sopivin ⇒vaihtoehtojen käyttäytyminen, vahvuudet ja heikkoudet on tunnettava Modernit ohjelmointikielet tarjoavat valmiina helppokäyttöisiä tietorakenteita. Näiden ominaisuuksien sekä käyttöön vaikuttavien rajoitteiden tuntemiseksi tarvitaan perustietorakenneosaamista COMP.CS.300 Tietorakenteet ja algoritmit 1 10 COMP.CS.300 Tietorakenteet ja algoritmit 1 11 Kuinka moni on turhautunut ohjelman tai esimerkiksi kännykän hitauteen? • toiminnallisuus on toki ensisijaisen tärkeää kaikille ohjelmille, mutta tehokkuus ei ole merkityksetön sivuseikka • on tärkeää huomioida ja miettiä ratkaisujaan myös ajan- ja muistinkäytön kannalta • valmiin kirjaston käyttö näyttää usein suoraviivaisemmalta kuin onkaan Näitä asioita käsitellään tällä kurssilla COMP.CS.300 Tietorakenteet ja algoritmit 1 12 2 Käsitteitä ja merkintöjä Tässä luvussa esitellään kurssilla käytettävää käsitteistöä ja merkintätapoja. Luvussa käsitellään myös pseudokoodiesityksen ja ohjelmointikielisen koodin eroja. Esimerkkinä käytetään järjestämisalgoritmia INSERTION-SORT. COMP.CS.300 Tietorakenteet ja algoritmit 1 13 2.1 Tavoitteet Kurssin keskeisenä tavoitteena on antaa opiskelijalle käyttöön peruskoneisto kuhunkin ohjelmointitehtävään sopivan ratkaisun valitsemiseen ja omien ratkaisujen tehokkuuden arvioimiseen karkealla tasolla. • Kurssilla keskitytään tilanteeseen sopivan tietorakenteen valintaan. • Lisäksi käsitellään käytännön tilanteissa usein vastaan tulevia ongelmatyyppejä ja algoritmeja, joilla ne voi ratkaista. COMP.CS.300 Tietorakenteet ja algoritmit 1 14 • Kurssilla keskitytään lähinnä ns. hyviin algoritmeihin. • Painotus on siis siinä, miten algoritmin ajankulutus kasvaa syötekoon kasvaessa, eikä niinkään yksityiskohtien optimoinnissa. COMP.CS.300 Tietorakenteet ja algoritmit 1 15 2.2 Peruskäsitteistöä Tietorakenne COMP.CS.300 Tietorakenteet ja algoritmit 1 16 Tapa tallettaa ja järjestää tietoa: • tietoa pystytään lisäämään ja hakemaan algoritmien avulla. • talletettua tietoa voidaan muokata • tietorakenteita on useamman tasoisia: tietorakenne voi koostua toisista tietorakenteista COMP.CS.300 Tietorakenteet ja algoritmit 1 17 Algoritmi: Kuva 1: kuva: Aldo Cortesi COMP.CS.300 Tietorakenteet ja algoritmit 1 18 • Joukko ohjeita tai askeleita jonkin ongelman ratkaisemiseksi • Hyvin määritelty laskentamenetelmä, joka saa syötteenään alkion tai joukon alkioita ja ja tuottaa tuloksenaan alkion tai joukon alkioita • Tapa tuottaa syötteestä tulos COMP.CS.300 Tietorakenteet ja algoritmit 1 19 • hyvin määritelty = – jokainen askel on kuvattu niin tarkasti, että lukija (ihminen tai kone) osaa suorittaa sen – jokainen askel on määritelty yksikäsitteisesti – samat vaatimukset pätevät askelten suoritusjärjestykselle – suorituksen tulee päättyä äärellisen askelmäärän jälkeen COMP.CS.300 Tietorakenteet ja algoritmit 1 20 Algoritmi ratkaisee jonkin hyvin määritellyn (laskenta)tehtävän. • laskentatehtävä määrittelee, missä suhteessa tulosten tulee olla annettuihin syötteisiin • esimerkiksi: – taulukon järjestäminen syötteet: jono lukuja a1, a2, . . . , an tulokset: luvut a1, a2, . . . , an suuruusjärjestyksessä pienin ensin – lentoyhteyksien etsiminen syötteet: lentoreittiverkosto eli kaupunkeja joiden välillä lentoyhteyksiä tulokset: Lentojen numerot, yhteyden tiedot ja hinta. COMP.CS.300 Tietorakenteet ja algoritmit 1 21 • laskentatehtävän esiintymä eli instanssi saadaan antamalla tehtävän syötteille lailliset arvot – järjestämistehtävän instanssiesimerkki: 31, 41, 59, 26, 41, 58 Algoritmi on oikea (correct), jos se pysähtyy ja antaa oikeat tulokset aina kun sille on annettu laillinen syöte. COMP.CS.300 Tietorakenteet ja algoritmit 1 22 • algoritmin tai laskentatehtävän määritelmä saa kieltää osan muodollisesti mahdollisista syötteistä COMP.CS.300 Tietorakenteet ja algoritmit 1 23 algoritmi voi olla virheellinen kolmella tavalla – antaa väärän lopputuloksen – kaatuu kesken suorituksen – ei koskaan lopeta virheellinenkin algoritmi on joskus hyvin käyttökelpoinen, jos virhetiheys hallitaan! – esim. luvun testaus alkuluvuksi COMP.CS.300 Tietorakenteet ja algoritmit 1 24 Periaatteessa mikä tahansa menetelmä kelpaa algoritmien esittämiseen, kunhan tulos on tarkka ja yksikäsitteinen. • yleensä algoritmit toteutetaan tietokoneohjelmina tai laitteistoina • käytännön toteutuksessa on otettava huomioon monia insinöörinäkökohtia – sopeuttaminen käyttötilanteeseen – syötteiden laillisuuden tarkistukset – virhetilanteiden käsittely – ohjelmointikielen rajoitukset – laitteiston ja kielen aiheuttamat nopeus- ja tarkoituksenmukaisuusnäkökohdat – ylläpidettävyys ⇒modulaarisuus jne. ⇒algoritmin idea hukkuu helposti toteutusyksityiskohtien alle COMP.CS.300 Tietorakenteet ja algoritmit 1 25 Tällä kurssilla keskitytään algoritmien ideoihin ja algoritmit esitetään useimmiten pseudokoodina ilman laillisuustarkistuksia, virheiden käsittelyä yms. Otetaan esimerkiksi pienten taulukoiden järjestämiseen soveltuva algoritmi INSERTION-SORT: Kuva 2: kuva: Wikipedia COMP.CS.300 Tietorakenteet ja algoritmit 1 26 • periaate: – toiminnan aikana taulukon alkuo- sa on järjestyksessä ja loppuosa ei – osien raja lähtee liikkeelle paikko- jen 1 ja 2 välistä ja etenee askel ker- rallaan taulukon loppuun • kullakin siirtoaskeleella etsitään taulu- kon alkuosasta kohta, johon loppuo- san ensimmäinen alkio kuuluu – uudelle alkiolle raivataan tilaa siirtämällä isompia alkioita askel eteenpäin – lopuksi alkio sijoitetaan paikalleen ja alkuosaa kasvatetaan pykälällä 59 41 31 58 41 26 59 41 58 41 26 59 41 31 31 26 59 58 41 41 31 26 58 59 41 41 31 26 58 41 59 41 31 26 58 41 COMP.CS.300 Tietorakenteet ja algoritmit 1 27 Kurssilla käytetyllä pseudokoodiesityksellä INSERTION-SORT näyttää tältä: INSERTION-SORT(A) (syöte saadaan taulukossa A) 1 for j := 2 to A.length do (siirretään osien välistä rajaa) 2 key := A[j] (otetaan alkuosan uusi alkio käsittelyyn) 3 i := j −1 4 while i > 0 and A[i] > key do(etsitään uudelle alkiolle oikea paikka) 5 A[i + 1] := A[i] (raivataan uudelle alkiolle tilaa) 6 i := i −1 7 A[i + 1] := key (asetetaan uusi alkio oikealle paikalleen) • for- yms. rakenteellisten lauseiden rajaus osoitetaan sisennyksillä • (kommentit) kirjoitetaan sulkuihin kursiivilla • sijoitusoperaattorina on “:=” (“=” on yhtäsuuruuden vertaaminen) • ▷-merkkillä varustettu rivi antaa ohjeet vapaamuotoisesti COMP.CS.300 Tietorakenteet ja algoritmit 1 28 • tietueen (tai olion) kenttiä osoitetaan pisteen avulla – esim. opiskelija.nimi, opiskelija.numero • osoittimen x osoittaman tietueen kenttiä osoitetaan merkin →avulla – esim. x→nimi, x→numero • ellei toisin sanota, kaikki muuttujat ovat paikallisia • taulukoilla ja / tai osoittimilla kootun kokonaisuuden nimi tarkoittaa viitettä ko. kokonaisuuteen – tuollaiset isommat tietorakenteethan aina käytännössä kannattaa välittää viiteparametreina • yksittäisten muuttujien osalta aliohjelmat käyttävät arvonvälitystä (kuten C++-ohjelmatkin oletuksena) • osoitin tai viite voi kohdistua myös ei minnekään: NIL COMP.CS.300 Tietorakenteet ja algoritmit 1 29 2.3 Algoritmien toteutuksesta Käytännön toteutuksissa teoriaa tulee osata soveltaa. Esimerkki: järjestämisalgoritmin sopeuttaminen käyttötilanteeseen. • harvoin järjestetään pelkkiä lukuja; yleensä järjestetään tietueita, joissa on – avain (key) – oheisdataa (satellite data) • avain määrää järjestyksen ⇒sitä käytetään vertailuissa • oheisdataa ei käytetä vertailuissa, mutta sitä on siirreltävä samalla kuin avaintakin COMP.CS.300 Tietorakenteet ja algoritmit 1 30 Edellisessä kappaleessa esitelty INSERTION-SORTissa muuttuisi seuraavalla tavalla, jos siihen lisättäisi oheisdata: 1 for j := 2 to A.length do 2 temp := A[j] 3 i := j −1 4 while i > 0 and A[i].key > temp.key do 5 A[i + 1] := A[i] 6 i := i −1 7 A[i + 1] := temp • jos oheisdataa on paljon, kannattaa järjestää taulukollinen osoittimia tietueisiin ja siirtää lopuksi tietueet suoraan paikoilleen COMP.CS.300 Tietorakenteet ja algoritmit 1 31 Jotta tulokseksi saataisi ajokelpoinen ohjelma, joka toteuttaa INSERTION-SORT:n tarvitaan vielä paljon enemmän. • täytyy ottaa käyttöön oikea ohjelmointikieli muuttujien määrittelyineen ja aliohjelmineen • tarvitaan pääohjelma, joka hoitaa syötteenluvun ja sen laillisuuden tutkimisen ja vastauksen tulostamisen – on tavallista, että pääohjelma on selvästi algoritmia pidempi COMP.CS.300 Tietorakenteet ja algoritmit 1 32 Ohjelmointikieli määrää usein myös muita asioita, esim: • Indeksointi alkaa 0:sta (pseudokoodissa usein 1:stä) • Käytetäänkö edes indeksointia (tai taulukoita, tai...) • (C++) Onko data oikeasti tietorakenteen sisässä, vai osoittimen päässä (jolloin dataa ei tarvitse siirtää ja sen jakaminen on helpompaa) • Jos dataan viitataan epäsuorasti muualta, tapahtuuko se – Osoittimella – Älyosoittimella (esim. shared_ptr) – Iteraattorilla (jos data tietorakenteessa) – Indeksillä (jos data vektorissa tms.) – Hakuavaimella (jos data tietorakenteessa, josta haku nopeaa) • Toteutetaanko rekursio iteroinnilla vai ei (riippuu myös ongelmasta) • Ovatko algoritmin \"parametrit\"oikeasti parametreja, vai vain muuttujia tms. COMP.CS.300 Tietorakenteet ja algoritmit 1 33 Otetaan esimerkiksi edellä kuvatun ohjelman toteutus C++:lla: #include <iostream> #include <vector> typedef std::vector<int> Taulukko; void insertionSort( Taulukko & A ) { int key = 0; int i = 0; for( Taulukko::size_type j = 1; j < A.size(); ++j ) { key = A.at(j); i = j-1; while( i >= 0 && A.at(i) > key ) { A.at(i+1) = A.at(i); --i; } A.at(i+1) = key; } } int main() { unsigned int i; // haetaan järjestettävien määrä std::cout << \"Anna taulukon koko 0...: \"; std::cin >> i; COMP.CS.300 Tietorakenteet ja algoritmit 1 34 Taulukko A(i); // luodaan taulukko // luetaan järjestettävät for( i = 0; i < A.size(); ++i ) { std::cout << \"Anna A[\" << i+1 << \"]: \"; std::cin >> A.at(i); } insertionSort( A ); // järjestetään // tulostetaan siististi for( i = 0; i < A.size(); ++i ) { if( i % 5 == 0 ) { std::cout << std::endl; } else { std::cout << \" \"; } std::cout << A.at(i); } std::cout << std::endl; } COMP.CS.300 Tietorakenteet ja algoritmit 1 35 Ohjelmakoodi on huomattavasti pseudokoodia pidempi ja algoritmille ominaisten asioiden hahmottaminen on siitä paljon vaikeampaa. Tämä kurssi keskittyy algoritmien ja tietorakenteiden periaatteisiin, joten ohjelmakoodi ei palvele tarkoituksiamme. ⇒Tästä eteenpäin toteutuksia ohjelmointikielillä ei juurikaan esitetä. COMP.CS.300 Tietorakenteet ja algoritmit 1 52 4 Tehokkuus ja algoritmien suunnittelu Tässä luvussa pohditaan tehokkuuden käsitettä ja esitellään kurssilla käytetty kertaluokkanotaatio, jolla kuvataan algoritmin asymptoottista käyttäytymistä eli tapaa, jolla algoritmin resurssien kulutus muuttuu syötekoon kasvaessa. t n COMP.CS.300 Tietorakenteet ja algoritmit 1 53 4.1 Kertaluokat Algoritmin analysoinnilla tarkoitetaan sen kuluttamien resurssien määrän arvioimista Tyypillisesti analysoidaan syötekoon kasvun vaikutusta algoritmin resurssien kulutukseen Useimmiten meitä kiinnostaa algoritmin ajankäytön kasvu syötteen koon kasvaessa – Voimme siis tarkastella ajankäyttöä irrallaan toteutusympäristöstä – Itse asiassa voimme kuvata periaatteessa minkä tahansa peräkkäisiä operaatioita sisältävän toiminnan ajankulutusta COMP.CS.300 Tietorakenteet ja algoritmit 1 54 – Algoritmin ajankäyttö: Algoritmin suorittamien \"askelten\" suorituskertojen määrä – Askel: syötekoosta riippumattoman operaation viemä aika. – Emme välitä siitä, kuinka monta kertaa jokin operaatio suoritetaan kunhan se tehdään vain vakiomäärä kertoja. – Tutkimme kuinka monta kertaa algoritmin suorituksen aikana kukin rivi suoritetaan ja laskemme nämä määrät yhteen. COMP.CS.300 Tietorakenteet ja algoritmit 1 55 – Yksinkertaistamme vielä tulosta poistamalla mahdolliset vakiokertoimet ja alemman asteen termit. ⇒Näin voidaan tehdä, koska syötekoon kasvaessa riittävästi alemman asteen termit käyvät merkityksettömiksi korkeimman asteen termin rinnalla. ⇒Menetelmä ei luonnollisestikaan anna luotettavia tuloksia pienillä syöteaineistoilla, mutta niillä ohjelmat ovat tyypillisesti riittävän tehokkaita joka tapauksessa. – Kutsumme näin saatua tulosta algoritmin ajan kulutuksen kertaluokaksi, jota merkitään kreikkalaisella kirjaimella Θ (äännetään “theeta”). f(n) = 23n2 + 2n + 15 ⇒f ∈Θ(n2) f(n) = 1 2n lg n + n ⇒f ∈Θ(n lg n) COMP.CS.300 Tietorakenteet ja algoritmit 1 56 Esimerkki 1: taulukon alkioiden summaus 1 for i := 1 to A.length do 2 summa := summa + A[i] – jos taulukon A pituus (syötekoko) on n, rivi 1 suoritetaan n + 1 kertaa – rivi 2 suoritetaan n kertaa – ajankulutus kasvaa siis n:n kasvaessa seuraavalla tavalla: n aika = 2n + 1 1 3 10 21 100 201 1000 2001 10000 20001 ⇒n:n arvo hallitsee ajankulutusta COMP.CS.300 Tietorakenteet ja algoritmit 1 57 – suoritamme edellä sovitut yksinkertaistukset: poistamme vakiokertoimen ja alemman asteen termin: f(n) = 2n + 1 ⇒n ⇒saamme tulokseksi Θ(n) ⇒kulutettu aika riippuu lineaarisesti syötteen koosta COMP.CS.300 Tietorakenteet ja algoritmit 1 58 Esimerkki 2: alkion etsintä järjestämättömästä taulukosta 1 for i := 1 to A.length do 2 if A[i] = key then 3 return i – tässä tapauksessa suoritusaika riippuu syöteaineiston koon lisäksi sen koostumuksesta eli siitä, mistä kohtaa taulukkoa haluttu alkio löytyy – On tutkittava erikseen: paras, huonoin ja keskimääräinen tapaus COMP.CS.300 Tietorakenteet ja algoritmit 1 59 – paras tapaus: Kuva 6: Etsintä: paras tapaus, löytyy ensimmäisestä alkiosta ⇒alkio löytyy vakioajassa eli ajankulutus on Θ(1) COMP.CS.300 Tietorakenteet ja algoritmit 1 60 – huonoin tapaus Kuva 7: Etsintä: huonoin tapaus, löytyy viimeisestä tai ei ollenkaan rivi 1 suoritetaan n + 1 kertaa ja rivi 2 n kertaa ⇒suoritusaika on lineaarinen eli Θ(n). COMP.CS.300 Tietorakenteet ja algoritmit 1 61 – keskimääräinen tapaus: täytyy tehdä jonkinlainen oletus tyypillisestä eli keskimääräisestä aineistosta: * alkio on taulukossa todennäköisyydellä p (0 ≤p ≤1) * ensimmäinen haettu alkio löytyy taulukon jokaisesta kohdasta samalla todennäköisyydellä – voimme laskea suoraan todennäköisyyslaskennan avulla, kuinka monta vertailua keskimäärin joudutaan tekemään COMP.CS.300 Tietorakenteet ja algoritmit 1 62 – todennäköisyys sille, että alkio ei löydy taulukosta on 1 - p ⇒joudutaan tekemään n vertailua (huonoin tapaus) – todennäköisyys sille, että alkio löytyy kohdasta i, on p/n ⇒joudutaan tekemään i vertailua – odotusarvoinen tarvittavien vertailujen määrä saadaan siis seuraavasti: [1 · p n + 2 · p n + · · · + i · p n · · · + n · p n] + n · (1 −p) COMP.CS.300 Tietorakenteet ja algoritmit 1 63 – oletamme, että alkio varmasti löytyy taulukosta eli p = 1, saamme tulokseksi (n+1)/2 eli Θ(n) ⇒koska myös tapaus, jossa alkio ei löydy taulukosta, on ajankäytöltään lineaarinen, voimme olla varsin luottavaisia sen suhteen, että keskimääräinen ajankäyttö on kertaluokassa Θ(n) – kannattaa kuitenkin muistaa, että läheskään aina kaikki syötteet eivät ole yhtä todennäköisiä ⇒jokaista tapausta on syytä tutkia erikseen COMP.CS.300 Tietorakenteet ja algoritmit 1 64 Esimerkki 3: kahden taulukon yhteisen alkion etsintä 1 for i := 1 to A.length do 2 for j := 1 to B.length do 3 if A[i] = B[j] then 4 return A[i] – rivi 1 suoritetaan 1 .. (n + 1) kertaa – rivi 2 suoritetaan 1 .. (n · (n + 1)) kertaa – rivi 3 suoritetaan 1 .. (n · n) kertaa – rivi 4 suoritetaan korkeintaan kerran COMP.CS.300 Tietorakenteet ja algoritmit 1 65 – nopeimmillaan algoritmi on siis silloin kun molempien taulukoiden ensimmäinen alkio on sama ⇒parhaan tapauksen ajoaika on Θ(1) – pahimmassa tapauksessa taulukoissa ei ole ainuttakaan yhteistä alkiota tai ainoastaan viimeiset alkiot ovat samat ⇒tällöin suoritusajaksi tulee neliöllinen eli 2n2 + 2n + 1 = Θ(n2) – keskimäärin voidaan olettaa, että molempia taulukoita joudutaan käymään läpi noin puoleen väliin ⇒tällöin suoritusajaksi tulee Θ(n2) (tai Θ(nm) mikäli taulukot ovat eri mittaisia) COMP.CS.300 Tietorakenteet ja algoritmit 1 66 Palataan INSERTION-SORTiin. Sen ajankäyttö: INSERTION-SORT( A ) (syöte saadaan taulukossa A) 1 for j := 2 to A.length do (siirretään osien välistä rajaa) 2 key := A[ j ] (otetaan alkuosan uusi alkio käsittelyyn) 3 i := j −1 4 while i > 0 and A[ i ] > key do (etsitään uudelle alkiolle oikea paikka) 5 A[ i + 1 ] := A[ i ] (raivataan uudelle alkiolle tilaa) 6 i := i −1 7 A[ i + 1 ] := key (asetetaan uusi alki o oikealle paikalleen) – rivi 1 suoritetaan n kertaa – rivit 2 ja 3 suoritetaan n - 1 kertaa – rivi 4 suoritetaan vähintään n - 1, enintään (2 + 3 + 4 + · · · + n - 2) kertaa – rivit 5 ja 6 suoritetaan vähintään 0, enintään (1 + 2 + 3 + 4 + · · · + n - 3) kertaa COMP.CS.300 Tietorakenteet ja algoritmit 1 67 – parhaassa tapauksessa, kun taulukko on valmiiksi järjestyksessä, koko algoritmi siis kuluttaa vähintään Θ(n) aikaa – huonoimmassa tapauksessa, kun taulukko on käänteisessä järjestyksessä, aikaa taas kuluu Θ(n2) – keskimääräisen tapauksen selvittäminen on jälleen vaikeampaa: * oletamme, että satunnaisessa järjestyksessä olevassa taulukossa olevista elementtipareista puolet ovat keskenään epäjärjestyksessä. ⇒vertailuja joudutaan tekemään puolet vähemmän kuin pahimmassa tapauksessa, jossa kaikki elementtiparit ovat keskenään väärässä järjestyksessä ⇒keskimääräinen ajankulutus on pahimman tapauksen ajankäyttö jaettuna kahdella: [(n - 1)n]/ 4 = Θ(n2) COMP.CS.300 Tietorakenteet ja algoritmit 1 40 3.3 Suunnitteluperiaate: Hajota ja hallitse Suunnitteluperiaate hajota ja hallitse on todennäköisesti periaatteista kuuluisin. Se toimii useiden tunnettujen tehokkaiden algoritmien periaatteena Perusidea: • ongelma jaetaan alkuperäisen kaltaisiksi, mutta pienemmiksi osaongelmiksi. • pienet osaongelmat ratkaistaan suoraviivaisesti • suuremmat osaongelmat jaetaan edelleen pienempiin osiin • lopuksi osaongelmien ratkaisut kootaan alkuperäisen ongelman ratkaisuksi COMP.CS.300 Tietorakenteet ja algoritmit 1 41 Hajota ja hallitse on usein rekursiivinen rakenteeltaan: algoritmi kutsuu itseään osaongelmille Pienten osaongelmien ratkaisemiseksi voidaan myös hyödyntää toista algoritmia COMP.CS.300 Tietorakenteet ja algoritmit 1 42 3.4 QUICKSORT Ongelman jakaminen pienemmiksi osaongelmiksi • Valitaan jokin taulukon alkioista jakoalkioksi eli pivot-alkioksi. • Muutetaan taulukon alkioiden järjestystä siten, että kaikki jakoalkiota pienemmät tai yhtäsuuret alkiot ovat taulukossa ennen jakoalkiota ja suuremmat alkiot sijaitsevat jakoalkion jälkeen. • Jatketaan alku ja loppuosien jakamista pienemmiksi, kunnes ollaan päästy 0:n tai 1:n kokoisiin osataulukoihin. COMP.CS.300 Tietorakenteet ja algoritmit 1 43 QUICKSORT-algoritmi QUICKSORT( A, left, right ) 1 if left < right then (triviaalitapaukselle ei tehdä mitään) 2 p := PARTITION( A, left, right )(muuten jaetaan alkuosaan ja loppuosaan) 3 QUICKSORT( A, left, p −1 ) (järjestetään jakoalkiota pienemmät) 4 QUICKSORT( A, p + 1, right ) (järjestetään jakoalkiota suuremmat) Kuva 3: Jako pienempiin ja suurempiin COMP.CS.300 Tietorakenteet ja algoritmit 1 44 Pienet osaongelmat: • 0:n tai 1:n kokoiset osataulukot ovat valmiiksi järjestyksessä. Järjestyksessä olevien osataulukoiden yhdistäminen: • Kun alkuosa ja loppuosa on järjestetty on koko (osa)taulukko automaattisesti järjestyksessä. – kaikki alkuosan alkiothan ovat loppuosan alkioita pienempiä, kuten pitääkin Ositus- eli partitiointialgoritmi jakaa taulukon paikallaan vaaditulla tavalla. PARTITION( A, left, right ) 1 p := A[ right ] (otetaan pivotiksi viimeinen alkio) 2 i := left −1 (merkitään i:llä pienten puolen loppua) 3 for j := left to right −1 do (käydään läpi toiseksi viimeiseen alkioon asti) 4 if A[ j ] ≤p (jos A[ j] kuuluu pienten puolelle...) 5 i := i + 1 (... kasvatetaan pienten puolta...) 6 exchange A[ i ] ↔A[ j ] (... ja siirretään A[j] sinne) 7 exchange A[ i + 1 ] ↔A[ right ] (sijoitetaan pivot pienten ja isojen puolten väliin) 8 return i + 1 (palautetaan pivot-alkion sijainti) COMP.CS.300 Tietorakenteet ja algoritmit 1 45 3.5 MERGESORT Erinomainen esimerkki hajota ja hallitse -periaatteesta on MERGE-SORT järjestämisalgoritmi: 1. Taulukko jaetaan kahteen osaan A[1..⌊n/2⌋] ja A[⌊n/2⌋+ 1..n]. 2. Järjestetään puolikkaat rekursiivisesti 3. Limitetään järjestetyt puolikkaat järjestetyksi taulukoksi COMP.CS.300 Tietorakenteet ja algoritmit 1 46 • MERGE-SORT -ALGORITMI MERGE-SORT( A, left, right ) 1 if left < right then (jos taulukossa on alkioita...) 2 mid := ⌊( left + right )/2⌋ (... jaetaan se kahtia) 3 MERGE-SORT( A, left, mid ) (järjestetään alkuosa...) 4 MERGE-SORT( A, mid + 1, right ) (... ja loppuosa) 5 MERGE( A, left, mid, right ) (limitetään osat siten, että järjestys säilyy) COMP.CS.300 Tietorakenteet ja algoritmit 1 47 Hajota: Kuva 4: Jako osaongelmiin COMP.CS.300 Tietorakenteet ja algoritmit 1 48 Hallitse: Kuva 5: Osaongelmien ratkaisujen limitys COMP.CS.300 Tietorakenteet ja algoritmit 1 49 Eli: – jaetaan järjestettävä taulukko kah- teen osaan – jatketaan edelleen osien jakamista kahtia, kunnes osataulukot ovat 0 tai 1 alkion kokoisia – 0 ja 1 kokoiset taulukot ovat valmiiksi järjestyksessä eivätkä vaadi mitään toimenpiteitä – lopuksi yhdistetään järjestyksessä olevat osataulukot limittämällä – huomaa, että rekursiivinen algorit- mi ei toimi kuvan tavalla molemmat puolet rinnakkain 6 8 5 3 6 6 1 8 5 3 6 6 1 8 3 5 6 1 5 6 1 3 6 8 5 6 1 3 6 8 5 3 6 6 8 1 5 6 1 3 6 8 COMP.CS.300 Tietorakenteet ja algoritmit 1 50 – limityksen suorittava MERGE-algoritmi: MERGE( A, left, mid, right ) 1 for i := left to right do (käydään koko alue läpi...) 2 B[ i ] := A[ i ] (... ja kopioidaan se aputaulukkoon) 3 i := left (asetetaan i osoittamaan valmiin osan loppua) 4 j := left; k := mid + 1 (asetetaan j ja k osoittamaan osien alkuja) 5 while j ≤mid and k ≤right do (käydään läpi, kunnes jompikumpi osa loppuu) 6 if B[ j ] ≤B[ k ] then (jos alkuosan ensimmäinen alkio on pienempi...) 7 A[ i ] := B[ j ] (... sijoitetaan se tulostaulukkoon...) 8 j := j + 1 (... ja siirretään alkuosan alkukohtaa) 9 else (muuten...) 10 A[ i ] := B[ k ] (... sijoitetaan loppuosan alkio tulostaulukkoon...) 11 k := k + 1 (... ja siirretään loppuosan alkukohtaa) 12 i := i + 1 (siirretään myös valmiin osan alkukohtaa) 13 if j > mid then 14 k := 0 15 else 16 k := mid −right 17 for j := i to right do (siirretään loput alkiot valmiin osan loppuun) 18 A[ j ] := B[ j + k ] MERGE limittää taulukot käyttäen “pala kerrallaan” -menetelmää. COMP.CS.300 Tietorakenteet ja algoritmit 1 51 Tuottaako hajota ja hallitse tehokkaamman ratkaisun kuin pala kerrallaan? Ei aina, mutta tarkastellaksemme tilannetta tarkemmin, meidän täytyy tutustua algoritmin analyysiin COMP.CS.300 Tietorakenteet ja algoritmit 1 75 5 Kertaluokkamerkinnät Tässä luvussa käsitellään asymptoottisessa analyysissa käytettyjä matemaattisia merkintätapoja Määritellään tarkemmin Θ, sekä kaksi muuta saman sukuista merkintää O ja Ω. COMP.CS.300 Tietorakenteet ja algoritmit 1 76 5.1 Asymptoottinen aika-analyysi Edellisessä luvussa yksinkertaistimme suoritusajan lauseketta melkoisesti: • jätimme jäljelle ainoastaan eniten merkitsevän termin • poistimme sen edestä vakiokertoimen ⇒kuvastavat algoritmin käyttäytymistä, kun syötekoko kasvaa kohti ääretöntä • siis kuvaavat asymptoottista suorituskykyä ⇒antavat käyttökelpoista tietoa vain jotain rajaa suuremmilla syötteillä • todettiin, että usein raja on varsin alhaalla ⇒Θ- yms. merkintöjen mukaan nopein on myös käytännössä nopein, paitsi aivan pienillä syötteillä COMP.CS.300 Tietorakenteet ja algoritmit 1 77 Θ-merkintä Kuva 8: Θ-merkintä COMP.CS.300 Tietorakenteet ja algoritmit 1 78 eli matemaattisesti • olkoon g(n) funktio luvuilta luvuille Θ(g(n)) on niiden funktioiden f(n) joukko, joille on olemassa positiiviset vakiot c1, c2 ja n0 siten, että aina kun n ≥n0, niin 0 ≤c1 · g(n) ≤f(n) ≤c2 · g(n) – Θ(g(n)) on joukko funktioita ⇒tulisi kirjoittaa esim. f(n) ∈Θ(g(n)) ⇒ohjelmistotieteessä vakiintunut käytäntö kuitenkin on käyttää = -merkintää COMP.CS.300 Tietorakenteet ja algoritmit 1 79 Funktion f(n) kuuluvuuden kertaluokkaan Θ(g(n)), voi siis todistaa etsimällä jotkin arvot vakioille c1, c2 ja n0 ja osoittamalla, että funktion arvo pysyy n:n arvoilla n0:sta alkaen arvojen c1g(n) ja c2g(n) välillä (eli suurempana tai yhtäsuurena kuin c1g(n) ja pienempänä tai yhtäsuurena, kuin c2g(n)). Esimerkki: 3n2 + 5n −20 = Θ(n2) • valitaan c1 = 3, c2 = 4 ja n0 = 4 • 0 ≤3n2 ≤3n2 + 5n −20 ≤4n2 kun n ≥4, koska silloin 0 ≤5n −20 ≤n2 • yhtä hyvin olisi voitu valita c1 = 2, c2 = 6 ja n0 = 7 tai c1 = 0,000 1, c2 = 1 000 ja n0 = 1 000 • tärkeää on vain, että voidaan valita jotkut positiiviset, ehdot täyttävät c1, c2 ja n0 COMP.CS.300 Tietorakenteet ja algoritmit 1 80 Tärkeä tulos: jos ak > 0, niin aknk + ak−1nk−1 + · · · + a2n2 + a1n + a0 = Θ(nk) • toisin sanoen, jos polynomin eniten merkitsevän termin kerroin on positiivinen, Θ-merkintä sallii kaikkien muiden termien sekä e.m. kertoimen abstrahoinnin pois Vakiofunktiolle pätee c = Θ(n0) = Θ(1) • Θ(1) ei kerro, minkä muuttujan suhteen funktioita tarkastellaan ⇒sitä saa käyttää vain kun muuttuja on asiayhteyden vuoksi selvä ⇒yleensä algoritmien tapauksessa on COMP.CS.300 Tietorakenteet ja algoritmit 1 81 O-merkintä Kuva 9: O-merkintä COMP.CS.300 Tietorakenteet ja algoritmit 1 82 O-merkintä on muuten samanlainen kuin Θ-merkintä, mutta se rajaa funktion ainoastaan ylhäältä. ⇒asymptoottinen yläraja Määritelmä: O(g(n)) on niiden funktioiden f(n) joukko, joille on olemassa positiiviset vakiot c ja n0 siten, että aina kun n ≥n0, niin 0 ≤f(n) ≤c · g(n) COMP.CS.300 Tietorakenteet ja algoritmit 1 83 • pätee: jos f(n) = Θ(g(n)), niin f(n) = O(g(n)) • päinvastainen ei aina päde: n2 = O(n3), mutta n2 ̸= Θ(n3) • tärkeä tulos: jos k ≤m, niin nk = O(nm) • jos hitaimman tapauksen suoritusaika on O(g(n)), niin jokaisen tapauksen suoritusaika on O(g(n)) COMP.CS.300 Tietorakenteet ja algoritmit 1 84 Usein algoritmin hitaimman (ja samalla jokaisen) tapauksen suoritusajalle saadaan pelkällä vilkaisulla jokin O-merkinnällä ilmoitettava yläraja. Usein vain yläraja onkin kiinnostava ⇒O-merkinnällä on suuri käytännön merkitys COMP.CS.300 Tietorakenteet ja algoritmit 1 85 Esimerkki: INSERTION-SORT rivi kerta-aika for j := 2 to A.length do O(n) key := A[j] · O(1) i := j −1 · O(1) while i > 0 and A[i] > key do · O(n) A[i + 1] := A[i] · · O(1) i := i −1 · · O(1) A[i + 1] := key · O(1) Jolloin pahimmalle tapaukselle saadaan suoritusaika O(n) · O(n) · O(1) = O(n2) COMP.CS.300 Tietorakenteet ja algoritmit 1 86 Ω-merkintä (äännetään “iso oomega”) Kuva 10: Ω-merkintä COMP.CS.300 Tietorakenteet ja algoritmit 1 87 Ω-merkintä on muuten täysin samanlainen kuin Θ-merkintä, mutta se rajaa funktion vain alhaalta. ⇒asymptoottinen alaraja määritelmä: Ω(g(n)) on niiden funktioiden f(n) joukko, joille on olemassa positiiviset vakiot c ja n0 siten, että aina kun n ≥n0, niin 0 ≤c · g(n) ≤f(n) COMP.CS.300 Tietorakenteet ja algoritmit 1 88 • määritelmistä seuraa tärkeä tulos: f(n) = Θ(g(n)) jos ja vain jos f(n) = O(g(n)) ja f(n) = Ω(g(n)). • jos nopeimman tapauksen suoritusaika on Ω(g(n)), niin jokaisen tapauksen suoritusaika on Ω(g(n)) Käytännön hyötyä Ω-merkinnästä on lähinnä tilanteissa, joissa jonkin ratkaisuvaihtoehdon parhaan tapauksenkin tehokkuus on epätyydyttävä, jolloin ratkaisu voidaan hylätä välittömästi. COMP.CS.300 Tietorakenteet ja algoritmit 1 89 Merkintöjen keskinäiset suhteet f(n) = Ω(g(n)) ja f(n) = O(g(n)) ⇐⇒f(n) = Θ(g(n)) Kertaluokkamerkinnöillä on samankaltaisia ominaisuuksia kuin lukujen vertailuilla: f(n) = O(g(n)) a ≤b f(n) = Θ(g(n)) a = b f(n) = Ω(g(n)) a ≥b Eli, jos f(n):n korkeimman asteen termi, josta on poistettu vakiokerroin ≤g(n): n vastaava, f(n) = O(g(n)) jne. Yksi merkittävä ero: reaaliluvuille pätee aina tasan yksi kaavoista a < b, a = b ja a > b, mutta vastaava ei päde kertaluokkamerkinnöille. ⇒Kaikkia funktioita ei pysty mielekkäällä tavalla vertaamaan toisiinsa kertaluokkamerkintöjen avulla (esim. n ja n1+sin n). COMP.CS.300 Tietorakenteet ja algoritmit 1 90 Hieman yksinkertaisten: • Jos algoritmi on Ω(g(n)), sen resurssien kulutus on ainakin kertaluokassa g(n). – vrt. kirja maksaa ainakin noin kympin. • Jos algoritmi on O(g(n)), sen resurssien kulutus on korkeintaan kertaluokassa g(n). – vrt. kirja maksaa korkeintaan noin kympin. • Jos algoritmi on Θ(g(n)), sen resurssien kulutus on aina kertaluokassa g(n). – vrt. kirja maksaa suunnilleen kympin. COMP.CS.300 Tietorakenteet ja algoritmit 1 91 Kaikkien algoritmien kaikkien tapausten suoritusajalle ei välttämättä voida antaa mitään ratkaisua Θ-notaatiolla. Esimerkkinä Insertion-Sort: • paras tapaus on Ω(n), mutta ei Ω(n2) • pahin tapaus on O(n2), mutta ei O(n) ⇒kaikille tapauksille yhteistä Θ-arvoa ei voida määrittää COMP.CS.300 Tietorakenteet ja algoritmit 1 92 Esimerkki Otetaan funktio f(n) = 3n2 + 5n + 2. Suoritetaan sille aiemmin sovitut yksinkertaistukset: • alemman asteen termit pois • vakiokertoimet pois ⇒f(n) = Θ(n2) Vakuuttuaksemme asiasta etsim- me kertoimet c1 ja c2: 3n2 ≤3n2 + 5n + 2 ≤4n2, kun n ≥6 ⇒c1 = 3, c2 = 4 ja n0 = 6 toimivat ⇒f(n) = O(n2) ja Ω(n2) ⇒f(n) = Θ(n2) 0 5 10 15 0 100 200 300 400 500 600 700 800 f(n) n² 0 5 10 15 0 100 200 300 400 500 600 700 800 n0 4n² 3n² COMP.CS.300 Tietorakenteet ja algoritmit 1 93 Selvästi kerroin c2 = 4 toimii myös kun g(n) = n3, sillä kun n ≥6, n3 > n2 ⇒f(n) = O(n3) • sama pätee kun g(n) = n4... Ja alapuolella kerroin c1 = 3 toimii myös kun g(n) = n lg n, sillä kun n ≥ 6, n2 > n lg n ⇒f(n) = Ω(n lg n) • sama pätee kun g(n) = n tai g(n) = lg n 0 5 10 15 0 100 200 300 400 500 600 700 800 n0 4n² 3n² 3n 3nlog(n) 4n³ COMP.CS.300 Tietorakenteet ja algoritmit 1 94 5.2 Suorituskykykäsitteitä Tähän mennessä algoritmien suorituskykyä on arvioitu lähinnä suoritusajan näkökulmasta. Muitakin vaihtoehtoja kuitenkin on: • Voidaan mitata myös esimerkiksi muistinkulutusta tai kaistanleveyttä. Lisäksi käytännössä tulee ottaa huomioon ainakin seuraavat seikat: • Millä mittayksiköllä resurssien kulutusta mitataan? • Miten syötekoko määritellään? • Mitataanko huonoimman, parhaan vai keskimääräisen tapauksen resurssien kulutusta? • Millaiset syötekoot tulevat kysymykseen? • Riittääkö kertaluokkamerkintöjen tarkkuus vai tarvitaanko tarkempaa tietoa? COMP.CS.300 Tietorakenteet ja algoritmit 1 95 Ajoajan mittayksiköt Valittaessa ajoajan mittayksikköä “askelta” pyritään yleensä mahdollisimman koneriippumattomaan ratkaisuun: • Todelliset aikayksiköt kuten sekunti eivät siis kelpaa. • Vakiokertoimet käyvät merkityksettömiksi. ⇒Jäljelle jää kertaluokkamerkintöjen tarkkuustaso. ⇒askeleeksi voidaan katsoa mikä tahansa enintään vakioajan vievä operaatio. • Vakioaikaiseksi tulkitaan mikä tahansa operaatio, jonka ajankulutus on riippumaton syötekoosta. • Tällöin on olemassa jokin syötteen koosta riippumaton aikamäärä, jota operaation kesto ei milloinkaan ylitä. • Yksittäisiä askeleita ovat esimerkiksi yksittäisen muuttujan sijoitus, if-lauseen ehdon testaus etc. • Askeleen rajauksen kanssa ei tarvitse olla kovin tarkka, koska Θ(1) + Θ(1) = Θ(1). COMP.CS.300 Tietorakenteet ja algoritmit 1 96 Muistin käytön mittayksiköt Tarkkoja yksiköitä ovat lähes aina bitti, tavu (8 bittiä) ja sana (jos sen pituus tunnetaan). Eri tyyppien muistin käyttö on usein tunnettu, joskin se vaihtelee vähän eri tietokoneiden ja kielten välillä. • kokonaisluku on yleensä 16 tai 32 bittiä • merkki on yleensä 1 tavu = 8 bittiä • osoitin on yleensä 4 tavua = 32 bittiä • taulukko A[1 . . . n] on usein n · <alkion koko> ⇒Tarkka muistin käytön arvioiminen on usein mahdollista, joskin huolellisuutta vaativaa. COMP.CS.300 Tietorakenteet ja algoritmit 1 97 Kertaluokkamerkinnät ovat käteviä silloin, kun tarkka tavujen laskeminen ei ole vaivan arvoista. Jos algoritmi säilyttää yhtäaikaa koko syöteaineiston, niin arvioinnissa kannattaa erottaa syöteaineiston kuluttama muisti muusta muistin tarpeesta. • Θ(1) lisämuistin tarve vs. Θ(n) lisämuistin tarve • Kannattaa kuitenkin huomata, että esimerkiksi merkkijonon etsintä syötetiedostosta ei talleta yhtäaikaa koko syöteaineistoa, vaan selaa sen läpi. COMP.CS.300 Tietorakenteet ja algoritmit 1 98 6 Pikalajittelu ja satunnaistaminen MERGE-SORTilla osaongelmiin jako oli helppoa ja ratkaisujen yhdistämisessä nähtiin paljon työtä. Tutustutaan seuraavaksi keskimäärin erittäin nopeaan järjestämisalgoritmiin QUICKSORT, jossa työ tehdään jakovaiheessa. QUICKSORTin kautta kohdataan uusi suunnitteluperiaate: satunnaistaminen. COMP.CS.300 Tietorakenteet ja algoritmit 1 99 6.1 QUICKSORT Ongelman jakaminen pienemmiksi osaongelmiksi • Valitaan jokin taulukon alkioista jakoalkioksi eli pivot-alkioksi. • Muutetaan taulukon alkioiden järjestystä siten, että kaikki jakoalkiota pienemmät tai yhtäsuuret alkiot ovat taulukossa ennen jakoalkiota ja suuremmat alkiot sijaitsevat jakoalkion jälkeen. • Jatketaan alku ja loppuosien jakamista pienemmiksi, kunnes ollaan päästy 0:n tai 1:n kokoisiin osataulukoihin. COMP.CS.300 Tietorakenteet ja algoritmit 1 100 Kertaus: QUICKSORT-algoritmi QUICKSORT( A, left, right ) 1 if left < right then (triviaalitapaukselle ei tehdä mitään) 2 p := PARTITION( A, left, right )(muuten jaetaan alkuosaan ja loppuosaan) 3 QUICKSORT( A, left, p −1 ) (järjestetään jakoalkiota pienemmät) 4 QUICKSORT( A, p + 1, right ) (järjestetään jakoalkiota suuremmat) Kuva 11: Jako pienempiin ja suurempiin COMP.CS.300 Tietorakenteet ja algoritmit 1 101 Pienet osaongelmat: • 0:n tai 1:n kokoiset osataulukot ovat valmiiksi järjestyksessä. Järjestyksessä olevien osataulukoiden yhdistäminen: • Kun alkuosa ja loppuosa on järjestetty on koko (osa)taulukko automaattisesti järjestyksessä. – kaikki alkuosan alkiothan ovat loppuosan alkioita pienempiä, kuten pitääkin Ositus- eli partitiointialgoritmi jakaa taulukon paikallaan vaaditulla tavalla. PARTITION( A, left, right ) 1 p := A[ right ] (otetaan pivotiksi viimeinen alkio) 2 i := left −1 (merkitään i:llä pienten puolen loppua) 3 for j := left to right −1 do (käydään läpi toiseksi viimeiseen alkioon asti) 4 if A[ j ] ≤p (jos A[ j] kuuluu pienten puolelle...) 5 i := i + 1 (... kasvatetaan pienten puolta...) 6 exchange A[ i ] ↔A[ j ] (... ja siirretään A[j] sinne) 7 exchange A[ i + 1 ] ↔A[ right ] (sijoitetaan pivot pienten ja isojen puolten väliin) 8 return i + 1 (palautetaan pivot-alkion sijainti) COMP.CS.300 Tietorakenteet ja algoritmit 1 102 Kuinka nopeasti PARTITION toimii? • For-silmukka tekee n - 1 kierrosta, kun n on right - left • Kaikki muut operaatiot ovat vakioaikaisia. ⇒Sen suoritusaika on Θ(n). Kuten MERGE-SORTilla QUICKSORTin analyysi ei ole yhtä suoraviivainen rekursion vuoksi COMP.CS.300 Tietorakenteet ja algoritmit 1 103 • Koska PARTITIONIA ja rekursiivista kutsua lukuunottamatta kaikki QUICKSORTIN operaatiot ovat vakioaikaisia, keskitymme tarkastelemaan PARTITIONIN instanssien käyttämää aikaa. 1 1 1 1 1 n n n 1 1 1 1 1 2 2 n − n − 1 n − n − 1 COMP.CS.300 Tietorakenteet ja algoritmit 1 104 • Kokonaisaika on edellisen kuvan esittämän puun solmujen aikojen summa. • 1 kokoiselle taulukolle suoritus on vakioaikaista. • Muille suoritus on lineaarista osataulukon kokoon nähden. ⇒Kokonaisaika on siis Θ(solmujen numeroiden summa). COMP.CS.300 Tietorakenteet ja algoritmit 1 105 Pahimman tapauksen suoritusaika • Solmun numero on aina pienempi kuin isäsolmun numero, koska jakoalkio on jo oikealla paikallaan, eikä se kuulu kumpaankaan järjestettävään osa- taulukkoon ⇒puussa voi siis olla kerroksia enintään n kappa- letta • pahin tapaus realisoituu, kun jakoalkioksi valitaan aina pienin tai suurin alkio – näin tapahtuu esimerkiksi valmiiksi järjestetyllä taulukolla • solmujen numeroiden summa on n + n - 1 + · · · + 2 + 1 ⇒QUICKSORTIN suoritusaika on O(n2) n−1 n−2 k 2 1 n COMP.CS.300 Tietorakenteet ja algoritmit 1 106 Paras tapaus realisoituu kun taulukko jakautuu aina tasan. • Alla oleva kuva näyttää kuinka osataulukoiden koot pienenevät. – harmaat ruudut ovat jo oikealla paikallaan olevia alkioita • Jokaisella tasolla tehtävä työ on kertaluokassa Θ(n). – tästä saamme pessimistisen arvion suorituspuun korkeudelle parhaassa tapauksessa ⇒O(lg n) ⇒Parhaan tapauksen suoritusajan yläraja on O(n lg n). O(lg n) O(n) O(n) O(n) O(n) O(n) COMP.CS.300 Tietorakenteet ja algoritmit 1 107 QUICKSORTIN kohdalla parhaan ja huonoimman tapauksen suoritusajat eroavat toisistaan selvästi. • Olisi mielenkiintoista tietää keskimääräinen suoritusaika. • Sen analysoiminen menee tämän kurssin tavoitteiden ulkopuolelle, mutta on osoitettu, että jos aineisto on tasajakautunutta, keskimääräinen suoritusaika on Θ(n lg n). • Keskimääräinen suoritusaika on siis varsin hyvä. COMP.CS.300 Tietorakenteet ja algoritmit 1 108 QUICKSORTIIN liittyy kuitenkin se kiusallinen seikka, että sen pahimman tapauksen suoritus on hidasta ja sen esiintyminen on käytännössä varsin todennäköistä. • On helppoa kuvitella tilanteita, joissa aineisto on jo järjestyksessä tai melkein järjestyksessä. ⇒Tarvitaan keino, jolla pahimman tapauksen systemaattisen esiintymisen riskiä saadaan pienennettyä. Satunnaistaminen on osoittautunut tässä varsin tehokkaaksi. COMP.CS.300 Tietorakenteet ja algoritmit 1 109 QUICKSORTIN etuja ja haittoja Etuja: • järjestää taulukon keskimäärin erittäin tehokkaasti – ajoaika on keskimäärin Θ(n lg n) – vakiokerroin on pieni • tarvitsee vain vakiomäärän lisämuistia • sopii hyvin virtuaalimuistiympäristöön • käyttää tehokkaasti välimuistia Haittoja: • ajoaika on hitaimmillaan Θ(n2) • hitaimman tapauksen tuottava syöte on kiusallisen tavallinen ilman satunnaistamista • rekursiivisuus ⇒tarvitsee lisämuistia pinolle • epävakaus COMP.CS.300 Tietorakenteet ja algoritmit 1 119 7 C++:n standardikirjasto Tässä luvussa käsitellään C++:n standardikirjaston tietorakenteita ja algoritmeja. Tarkoituksena on käsitellä sellaisia asioita, joihin tulee kiinnittää huomiota, jotta kirjastoa tulisi käyttäneeksi tarkoituksenmukaisesti ja tehokkaasti. COMP.CS.300 Tietorakenteet ja algoritmit 1 120 7.1 Yleistä C++:n standardikirjastosta Standardikirjasto standardoitiin C++-kielen mukana syksyllä 1998, ja sitä on jonkin verran laajennettu myöhemmissä versioissa. Uusin standardiversio on C++17 vuodelta 2017. Kirjasto sisältää tärkeimmät perustietorakenteet ja algoritmit. • useimmat tämän aineiston alkupuolen tietorakenteet ja algoritmit mukana muodossa tai toisessa Rajapinnat ovat harkittuja, joustavia, geneerisiä ja tyyppiturvallisia. Rajapintojen tarjoamien operaatioiden tehokkuudet on ilmaistu O-merkinnällä. Kirjaston geneerisyys on toteutettu käännösaikaisella mekanismilla: C++:n malli (template) COMP.CS.300 Tietorakenteet ja algoritmit 1 121 Tietorakennekurssin kannalta kiinnostavin standardikirjaston elementti on ns. STL (Standard Template Library): säiliöt eli kirjaston tarjoamat tietorakenteet, geneeriset algoritmit sekä iteraattorit, joiden avulla säiliöiden alkioita käsitellään. Kuva 12: STL:n osaset C++11:n mukana tulleet lambdat ovat myös keskeisiä COMP.CS.300 Tietorakenteet ja algoritmit 1 122 7.2 Iteraattorit Kaikki standardikirjaston tietora- kenteet näyttäytyvät meille var- sin samanlaisina mustina laati- koina, joista oikeastaan tiedäm- me ainoastaan, että ne sisältä- vät tallettamiamme alkioita ja että ne toteuttavat tietyt raja- pintafunktiot. Pystymme käsittelemään säiliöi- den sisältöä ainoastaan noi- den rajapintafunktioiden sekä iteraattorien avulla. Alkio2 Alkio3 Alkio4 Alkio5 Iteraattori Iteraattori Iteraattori Alkio1 Säiliö lukeminen muuttaminen x q siirto p COMP.CS.300 Tietorakenteet ja algoritmit 1 123 Iteraattorit ovat kahvoja tai “kirjanmerkkejä” tietorakenteen alkioihin. • kukin iteraattori osoittaa joko tietorakenteen alkuun, loppuun tai kahden alkion väliin. • säiliöiden rajapinnassa on yleensä funktiot begin() ja end(), jotka palauttavat säiliön alkuun ja loppuun osoittavat iteraattorit • iteraattorin läpi pääsee käsiksi sen oikealla puolella olevaan alkioon, paitsi jos kysymyksessä on käänteisiteraattori (reverse iterator), joilloin sen läpi käsitellään vasemmanpuoleista alkiota • käänteisiteraattorille myös siirto-operaatiot toimivan käänteisesti, esimerkiksi ++ siirtää iteraattoria pykälän vasemmalle • begin():ä ja end():ä vastaavat käänteisiteraattorit saa rbegin():llä ja rend():llä COMP.CS.300 Tietorakenteet ja algoritmit 1 124 • nimensä mukaisesti iteraattoria voi siirtää säiliön sisällä, ja sen avulla säiliön voi käydä läpi • iteraattorin avulla voi lukea ja kirjoittaa • säiliöön lisättävien ja siitä poistettavien alkioiden sijainti yleensä ilmaistaan iteraattoreiden avulla Kullakin säiliöllä on oma iteraattorityyppinsä. • eri säiliöt tarjoavat erilaisia mahdollisuuksia siirtää iteraattoria nopeasti paikasta toiseen (vrt. taulukon/listan mielivaltaisen alkion lukeminen) • suunnitteluperiaatteena on ollut, että kaikkien iteraattoreille tehtävien operaatioiden tulee onnistua vakioajassa, jotta geneeriset algoritmit toimisivat luvatulla tehokkuudella riippumatta siitä mikä iteraattori niille annetaan • iteraattorit voidaan jakaa kategorioihin sen mukaan, millaisia vakioaikaisia operaatioita ne pystyvät tarjoamaan COMP.CS.300 Tietorakenteet ja algoritmit 1 125 Syöttöiteraattorin (input itera- tor) avulla voi vain lukea al- kioita, mutta ei muuttaa niitä • iteraattorin osoittaman al- kion arvon voi lukea (*p) • iteraattorin osoittaman al- kion kentän voi lukea tai kutsua sen jäsenfunktiota (p->) • iteraattoria voi siirtää as- keleen eteenpäin (++p tai p++) • iteraattoreita voi sijoittaa ja vertailla toisiinsa (p=q, p==q, p!=) input iterator * (luku), ==, !=, ++, =, −> syöttöiteraattori output iterator tulostusiteraattori * (kirjoitus), ==, !=, ++, =, −> forward iterator * (luku/kirjoitus) eteenpäin−iteraattori bidirectional iterator −− kaksisuuntainen iteraattori random access iterator +=, −=, +, −, [], <, >, <=, >= hajasaanti−iteraattori deque vector set multiset map multimap list T[] Tulostusiteraattori (output iterator) on kuten syöttöiteraattori, mutta sen avulla voi vain muuttaa alkioita. (*p=x) COMP.CS.300 Tietorakenteet ja algoritmit 1 126 Eteenpäin-iteraattori (forward iterator) on yhdistelmä syöttö- ja tulostusiteraattorien rajapinnoista. Kaksisuuntainen iteraattori (bidirectional iterator) osaa lisäksi siirtyä yhden askeleen kerrallaan taaksepäin. (--p tai p--) Hajasaanti-iteraattori (random access iterator) on kuin kaksisuuntainen iteraattori, mutta sitä voi lisäksi siirtää mielivaltaisen määrän eteen- tai taaksepäin. • iteraattoria voi siirtää n askelta eteen- tai taaksepäin (p+=n, p-=n, q=p+n, q=p-n) • iteraattorista n:n alkion päässä olevan alkion pystyy lukemaan ja sitä pystyy muokkaamaan (p[n]) • kahden iteraattorin välisen etäisyyden pystyy selvittämään (p-q) • iteraattoreiden keskinäistä suuruusjärjestystä voi vertailla, iteraattori on toista “pienempi”, jos sen paikka on säiliössä ennen tätä (p<q, p<=q, p>q, p>=q) COMP.CS.300 Tietorakenteet ja algoritmit 1 127 Iteraattorit ovat osoitinabstraktio →Iteraattorien operaatioiden syntaksi muistuttaa selvästi C++:n osoitinsyntaksia. Iteraattorit saadaan otettua käyttöön komennolla #include <iterator> Oikean tyyppinen iteraattori saadaan luotua esimerkiksi seuraavalla syntaksilla. säiliö<talletettava tyyppi>::iterator p; Iteraattoreiden kanssa avainsana auto on käyttökelpoinen: auto p = begin( säiliö ); // →std::vector<std::string>::iterator Säiliöihin tehtävät poistot ja lisäykset saattavat mitätöidä säiliöön jo osoittavia iteraattoreita. • ominaisuus on säiliö-kohtainen, joten sen yksityiskohdat käsitellään säiliöiden yhteydessä COMP.CS.300 Tietorakenteet ja algoritmit 1 128 Tavallisten iteraattorien lisäksi STL tarjoaa joukon iteraattorisovittimia. • niiden avulla voidaan muunnella geneeristen algoritmien toiminnallisuutta • edellä mainitut käänteisiteraattorit (reverse iterator) ovat iteraattorisovittimia • lisäysiteraattorit (insert iterator/inserter) ovat tärkeitä iteraattorisovittimia. – ne ovat tulostusiteraattoreita, jotka lisäävät alkioita halutulle paikalle säiliöön kopioimisen sijasta – säiliön alkuun lisäävän iteraattorin saa funktiokutsulla front_inserter(säiliö) – säiliön loppuun lisäävän iteraattorin saa funktiokutsulla back_inserter(säiliö) – annetun iteraattorin kohdalle lisäävän iteraattorin saa funktiokutsulla inserter(säiliö, paikka) COMP.CS.300 Tietorakenteet ja algoritmit 1 129 • virtaiteraattorit (stream iterator) ovat syöttö- ja tulostusiteraattoreita, jotka käyttävät säiliöiden sijaista C++:n tiedostovirtoja – cin virrasta haluttua tyyppiä lukevan syöttöiteraattorin saa syntaksilla istream_iterator<tyyppi> (cin) – cout virtaan haluttua tyyppiä pilkuilla erotettuna tulostavan tulostusiteraattorin saa syntaksilla ostream_iterator<tyyppi> (cout, ’,’) • siirtoiteraattorit (move iterator) muuttavat alkion kopioinnin iteraattorin avulla alkion siirtämiseksi. COMP.CS.300 Tietorakenteet ja algoritmit 1 130 7.3 Säiliöt Standardikirjaston säiliöt kuuluvat pääsääntöisesti kahteen kategoriaan rajapintojensa puolesta: • sarjat (sequence) – alkioita pystyy hakemaan niiden järjestysnumeron perusteella – alkioita pystyy lisäämään ja poistamaan halutusta kohdasta – alkioita pystyy selaamaan järjestyksessä • assosiatiiviset säiliöt (associative container) – alkiot sijoitetaan säiliöön avaimen määräämään kohtaan – talletettavien alkioiden avainten arvoja pitää pystyä vertaamaan toisiinsa oletusarvoisesti operaattorilla < järjestetyissä säiliöissä • Rajapintojen tarjoamista jäsenfunktiosta näkee, mikä säiliön mielekäs käyttötarkoitus on COMP.CS.300 Tietorakenteet ja algoritmit 1 131 Kirjaston säiliöt: Säiliötyyppi Kirjasto Sarjat array vector deque list (forward_list) Assosiatiiviset map set Järjestämättömät unordered_map assosiatiiviset unordered_set Säiliösovittimet queue stack COMP.CS.300 Tietorakenteet ja algoritmit 1 132 Säiliöt noudattavat arvon välitystä. • säiliö ottaa talletettavasta datasta kopion • säiliö palauttaa kopioita sisältämästään tiedosta ⇒säiliön ulkopuolella tehtävät muutokset eivät vaikuta säiliön sisältämään dataan • kaikilla säiliöihin talletettavilla alkioilla tulee olla kopiorakentaja ja sijoitusoperaattori. – perustyypeillä sellaiset ovat valmiina • itse määriteltyä tyyppiä olevat alkiot kannattaa tallettaa osoittimen päähän – näinhän kannattaisi tehdä tehokkuussyistä joka tapauksessa COMP.CS.300 Tietorakenteet ja algoritmit 1 133 • C++11 tarjoaa kätevän työkalun muistinhallinnan helpottamiseksi shared_pointer tilanteissa, joissa useampi taho tarvitsee resurssia – sisältää sisäänrakennetun viitelaskurin ja tuhoaa alkion kun viitelaskuri nollautuu – deleteä ei tarvitse eikä saakaan kutsua – luominen: auto pi = std::make_shared<Olio>(params); – näppärä erityisesti jos halutaan tehdä kahden avaimen mukaan järjestetty tietorakenne: * sijoitetaan oheisdata shared_pointerin päähän * sijoitetaan osoittimet kahteen eri säiliöön kahden eri avaimen mukaan COMP.CS.300 Tietorakenteet ja algoritmit 1 134 Sarjat: Taulukko array<tyyppi> on vakiokokoinen taulukko. • Luodaan std::array<tyyppi, koko> a = {arvo, arvo,...}; • Indeksointi jäsenfunktiolla .at() tai []-operaatiolla. Funktioilla front() ja back() voidaan käsitellä ensimmäistä ja viimeistä alkiota. • Tarjoaa iteraattorit ja käänteisiteraattorit • empty(), size() ja max_size() • Funktion data() avulla päästään suoraan käsiksi sisällä olevaan taulukkoon Taulukon operaatiot ovat vakioaikaisia, mutta fill() ja swap() ovat O(n) COMP.CS.300 Tietorakenteet ja algoritmit 1 135 Vektori vector<tyyppi> on lopustaan joustavarajainen taulukko • Luodaan vector<int> v {arvo, arvo, ...}; • Vakioaikainen indeksointi .at(), [] sekä (tasatusti) vakioaikainen lisäys push_back() ja poisto pop_back() vektorin lopussa • alkion lisääminen muualle insert():illä ja poistaminen erase:lla on lineaarista, O(n) • emplace_back(args); rakentaa alkion suoraan vektoriin • Vektorin kokoa voi kasvattaa funktiolla .resize(koko, alkuarvo); – alkuarvo on vapaaehtoinen – tarvittaessa vektori varaa lisää muistia automaattisesti – muistia voi varata myös ennakkoon: .reserve(koko), .capacity() • iteraattorien mitätöitymistä tapahtuu seuraavissa tilanteissa – mikäli vectorille ei ole etukäteen varattu riittävää tilaa, voi mikä tahansa lisäys aiheuttaa kaikkien iteraattoreiden mitätöitymisen COMP.CS.300 Tietorakenteet ja algoritmit 1 136 – poistot aiheuttavat mitätöitymisen ainoastaan poistopaikan jälkeen tuleville iteraattoreille – lisäykset keskelle aiheuttavat aina lisäyspaikan jälkeen tulevien iteraattoreiden mitätöitymisen • vector<bool>:ille on määritelty erikoistoteutus, joka poikkeaa siitä mitä yleinen toteutus tekisi muistinkäytön tehostamiseksi – tavoite: mahdollistaa 1 bitti / alkio, kun tavallinen toteutus luultavasti veisi 1 tavu / alkio eli 8 bittiä / alkio COMP.CS.300 Tietorakenteet ja algoritmit 1 137 Pakka deque<tyyppi> on molemmista päistään avoin taulukko • luodaan deque<tyyppi> d {arvo, arvo, arvo...}; • Rajapinta vektorin kanssa yhtenevä, mutta tarjoaa tehokkaan (O(1) tasattu suoritusaika) lisäyksen ja poiston molemmissa päissä: .push_front(alkio), .emplace_front(args), .pop_front() • iteraattorien mitätöitymistä tapahtuu seuraavissa tilanteissa – kaikki lisäykset voivat mitätöidä iteraattorit – poistot keskelle mitätöivät kaikki iteraattorit – kaikki paitsi päihin kohdistuvat lisäys- ja poisto-operaatiot voivat mitätöidä viitteet ja osoittimet COMP.CS.300 Tietorakenteet ja algoritmit 1 138 Lista on säiliö, joka tukee kaksisuuntaista iterointia • luodaan list<tyyppi> l {arvo, arvo, arvo }; • lisäys ja poisto kaikkialla vakioaikaista, indeksointioperaatiota ei ole • lisäys ja poisto eivät mitätöi iteraattoreita ja viitteitä (paitsi tietysti poistettuihin alkioihin) • listalla on monenlaisia erikoispalveluja – .splice(paikka, toinen_lista) siirtää toisen listan nykyisen osaksi paikan eteen, O(1). – .splice(paikka, lista, alkio) siirtää alkion toisesta tai samasta listasta paikan eteen, O(1). – .splice(paikka, lista, alku, loppu) siirtää välin alkiot paikan eteen, O(1) tai lineaarinen – .merge(toinen_lista) ja .sort(), vakaa, keskimäärin O(nlogn) – .reverse(), lineaarinen COMP.CS.300 Tietorakenteet ja algoritmit 1 142 Assosiatiiviset säiliöt: Joukko set<tyyppi> ja monijoukko multiset<tyyppi> on dynaaminen joukko, josta voi • etsiä, lisätä ja poistaa logaritmisessa ajassa • selata suuruusjärjestyksessä tasatussa vakioajassa siten että läpikäynti alusta loppuun on aina lineaarinen operaatio • alkioilla on oltava suuruusjärjestys “<” • voi määritellä erikseen joko osana tyyppiä tai rakentajan parametrina • tutkii yhtäsuuruuden kaavalla ¬(x<y ∨y<x) ⇒“<” määriteltävä järkevästi ja tehokkaaksi Monijoukossa sama alkio voi olla moneen kertaan, joukossa ei COMP.CS.300 Tietorakenteet ja algoritmit 1 143 Luodaan std::set<tyyppi> s {arvo, arvo, arvo...}; • alkion arvon muuttaminen on pyritty estämään – sen sijaan pitää poistaa vanha alkio ja lisätä uusi • mielenkiintoisia operaatioita: – .find(alkio) etsii alkion (monijoukolle ensimmäisen monesta), tai palauttaa .end() jollei löydä – .lower_bound(alkio) etsii ensimmäisen, joka on ≥alkio – .upper_bound(alkio) etsii ensimmäisen, joka on > alkio – .equal_range(alkio) palauttaa make_pair( .lower_bound(alkio), .upper_bound(alkio) ), mutta selviää yhdellä etsinnällä (joukolle ko. välin leveys on 0 tai 1) – joukoille insert palauttaa parin (paikka, lisättiin), koska alkiota ei saa lisätä, jos se on jo joukossa • standardi lupaa, että iteraattorit eivät vanhene lisäyksessä ja poistossa (paitsi tietysti poistettuihin alkioihin kohdistuneet) COMP.CS.300 Tietorakenteet ja algoritmit 1 144 Kuvaus map<avaimen_tyyppi, alkion_tyyppi> ja monikuvaus multimap<avaimen_tyyppi, alkion_tyyppi> • alkiot avain-oheisdatapareja – parin tyyppi on pair<tyyppi1, tyyppi2> – parin voi tehdä funktiolla make_pair:illä – parin kentät saa operaatioilla .first(), .second() • luodaan std::map m<avain_tyyppi, alkio_tyyppi> m {{avain1, arvo1}, {avain2, arvo2}, avain3, arvo3},...}; esim. std::map<std::string,int> anim { {\"bear\",4}, {\"giraffe\",2}, {\"tiger\",7} }; • map:ia voi poikkeuksellisesti indeksoida avaimen avulla O(logn) – Jos avainta ei löydy, lisää arvoparin avain-arvo rakenteeseen • nytkään iteraattorit eivät vanhene lisäyksessä ja poistossa COMP.CS.300 Tietorakenteet ja algoritmit 1 145 Hajautustaulu, Unordered set/multiset, joka sisältää joukon alkioita ja unordered map/multimap, joka sisältää joukon alkioita, jotka assosioidaan avainarvojoukolle. • unordered map/set muistuttavat rajapinnaltaan mapia ja setia • tärkeimmät erot: – alkiot eivät ole järjestyksessä (unordered) – lisäys, poisto ja etsintä ovat keskimäärin vakioaikaisia ja pahimmassa tapauksessa lineaarisia – tarjoavat hajautuksen kannalta olennaisia funktioita, kuten rehash(koko), load_factor(), hash_function() ja bucket_size(). COMP.CS.300 Tietorakenteet ja algoritmit 1 146 • hajautustalun kokoa kasvatetaan automaattisesti, jotta lokeroiden keskimääräinen täyttöaste saadaan pidettyä sovitun rajan alapuolella – hajautustaulun koon muuttaminen (rehashing) on keskimäärin lineaarinen pahimmillaan neliöllinen operaatio – koon muuttaminen mitätöi kaikki iteraattorit, muttei osoittimia eikä viitteitä COMP.CS.300 Tietorakenteet ja algoritmit 1 147 Lisäksi Standardikirjastosta löytyy joitakin muita säiliöitä: Bittivektori bitset<bittien_määrä> • #include<bitset> • tarkoitettu kiinteän kokoisten binääristen bittisarjojen käsittelyyn • tarjoaa tyypillisiä binäärisiä operaatioita (AND, OR, XOR, NOT) Merkkijonot string • #include<string> • vaikka C++:n merkkijonot on optimoitu muuhun tarkoitukseen eikä niitä yleensä ajatella säiliöinä, ne ovat muun lisäksi säiliöitäkin • säilövät merkkejä, mutta saadaan säilömään muutakin • niillä on mm. iteraattorit, [. . . ], .at(. . . ), .size(), .capacity() ja swap • merkkijonot voivat kasvaa hyvin suuriksi ja varaavat tarvittaessa automaattisesti lisää muistia COMP.CS.300 Tietorakenteet ja algoritmit 1 148 • merkkijonojen muokkausoperaatioiden (katenointi, poisto) kanssa kannattaa olla varovainen, koska niissä suoritetaan muistinvarausta ja kopiointia, minkä vuoksi ne ovat pitkille merkkijonoille varsin raskaita • usein on muutenkin järkevää sijoittaa merkkijonot esimerkiksi osoittimen päähän sijoitettaessa niitä säiliöihin, jottei niitä turhaan kopioitaisi • samasta syystä merkkijonot tulee välittää viiteparametreina Säiliöiden lisäksi STL tarjoaa joukon säiliösovittimia, jotka eivät itsessään ole säiliöitä, mutta joiden avulla säiliön rajapinnan saa “sovitettua toiseen muottiin”: COMP.CS.300 Tietorakenteet ja algoritmit 1 149 Pino stack<alkion_tyyppi, säiliön_tyyppi> • tarjoaa normaalien luokka-operaatioiden lisäksi vain – pino-operaatiot, .push(. . . ), .top(), .pop() – koon kyselyt .size() ja .empty() – vertailut “==”, “<” jne. • .pop() ei palauta mitään, ylimmän alkion saa tarkasteltavaksi .top():illa • pinon ylintä alkiota voi muuttaa paikallaan: pino.top() = 35; • kurssin kannalta kiinnostavaa on, että käyttäjä voi valita taulukkoon tai listaan perustuvan toteutuksen – mikä tahansa säiliö, joka tarjoaa back(), push_back() ja pop_back() käy, erityisesti vector, list ja deque. – stack<tyyppi> perus_pino; (deque) – stack<tyyppi, list<tyyppi> > lista_pino; COMP.CS.300 Tietorakenteet ja algoritmit 1 150 Jono queue<alkion_tyyppi, säiliön_tyyppi> • jono-operaatiot .push(. . . ), .pop(), .front(), .back()(!) • mikä tahansa säiliö, joka tarjoaa front(), back(), push_back() ja pop_front käy • muuten kutakuinkin samanlainen kuin pino Prioriteettijono priority_queue<alkion_tyyppi,säiliön_tyyppi> • lähes täysin samanlainen rajapinta kuin jonolla • toteutus kekona • mikä tahansa säiliö, jolla front(), push_back() ja pop_back() ja hajasaanti-iteraattoreita tukeva käy, erityisesti vector (oletus) ja deque • alkioilla eri järjestys: .top() palauttaa suurimman • yhtäsuurista palauttaa minkä vain • ylintä alkiota ei voi muuttaa top:in avulla paikallaan • kuten assosiatiivisilla säiliöillä, järjestämisperiaatteen voi antaa <>-parametrina tai rakentajan parametrina (strict weak ordering) COMP.CS.300 Tietorakenteet ja algoritmit 1 151 tieto- lisäys lisäys 1. alkion n:s alkio tietyn suurimman rakenne loppuun muualle poisto poisto (indeks.) etsintä poisto array O(1) O(n)[2] vector O(1) O(n) O(n) O(n)[1] O(1) O(n)[2] O(n)[3] list O(1) O(1) O(1) O(1) O(n) O(n) O(n)[3] deque O(1) O(n)[4] O(1) O(n)[1] O(1) O(n)[2] O(n)[3] stack[9] O(1) O(1)[5] queue[9] O(1)[6] O(1)[7] priority O(log n) O(log n)[8] queue[9] [10] set O(log n) O(log n) O(log n) O(n) O(log n) O(log n) (multiset) map O(log n) O(log n) O(log n) O(n) O(log n) O(log n) (multimap) unordered_ O(n) O(n) O(n) O(n) (multi)set ≈Θ(1) ≈Θ(1) ≈Θ(1) O(n) unordered_ O(n) O(n) O(n) O(n) (multi)map ≈Θ(1) ≈Θ(1) ≈Θ(1) O(n) COMP.CS.300 Tietorakenteet ja algoritmit 1 152 [1] vakioaikainen viimeiselle alkiolle, muuten lineaarinen [2] logaritminen jos tietorakenne on järjestetty, muuten lineaarinen [3] vakioaikainen jos tietorakenne on järjestetty, muuten lineaarinen [4] vakioaikainen ensimmäiselle alkiolle, muuten lineaarinen [5] mahdollinen vain viimeiselle alkiolle [6] vain alkuun lisääminen on mahdollista [7] vain lopusta poistaminen on mahdollista [8] kysyminen vakioajassa, poistaminen logaritmisessa ajassa [9] säiliösovitin [10] lisäys tapahtuu automaattisesti kekojärjestyksen mukaiselle paikalle COMP.CS.300 Tietorakenteet ja algoritmit 1 153 7.4 Geneeriset algoritmit Standardikirjasto tarjoaa useimmat tähän mennessä käsitellyistä algoritmeista. Algoritmit on kaikki toteutettu funktiomalleina, jotka saavat kaikki tarvitsemansa tiedon käsiteltävistä säiliöistä parametrien avulla. Algoritmeille ei kuitenkaan koskaan anneta parametrina kokonaisia säiliöitä vaan ainoastaan iteraattoreita niihin. • algoritmeilla voidaan käsitellä myös säiliön osia kokonaisten säiliöiden sijasta • algoritmi voi saada parametrinaan iteraattoreita erityyppisiin säiliöihin, jolloin yhdellä funktiokutsulla voidaan yhdistää esimerkiksi vectorin ja listan sisällöt ja tallettaa tulos joukkoon • algoritmien toimintaa voidaan muuttaa iteraattorisovittimien avulla • ohjelmoija voi toteuttaa omiin tietorakenteisiinsa iteraattorit, jonka jälkeen algoritmit toimivat myös niille COMP.CS.300 Tietorakenteet ja algoritmit 1 154 Kaikkia algoritmeja ei kuitenkaan pystytä suorittamaan kaikille tietorakenteille tehokkaasti. ⇒osa algoritmeista hyväksyy parametreikseen vain tietyn iteraattorikategorian iteraattoreita. • tämä takaa algoritmien tehokkuuden, koska kaikki iteraattorin tarjoamat operaatiot ovat vakioaikaisia • jos iteraattori on väärää tyyppiä, annetaan käännösaikainen virhe-ilmoitus ⇒jos algoritmille annetaan tietorakenne, jolle sitä ei voida toteuttaa tehokkaasti, se ei edes käänny Standardikirjaston algoritmit ovat kirjastossa algorithm. Lisäksi standardi määrittelee C-kielen algoritmikirjaston cstdlib. jakaa algoritmit kolmee pääryhmään: muuttamattomat sarjalliset operaatiot, muuttavat sarjalliset operaatiot ja järjestäminen sekä siihen liittyvät operaatiot. Seuraavaksi lyhyt kuvaus joistakin kurssin kannalta kiinnostavimmista algoritmeista (näiden lisäksi on vielä COMP.CS.300 Tietorakenteet ja algoritmit 1 155 runsaasti suoraviivaisia selaamiseen yms. perustuvia algoritmeja): COMP.CS.300 Tietorakenteet ja algoritmit 1 156 Puolitushaku • binary_search(eka, loppu, arvo) kertoo onko arvo järjestetyssä jononpätkässä – eka ja loppu ovat iteraattoreita, jotka osoittavat etsittävän alueen alkuun ja loppuun, muttei välttämättä säiliön alkuun ja loppuun • samaa arvoa voi olla monta peräkkäin ⇒lower_bound ja upper_bound palauttavat sen alueen rajat, jolla on arvoa – alaraja on, yläraja ei ole mukana alueessa • rajat saa myös pariksi yhdistettynä yhdellä etsinnällä: equal_range • vertaa BIN-SEARCH sivu 74 COMP.CS.300 Tietorakenteet ja algoritmit 1 157 Järjestämisalgoritmit • sort(alku, loppu) ja stable_sort(alku, loppu) • sortin suoritusaika O(nlogn) ja stable_sortin O(nlogn) jos tarpeeksi lisämuistia on saatavilla, muuten O(nlog2n) • järjestelyalgoritmit vaativat parametreikseen hajasaanti-iteraattorit ⇒eivät toimi listoille, mutta niissä on oma sort (ja ei-kopioiva merge) jäsenfunktiona • löytyy myös järjestäminen, joka lopettaa, kun halutun mittainen alkuosa on järjestyksessä: partial_sort(alku, keski, loppu) • lisäksi is_sorted(alku, loppu) ja is_sorted_until(alku, loppu) nth_element( eka, ¨ann¨as, loppu ) • etsii alkion, joka järjestetyssä säiliössä olisi kohdalla ¨ann¨as • muistuttaa algoritmia RANDOMIZED-SELECT • iteraattoreiden tulee olla hajasaanti-iteraattoreita COMP.CS.300 Tietorakenteet ja algoritmit 1 158 Ositus (partitiointi) • partition(eka, loppu, ehtofunktio) epävakaa, erikseen stable_partition. • stable_partition(eka, loppu, ehtofunktio) vakaa, mutta hitaampi ja/tai varaa enemmän muistia • järjestää välillä eka - loppu olevat alkiot siten, että ensin tulevat alkiot, joille ehtofunktio palauttaa true ja sitten, ne joille se palauttaa false. • vrt. QUICK-SORTn yhteudessä esitelty PARTITION • partition on tehokkuudeltaan lineaarinen • lisäksi is_partitioned ja partition_point COMP.CS.300 Tietorakenteet ja algoritmit 1 159 merge( alku1, loppu1, alku2, loppu2, maali) • Algoritmi limittää välien alku1 - loppu1 ja alku2 - loppu2 alkiot ja kopioi ne suuruusjärjestyksessä iteraattorin maali päähän • algoritmi edellyttää, että alkiot yhdistettävillä väleillä ovat järjestyksessä • vertaa sivun 45 MERGE • algoritmi on lineaarinen • alku- ja loppu-iteraattorit ovat syöttöiteraattoreita ja maali on tulostusiteraattori Keot • STL:stä löytyy myös vastineet luvun 3.1 kekoalgoritmeille • push_heap( eka, loppu) HEAP-INSERT • pop_heap( eka, loppu ) vaihtaa huippualkion viimeiseksi (eli paikkaan loppu −1) ja ajaa HEAPIFY:n osalle eka . . . loppu −1 – vrt. HEAP-EXTRACT-MAX • make_heap( eka, loppu) BUILD-HEAP COMP.CS.300 Tietorakenteet ja algoritmit 1 160 • sort_heap( eka, loppu ) HEAPSORT • lisäksi is_heap ja is_heap_until • iteraattoreiden tulee olla hajasaanti-iteraattoreita Joukko-operaatiot • C++:n standardikirjasto sisältää tätä tukevia funktioita • includes( eka1, loppu1, eka2, loppu2 ) osajoukko ⊆ • set_union( eka1, loppu1, eka2, loppu2, tulos ) unioni ∪ • set_intersection(. . . ) leikkaus ∩ • set_difference(. . . ) erotus - • set_symmetric_difference(. . . ) • alku- ja loppu-iteraattorit ovat syöttöiteraattoreita ja tulos on tulostusiteraattori find_first_of( eka1, loppu1, eka2, loppu2 ) • lopussa voi lisäksi olla tutkittavia alkioita rajaava ehto COMP.CS.300 Tietorakenteet ja algoritmit 1 161 • etsii ensimmäisestä jonosta ensimmäisen alkion, joka on myös toisessa jonossa • jono voi olla taulukko, lista, joukko, . . . • yksinkertainen toteutus on hitaimmillaan Θ(nm), missä n ja m ovat jonojen pituudet • toinen jono selataan jokaiselle ensimmäisen jonon alkiolle – hitain tapaus kun ei löydy ⇒hidasta, jos molemmat jonot pitkiä • toteutus saataisiin yksinkertaiseksi, nopeaksi ja muistia säästäväksi vaatimalla, että jonot ovat järjestyksessä HUOM! Mikään STL:n algoritmi ei automaattisesti tee säiliöihin lisäyksiä eikä poistoja, vaan ainoastaan muokkaa olemassa olevia alkioita. • esimerkiksi merge ei toimi, jos sille annetaan tulostusiteraattoriksi iteraattori tyhjän säiliön alkuun • jos tulostusiteraattorin halutaan tekevän lisäyksiä kopioinnin sijasta, tulee käyttää iteraattorisovitinta lisäysiteraattori COMP.CS.300 Tietorakenteet ja algoritmit 1 162 7.5 Lambdat: [](){} Algoritmikirjaston yhteydessä on paljon tilanteita, joissa on tarve välittää funktiolle toiminnallisuutta – esim. find_if, for_each, sort Lambdat ovat nimettömiä, määrittelemättömän tyyppisiä funktion kaltaisia. Ne ottavat parametreja, palauttavat paluuarvon ja pystyvät viittaamaan luontiympäristönstä muuttujiin sekä muuttamaan niitä. Syntaksi: [ympäristö](parametrit)->paluutyyppi {runko} – Jos lambda ei viittaa ympäristöönsä ympäristö on tyhjä – parametrit voi puuttua – jos ->paluutyyppiä ei ole annettu, se on void. Yksittäisestä return-lauseesta se voidaan päätellä – esim. [](int x, int y){ return x+y;} for_each( v.begin(), v.end(),[] (int val) {cout<<val<<endl;}); std::cin >> raja; //paikallinen muuttuja std:find_if(v.begin(), v.end(),[raja](int a){return a<raja;}); COMP.CS.300 Tietorakenteet ja algoritmit 1 163 STL:n algoritmeja voi ajatella nimettyinä erityissilmukoina, joiden runko lambda on bool kaikki = true; for (auto i : v) { if (i%10 != 0) { kaikki = false; break; } } if (kaikki) {...} if (std::all_of(v.begin(), v.end(), [](int i){return i%10==0;}){...} COMP.CS.300 Tietorakenteet ja algoritmit 1 169 8.1 Puu Ennen kuin käydään käsiksi kekoon, määritellään sen tueksi käsite puu. Puu on: lehtiä sisäsolmuja juuri • rakenne, joka koostuu solmuista, joilla on mielivaltainen määrä lapsia. COMP.CS.300 Tietorakenteet ja algoritmit 1 170 • Binääripuussa lasten määrä on rajoitettu välille 0–2. Tällöin lapset nimetään vasen (left) ja oikea (right) • solmu on lapsiensa isä (parent) • lapseton solmu on lehti (leaf), ja muut solmut ovat sisäsolmuja (internal node) • puussa on korkeintaan yksi solmu, jolla ei ole isää. Isätön solmu on puun juuri (root). – kaikki muut solmut ovat juuren lapsia, lastenlapsia jne. COMP.CS.300 Tietorakenteet ja algoritmit 1 171 • puun rakenne on rekursiivinen: kunkin solmun jälkeläiset muodostavat puun alipuun, jonka juuri kyseinen solmu on Kuva 13: Binääripuun rekursiivisuus • puun solmun korkeus (height) on pisimmän solmusta COMP.CS.300 Tietorakenteet ja algoritmit 1 172 suoraan alas lehteen vievän polun pituus – pituus lasketaan kaarien mukaan, jolloin lehden korkeus on 0 • puun korkeus on sen juuren korkeus • puu on täydellisesti tasapainotettu (completely balanced), jos sen juuren lasten määräämien alipuiden korkeudet eroavat toisistaan enintään yhdellä, ja alipuut on täydellisesti tasapainotettu • n-solmuisen puun korkeus on vähintään ⌊lg n⌋ja korkeintaan n - 1 (logaritmin kantaluku riippuu lasten maksimimäärästä) ⇒O(n) ja Ω(lg n) COMP.CS.300 Tietorakenteet ja algoritmit 1 173 Puun solmut voidaan käsitellä monessa eri järjestyksessä. • esijärjestys (preorder) eli ensin käsi- tellään juuri, sitten rekursiivisesti lap- set. – kutsu: PREORDER-TREE-WALK(T.root) – esimerkin käsittelyjärjestys on 18, 13, 8, 5, 3, 6, 9, 15, 14, 25, 22, 23, 30, 26, 33, 32, 35 PREORDER-TREE-WALK(x) 1 if x ̸= NIL then 2 käsittele alkio x 3 for child in x→children do 4 PREORDER-TREE-WALK(child) 13 25 33 lehtiä sisäsolmuja juuri 14 18 9 23 5 15 3 8 35 32 26 22 30 6 COMP.CS.300 Tietorakenteet ja algoritmit 1 174 • välijärjestys (inorder) – välijärjestys koskee lähinnä bi- nääripuuta, siinä käsitellään en- sin rekursiivisesti vasen lapsi, sitten juuri ja lopuksi rekursiivisesti oikea lapsi – esimerkissä 3, 5, 6, 8, 9, 13, 14, 15, 18, 22, 23, 25, 26, 30, 32, 33, 35 INORDER-TREE-WALK(x) 1 if x ̸= NIL then 2 INORDER-TREE-WALK(x→left) 3 käsittele alkio x 4 INORDER-TREE-WALK(x→right) 13 25 33 lehtiä sisäsolmuja juuri 14 18 9 23 5 15 3 8 35 32 26 22 30 6 COMP.CS.300 Tietorakenteet ja algoritmit 1 175 • jälkijärjestys (postorder), eli ensin käsitellään rekursiivisesti lapset, lo- puksi vasta juuri – esimerkissä 3, 6, 5, 9, 8, 14, 15, 13, 23, 22, 26, 32, 35, 33, 30, 25, 18 POSTORDER-TREE-WALK(x) 1 if x ̸= NIL then 2 for child in x→children do 3 POSTORDER-TREE-WALK(child) 4 käsittele alkio x 13 25 33 lehtiä sisäsolmuja juuri 14 18 9 23 5 15 3 8 35 32 26 22 30 6 COMP.CS.300 Tietorakenteet ja algoritmit 1 176 Puun läpikäynnin ajankäyttö: • ajoaika Θ(n), algoritmit kutsuvat itseään kahdesti joka solmussa: kerran vasemmalle ja kerran oikealle lapselle • lisämuistin tarve = Θ(rekursion maksimisyvyys) = Θ(h + 1) = Θ(h) COMP.CS.300 Tietorakenteet ja algoritmit 1 139 Tasattu eli amortisoitu ajoaika Vektori on joustavarajainen taulukko eli sen kokoa kasvatetaan tarvittaessa. • kun uusi alkio ei enää mahdu taulukkoon, varataan uusi suurempi ja siirretään kaikki alkiot sinne • taulukko ei koskaan kutistu ⇒muistin varaus ei vähene muuten kuin kopioimalla vektoriin kokonaan uusi sisältö ⇒Alkion lisäämisen vectorin loppuun sanottiin olevan tasatusti (amortisoidusti) vakioaikaista. COMP.CS.300 Tietorakenteet ja algoritmit 1 140 • Amortisoidusti lähestyttäessä suoritusaikaa tarkastellaan kokonaisuutena, tutkitaan operaatiosarjojen suoritusaikaa yksittäisten operaatioiden sijaan – jokaista kallista muistinvarausta vaativaa lisäysoperaatiota edeltää kalliin operaation hintaan suoraan verrannollinen määrä halpoja lisäysoperaatioita – kalliin operaation kustannus voidaan jakaa tasan halvoille operaatioille – tällöin halvat operaatiot ovat edelleen vakioaikaisia, tosin vakiokertoimen verran hitaampia kuin oikeasti – kallis operaatio voidaan maksaa säästöillä ⇒kaikki lisäysoperaatiot vektorin loppuun ovat tasatusti vakioaikaisia COMP.CS.300 Tietorakenteet ja algoritmit 1 141 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 Tämä voidaan todentaa vaikkapa kirjanpito- menetelmällä: • laskutetaan jokaisesta lisäyksestä kolme ra- haa • yksi raha käytetään lisäyksen todellisiin kus- tannuksiin • yksi raha laitetaan säästöön lisätyn alkion i kohdalle • yksi raha laitetaan säästöön alkion i −1 2 · vector.capacity() kohdalle • kun tulee tarve laajentaa taulukkoa, jokai- sella alkiolla on yksi raha säästössä, ja kallis kopiointi voidaan maksaa niillä COMP.CS.300 Tietorakenteet ja algoritmit 1 110 6.2 Suunnitteluperiaate: Satunnaistaminen Satunnaista on eräs algoritmien suunnitteluperiaatteista. • Sen avulla voidaan usein estää huonoimpien tapausten patologinen ilmeneminen. • Parhaan ja huonoimman tapauksen suoritusajat eivät useinkaan muutu, mutta niiden esiintymistodennäköisuus käytännössä laskee. • Huonot syötteet ovat täsmälleen yhtä todennäköisiä kuin mitkä tahansa muut syötteet riippumatta alkuperäisestä syötteiden jakaumasta. • Satunnaistaminen voidaan suorittaa joko ennen algoritmin suoritusta satunnaistamalla sen saama syöteaineisto tai upottamalla satunnaistaminen algoritmin sisälle. – jälkimmäisellä tavalla päästään usein parempaan tulokseen – usein se on myös helpompaa kuin syötteen esikäsittely COMP.CS.300 Tietorakenteet ja algoritmit 1 111 • Satunnaistaminen on hyvä ratkaisu yleensä silloin, kun – algoritmi voi jatkaa suoritustaan monella tavalla – on vaikea arvata etukäteen, mikä tapa on hyvä – suuri osa tavoista on hyviä – muutama huono arvaus hyvien joukossa ei haittaa paljoa • Esimerkiksi QUICKSORT voi valita jakoarvoksi minkä tahansa taulukon alkion – hyviä valintoja ovat kaikki muut, paitsi lähes pienimmät ja lähes suurimmat taulukossa olevat arvot – on vaikea arvata valintaa tehdessä, onko ko. arvo lähes pienin / suurin – muutama huono arvaus silloin tällöin ei turmele QUICKSORTin suorituskykyä ⇒satunnaistaminen sopii QUICKSORTille COMP.CS.300 Tietorakenteet ja algoritmit 1 112 Satunnaistamisen avulla voidaan tuottaa algoritmi RANDOMIZED-QUICKSORT, joka käyttää satunnaistettua PARTITIONIA. • Ei valita jakoarvoksi aina A[ right ]:tä, vaan valitaan jakoarvo satunnaisesti koko osataulukosta. • Jotta PARTITION ei menisi rikki, sijoitetaan jakoarvo silti kohtaan right taulukkoa ⇒Nyt jako on todennäköisesti melko tasainen riippumatta siitä, mikä syöte saatiin ja mitä taulukolle on jo ehditty tehdä. COMP.CS.300 Tietorakenteet ja algoritmit 1 113 RANDOMIZED-PARTITION( A, left, right ) 1 p := RANDOM(left, right) (valitaan satunnainen alkio pivotiksi) 2 exchange A[ right ] ↔A[ p ] (asetetaan se taulukon viimeiseksi) 3 return PARTITION( A, left, right ) (kutsutaan tavallista partitiointia) RANDOMIZED-QUICKSORT( A, left, right ) 1 if left < right then 2 p := RANDOMIZED-PARTITION( A, left, right ) 3 RANDOMIZED-QUICKSORT( A, left, p −1 ) 4 RANDOMIZED-QUICKSORT( A, p + 1, right ) RANDOMIZED-QUICKSORTIN ajoaika on keskimäärin Θ(n lg n) samoin kuin tavallisenkin QUICKSORTIN. COMP.CS.300 Tietorakenteet ja algoritmit 1 114 • RANDOMIZED-QUICKSORTILLE kuitenkin varmasti pätee keskimääräisen ajankäytön analyysin yhteydessä tehtävä oletus, jonka mukaan pivot-alkio on osataulukon pienin, toiseksi pienin jne. aina samalla todennäköisyydellä. • Tavalliselle QUICKSORTILLE tämä pätee ainoastaan, jos aineisto on tasaisesti jakautunutta. ⇒RANDOMIZED-QUICKSORT on yleisessä tapauksessa tavallista QUICKSORTIA parempi. COMP.CS.300 Tietorakenteet ja algoritmit 1 115 QUICKSORTIA voidaan tehostaa myös muilla keinoilla: • Voidaan järjestää pienet osataulukot pienille taulukoille tehokkaalla algoritmilla (esim. INSERTIONSORT) avulla. – voidaan myös jättää ne vain järjestämättä ja järjestää taulukko lopuksi INSERTIONSORTIN avulla • Jakoarvo voidaan valita esimerkiksi kolmen satunnaisesti valitun alkion mediaanina. • On jopa mahdollista käyttää aina mediaanialkiota jakoalkiona. COMP.CS.300 Tietorakenteet ja algoritmit 1 116 Mediaani on mahdollista etsiä nopeasti niin sanotun laiskan QUICKSORTIN avulla. • Jaetaan taulukko “pienten alkioiden” alaosaan ja “suurten alkioiden” yläosaan kuten QUICKSORTissa. • Lasketaan, kumpaan osaan i:s alkio kuuluu, ja jatketaan rekursiivisesti sieltä. • Toiselle osalle ei tarvitse tehdä enää mitään. RANDOMIZED-SELECT( A, left, right, goal ) 1 if left = right then (jos osataulukko on yhden kokoinen...) 2 return A[ left ] (... palautetaan ainoa alkio) 3 p := RANDOMIZED-PARTITION( A, left, right ) (jaetaan taulukko pieniin ja isoihin) 4 k := p −left + 1 (lasketaan monesko jakoalkio on) 5 if i = k then (jos jakoalkio on taulukon i:s alkio...) 6 return A[ p ] (...palautetaan se) 7 else if i < k then (jatketaan etsintää pienten puolelta) 8 return RANDOMIZED-SELECT( A, left, p −1, goal ) 9 else (jatketaan etsintää suurten puolelta) 10 return RANDOMIZED-SELECT( A, p + 1, right, goal −k ) COMP.CS.300 Tietorakenteet ja algoritmit 1 117 RANDOMIZED-SELECTIN suoritusajan alaraja: • Jälleen kaikki muu on vakioaikaista paitsi RANDOMIZED-PARTITION ja rekursiivinen kutsu. • Parhaassa tapauksessa RANDOMIZED-PARTITIONIN valitsema jakoalkio on taulukon i:s alkio, ja ohjelman suoritus loppuu. • RANDOMIZED-PARTITION ajetaan kerran koko taulukolle. ⇒Algoritmin suoritusaika on Ω(n). RANDOMIZED-SELECTIN suoritusajan yläraja: • RANDOMIZED-PARTITION sattuu aina valitsemaan pienimmän tai suurimman alkion, ja i:s alkio jää suuremmalle puoliskolle • työmäärä pienenee vain yhdellä askeleella joka rekursiotasolla. ⇒Algoritmin suoritusaika on O(n2). COMP.CS.300 Tietorakenteet ja algoritmit 1 118 Keskimääräisen tapauksen ajoaika on kuitenkin O(n). Algoritmi löytyy esimerkiksi STL:stä nimellä nth_element. Algoritmi on mahdollista muuttaa myös toimimaan aina lineaarisessa ajassa. COMP.CS.300 Tietorakenteet ja algoritmit 1 177 8.2 Suunnitteluperiaate: Muunna ja hallitse Muunna ja hallitse on suunnitteluperiaate, joka • Ensin muokkaa ongelman instanssia muotoon, joka on helpompi ratkaista – muunnosvaihe • Sitten ongelma voidaan ratkaista – hallintavaihe Ongelman instanssi voidaan muuntaa kolmella eri tavalla: • Yksinkertaistaminen (Instance simpliﬁcation): yksinkertaisempi tai kätevämpi instanssi samasta ongelmasta • Esitystavan muutos (Representation change): saman instanssin toinen esitystapa • Ongelman muunnos (Problem reduction): ratkaistaan sellaisen ongelman, jolle algoritmi on jo valmiina, instanssi COMP.CS.300 Tietorakenteet ja algoritmit 1 178 Keko Taulukko A[1 . . . n] on keko, jos A[i] ≥A[2i] ja A[i] ≥A[2i + 1] aina kun 1 ≤i ≤⌊n 2⌋(ja 2i + 1 ≤n). Tämä on helpompi ymmärtää, kun tulkitaan keko täydellisesti tasapainotetuksi binääripuuksi, jonka • juuri on talletettu taulukon paik- kaan 1 • paikkaan i talletetun solmun lapset (jos olemassa) on talletet- tu paikkoihin 2i ja 2i + 1 • paikkaan i talletetun solmun isä on talletettu paikkaan ⌊i 2⌋ 12 15 14 7 17 5 10 8 2 7 COMP.CS.300 Tietorakenteet ja algoritmit 1 179 Tällöin jokaisen solmun arvo on suurempi tai yhtä suuri kuin sen lasten arvot. Kekopuun jokainen kerros on täysi, paitsi ehkä alin, joka on täytetty vasemmasta reunasta alkaen. COMP.CS.300 Tietorakenteet ja algoritmit 1 180 Jotta kekoa olisi helpompi ajatella puuna, määrittelemme isä- ja lapsisolmut löytävät aliohjelmat. • ne ovat toteutettavissa hyvin tehokkaasti bittisiirtoina • kunkin suoritusaika on aina Θ(1) PARENT(i) return ⌊i/2⌋ LEFT(i) return 2i RIGHT(i) return 2i + 1 COMP.CS.300 Tietorakenteet ja algoritmit 1 181 ⇒Nyt keko-ominaisuus voidaan lausua seuraavasti: A[PARENT(i)] ≥A[i] aina kun 2 ≤i ≤A.heapsize • A.heapsize kertoo keon koon (myöhemmin nähdään, ettei se aina ole välttämättä sama kuin taulukon koko) Keko-ominaisuudesta seuraa, että keon suurin alkio on aina keon juuressa, siis taulukon ensimmäisessa lokerossa. Jos keon korkeus on h, sen solmujen määrä on välillä 2h . . . 2h+1 −1. ⇒Jos keossa n solmua, sen korkeus on Θ(lg n). COMP.CS.300 Tietorakenteet ja algoritmit 1 182 Alkion lisääminen kekoon ylhäältä: • oletetaan, että A[1 . . . n] on muuten keko, mutta keko- ominaisuus ei päde kekopuun juurelle – toisin sanoen A[1] < A[2] tai A[1] < A[3] 12 10 9 8 3 4 5 6 7 2 1 15 14 7 5 9 8 10 2 7 • ongelma saadaan siirrettyä alemmas puussa valitsemal- la juuren lapsista suurempi, ja vaihtamalla se juuren kanssa – jotta keko-ominaisuus ei ha- joaisi, pitää valita lapsista suu- rempi - siitähän tulee toisen lapsen uusi isä 12 10 9 8 3 4 5 6 7 2 1 9 7 14 5 10 8 15 7 2 COMP.CS.300 Tietorakenteet ja algoritmit 1 183 • sama voidaan tehdä alipuulle, jonka juureen ongelma siirtyi, ja sen alipuulle jne. kunnes ongel- ma katoaa – ongelma katoaa viimeistään kun saavutetaan lehti ⇒puu muuttuu keoksi 12 10 9 8 3 4 5 6 7 2 1 14 7 5 10 15 8 9 7 2 COMP.CS.300 Tietorakenteet ja algoritmit 1 184 Sama pseudokoodina: HEAPIFY( A, i ) (i kertoo paikan, jonka alkio saattaa olla liian pieni) 1 repeat (toistetaan, kunnes keko on ehjä) 2 old_i := i (otetaan i:n arvo talteen) 3 l := LEFT( i ) 4 r := RIGHT( i ) 5 if l ≤A.heapsize and A[ l ] > A[ i ] then (vasen lapsi on suurempi kuin i) 6 i := l 7 if r ≤A.heapsize and A[ r ] > A[ i ] then (oikea lapsi on vielä suurempi) 8 i := r 9 if i ̸= old_i then (jos suurempi lapsi löytyi...) 10 exchange A[ old_i ] ↔A[ i ] (...siirretään rike alaspäin) 11 until i = old_i (jos keko oli jo ehjä, lopetetaan) • Suoritus on vakioaikaista kun rivin 11 ehto toteutuu heti ensimmäisellä kerralla kun sinne päädytään: Ω(1). • Pahimmassa tapauksessa uusi alkio joudutaan siirtämään lehteen asti koko korkeuden verran. ⇒Suoritusaika on O(h) = O(lg n). COMP.CS.300 Tietorakenteet ja algoritmit 1 185 Keon rakentaminen • seuraava algoritmi järjestää taulukon uudelleen niin, että siitä tulee keko: BUILD-HEAP(A) 1 A.heapsize := A.length (koko taulukosta tehdään keko) 2 for i := ⌊A.length/2⌋downto 1 do (käydään taulukon alkupuolisko läpi) 3 HEAPIFY(A, i) (kutsutaan Heapifyta) • Lähdetään käymään taulukkoa läpi lopusta käsin ja kutsutaan HEAPIFYTA kaikille alkioille. – ennen HEAPIFY-funktion kutsua keko-ominaisuus pätee aina i:n määräämälle alipuulle, paitsi että paikan i alkio on mahdollisesti liian pieni – yhden kokoisia alipuita ei tarvitse korjata, koska niissä keko-ominaisuus pätee triviaalisti – HEAPIFY(A, i):n jälkeen i:n määräämä alipuu on keko ⇒HEAPIFY(A, 1):n jälkeen koko taulukko on keko COMP.CS.300 Tietorakenteet ja algoritmit 1 186 • BUILD-HEAP ajaa for-silmukan ⌊n 2⌋kertaa ja HEAPIFY on Ω(1) ja O(lg n), joten – nopein suoritusaika on ⌊n 2⌋· Ω(1) + Θ(n) = Ω(n) – ohjelma ei voi koskaan käyttää enempää aikaa kuin ⌊n 2⌋· O(lg n) + Θ(n) = O(n lg n) • Näin saamamme hitaimman tapauksen suoritusaika on kuitenkin liian pessimistinen: COMP.CS.300 Tietorakenteet ja algoritmit 1 187 – HEAPIFY on O(h), missä h on kekopuun korkeus – i:n muuttuessa myös puun korkeus vaihtelee kerros h HEAPIFY-suoritusten määrä alin 0 0 toinen 1 ⌊n 4⌋ kolmas 2 ⌊n 8⌋ ... ... ... ylin ⌊lg n⌋ 1 – siis pahimman tapauksen suoritusaika onkin n 4 · 1 + n 8 · 2 + n 16 · 3 + · · · = n 2 · P∞ i=1 i 2i = n 2 · 2 = n ⇒O(n) ⇒BUILD-HEAPIN suoritusaika on aina Θ(n) COMP.CS.300 Tietorakenteet ja algoritmit 1 188 8.3 Järjestäminen keon avulla Taulukon alkioiden ärjestäminen voidaan toteuttaa tehokkaasti kekoa hyödyntäen: HEAPSORT( A ) 1 BUILD-HEAP( A ) (muutetaan taulukko keoksi) 2 for i := A.length downto 2 do (käydään taulukko läpi lopusta alkuun) 3 exchange A[ 1 ] ↔A[ i ] (siirretään keon suurin alkio keon viimeiseksi) 4 A.heapsize := A.heapsize −1 (siirretään suurin alkio keon ulkopuolelle) 5 HEAPIFY( A, 1 ) (korjataan keko, joka on muuten kunnossa, mutta...) (... sen ensimmäinen alkio saattaa olla liian pieni) Esitetäänpä sama kuvien avulla: • ensin taulukko muutetaan keoksi • esimerkistä on helppo havaita, et- tei operaatio ole kovin raskas – keko-ominaisuus on selvästi jär- jestystä heikompi 9 12 8 14 7 15 9 9 9 12 12 12 8 8 8 7 7 7 14 14 14 15 15 15 COMP.CS.300 Tietorakenteet ja algoritmit 1 189 • kuvassa voi nähdä kuinka järjeste- tyn loppuosan koko kasvaa, kun- nes koko taulukko on järjestyksessä • kasvatusten välillä keko-osuus kor- jataan • korjaus näyttää tällaisessa pienes- sä esimerkissä tarpeettoman moni- mutkaiselta – korjaukseen ei suurillakaan taulu- koilla kulu kovin montaa askelta, ainoastaan logaritminen määrä 7 8 14 9 12 15 15 15 15 15 15 12 8 7 9 14 7 14 14 15 9 12 8 7 14 15 8 14 9 12 7 14 14 14 14 12 12 12 12 7 9 8 9 9 7 8 8 7 8 7 9 14 12 9 8 12 7 9 8 8 7 9 12 12 14 7 8 9 15 15 15 15 COMP.CS.300 Tietorakenteet ja algoritmit 1 190 HEAPSORTIN suoritusaika koostuu seuraavista osista. • BUILD-HEAP rivillä 1 suoritetaan kerran: Θ(n) • for-silmukan sisältö suoritetaan n - 1 kertaa – rivien 3 ja 4 operaatiot ovat vakioaikaisia – HEAPIFY käyttää aikaa Ω(1) ja O(lg n) ⇒Saadaan yhteensä Ω(n) ja O(n lg n) • alaraja on tarkka – jos kaikki alkiot ovat samanarvoisia, keon korjaustoimenpiteitä ei tarvita koskaan ja HEAPIFY on aina vakioaikainen • myös yläraja on tarkka – tämän osoittaminen on hieman hankalampaa ja tyydymmekin myöhemmin saatavaan tulokseen vertailuun perustuvan järjestämisen nopeudesta COMP.CS.300 Tietorakenteet ja algoritmit 1 191 Huom! Edelliset suoritusaikalaskelmat olettavat, että keon pohjana käytettävällä tietorakenteella on vakioaikainen indeksointi. • Kekoa kannattaa käyttää ainoastaan silloin! HEAPSORTIN etuja ja haittoja Etuja: • järjestää taulukon paikallaan • ei koskaan käytä enempää kuin Θ(n lg n) aikaa Haittoja: • suoritusajan vakiokerroin on suurehko • epävakaus – samanarvoisten alkioiden keskinäinen järjestys ei säily COMP.CS.300 Tietorakenteet ja algoritmit 1 192 8.4 Prioriteettijono Prioriteettijono (priority queue) on tietorakenne, joka pitää yllä joukkoa S alkioita, joista jokaiseen liittyy avain (key), ja sallii seuraavat operaatiot: • INSERT(S, x) lisää alkion x joukkoon S • MAXIMUM(S) palauttaa sen alkion, jonka avain on suurin – jos monella eri alkiolla on sama, suurin avain, valitsee vapaasti minkä tahansa niistä • EXTRACT-MAX(S) poistaa ja palauttaa sen alkion, jonka avain on suurin • vaihtoehtoisesti voidaan toteuttaa operaatiot MINIMUM(S) ja EXTRACT-MIN(S) – samassa jonossa on joko vain maksimi- tai vain minimioperaatiot! COMP.CS.300 Tietorakenteet ja algoritmit 1 193 Prioriteettijonoilla on monia käyttökohteita • tehtävien ajoitus käyttöjärjestelmässä – uusia tehtäviä lisätään komennolla INSERT – kun edellinen tehtävä valmistuu tai keskeytetään, seuraava valitaan komennolla EXTRACT-MAX • tapahtumapohjainen simulointi – jono tallettaa tulevia (= vielä simuloimattomia) tapahtumia – avain on tapahtuman tapahtumisaika – tapahtuma voi aiheuttaa uusia tapahtumia ⇒lisätään jonoon operaatiolla INSERT – EXTRACT-MIN antaa seuraavan simuloitavan tapahtuman • lyhimmän reitin etsintä kartalta – simuloidaan vakionopeudella ajavia, eri reitit valitsevia autoja, kunnes ensimmäinen perillä – prioriteettijonoa tarvitaan käytännössä myöhemmin esiteltävässä lyhimpien polkujen etsintäalgoritmissa COMP.CS.300 Tietorakenteet ja algoritmit 1 194 Prioriteettijonon voisi käytännössä toteuttaa järjestämättömänä tai järjestettynä taulukkona, mutta se olisi tehotonta. • järjestämättömässä taulukossa MAXIMUM ja EXTRACT-MAX ovat hitaita • järjestetyssä taulukossa INSERT on hidas Sen sijaan keon avulla prioriteettijonon voi toteuttaa tehokkaasti. • Joukon S alkiot talletetaan kekoon A. • MAXIMUM( S ) on hyvin helppo, ja toimii ajassa Θ(1). HEAP-MAXIMUM( A ) 1 if A.heapsize < 1 then (tyhjästä keosta ei löydy maksimia) 2 error “heap underﬂow” 3 return A[ 1 ] (muuten palautetaan taulukon ensimmäinen alkio) COMP.CS.300 Tietorakenteet ja algoritmit 1 195 • EXTRACT-MAX(S) voidaan toteuttaa korjaamalla keko poiston jälkeen HEAPIFYN avulla. • HEAPIFY dominoi algoritmin ajoaikaa: O(lg n). HEAP-EXTRACT-MAX( A ) 1 if A.heapsize < 1 then (tyhjästä keosta ei löydy maksimia) 2 error “heap underﬂow” 3 max := A[ 1 ] (suurin alkio löytyy taulukon alusta) 4 A[ 1 ] := A[ A.heapsize ] (siirretään viimeinen alkio juureen) 5 A.heapsize := A.heapsize −1 (pienennetään keon kokoa) 6 HEAPIFY( A, 1 ) (korjataan keko) 7 return max COMP.CS.300 Tietorakenteet ja algoritmit 1 196 • INSERT(S, x) lisää uuden alkion kekoon asettamalla sen uudeksi lehdeksi, ja nostamalla sen suuruutensa mukaiselle paikalle. – se toimii kuten HEAPIFY, mutta alhaalta ylöspäin – lehti joudutaan nostamaan pahimmassa tapauksessa juureen asti: ajoaika O(lg n) HEAP-INSERT( A, key ) 1 A.heapsize := A.heapsize + 1 (kasvatetaan keon kokoa) 2 i := A.heapsize (lähdetään liikkeelle taulukon lopusta) 3 while i > 1 and A[ PARENT(i) ] < key do (edetään kunnes ollaan juuressa tai ...) (...kohdassa jonka isä on avainta suurempi) 4 A[ i ] := A[ PARENT(i) ] (siirretään isää alas päin) 5 i := PARENT(i) (siirrytään ylöspäin) 6 A[ i ] := key (asetetaan avain oikealle paikalleen) ⇒Keon avulla saadaan jokainen prioriteettijonon operaatio toimimaan ajassa O(lg n). COMP.CS.300 Tietorakenteet ja algoritmit 1 197 Prioriteettijonoa voidaan ajatella abstraktina tietotyyppinä, johon kuuluu talletettu data (joukko S) ja operaatiot (INSERT, MAXIMUM,EXTRACT-MAX. • käyttäjälle kerrotaan ainoastaan operaatioiden nimet ja merkitykset, muttei toteutusta • toteutus kapseloidaan esimerkiksi pakkaukseksi (Ada), luokaksi (C++) tai itsenäiseksi tiedostoksi (C) ⇒Toteutusta on helppo ylläpitää, korjata ja tarvittaessa vaihtaa toiseen, ilman että käyttäjien koodiin tarvitsee koskea. COMP.CS.300 Tietorakenteet ja algoritmit 1 231 11.1 Hajautustaulu Hajautustaulun ideana on tiivistää dynaamisen joukon avainten arvoalue pienemmäksi hajautusfunktion (hash function) h avulla, siten että ne voidaan tallettaa taulukkoon. • taulukon etuna on sen tarjoama tehokas vakioaikainen indeksointi 123456 121212 201304 111222 201304 13579 567135 N=30000 h(k) 201304 123456 201304 121212 121212 13579 567135 111222 COMP.CS.300 Tietorakenteet ja algoritmit 1 232 Avainten arvoalueen tiivistämisestä seuraa kuitenkin ongelma: törmäykset. • useampi kuin yksi alkio voi hajautua samaan hajautustaulun lokeroon Tavallisin tapa ratkaista ongelma on ketjuttaminen (chaining). • samaan lokeroon hajautuvat alkiot talletetaan listoihin • muitakin ratkaisutapoja on – avoimen osoituksen käsittelytavalla alkio laitetaan sekundääriseen lokeroon, mikäli primäärinen ei ole vapaana – joissakin tapauksissa avainten arvoalue on niin pieni, että arvoalueen tiivistämistä ei tarvita, eikä siis synny törmäyksiäkään * tällainen suoraosoitustaulu (direct-access table) on hyvin yksinkertainen ja tehokas – tällä kurssilla kuitenkin käsitellään ainoastaan ketjutettuja hajautustauluja COMP.CS.300 Tietorakenteet ja algoritmit 1 233 Alla oleva kuva esittää ketjutettua hajautustaulua, jonka avaimet on hajautettu etukirjaimen mukaan viereisen taulukon avulla. h(k) alkukirjain 0 H P X 1 A I Q Y 2 B J R Z 3 C K S Ä 4 D L T Ö 5 E M U Å 6 F N V 7 G O W Onko tämä hyvä hajautus? • Ei. Katsotaan seuraavaksi, miksei. COMP.CS.300 Tietorakenteet ja algoritmit 1 234 Ketjutettu hajautustaulu tarjoaa ainoastaan sanakirjan operaatiot, mutta ne ovat hyvin yksinkertaisia: CHAINED-HASH-SEARCH(T, k) ▷etsi listasta T[h(k)] alkio, jonka avain on k CHAINED-HASH-INSERT(T, x) ▷lisää x listan T[h(x→key)] alkuun CHAINED-HASH-DELETE(T, x) ▷poista x listasta T[h(x→key)] COMP.CS.300 Tietorakenteet ja algoritmit 1 235 Suoritusajat: • lisäys: Θ(1) • etsintä: hitaimmassa tapauksessa Θ(n) • poisto: jos lista kaksisuuntainen, niin Θ(1); yksisuuntaisella hitaimmillaan Θ(n), koska poistettavan edeltäjä on ensin etsittävä listasta – käytännössä ero ei kuitenkaan ole kovin merkittävä, koska yleensä poistettava alkio joudutaan joka tapauksessa etsimään listasta Ketjutetun hajautustaulun operaatioiden keskimääräiset suoritusajat riippuvat listojen pituuksista. COMP.CS.300 Tietorakenteet ja algoritmit 1 236 • huonoimmassa tapauksessa kaikki alkiot joutuvat samaan listaan jolloin suoritusajat ovat Θ(n) • keskimääräisen tapauksen selville saamiseksi käytämme seuraavia merkintöjä: – m = hajautustaulun koko – n = alkioiden määrä taulussa – α = n m = täyttöaste (load factor) eli listan keskimääräinen pituus • lisäksi keskimääräisen suoritusajan arvioimiseksi on tehtävä oletus siitä, miten hyvin h hajauttaa alkiot – jos esim. h(k) = nimen alkukirjaimen 3 ylintä bittiä, niin kaikki osuvat samaan listaan – usein oletetaan että, jokaisella alkiolla on yhtä suuri todennäköisyys osua mihin tahansa lokeroon – tasainen hajautus (simple uniform hashing) – oletetaan myös, että h(k):n laskenta kuluttaa Θ(1) aikaa COMP.CS.300 Tietorakenteet ja algoritmit 1 237 • jos etsitään alkiota, jota ei ole taulussa, niin joudutaan selaamaan koko lista läpi ⇒joudutaan tutkimaan keskimäärin α alkiota ⇒suoritusaika keskimäärin Θ(1 + α) • jos oletetaan, että listassa oleva avain on mikä tahansa listan alkio samalla todennäköisyydellä, joudutaan listaa selamaan etsinnän yhteydessä keskimäärin puoleen väliin siinäkin tapauksessa, että avain löytyy listasta ⇒suoritusaika keskimäärin Θ(1 + α 2) = Θ(1 + α) • jos täyttöaste pidetään alle jonkin kiinteän rajan (esim. α < 50 %), niin Θ(1 + α) = Θ(1) ⇒ketjutetun hajautustaulun kaikki operaatiot voi toteuttaa keskimäärin ajassa Θ(1) – tämä edellyttää, että hajautustaulun koko on samaa luokkaa kuin sinne talletettavien alkioiden määrä COMP.CS.300 Tietorakenteet ja algoritmit 1 238 Laskiessamme keskimääräistä suoritusaikaa oletimme, että hajautusfunktio hajauttaa täydellisesti. Ei kuitenkaan ole mitenkään itsestään selvää, että näin tapahtuu. Hajautusfunktion laatu on kriittisin tekijä hajautustaulun suorituskyvyn muodostumisessa. Hyvän hajautusfunktion ominaisuuksia: • hajautusfunktion on oltava deterministinen – muutoin kerran tauluun pantua ei välttämättä enää koskaan löydetä! • tästä huolimatta olisi hyvä, että hajautusfunktion arvo olisi mahdollisimman “satunnainen” – kuhunkin lokeroon tulisi osua mahdollisimman tarkasti 1 m avaimista COMP.CS.300 Tietorakenteet ja algoritmit 1 239 • valitettavasti täysin tasaisesti hajottavan hajautusfunktion teko on useinmiten mahdotonta – eri arvojen esiintymistodennäköisyydet aineistossa ovat yleensä tuntemattomia – aineisto ei yleensä ole tasaisesti jakautunut * lähes mikä tahansa järkevä hajautusfunktio jakaa tasaisesti jakautuneen aineiston täydellisesti • yleensä hajautusfunktio pyritään muodostamaan siten, että se sotkee tehokkaasti kaikki syöteaineistossa luultavasti esiintyvät säännönmukaisuudet – esimerkiksi nimien tapauksessa ei katsota yksittäisiä kirjaimia, vaan otetaan jotenkin huomioon nimen kaikki bitit COMP.CS.300 Tietorakenteet ja algoritmit 1 240 • esittelemme kaksi yleistä usein hyvin toimivaa hajautusfunktion luontimenetelmää • oletamme, että avaimet ovat luonnollisia lukuja 0, 1, 2, . . . – jollei näin ole, avain voidaan usein tulkita luonnolliseksi luvuksi – esim. nimen saa luvuksi muuttamalla kirjaimet numeroiksi ASCII-koodiarvon mukaan, ja laskemalla ne sopivasti painottaen yhteen COMP.CS.300 Tietorakenteet ja algoritmit 1 241 Hajautusfunktion luonti jakomenetelmällä on yksinkertaista ja nopeaa. • h(k) = k mod m • sitä kannattaa kuitenkin käyttää vain, jos m:n arvo on sopiva • esim. jos m = 2b jollekin b ∈N = {0, 1, 2, . . .}, niin h(k) = k:n b alinta bittiä ⇒funktio ei edes katso kaikkia k:n bittejä ⇒funktio todennäköisesti hajauttaa huonosti, jos avaimet ovat peräisin binäärijärjestelmästä COMP.CS.300 Tietorakenteet ja algoritmit 1 242 • samasta syystä tulee välttää m:n arvoja muotoa m = 10b, jos avaimet ovat peräisin kymmenjärjestelmän luvuista • jos avaimet ovat muodostetut tulkitsemalla merkkijono 128-järjestelmän luvuksi, niin m = 127 on huono valinta, koska silloin saman merkkijonon kaikki permutaatiot osuvat samaan lokeroon • hyviä m:n arvoja ovat yleensä alkuluvut, jotka eivät ole lähellä 2:n potensseja – esim. halutaan ≈700 listaa ⇒701 kelpaa • kannattaa tarkistaa kokeilemalla pienellä “oikealla” aineistolla, hajauttaako funktio avaimet tehokkaasti COMP.CS.300 Tietorakenteet ja algoritmit 1 243 Hajautusfunktion luonti kertomenetelmällä ei aseta suuria vaatimuksia m:n arvolle. • valitaan vakio A siten, että 0 < A < 1 • h(k) = ⌊m(kA −⌊kA⌋)⌋ • jos m = 2b, koneen sanapituus on w, ja k ja 2w · A mahtuvat yhteen sanaan, niin h(k) voidaan laskea helposti seuraavasti: h(k) = ⌊(((2w · A) · k) mod 2w) 2w−b ⌋ • mikä arvo A:lle tulisi valita? – kaikki A:n arvot toimivat ainakin jollain lailla – kuulemma A ≈ √ 5−1 2 toimii usein aika hyvin COMP.CS.300 Tietorakenteet ja algoritmit 1 244 12 Binäärihakupuu http://imgur.com/L77FY5X Tässä luvussa käsitellään erilaisia yleisiä puurakenteita. • Ensin opitaan, millainen rakenne on binäärihakupuu, • ja tasapainotetaan binäärihakupuu muuttamalla se puna-mustaksi puuksi. • Sitten tutustutaan monihaaraisiin puihin: merkkijonopuu Trie ja B-puu. • Lopuksi vilkaistaan splay- ja AVL-puita. COMP.CS.300 Tietorakenteet ja algoritmit 1 245 12.1 Tavallinen binäärihakupuu Kertauksena: Binääripuu (binary tree) on äärellinen solmuista (node) koostuva rakenne, joka on joko • tyhjä, tai • sisältää yhden solmun nimeltä juuri (root), sekä kaksi binääripuuta nimeltä vasen alipuu (left subtree) ja oikea alipuu (right subtree). Kuva 14: Kertaus: Binääripuu COMP.CS.300 Tietorakenteet ja algoritmit 1 246 Lisäksi määritellään: • Lapseton solmu: lehti (leaf). • Muut solmut sisäsolmuja. • Solmu on lastensa isä (pa- rent) ja solmun esi-isiä (ancestor) ovat solmu itse, solmun isä, tämän isä jne. • Jälkeläinen (descendant) vastaavasti. 13 25 33 lehtiä sisäsolmuja juuri 14 18 9 23 5 15 3 8 35 32 26 22 30 6 COMP.CS.300 Tietorakenteet ja algoritmit 1 247 Binäärihakupuu (binary search tree) on binääripuu, jonka kaikille solmuille x pätee: Jos l on mikä tahansa solmu x:n vasemmassa alipuussa ja r mikä tahansa solmu x:n oikeassa alipuussa, niin l.key ≤x.key ≤r.key • Edellisen sivun binääripuu on binäärihakupuu • Luvussa 8.2 esitelty kekorakenne on binääripuu muttei binäärihakupuu Useinmiten binäärihakupuu esitetään linkitettynä rakenteena, jossa jokaisessa alkiossa on kentät avain (key), vasen lapsi (left), oikea lapsi (right) ja vanhempi (p (parent)). Lisäksi alkiolla on oheisdataa. COMP.CS.300 Tietorakenteet ja algoritmit 1 248 Kuva 15: Hakupuita COMP.CS.300 Tietorakenteet ja algoritmit 1 249 Avaimen haku binäärihakupuusta : • koko puusta haku R-TREE-SEARCH(T.root, k) • palauttaa osoittimen x solmuun, jolle x→key = k, tai NIL, jos tällaista solmua ei ole R-TREE-SEARCH(x, k) 1 if x = NIL or k = x→key then 2 return x (etsitty avain löytyi) 3 if k < x→key then (jos etsitty on pienempi kuin avain...) 4 return R-TREE-SEARCH(x→left, k) (...etsitään vasemmasta alipuusta) 5 else (muuten...) 6 return R-TREE-SEARCH(x→right, k) (...etsitään oikeasta alipuusta) COMP.CS.300 Tietorakenteet ja algoritmit 1 250 Algoritmi suunnistaa juuresta alaspäin huonoimmassa tapauksessa pisimmän polun päässä olevaan lehteen asti. • suoritusaika O(h), missä h on puun korkeus • lisämuistin tarve O(h), rekursion vuoksi Saman voi tehdä myös ilman rekursiota, mikä on suositeltavaa. • tällöin lisämuistin tarve on vain Θ(1) • ajoaika on yhä O(h) TREE-SEARCH(x, k) 1 while x ̸= NIL and k ̸= x→key do (kunnes avain on löytynyt tai ollaan lehdessä) 2 if k < x→key then (jos etsitty on pienempi kuin avain...) 3 x := x→left (...siirrytään vasemmalle) 4 else (muuten...) 5 x := x→right (...siirrytään oikealle) 6 return x (palautetaan tulos) COMP.CS.300 Tietorakenteet ja algoritmit 1 251 Minimi ja maksimi: • minimi löydetään menemällä vasemmalle niin kauan kun se on mahdollista TREE-MINIMUM(x) 1 while x→left ̸= NIL do 2 x := x→left 3 return x • maksimi löydetään vastaavasti menemällä oikealle niin kauan kun se on mahdollista TREE-MAXIMUM(x) 1 while x→right ̸= NIL do 2 x := x→right 3 return x • molempien ajoaika on O(h) ja lisämuistin tarve Θ(1) COMP.CS.300 Tietorakenteet ja algoritmit 1 252 Solmun seuraajaa ja edeltäjää kannattaa etsiä binäärihakupuusta puun rakenteen avulla mieluummin kuin avainten arvojen perusteella. • tällöin kaikki alkiot saadaan käytyä niiden avulla läpi, vaikka puussa olisi yhtä suuria avaimia ⇒tarvitaan siis algoritmi, joka etsii annettua solmua välijärjestyksessä seuraavan solmun • sellainen voidaan rakentaa algoritmin TREE-MINIMUM avulla Kuva 16: Solmun seuraaja? COMP.CS.300 Tietorakenteet ja algoritmit 1 253 Binäärihakupuun solmun seuraaja on joko: • oikean alipuun pienin alkio • tai solmusta juureen vievällä polulla ensimmäinen kohdattu solmu, jonka vasempaan alipuuhun solmu kuuluu jos edellä mainittuja solmuja ei löydy, on kysymyksessä puun suurin solmu Kuva 17: Seuraajat COMP.CS.300 Tietorakenteet ja algoritmit 1 254 TREE-SUCCESSOR(x) 1 if x→right ̸= NIL then (jos oikea alipuu löytyy...) 2 return TREE-MINIMUM(x→right) (...etsitään sen minimi) 3 y := x→p (muuten lähdetään kulkemaan kohti juurta) 4 while y ̸= NIL and x = y→right do (kunnes ollaan tultu vasemmasta lapsesta) 5 x := y 6 y := y→p 7 return y (palautetaan löydetty solmu) • huomaa, että avainten arvoja ei edes katsota! • vrt. seuraajan löytäminen järjestetystä listasta • ajoaika O(h), lisämuistin tarve Θ(1) • TREE-PREDECESSOR voidaan toteuttaa vastaavalla tavalla COMP.CS.300 Tietorakenteet ja algoritmit 1 255 TREE-SUCCESSORIN ja TREE-MINIMUMIN avulla voidaan rakentaa toinen tapa selata puu läpi välijärjestyksessä. TREE-SCAN-ALL(T) 1 if T.root ̸= NIL then 2 x := TREE-MINIMUM(T.root) (aloitetaan selaus puun minimistä) 3 else 4 x := NIL 5 while x ̸= NIL do (selataan niin kauan kun seuraajia löytyy) 6 käsittele alkio x 7 x := TREE-SUCCESSOR(x) • jokainen kaari kuljetaan kerran molempiin suuntiin ⇒TREE-SCAN-ALL selviää ajassa Θ(n), vaikka kutsuukin TREE-SUCCESSORia n kertaa COMP.CS.300 Tietorakenteet ja algoritmit 1 256 • lisämuistin tarve Θ(1) ⇒TREE-SCAN-ALL on asymptoottisesti yhtä nopea, ja muistinkulutukseltaan asymptoottisesti parempi kuin INORDER-TREE-WALK – vakiokertoimissa ei suurta eroa ⇒kannattaa valita TREE-SCAN-ALL, jos tietueissa on p-kentät • TREE-SCAN-ALL sallii useat yhtäaikaiset selaukset, INORDER-TREE-WALK ei COMP.CS.300 Tietorakenteet ja algoritmit 1 257 Lisäys binäärihakupuuhun: TREE-INSERT(T, z) (z osoittaa käyttäjän varaamaa alustettua tietuetta) 1 y := NIL; x := T.root (aloitetaan juuresta) 2 while x ̸= NIL do (laskeudutaan kunnes kohdataan tyhjä paikka) 3 y := x (otetaan potentiaalinen isä-solmu talteen) 4 if z→key < x→key then (siirrytään oikealle tai vasemmalle) 5 x := x→left 6 else 7 x := x→right 8 z→p := y (sijoitetaan löydetty solmu uuden solmun isäksi) 9 if y = NIL then 10 T.root := z (puun ainoa solmu on juuri) 11 else if z→key < y→key then (sijoitetaan uusi solmu isänsä vasemmaksi . . . ) 12 y→left := z 13 else (. . . tai oikeaksi lapseksi) 14 y→right := z 15 z→left := NIL; z→right := NIL Algoritmi suunnistaa juuresta lehteen; uusi solmu sijoitetaan aina lehdeksi. ⇒ajoaika O(h), lisämuistin tarve Θ(1) COMP.CS.300 Tietorakenteet ja algoritmit 1 258 Poisto on monimutkaisempaa, koska se voi kohdistua sisäsolmuun: TREE-DELETE(T, z) (z osoittaa poistettavaa solmua) 1 if z→left = NIL or z→right = NIL then (jos z:lla on vain yksi lapsi . . . ) 2 y := z (. . . asetetaan z poistettavaksi tietueeksi) 3 else 4 y := TREE-SUCCESSOR(z) (muuten poistetaan z:n seuraaja) 5 if y→left ̸= NIL then (otetaan talteen poistettavan ainoa lapsi) 6 x := y→left 7 else 8 x := y→right 9 if x ̸= NIL then (jos lapsi on olemassa . . . ) 10 x→p := y→p (. . . linkitetään se poistettavan tilalle) 11 if y→p = NIL then (jos poistettava oli juuri . . . ) 12 T.root := x (. . . merkitään x uudeksi juureksi) 13 else if y = y→p→left then (sijoitetaan x poistettavan tilalle . . . ) 14 y→p→left := x (. . . sen isän vasemmaksi . . . ) 15 else 16 y→p→right := x (. . . tai oikeaksi lapseksi) 17 if y ̸= z then (jos poistettiin joku muu kuin z . . . ) 18 z→key := y→key (. . . vaihdetaan poistetun ja z:n datat) 19 z→satellitedata := y→satellitedata 20 return y (palautetaan osoitin poistettuun solmuun) COMP.CS.300 Tietorakenteet ja algoritmit 1 259 Huom! Rivillä 5 todellakin tiedetään, että y:llä on korkeintaan yksi lapsi. • jos z:lla on vain yksi lapsi, y on z • jos rivillä 4 kutsutaan TREE-SUCCESSORIA tiedetään, että z:lla on oikea alipuu, jonka minimi on y – minimillä ei voi olla vasenta lasta Algoritmi näyttää monimutkaiselta, mutta rivin 4 TREE-SUCCESSORIA lukuunottamatta kaikki operaatiot ovat vakioaikaisia. ⇒ajoaika on siis O(h) ja lisämuistin tarve Θ(1) Siis kaikki dynaamisen joukon perusoperaatiot saadaan binäärihakupuulla toimimaan ajassa O(h) ja lisämuistilla Θ(1): SEARCH, INSERT, DELETE, MINIMUM, MAXIMUM, SUCCESSOR ja PREDECESSOR COMP.CS.300 Tietorakenteet ja algoritmit 1 260 Binäärihakupuita - kuten muitakin tietorakenteita - voi sovittaa uusiin tehtäviin lisäämällä uusia tehtävän kannalta olennaista tietoa sisältäviä kenttiä. • tällöin perusoperaatiot tulee muokata ylläpitämään myös uusien kenttien sisältöä • esimerkiksi lisäämällä solmuihin kenttä, joka kertoo solmun virittämän alipuun koon – saadaan toteutettua algoritmi, joka palauttaa puun korkeuteen nähden lineaarisessa ajassa i:nnen alkion – saadaan toteutettua algoritmi, joka puun korkeuteen nähden lineaarisessa ajassa kertoo, monesko kysytty alkio on suuruusjärjestyksessä – ilman ylimääräistä kenttää algoritmit täytyisi toteuttaa huomattavasti tehottomammin puun alkioiden määrään nähden lineaarisessa ajassa COMP.CS.300 Tietorakenteet ja algoritmit 1 261 12.2 Kuinka korkeita binäärihakupuut yleensä ovat? Kaikki dynaamisen joukon perusoperaatiot saatiin binäärihakupuulla toimimaan ajassa O(h). ⇒puun korkeus on tehokkuuden kannalta keskeinen tekijä. Jos oletetaan, että alkiot on syötetty satunnaisessa järjestyksessä, ja jokainen järjestys on yhtä todennäköinen, suoraan INSERTillä rakennetun binäärihakupuun korkeus on keskimäärin Θ(lg n). ⇒kaikki operaatiot keskimäärin Θ(lg n) COMP.CS.300 Tietorakenteet ja algoritmit 1 262 Valitettavasti lopputulos on surkeampi, jos avai- met syötetään (lähes) suuruusjärjestyksessä, ku- ten viereisestä kuvasta voi havaita. • korkeus on n -1, surkeaa! Ongelmaa ei pystytä ratkaisemaan järkevästi esimerkiksi satunnaistamalla, jos halutaan säilyt- tää kaikki dynaamisen joukon operaatiot. Puu pitää siis tasapainottaa. Siihen palataan myöhemmin. 7 6 5 1 4 3 2 COMP.CS.300 Tietorakenteet ja algoritmit 1 309 15 Tasapainotetut puurakenteet Binäärihakupuu toteuttaa kaikki dynaamisen joukon operaatiot O(h) ajassa Kääntöpuolena on, että puu voi joskus litistyä listaksi, jolloin tehokkuus menetetään (O(n)) Tässä luvussa käsitellään tapoja pitää huolta siitä, ettei litistymistä käy Ensin opitaan tasapainoitus puna-mustan puun invarianttia ylläpitämällä Lopuksi vilkaistaan muista tasapainotetuista binäärihakupuista Splay- ja AVL-puita COMP.CS.300 Tietorakenteet ja algoritmit 1 310 15.1 Puna-musta binäärihakupuu Puna-mustat puut ovat tasapainotettuja binäärihakupuita. Ne tekevät lisäysten ja poistojen yhteydessä tasapainotustoimenpiteitä, jotka takaavat, ettei haku ole koskaan tehoton vaikka alkiot olisikin lisätty puuhun epäsuotuisassa järjestyksessä. • puna-musta puu ei voi koskaan litistyä listaksi, kuten perusbinäärihakupuu Kuva 23: Punamustapuu (via Wikipedia, ©Colin M.L. Burnett (CC BY-SA 3.0)) COMP.CS.300 Tietorakenteet ja algoritmit 1 311 Puna-mustien puiden perusidea: • jokaisessa solmussa on yksi lisäbitti: väri (colour) – arvot punainen ja musta • muut kentät ovat vanhat tutut key, left, right ja p – jätämme oheisdatan näyttämättä, jotta pääideat eivät hukkuisi yksityiskohtien taakse • värikenttien avulla ylläpidetään puna-mustan puun invarianttia, joka takaa, että puun korkeus on aina kertaluokassa Θ(lg n) COMP.CS.300 Tietorakenteet ja algoritmit 1 312 Puna-mustien puiden invariantti: 1. Jos solmu on punainen, niin sillä joko • ei ole lapsia, tai • on kaksi lasta, ja ne molemmat ovat mustia. 2. Jokaiselle solmulle pätee: jokainen solmusta alas 1- tai 0-lapsiseen solmuun vievä polku sisältää saman määrän mustia solmuja. 3. Juuri on musta. Solmun x musta-korkeus (black-height) bh(x) on siitä alas 1- tai 0-lapsiseen solmuun vievällä polulla olevien mustien solmujen määrä. • invariantin osan 3 mukaisesti jokaisen solmun mustakorkeus on yksikäsitteinen • jokaisella vaihtoehtoisella polulla on sama määrä mustia solmuja • koko puun mustakorkeus on sen juuren mustakorkeus COMP.CS.300 Tietorakenteet ja algoritmit 1 313 Puna-mustan puun maksimikorkeus • merkitään korkeus = h ja solmujen määrä = n • kunkin juuresta lehteen vievän polun solmuista vähintään puolet (⌊h 2⌋+ 1) ovat mustia (invariantin osat 1 ja 3) • jokaisella juuresta lehteen vievällä polulla on saman verran mustia solmuja (invariantin osa 2) ⇒ainakin ⌊h 2⌋+ 1 ylintä tasoa täysiä ⇒n ≥2 h 2 ⇒h ≤2 lg n Invariantti siis todellakin takaa puun korkeuden pysymisen logaritmisena puun alkioiden määrään nähden. ⇒Dynaamisen joukon operaatiot SEARCH, MINIMUM, MAXIMUM, SUCCESSOR ja PREDECESSOR saadaan toimimaan puna-mustille puille ajassa O(lg n). • binäärihakupuulle operaatiot toimivat ajassa O(h), ja puna-musta puu on binäärihakupuu, jolle h = Θ(lg n) COMP.CS.300 Tietorakenteet ja algoritmit 1 314 Puna-mustien puiden ylläpitämiseen ei kuitenkaan voida käyttää samoja lisäys- ja poistoalgoritmeja kuin tavallisilla binäärihakupuilla, koska ne saattavat rikkoa invariantin. Niiden sijaan käytetään algoritmeja RB-INSERT ja RB-DELETE. • operaatiot RB-INSERT ja RB-DELETE perustuvat kiertoihin (rotation) • kiertoja on kaksi: vasemmalle ja oikealle • ne muuttavat puun rakennetta, mutta säilyttävät binäärihakupuiden perusominaisuuden kaikille solmuille x y C A B C B A x y x y • kierto vasemmalle – olettaa, että solmut x ja y ovat olemassa • kierto oikealle vastaavasti – left ja right vaihtaneet paikkaa COMP.CS.300 Tietorakenteet ja algoritmit 1 315 LEFT-ROTATE(T, x) 1 y := x→right; x→right := y→left 2 if y→left ̸= NIL then 3 y→left→p := x 4 y→p := x→p 5 if x→p = NIL then 6 T.root := y 7 else if x = x→p→left then 8 x→p→left := y 9 else 10 x→p→right := y 11 y→left := x; x→p := y • molempien kiertojen ajoaika on Θ(1) • ainoastaan osoittimia muutetaan COMP.CS.300 Tietorakenteet ja algoritmit 1 316 Lisäyksen perusidea • ensin uusi solmu lisätään kuten tavalliseen binäärihakupuuhun • sitten lisätty väritetään punaiseksi • mitä puna-mustien puiden perusominaisuuksia näin tehty lisäys voi rikkoa? COMP.CS.300 Tietorakenteet ja algoritmit 1 317 • Invariantin osa – 1 rikkoutuu lisätyn solmun osalta, jos sen isä on punainen; muuten se ei voi rikkoutua. – 2 ei rikkoudu, koska minkään solmun alla olevien mustien solmujen määrät ja sijainnit eivät muutu, ja lisätyn alla ei ole solmuja. – 3 rikkoutuu, jos puu oli alun perin tyhjä. COMP.CS.300 Tietorakenteet ja algoritmit 1 318 • korjataan puu seuraavasti: – ominaisuutta 2 pilaamatta siirretään 1:n rike ylöspäin kunnes se katoaa – lopuksi 3 korjataan värittämällä juuri mustaksi (ei voi pilata ominaisuuksia 1 ja 2) • 1:n rike = sekä solmu että sen isä ovat punaisia • siirto tapahtuu värittämällä solmuja ja tekemällä kiertoja COMP.CS.300 Tietorakenteet ja algoritmit 1 319 RB-INSERT(T, x) 1 TREE-INSERT(T, x) 2 x→colour := RED (suoritetaan silmukkaa kunnes rike on hävinnyt tai ollaan saavutettu juuri) 3 while x ̸= T.root and x→p→colour = RED do 4 if x→p = x→p→p→left then 5 y := x→p→p→right 6 if y ̸= NIL and y→colour = RED then (siirretään rikettä ylöspäin) 7 x→p→colour := BLACK 8 y→colour := BLACK 9 x→p→p→colour := RED 10 x := x→p→p 11 else (siirto ei onnistu →korjataan rike) 12 if x = x→p→right then 13 x := x→p; LEFT-ROTATE(T, x) 14 x→p→colour := BLACK 15 x→p→p→colour := RED 16 RIGHT-ROTATE(T, x→p→p) 17 else . . . ▷sama kuin rivit 5. . . 16 paitsi “left” ja “right” vaihtaneet paikkaa 30 T.root→colour := BLACK (väritetään juuri mustaksi) COMP.CS.300 Tietorakenteet ja algoritmit 1 320 x−>p−>p x−>p x y C B x A A C B Ominaisuuden 1 rikkeen siirto ylöspäin: • solmu x ja sen isä ovat molemmat punai- sia. • myös solmun x setä on punainen ja isoisä musta. ⇒rike siirretään ylöspäin värittämällä sekä x:n setä että isä mustiksi ja isoisä punaiseksi. Korjauksen jälkeen: • ominaisuus 1 saattaa olla edelleen rikki – solmu x ja sen isä saattavat molemmat olla punaisia • ominaisuus 2 ei rikkoudu – kaikkien polkujen mustien solmujen määrä pysyy samana • ominaisuus 3 saattaa rikkoutua – jos ollaan noustu juureen asti, se on saatettu värittää punaiseksi COMP.CS.300 Tietorakenteet ja algoritmit 1 321 x−>p−>p x−>p y x x x−>p x−>p−>p y C D B A D C A B Mikäli punaista setää ei ole olemassa, rikettä ei voi siirtää ylöspäin vaan se täytyy poistaa käyttäen monimutkaisem- paa menetelmää: • Varmistetaan ensin, että x on isänsä vasen lapsi tekemällä tarvittaessa kier- to vasemmalle. COMP.CS.300 Tietorakenteet ja algoritmit 1 322 y x x−>p−>p x−>p x D C B A D A B C • tämän jälkeen väritetään x:n isä mustaksi ja isoisä punaiseksi, ja suo- ritetaan kierto oikealle – isoisä on varmasti musta, koska muuten puussa olisi ollut kaksi pu- naista solmua päällekkäin jo en- nen lisäystä Korjauksen jälkeen: • puussa ei enää ole päällekkäisiä punaisia solmuja • korjausoperaatiot yhdessä eivät ri- ko 2. ominaisuutta ⇒puu on ehjä ja korjausalgoritmin suorittaminen voidaan lopettaa COMP.CS.300 Tietorakenteet ja algoritmit 1 323 Poistoalgoritmin yleispiirteet • ensin solmu poistetaan kuten tavallisesta binäärihakupuusta – w osoittaa poistettua solmua • jos w oli punainen tai puu tyhjeni kokonaan, puna-musta-ominaisuudet säilyvät voimassa ⇒ei tarvitse tehdä muuta • muussa tapauksessa korjataan puu RB-DELETE-FIXUPin avulla aloittaen w:n (mahdollisesta) lapsesta x ja sen isästä w→p – TREE-DELETE takaa, että w:llä oli enintään yksi lapsi RB-DELETE(T, z) 1 w := TREE-DELETE(T, z) 2 if w→colour = BLACK and T.root ̸= NIL then 3 if w→left ̸= NIL then 4 x := w→left 5 else 6 x := w→right 7 RB-DELETE-FIXUP(T, x, w→p) 8 return w COMP.CS.300 Tietorakenteet ja algoritmit 1 324 RB-DELETE-FIXUP(T, x, y) 1 while x ̸= T.root and (x = NIL or x→colour = BLACK) do 2 if x = y→left then 3 w := y→right 4 if w→colour = RED then 5 w→colour := BLACK; y→colour := RED 6 LEFT-ROTATE(T, y); w := y→right 7 if (w→left = NIL or w→left→colour = BLACK) and (w→right = NIL or w→right→colour = BLACK) then 8 w→colour := RED; x := y 9 else 10 if w→right = NIL or w→right→colour = BLACK then 11 w→left→colour := BLACK 12 w→colour := RED 13 RIGHT-ROTATE(T, w); w := y→right 14 w→colour := y→colour; y→colour := BLACK 15 w→right→colour := BLACK; LEFT-ROTATE(T, y) 16 x := T.root 17 else . . . ▷sama kuin rivit 3. . . 16 paitsi “left” ja “right” vaihtaneet paikkaa 32 y := y→p 33 x→colour := BLACK COMP.CS.300 Tietorakenteet ja algoritmit 1 325 15.2 AVL-puut ja Splay-puut AVL puu (Adelson-Velsky, Landis mukaan) on binäärihakupuu, jossa jokaisella solmulla on tasapainokerroin: 0, +1, tai -1, kun tasapainossa. • kerroin määräytyy solmun oikean ja vasemman alipuun korkeuksien erotuksesta. Kun uuden solmun lisäys tekee AVL-puusta epätasapainoisen, puu palautetaan tasapainoiseksi tekemällä rotaatioita. COMP.CS.300 Tietorakenteet ja algoritmit 1 326 Mahdollisia rotaatioita on neljä: • Oikealle • Vasemmalle • Kaksois-rotaatio vasen-oikea • Kaksois-rotaatio oikea-vasen 1 2 3 1 2 3 1 2 3 1 2 3 3 1 2 R 1 2 3 1 3 2 1 2 3 L LR RL COMP.CS.300 Tietorakenteet ja algoritmit 1 327 Splay puu on binäärihakupuu, jossa lisäominaisuutena viimeksi haetut alkiot ovat nopeita hakea uudelleen. Splay-operaatio suoritetaan solmulle haun yhteydessä. Tämä ns. splay-askelien sekvenssi siirtää solmun askel askeleelta lähemmäksi juurta ja lopulta juureksi. • Zig-askel: COMP.CS.300 Tietorakenteet ja algoritmit 1 328 • Zig-Zig-askel: • Zig-Zag-askel: COMP.CS.300 Tietorakenteet ja algoritmit 1 263 13 Graaﬁt Seuraavaksi tutustutaan tietorakenteeseen, jonka muodostavat pisteet ja niiden välille muodostetut yhteydet – graaﬁin. Keskitymme myös tyypillisimpiin tapoihin etsiä tietoa graaﬁsta eli graaﬁalgoritmeihin. Kuva: Flickr, Rachel D COMP.CS.300 Tietorakenteet ja algoritmit 1 264 13.1 Graaﬁen esittäminen tietokoneessa Graaﬁon ohjelmistotekniikassa keskeinen rakenne, joka koostuu solmuista (vertex, node) ja niitä yhdistävistä kaarista (edge, arc). Graaﬁvoi olla suuntaamaton (undirected) tai suunnattu (directed). COMP.CS.300 Tietorakenteet ja algoritmit 1 265 Kuva 18: Kuva: Getty Images • kaaviokuvat voidaan usein ajatella graafeiksi • asioiden välisiä suhteita voi usein esittää graafeina • monet tehtävät voidaan palauttaa graaﬁtehtäviksi COMP.CS.300 Tietorakenteet ja algoritmit 1 266 Kuva 19: Kuva: Wikimedia Commons, User:Jpatokal COMP.CS.300 Tietorakenteet ja algoritmit 1 267 Kuva 20: Kuva: Flickr, yaph,http://exploringdata.github.io/vis/footballers-search-relations/ COMP.CS.300 Tietorakenteet ja algoritmit 1 268 Kuva 21: Kuva: Flickr, Andy Lamb COMP.CS.300 Tietorakenteet ja algoritmit 1 269 Kuva 22: Kuva: Tokyo Metro COMP.CS.300 Tietorakenteet ja algoritmit 1 270 Matematiikassa graaﬁG ilmoitetaan usein parina G = (V, E). • V = solmujen joukko • E = kaarien joukko • tällöin samojen solmujen välillä saa yleensä olla vain yksi kaari molempiin suuntiin – aina tämä ei kuitenkaan käytännön sovelluksessa riitä – esimerkiksi juna-aikatauluja esittävässä graaﬁssa kaupunkien välillä on yleensä useampia vuoroja – tällaista graaﬁa kutsutaan monigraaﬁksi (multigraph) Kuva: HSL COMP.CS.300 Tietorakenteet ja algoritmit 1 271 • jos solmujen välillä sallitaan vain yksi kaari suuntaansa ⇒ E ⊆V 2 – suunnatulla graaﬁlla |E| voi vaihdella välillä 0, . . . , |V |2 – laskettaessa graaﬁalgoritmien suoritusaikoja oletamme tämän Graaﬁalgoritmin suorituskyky ilmoitetaan yleensä sekä |V |:n että |E|:n funktiona – helppouden vuoksi jätämme itseisarvomerkit pois kertaluokkamerkintöjen sisällä ts. O(V E) = O(|V | · |E|) COMP.CS.300 Tietorakenteet ja algoritmit 1 272 Graaﬁn esittämiseen tietokoneessa on kaksi perusmenetelmää kytkentälista (adjacency list) ja kytkentämatriisi (adjacency matrix). Näistä kahdesta kytkentälistaesitys on yleisempi, ja tällä kurssilla keskitytään siihen. • Solmut on talletettu johonkin tietorakenteeseen (valittu tietorakenne riippuu siitä, mitä oheisdataa solmut sisältävät, miten niitä pitää pystyä hakemaan, lisäämään jne.) • Yksinkertaisimmillaan jokaisessa solmussa on tietorakenne, jossa on tallessa mihin solmuihin tästä solmusta on kaari. – Valittu tietorakenne riippuu siitä, paljonko kaaria arvellaan olevan, lisätäänkö/poistetaanko niitä jatkuvasti, täytyykö tiettyä kaarta pystyä hakemaan nopeasti jne.) – Tieto kohdesolmusta voidaan tallettaa osoittimena, solmun indeksinä (jos solmut indeksoitavassa tietorakenteessa) tms. – Solmujen järjestyksellä kytkentälistassa ei yleensä ole väliä COMP.CS.300 Tietorakenteet ja algoritmit 1 273 • Graaﬁn kytkentälistojen yh- teiskoko on – |E|, jos graaﬁon suunnat- tu, 2·|E|, jos graaﬁon suun- taamaton ⇒kytkentälistaesityksen muistin kulutus on O(max(V, E)) = O(V + E) • Tiedon “onko kaarta solmusta v solmuun u” haku edellyttää yhden kytkentälistan läpi selaamista mikä vie hitaimmillaan aikaa Θ(V ) (ellei solmusta lähteviä kaaria talleteta johonkin tietorakenteeseen, josta niistä pystytään nopeasti hakemaan kohdekaaren perusteella) • Jos kaari on painotettu tai siihen liittyy oheisdataa, täytyy kaaresta tallettaa kohdesolmun lisäksi myös oheisdata (esim. struct, jossa kohdesolmu ja oheisdata) • Joskus on tarpeen tallettaa myös tieto solmuun tulevista kaarista samaan tapaan (esim. solmujen ja kaarien poistamisen helpottamiseksi) COMP.CS.300 Tietorakenteet ja algoritmit 1 274 Kytkentämatriisiesityksen avulla edelliseen kysymykseen pystytään vastaamaan helposti. • kytkentämatriisi on |V | × |V | -matriisi A, jonka alkio aij on – 0, jos solmusta i ei ole kaarta solmuun j – 1, jos solmusta i on kaari solmuun j • edellisen esimerkin graaﬁen kytkentämatriisit ovat 1 2 3 4 5 1 2 3 4 5 1 0 1 0 1 1 1 0 1 0 1 0 2 1 0 0 1 0 2 0 1 0 0 0 3 0 0 0 0 0 3 0 0 0 0 0 4 1 1 0 0 1 4 0 1 0 0 1 5 1 0 0 1 0 5 1 0 0 1 0 COMP.CS.300 Tietorakenteet ja algoritmit 1 275 • muistin kulutus on aina Θ(V 2) – jokainen alkio tarvitsee vain bitin muistia, joten useita alkioita voidaan tallettaa yhteen sanaan ⇒vakiokerroin saadaan aika pieneksi • kytkentämatriisiesitystä kannattaa käyttää lähinnä vain hyvin tiheiden graaﬁen yhteydessä. COMP.CS.300 Tietorakenteet ja algoritmit 1 276 Tarkastellaan tarkemmin kytkentälistaesitystavan toteutusta: • käytännön sovelluksissa solmuun kertyy usein monenlaista tehtävän tai algoritmin vaatimaa tietoa – nimi – bitti, joka kertoo, onko solmussa käyty – osoitin, joka kertoo, mistä solmusta tähän solmuun viimeksi tultiin – . . . ⇒solmusta kannattaa tehdä itsenäinen alkio, jossa on tarpeelliset kentät • yleensä sama pätee myös kaariin COMP.CS.300 Tietorakenteet ja algoritmit 1 277 • pääperiaate: – talleta jokainen asia yhteen kertaan – ota käyttöön osoittimet, joilla voit helposti kulkea haluamiisi suuntiin COMP.CS.300 Tietorakenteet ja algoritmit 1 278 13.2 Yleistä graaﬁalgoritmeista Käsitteitä: • askel = siirtyminen solmusta toiseen yhtä kaarta pitkin – suunnatussa graaﬁssa askel on otettava kaaren suuntaan • solmun v2 etäisyys (distance) solmusta v1 on lyhimmän v1:stä v2:een vievän polun pituus. – jokaisen solmun etäisyys itsestään on 0 – merkitään δ(v1, v2) – suunnatussa graaﬁssa on mahdollista (ja tavallista), että δ(v1, v2) ̸= δ(v2, v1) – jos v1:stä ei ole polkua v2:een, niin δ(v1, v2) = ∞ COMP.CS.300 Tietorakenteet ja algoritmit 1 279 Algoritmien ymmärtämisen helpottamiseksi annamme usein solmuille värit. • valkoinen = solmua ei ole vielä löydetty • harmaa = solmu on löydetty, mutta ei loppuun käsitelty • musta = solmu on löydetty ja loppuun käsitelty • solmun väri muuttuu järjestyksessä valkoinen →harmaa → musta • värikoodaus on lähinnä ajattelun apuväline, eikä sitä välttämättä tarvitse toteuttaa täysin, yleensä riittää tietää, onko solmu löydetty vai ei. – usein tämäkin informaatio on pääteltävissä nopeasti muista kentistä COMP.CS.300 Tietorakenteet ja algoritmit 1 280 Monet graaﬁalgoritmit perustuvat graaﬁn tai sen osan läpikäyntiin tietyssä järjestyksessä. • perusläpikäyntijärjestyksiä on kaksi, leveyteen ensin -haku (BFS) ja syvyyteen ensin -haku (DFS). • läpikäynnillä tarkoitetaan algoritmia, jossa – käydään kerran graaﬁn tai sen osan jokaisessa solmussa – kuljetaan kerran graaﬁn tai sen osan jokainen kaari Hakualgoritmit käyttävät lähtökohtana jotain annettua graaﬁn solmua, lähtösolmua (source) ja etsivät kaikki ne solmut, joihin pääsee lähtösolmusta nollalla tai useammalla askeleella. COMP.CS.300 Tietorakenteet ja algoritmit 1 281 13.3 Leveyteen ensin -haku (breadth-ﬁrst) Leveyteen ensin -hakua voi käyttää esimerkiksi • kaikkien solmujen etäisyyden määrittämiseen lähtösolmusta • (yhden) lyhimmän polun löytämiseen lähtösolmusta jokaiseen solmuun Leveyteen ensin -haun nimi tulee siitä, että se tutkii tutkitun ja tuntemattoman graaﬁn osan välistä rajapintaa koko ajan koko sen leveydeltä. Solmujen kentät: • v→d = jos solmu v on löydetty niin sen etäisyys s:tä, muutoin ∞ • v→π = osoitin solmuun, josta haku ensi kerran tuli v:hen, löytämättömille solmuille NIL • v→colour = solmun v väri • v→Adj = solmun v naapurisolmujen joukko COMP.CS.300 Tietorakenteet ja algoritmit 1 282 Algoritmin käyttämä tietorakenne Q on jono (noudattaa FIFO-jonokuria). BFS(s) (algoritmi saa parametrinaan aloitussolmun s) 1 ▷alussa kaikkien solmujen kentät ovat arvoiltaan colour = WHITE, d = ∞, π = NIL 2 s→colour := GRAY (merkitään alkutila löydetyksi) 3 s→d := 0 (etäisyys alkutilasta alkutilaan on 0) 4 PUSH(Q, s) (työnnetään alkutila jonoon) 5 while Q ̸= ∅do (toistetaan niin kauan kun tiloja riittää) 6 u := POP(Q) (vedetään jonosta seuraava tila) 7 for each v ∈u→Adj do (käydään u:n naapurit läpi) 8 if v→colour = WHITE then (jos solmua ei ole vielä löydetty . . . ) 9 v→colour := GRAY (. . . merkitään se löydetyksi) 10 v→d := u→d + 1 (kasvatetaan etäisyyttä yhdellä) 11 v→π := u (tilaan v tultiin tilan u kautta) 12 PUSH(Q, v) (työnnetään tila jonoon odottamaan käsittelyä) 13 u→colour := BLACK (merkitään tila u käsitellyksi) Kaikkia algoritmin käyttämiä solmun kenttiä ei välttämättä käytännön toteutuksessa tarvita, vaan osan arvoista voi päätellä toisistaan. COMP.CS.300 Tietorakenteet ja algoritmit 1 283 Alla olevassa kuvassa graaﬁn solmut on numeroitu siinä järjestyksessä, jossa BFS löytää ne. Solmujen etäisyys alkutilasta on merkitty solmun viereen ja haun kulkureitti tummennettu. 8 1 1 alkutila 1 2 9 3 2 2 2 5 4 6 7 3 3 4 COMP.CS.300 Tietorakenteet ja algoritmit 1 284 Suoritusaika solmujen (V ) ja kaarien (E) määrien avulla ilmaistuna: • ennen algoritmin kutsumista solmut pitää alustaa – järkevässä ratkaisussa tämä on tehtävissä ajassa O(V ) • rivillä 7 algoritmi selaa solmun lähtökaaret – onnistuu käyttämällämme kytkentälistaesityksellä solmun kaarien määrään nähden lineaarisessa ajassa • kukin jono-operaatio vie vakiomäärän aikaa • while-silmukan kierrosten määrä – vain valkoisia solmuja laitetaan jonoon – samalla solmun väri muuttuu harmaaksi ⇒kukin solmu voi mennä jonoon korkeintaan kerran ⇒while-silmukka pyörähtää siis korkeintaan O(V ) määrän kertoja COMP.CS.300 Tietorakenteet ja algoritmit 1 285 • for-silmukan kierrosten määrä – algoritmi kulkee jokaisen kaaren korkeintaan kerran molempiin suuntiin ⇒for-silmukka käydään läpi yhteensä korkeintaan O(E) kertaa ⇒koko algoritmin suoritusaika on siis O(V + E) COMP.CS.300 Tietorakenteet ja algoritmit 1 286 Algoritmin lopetettua π-osoittimet määrittelevät puun, joka sisältää löydetyt solmut, ja jonka juurena on lähtösolmu s. • leveyteen ensin -puu (breadth-ﬁrst tree) • π-osoittimet määräävät puun kaaret “takaperin” – osoittavat juurta kohti – v→π = v:n edeltäjä (predecessor) eli isä (parent) • kaikki lähtösolmusta saavutettavissa olevat solmut kuuluvat puuhun • puun polut ovat mahdollisimman lyhyitä polkuja s:stä löydettyihin solmuihin COMP.CS.300 Tietorakenteet ja algoritmit 1 287 Lyhimmän polun tulostaminen • kun BFS on asettanut π-osoittimet kohdalleen, lyhin polku lähtösolmusta s solmuun v voidaan tulostaa seuraavasti: PRINT-PATH(G, s, v) 1 if v = s then (rekursion pohjatapaus) 2 print s 3 else if v→π = NIL then (haku ei ole saavuttanut solmua v lainkaan) 4 print “ei polkua” 5 else 6 PRINT-PATH(G, s, v→π) (rekursiokutsu . . . ) 7 print v (. . . jonka jälkeen suoritetaan tulostus) • ei-rekursiivisen version voi tehdä esim. – kokoamalla solmujen numerot taulukkoon kulkemalla π-osoittimia pitkin, ja tulostamalla taulukon sisällön takaperin – kulkemalla polku kahdesti, ja kääntämällä π-osoittimet takaperin kummallakin kertaa (jälkimmäinen käännös ei tarpeen, jos π-osoittimet saa turmella) COMP.CS.300 Tietorakenteet ja algoritmit 1 288 13.4 Syvyyteen ensin -haku (depth-ﬁrst) Syvyyteen ensin -haku on toinen perusläpikäyntijärjestyksistä. Siinä missä leveyteen ensin -haku tutkii koko hakurintamaa sen koko leveydeltä, syvyyteen ensin -haku menee yhtä polkua eteen päin niin kauan kuin se on mahdollista. • polkuun hyväksytään vain solmuja, joita ei ole aiemmin nähty • kun algoritmi ei enää pääse eteenpäin, se peruuttaa juuri sen verran kuin on tarpeen uuden etenemisreitin löytämiseksi, ja lähtee sitä pitkin • algoritmi lopettaa, kun se peruuttaa viimeisen kerran takaisin lähtösolmuun, eikä löydä enää sieltäkään tutkimattomia kaaria Algoritmi muistuttaa huomattavasti leveyteen ensin -haun pseudokoodia. COMP.CS.300 Tietorakenteet ja algoritmit 1 289 Merkittäviä eroja on oikeastaan vain muutama: • jonon sijasta käsittelyvuoroaan odottavat tilat talletetaan pinoon • algoritmi ei löydä lyhimpiä polkuja, vaan ainoastaan jonkin polun – tästä syystä esimerkkipseudokoodia on yksinkertaistettu jättämällä π-kentät pois Algoritmin käyttämä tietorakenne S on pino (noudattaa LIFO-jonokuria). COMP.CS.300 Tietorakenteet ja algoritmit 1 290 DFS(s) (algoritmi saa parametrinaan aloitussolmun s) 1 ▷alussa kaikkien (käsittelemättömien) solmujen värikenttä colour = WHITE 2 PUSH(S, s) (työnnetään alkutila pinoon) 3 while S ̸= ∅do (jatketaan niin kauan kun pinossa on tavaraa) 4 u := POP(S) (vedetään pinosta viimeisin sinne lisätty tila) 5 if u→colour = WHITE then (jos solmua ei ole vielä käsitelty . . . ) 6 u→colour := GRAY (merkitään tila käsittelyssä olevaksi) 7 PUSH(S, u) (työnnetään taas pinoon (mustaksi värjäys)) 8 for each v ∈u→Adj do (käydään u:n naapurit läpi) 9 if v→colour = WHITE then (jos solmua ei ole vielä käsitelty . . . ) 10 PUSH(S, v) (. . . työnnetään se pinoon odottamaan käsittelyä) 11 else if v→colour = GRAY then (harmaa solmu! Sykli löytynyt! . . . ) 12 ???? (käsittele sykli, jos se kiinnostaa) 13 else 14 u→colour := BLACK (kaikki lapset käsitelty, solmu on valmis) Jos halutaan tutkia koko graaﬁ, voidaan kutsua syvyyteen ensin -hakua kertaalleen kaikista vielä tutkimattomista solmuista. • tällöin solmuja ei väritetä valkoisiksi kutsukertojen välillä COMP.CS.300 Tietorakenteet ja algoritmit 1 291 Rivin 5 perään voitaisi lisätä operaatio, joka kaikille graaﬁn alkioille halutaan tehdä. Voidaan esimerkiksi • tutkia onko tila maalitila, ja lopettaa jos on • ottaa talteen solmuun liittyvää oheisdataa • muokata solmuun liittyvää oheisdataa COMP.CS.300 Tietorakenteet ja algoritmit 1 292 Suoritusaika voidaan laskea samoin kuin leveyteen ensin -haun yhteydessä: • ennen algoritmin kutsumista solmut pitää alustaa – järkevässä ratkaisussa tämä on tehtävissä ajassa O(V ) • rivillä 6 algoritmi selaa solmun lähtökaaret – onnistuu käyttämällämme kytkentälistaesityksellä solmun kaarien määrään nähden lineaarisessa ajassa • kukin pino-operaatio vie vakiomäärän aikaa • while-silmukan kierrosten määrä – vain valkoisia solmuja laitetaan pinoon – samalla solmun väri muuttuu harmaaksi ⇒kukin solmu voi mennä pinoon korkeintaan kerran ⇒while-silmukka pyörähtää siis korkeintaan O(V ) määrän kertoja COMP.CS.300 Tietorakenteet ja algoritmit 1 293 • for-silmukan kierrosten määrä – algoritmi kulkee jokaisen kaaren korkeintaan kerran molempiin suuntiin ⇒for-silmukka käydään läpi yhteensä korkeintaan O(E) kertaa ⇒koko algoritmin suoritusaika on siis O(V + E) COMP.CS.300 Tietorakenteet ja algoritmit 1 294 DFS on myös mahdollista toteuttaa rekursiivisesti, jolloin algoritmin pinona toimii funktioiden kutsupino. • Rekursiivinen versio on itse asiassa jonkin verran yksinkertaisempi kuin iteratiivinen versio! • (Keksitkö, miksi?) Huom! Ennen algoritmin kutsumista kaikki solmut tulee alustaa valkoisiksi! DFS(u) 1 u→colour := GRAY (merkitään tila löydetyksi) 2 for each v ∈u→Adj do (käydään kaikki u:n naapurit läpi) 3 if v→colour = WHITE then (jos ei olla vielä käyty v:ssä. . . ) 4 DFS(v) (. . . jatketaan etsintää rekursiivisesti tilasta v) 5 else if v→colour = GRAY then (jos on jo käyty, muttei loppuun käsitelty . . . ) 6 ▷silmukka on löytynyt (. . . silmukka on löytynyt) 7 u→colour := BLACK (merkitään tila käsitellyksi) COMP.CS.300 Tietorakenteet ja algoritmit 1 295 Suoritusaika: • rekursiivinen kutsu tehdään ainoastaan valkoisille solmuille • funktion alussa solmu väritetään harmaaksi ⇒DFS:ää kutsutaan korkeintaan O(V ) kertaa • kuten aiemmassakin versiossa for-silmukka kiertää korkeintaan kaksi kierrosta kutakin graaﬁn kaarta kohden koko algoritmin suorituksen aikana ⇒siis for-silmukan kierroksia tulee korkeintaan O(E) kappaletta • muut operaatioista ovat vakioaikaisia ⇒koko algoritmin suoritusaika on edelleen O(V + E) COMP.CS.300 Tietorakenteet ja algoritmit 1 296 Leveyteen ensin -haku vai syvyyteen ensin -haku: • lyhimmän polun etsimiseen täytyy käyttää leveyteen ensin -hakua • jos graaﬁn esittämä tilavaruus on hyvin suuri, käyttää leveyteen ensin -haku yleensä huomattavasti enemmän muistia – syvyyteen ensin -haun pinon koko pysyy yleensä pienempänä kuin leveyteen ensin -haun jonon koko – useissa sovelluksissa esimerkiksi tekoälyn alalla jonon koko estää leveyteen ensin -haun käytön • mikäli graaﬁn koko on ääretön, ongelmaksi nousee se, ettei syvyyteen ensin -haku välttämättä löydä ikinä maalitilaa, eikä edes lopeta ennen kuin muisti loppuu – näin tapahtuu, jos algoritmi lähtee tutkimaan hedelmätöntä äärettömän pitkää haaraa – tätä ongelmaa ei kuitenkaan esiinny äärellisten graaﬁen yhteydessä COMP.CS.300 Tietorakenteet ja algoritmit 1 297 • syvyyteen ensin -haun avulla voi ratkaista joitakin monimutkaisempia ongelmia, kuten graaﬁn silmukoiden etsintä – harmaat solmut muodostavat lähtösolmusta nykyiseen solmuun vievän polun – mustista solmuista pääsee vain mustiin ja harmaisiin solmuihin ⇒jos nykyisestä solmusta pääsee harmaaseen solmuun, niin graaﬁssa on silmukka COMP.CS.300 Tietorakenteet ja algoritmit 1 298 14 Lyhimmät painotetut polut BFS löytää lyhimmän polun lähtösolmusta graaﬁn saavutettaviin solmuihin. Se ei kuitenkaan enää suoriudu tehtävästä, jos kaarien läpi kulkeminen maksaa askelta enemmän. Tässä luvussa käsitellemme lyhimpien painotettujen polkujen etsintää graaﬁsta, jonka kaaripainot ovat positiivisia ja voivat poiketa ykkösestä. • negatiivisten kaaripainojen hallitsemiseen tarvitaan monimutkaisempia algoritmeja, esimerkiksi Bellman-Ford algoritmi COMP.CS.300 Tietorakenteet ja algoritmit 1 299 14.1 Lyhin polku Graaﬁn kaarilla voi olla ominaisuus nimeltä paino(weight). • paino voi edustaa vaikkapa reitin pituutta tai tilasiirtymän kustannusta • Graaﬁn G = (V, E) painofunktio w : E →R kaarilta reaalilukupainoille • Polun p = ⟨v0, v1, ..., vk⟩paino w(p) on sen muodostavien kaarten painojen summa w(p) = Pk i=1 w(vi−1, vi). Määritelmä: lyhimmän polun paino δ(u, v): δ(u, v) = ( min{w(p) : u p⇝v} jos on polkuu:sta v:hen, ∞ muuten. ja siten lyhin polku u:sta v:hen mikä tahansa polku p, jolle w(p) = δ(u, v) COMP.CS.300 Tietorakenteet ja algoritmit 1 300 Tämä mutkistaa lyhimmän reitin etsintää merkittävästi. • lyhin reitti on se lähtösolmusta etsittyyn solmuun kulkeva polku, jonka kaarien painojen summa on mahdollisimman pieni • jos jokaisen kaaren paino on 1, tehtävä voidaan ratkaista käymällä lähtösolmusta saavutettavissa oleva graaﬁn osa läpi leveyteen ensin -järjestyksessä • jos painot saattavat olla < 0, voi olla, että tehtävään ei ole ratkaisua, vaikka polkuja olisi olemassakin – jos graaﬁssa on silmukka, jonka kaaripainojen summa on negatiivinen saadaan mielivaltaisen pieni painojen summa kiertämällä silmukkaa tarpeeksi monta kertaa COMP.CS.300 Tietorakenteet ja algoritmit 1 301 14.2 Dijkstran algoritmi Suunnatun, painotetun graaﬁn G = (V, E), jossa kaaripainot ovat ei-negatiivisia, lyhimmät painotetut polut lähtösolmusta voi etsiä Dijkstran algoritmilla. • etsii lyhimmät polut lähtösolmusta s kaikkiin saavutettaviin solmuihin, painottaen kaarien pituuksia w:n mukaan • valitsee joka tilanteessa tutkittavakseen lyhimmän polun, jota se ei ole vielä tutkinut ⇒se on siis ahne algoritmi • oletus: w(u, v) ≥0 ∀(u, v) ∈E COMP.CS.300 Tietorakenteet ja algoritmit 1 302 DIJKSTRA(s, w) (algoritmi saa parametrinaan aloitussolmun s) 1 ▷alussa kaikkien solmujen kentät ovat arvoiltaan colour = WHITE, d = ∞, π = NIL 2 s→colour := GRAY (merkitään alkutila löydetyksi) 3 s→d := 0 (etäisyys alkutilasta alkutilaan on 0) 4 PUSH(Q, s) (työnnetään alkutila prioriteettijonoon) 5 while Q ̸= ∅do (jatketaan niin kauan kun solmuja riittää) 6 u := EXTRACT-MIN(Q) (otetaan prioriteettijonosta seuraava tila) 7 for each v ∈u→Adj do (käydään u:n naapurit läpi) 8 if v→colour = WHITE then (jos solmussa ei ole käyty . . . ) 9 v→colour := GRAY (. . . merkitään se löydetyksi) 10 PUSH(Q, v) (työnnetään tila jonoon odottamaan käsittelyä) 11 RELAX(u, v, w) 12 u→colour := BLACK (merkitään tila u käsitellyksi) RELAX(u, v, w) 1 if v→d > u→d + w(u, v) then (jos löydettiin uusi lyhyempi reitti tilaan v...) 2 v→d := u→d + w(u, v) (...pienennetään v:n etäisyyttä lähtösolmusta) 3 v→π := u (merkitään, että v:n tultiin u:sta) COMP.CS.300 Tietorakenteet ja algoritmit 1 303 Algoritmin käyttämä tietorakenne Q on prioriteettijono (luentomonisteen kohta 3.2). w sisältää kaikkien kaarien painot. Dijkstran algoritmi käyttää apufunktiota RELAX • kaaren (u, v) relaksointi (relaxation) testaa, voiko lyhintä löydettyä v:hen vievää polkua parantaa reitittämällä sen loppupää u:n kautta, ja tarvittaessa tekee niin Muilta osin algoritmi muistuttaa huomattavasti leveyteen ensin -hakua. • se löytää lyhimmät polut kasvavan pituuden mukaisessa järjestyksessä • kun solmu u otetaan Q:sta, sen painotettu etäisyys s:stä varmistuu u→d:ksi – jos prioriteettijonosta otettu tila on maalitila, voidaan algoritmin suoritus lopettaa saman tien COMP.CS.300 Tietorakenteet ja algoritmit 1 304 Alla olevassa kuvassa nähdään Dijkstran algoritmi tilanteessa, jossa mustalla ympyröidyt solmut on käsitelty. Alkutila 5 1 3 2 4 3 7 3 8 0 1 4 i i i 2 COMP.CS.300 Tietorakenteet ja algoritmit 1 305 Suoritusaika: • while-silmukka käy enintään O(V ) kierrosta ja for-silmukka yhteensä enintään O(E) kierrosta • prioriteettijonon voi toteuttaa tehokkaasti keon avulla tai vähemmän tehokkaasti listan avulla kekototeutuksella:listatoteutuksella: rivi 4: Θ(1) Θ(1) (tyhjään tietorakenteeseen lisääminen) rivi 5: Θ(1) Θ(1) (onko prioriteettijono tyhjä) rivi 6: O(lg V ) O(V ) (Extract-Min) rivi 10: Θ(1) Θ(1) (valkoisen solmun prioriteetti on ääretön, joten sen oikea paikka on keon lopussa) rivi 11: O(lg V ) Θ(1) (relaksoinnissa solmun prioriteetti voi muuttua) • käytettäessä kekototeutusta jokaisella while- ja for-silmukan kierroksella suoritetaan yksi O(lg V ) aikaa kuluttava operaatio ⇒algoritmin suoritusaika on O((V + E) lg V ) COMP.CS.300 Tietorakenteet ja algoritmit 1 306 14.3 A*-algoritmi Dijkstran algoritmi etsi lyhimmän painotetun reitin kartoittamalla solmuja lyhimmästä reitistä alkaen reitin pituusjärjestyksessä. Eli: Dijkstra käyttää hyväkseen vain jo kuljetuista kaarista saatavaa tietoa. A*-algoritmi tehostaa tätä lisäämällä heuristiikan (=oletuksen) lyhimmästä mahdollisesta etäisyydestä maaliin. (Esim. maantiereitin haussa etäisyys linnuntietä). • etsii lyhimmän painotetun polun lähtösolmusta s annettuun maalisolmuun g. Ei kartoita lyhintä reittiä kaikkiin solmuihin (kuten Dijkstra), vain maalisolmuun. • edellyttää, että painot ovat ei-negatiivisia (kuten Dijkstrakin) • edellyttää, että jokaiselle solmulle voidaan laskea sen minimietäisyys maalista (ts. löytynyt lyhin reitti ei voi olla lyhempi). • valitsee joka tilanteessa tutkittavakseen ei-tutkitun solmun, jossa (lyhin etäisyys lähdöstä solmuun + arvioitu minimietäisyys maaliin) on pienin. COMP.CS.300 Tietorakenteet ja algoritmit 1 307 Ainoa ero A*:n ja Dijkstran välillä on relaksointi (ja se, että A* kannattaa lopettaa heti maalisolmun löydyttyä, koska se ei kartoita lyhimpiä etäisyyksiä kaikkiin solmuihin). RELAX-A*(u, v, w) 1 if v→d > u→d + w(u, v) then (jos löydettiin uusi lyhyempi reitti tilaan v...) 2 v→d := u→d + w(u, v) (...uusi pituus tähän saakka...) 3 v→de := v→d + min_est(v, g) (...ja minimiarvio koko reitistä) 4 v→π := u (merkitään, että v:n tultiin u:sta) A*:n käyttämässä prioriteettijonossa käytetään prioriteettina koko reitin pituusarviota v→de. (Dijkstran algoritmi on A*:n erikoistapaus, jossa min_est(a, b) on aina 0.) COMP.CS.300 Tietorakenteet ja algoritmit 1 308 14.4 Kevyin virittävä puu Graaﬁn G = (V, E) kevyin virittävä puu on sen asyklinen aligraaﬁ, joka yhdistää kaikki graaﬁn solmut niin, että aligraaﬁn kaarien painojen summa on pienin mahdollinen. Puun löytämiseksi on kaksi algoritmia: Primin ja Kruskalin Prim muistuttaa Dijkstran algoritmin kun taas Kruskal lähestyy ongelmaa luomalla metsän, jossa on puu jokaiselle puulle ja sitten yhdistämällä näitä puuksi"
            },
            {
                "role": "user",
                "content": "Can you summarize the key topics covered in this lecture?"
            },
            {
                "role": "assistant",
                "content": "This lecture covers foundational algorithmic concepts including problem formulation, correctness proof using induction, efficiency analysis using asymptotic notation, data structures (arrays, linked lists, BSTs, heaps), and sorting methods (selection sort, merge sort, counting sort). Additionally, it introduces the Word-RAM computational model, providing a theoretical framework for measuring algorithm efficiency."
            }
        ]
    },
    {
        "system": "Hello! I am Kiran, an AI assistant from Tampere University, dedicated to providing you with accurate and insightful assistance. My goal is to help you understand concepts, summarize lectures, and answer questions effectively.",
        "conversations": [
            {
                "role": "host",
                "content": "TIE-20106 1 TIE-20106 DATA STRUCTURES AND ALGORITHMS TIE-20106 2 Bibliography These lecture notes are based on the notes for the course OHJ-2016 Utilization of Data Structures. All editorial work is done by Terhi Kilamo and the content is based on the work of Professor Valmari and lecturer Minna Ruuska. Most algorithms are originally from the book Introduction to Algorithms; Thomas H. Cormen, Charles E. Leiserson, Ronald L. Rivest, Clifford Stein. In addition the following books have been used when completing this material: • Introduction to The Design & Analysis of Algorithms; Anany Levitin • Olioiden ohjelmointi C++:lla; Matti Rintala, Jyke Jokinen • Tietorakenteet ja Algoritmit; Ilkka Kokkarinen, Kirsti Ala-Mutka TIE-20106 3 1 Introduction Let’s talk first about the motivation for studying data structures and algorithms Algorithms in the world TIE-20106 4 1.1 Why? What are the three most important algorithms that affect YOUR daily life? Picture: Chris Watt TIE-20106 5 There are no computer programs without algorithms • algorithms make for example the following applications possible: TIE-20106 6 TIE-20106 7 algorithms are at work whenever a computer is used TIE-20106 8 Data structures are needed to store and acces the data handled in the programs easily • there are several different types of data structures and not all of them are suitable for all tasks ⇒it is the programmer’s job to know which to choose ⇒the behaviour, strengths and weaknesses of the alternatives must be known Modern programming languages provide easy to use library implementations for data structures (C++ standard library, JCF). Understanding the properties of these and the limitations there may be for using them requires theoretical knowledge on basic data structures. TIE-20106 9 Ever gotten frustrated on a program running slowly? • functionality is naturally a top priority but effiency and thus the usability and user experience are not meaningless side remarks • it is important to take memory- and time consumption into account when making decisions in program implementation • using a library implementation seems more straightforward than is really is This course discusses these issues TIE-20106 10 2 Terminology and conventions This chapter covers the terminology and the syntax of the algorithms used on the course. The differences between algorithms represented in pseudocode and the actual solution in a programming language is discussed. The sorting algorithm INSERTION-SORT is used as an example. TIE-20106 11 2.1 Goals of the course As discussed earlier, the main goal of the course is to provide a sufficient knowledge on and the basic tools for choosing the most suitable solution to a given programming problem. The course also aims to give the student the ability to evaluate the decisions made during the design process on a basic level. The data structures and algorithms commonly used in programming are covered. • The course concentrates on choosing a suitable data structure for solving a given problem. • In addition, common types of problems and the algorithms to solve them are covered. TIE-20106 12 • The course concentrates on the so called “good algo- rithms” shown in the picture on the right. • The emphasis is on the time the algorithm uses to process the data as the size of the in- put gets larger. Less attention is paid to optimization details. SIZE OF INPUT TIME 1st solution Hacker optimized Good algorithm Hacker optimized good algorithm TIE-20106 13 2.2 Terminology A data structure is a collection of related data items stored in a segment of the computer’s memory. • data can be added and searched by using suitable algorithms. • there can be several different levels in a data structure: a data structure can consist of other data structures. An algorithm is a well defined set of instructions that takes in a set of input and produces a set of output, i.e. it gives a solution to a given problem. TIE-20106 14 • well defined = – each step is detailed enough for the reader (human or machine) to execute – each step in unambiguous – the same requirements apply to the execution order of the steps – the execution is finite, i.e. it ends after a finite amount of steps. TIE-20106 15 An algorithm solves a well defined problem. • The relation between the results and the given input is determined by the problem • for example: – sorting the contents of the array input: a sequence of numbers a1, a2, . . . , an results: numbers a1, a2, . . . , an sorted into an ascending order – finding flight connections input: a graph of flight connections, cities of departure and destination results: Flight numbers, connection and price information TIE-20106 16 • an instance of the problem is created by giving legal values to the elements of the problem’s input – for example: an instance of the sorting problem: 31, 41, 59, 26, 41, 58 An algorithm is correct, if it halts and gives the correct output as the result each time it is given a legal input. TIE-20106 17 • A certain set of formally possible inputs can be forbidden by the definition of the algorithm or the problem TIE-20106 18 an algorithm can be incorrect in three different ways: – it produces an incorrect result – it crashes during execution – it never halts, i.e. has infinite execution an incorrect algorithm may sometimes be a very usefull one as long as a certain amount of errors is tolerated. – for example, checking whether a number is prime TIE-20106 19 In principle any method of representing algorithms can be used as long as the result is precise and unambiguous • usually algorithms are implemented as computer programs or in hardware • in practise, the implementation must take several “engineering viewpoints” into accout – accomodation to the situation and environment – checking the legality of inputs – handling error situations – limitations of the programming language – speed limitations and practicality issues concerning the hardware and the programming language – maintenance issues ⇒modularity etc. ⇒the idea of the algorithm may get lost under the implementation details TIE-20106 20 On this course we concentrate on the algorithmic ideas and therefore usually represent the algorithms in pseudocode without legality checks, error handling etc. Let’s take, for example, an algorithm suitable for sorting small arrays called INSERTION-SORT: Figure 1: picture from Wikipedia TIE-20106 21 • the basic idea: – during execution the leftmost ele- ments in the array are sorted and the rest are still unsorted – the algorithm starts from the sec- ond element and iteratively steps through the elements upto the end of the array • on each step the algorithm searches for the point in the sorted part of the array, where the first element in the unsorted range should go to. – room is made for the new element by moving the larger elements one step to the right – the element is placed to it’s cor- rect position and the size of the sorted range in the beginning of the array is incremented by one. 59 41 31 58 41 26 59 41 58 41 26 59 41 31 31 26 59 58 41 41 31 26 58 59 41 41 31 26 58 41 59 41 31 26 58 41 TIE-20106 22 In pseudocode used on the course INSERTION-SORT looks like this: INSERTION-SORT(A) (input in array A) 1 for j := 2 to A.length do (increment the size of the sorted range) 2 key := A[j] (handle the first unsorted element) 3 i := j −1 4 while i > 0 and A[i] > key do(find the correct location for the new element) 5 A[i + 1] := A[i] (make room for the new element) 6 i := i −1 7 A[i + 1] := key (set the new element to its correct location) • indentation is used to indicate the range of conditions and loop structures • (comments) are written in parentheses in italics • the “:=” is used as the assignment operator (“=” is the comparison operator) • the lines starting with the character ▷give textual instructions TIE-20106 23 • members of structure elements (or objects) are referred to with the dot notation. – e.g. student.name, student.number • the members of a structure accessed through a pointer x are referred to with the →character – e.g. x→name, x→number • variables are local unless mentioned otherwise • a collection of elements, an array or a pointer, is a reference to the collection – larger data structures like the ones mentioned should always be passed by reference • a pass-by-value mechanism is used for single parameters (just like C++ does) • a pointer or a reference can also have no target: NIL TIE-20106 24 2.3 Implementing algorithms In the real world you need to be able to use theoretical knowledge in practise. For example: apply a given sorting algorithm ins a certain programming problem • numbers are rarely sorted alone, we sort structures with – a key – satellite data • the key sets the order ⇒it is used in the comparisons • the satellite data is not used in the comparison, but it must be moved around together with the key TIE-20106 25 The INSERTION-SORT algorithm from the previous chapter would change as follows if there were some satellite data used: 1 for j := 2 to A.length do 2 temp := A[j] 3 i := j −1 4 while i > 0 and A[i].key > temp.key do 5 A[i + 1] := A[i] 6 i := i −1 7 A[i + 1] := temp • An array of pointers to structures should be used with a lot of satellite data. The sorting is done with the pointers and the structures can then be moved directly to their correct locations. TIE-20106 26 The programming language and the problem to be solved also often dictate other implementation details, for example: • Indexing starts from 0 (in pseudocode often from 1) • Is indexing even used, or some other method of accessing data (or do we use arrays or some other data structures) • (C++) Is the data really inside the array/datastructure, or somewhere else at the end of a pointer (in which case the data doesn’t have to be moved and sharing it is easier). Many other programming languages always use pointers/references, so you don’t have to choose. • If you refer to the data indirectly from elsewhere, does it happen with – Pointers (or references) – Smart pointers (C++, shared_ptr) – Iterators (if the data is inside a datastructure) – Index (if the data is inside an array) – Search key (if the data is insde a data structure with fast search) TIE-20106 27 • Is recursion implemented really as recursion or as iteration • Are algorithm \"parameters\" in pseudocode really parameters in code, or just variables TIE-20106 28 In order to make an executable program, additional information is needed to implement INSERTION-SORT • an actual programming language must be used with its syntax for defining variables and functions • a main program that takes care of reading the input, checking its legality and printing the results is also needed – it is common that the main is longer than the actual algorithm TIE-20106 29 The implementation of the program described abowe in C++: #include <iostream> #include <vector> typedef std::vector<int> Array; void insertionSort( Array & A ) { int key, i; unsigned int j; for( j = 1; j < A.size(); ++j ) { key = A.at(j); i = j-1; while( i >= 0 && A.at(i) > key ) { A.at(i+1) = A.at(i); --i; } A.at(i+1) = key; } } int main() { unsigned int i; // getting the amount of elements std::cout << \"Give the size of the array 0...: \"; std::cin >> i; TIE-20106 30 Array A(i); // creating the array // reading in the elements for( i = 0; i < A.size(); ++i ) { std::cout << \"Give A[\" << i+1 << \"]: \"; std::cin >> A.at(i); } insertionSort( A ); // sorting // print nicely for( i = 0; i < A.size(); ++i ) { if( i % 5 == 0 ) { std::cout << std::endl; } else { std::cout << \" \"; } std::cout << A.at(i); } std::cout << std::endl; } TIE-20106 31 The program code is significantly longer than the pseudocode. It is also more difficult to see the central characteristics of the algorithm. This course concentrates on the principles of algorithms and data structures. Therefore using program code doesn’t serve the goals of the course. ⇒From now on, program code implementations are not normally shown. TIE-20106 32 3 Efficiency and algorithm design This chapter dicusses the analysis of algorithms: the efficiency of algorithms and the notations used to describe the asymptotic behavior of an algorithm. In addition the chapter introduces two algorithm design techniques: decrease and conquer and divide and conquer. t n TIE-20106 33 3.1 Asymptotic notations It is occasionally important to know the exact time it takes to perform a certain operation (in real time systems for example). Most of the time it is enough to know how the running time of the algorithm changes as the input gets larger. • The advantage: the calculations are not tied to a given processor, architecture or a programming language. • In fact, the analysis is not tied to programming at all but can be used to describe the efficiency of any behaviour that consists of successive operations. TIE-20106 34 • The time efficiency analysis is simplified by assuming that all operations that are independent of the size of the input take the same amount of time to execute. • Furthermore, the amount of times a certain operation is done is irrelevant as long as the amount is constant. • We investigate how many times each row is executed during the execution of the algorithm and add the results together. TIE-20106 35 • The result is further simplified by removing any constant coefficients and lower-order terms. ⇒This can be done since as the input gets large enough the lower-order terms get insigficant when compared to the leading term. ⇒The approach naturally doesn’t produce reliable results with small inputs. However, when the inputs are small, programs usually are efficient enough in any case. • The final result is the efficiency of the algorithm and is denoted it with the greek alphabet theta, Θ. f(n) = 23n2 + 2n + 15 ⇒f ∈Θ(n2) f(n) = 1 2n lg n + n ⇒f ∈Θ(n lg n) TIE-20106 36 Example 1: addition of the elements in an array 1 for i := 1 to A.length do 2 sum := sum + A[i] • if the size of the array A is n, line 1 is executed n + 1 times • line 2 is executed n times • the running time increases as n gets larger: n time = 2n + 1 1 3 10 21 100 201 1000 2001 10000 20001 • notice how the value of n dominates the running time TIE-20106 37 • let’s simplify the result as described earlier by taking away the constant coefficients and the lower-order terms: f(n) = 2n + 1 ⇒n ⇒we get f ∈Θ(n) as the result ⇒the running time depends linearly on the size of the input. TIE-20106 38 Example 2: searching from an unsorted array 1 for i := 1 to A.length do 2 if A[i] = x then 3 return i • the location of the searched element in the array affects the running time. • the running time depends now both on the size of the input and on the order of the elements ⇒we must separately handle the best-case, worst-case and average-case efficiencies. TIE-20106 39 • in the best case the element we’re searching for is the first element in the array. ⇒the element is found in constant time, i.e. the efficiency is Θ(1) • in the worst case the element is the last element in the array or there are no matching elements. • now line 1 gets executed n + 1 times and line 2 n times ⇒efficiency is Θ(n). • determining the average-case efficiency is not as straightforward TIE-20106 40 • first we must make some assumptions on the average, typical inputs: – the probability p that the element is in the array is (0 ≤p ≤1) – the probability of finding the first match in each position in the array is the same • we can find out the average amount of comparisons by using the probabilities • the probability that the element is not found is 1 - p, and we must make n comparisons • the probability for the first match occuring at the index i, is p/n, and the amount of comparisons needed is i • the number of comparisons is: [1 · p n + 2 · p n + · · · + i · p n · · · + n · p n] + n · (1 −p) TIE-20106 41 • if we assume that the element is found in the array, i.e. p = 1, we get (n+1)/2 which is Θ(n) ⇒since also the case where the element is not found in the array has linear efficiency we can be quite confident that the average efficiency is Θ(n) • it is important to keep in mind that all inputs are usually not as probable. ⇒each case needs to be investigated separately. TIE-20106 42 Example 3: finding the common element in two arrays 1 for i := 1 to A.length do 2 for j := 1 to B.length do 3 if A[i] = B[j] then 4 return A[i] • line 1 is executed 1 – (n + 1) times • line 2 is executed 1 – (n · (n + 1)) times • line 3 is executed 1 – (n · n) times • line 4 is executed at most once TIE-20106 43 • the algorithm is fastest when the first element of both arrays is the same ⇒the best case efficiency is Θ(1) • in the worst case there are no common elements in the arrays or the last elements are the same ⇒the efficiency is 2n2 + 2n + 1 = Θ(n2) • on average we can assume that both arrays need to be investigated approximately half way through. ⇒the efficiency is Θ(n2) (or Θ(nm) if the arrays are of different lengths) TIE-20106 44 3.2 Algorithm Design Technique: Decrease and conquer The most straightforward algorithm design technique covered on the course is decrease and conquer. • initially the entire input is unprocessed • the algorithm processes a small piece of the input on each round ⇒the amount of processed data gets larger and the amount of unprocessed data gets smaller • finally there is no unprocessed data and the algorithm halts These types of algorithms are easy to implement and work efficiently on small inputs. TIE-20106 45 The Insertion-Sort seen earlier is a “decrease and conquer” algorithm. • initially the entire array is (possibly) unsorted • on each round the size of the sorted range in the beginning of the array increases by one element • in the end the entire array is sorted TIE-20106 46 INSERTION-SORT INSERTION-SORT(A) (input in array A) 1 for j := 2 to A.length do (move the limit of the sorted range) 2 key := A[j] (handle the first unsorted element) 3 i := j −1 4 while i > 0 and A[i] > key do(find the correct location of the new element) 5 A[i + 1] := A[i] (make room for the new element) 6 i := i −1 7 A[i + 1] := key (set the new element to it’s correct location) • line 1 is executed n times • lines 2 and 3 are executed n - 1 times • line 4 is executed at least n - 1 and at most (2 + 3 + 4 + · · · + n - 2) times • lines 5 and 6 are executed at least 0 and at most (1 + 2 + 3 + 4 + · · · + n - 3) times TIE-20106 47 • in the best case the entire array is already sorted and the running time of the entire algorithm is at least Θ(n) • in the worst case the array is in a reversed order. Θ(n2) time is used • once again determining the average case is more difficult: • let’s assume that out of randomly selected element pairs half is in an incorrect order in the array ⇒the amount of comparisons needed is half the amount of the worst case where all the element pairs were in an incorrect order ⇒the average-case running time is the worst-case running time divided by two: [(n - 1)n]/ 4 = Θ(n2) TIE-20106 48 3.3 Algorithm Design Technique: Divide and Conquer We’ve earlier seen the decrease and conquer algorithm design technique and the algorithm INSERTION-SORT as an example of it. Now another technique called divide and conquer is introduced. It is often more efficient than the decrease and conquer approach. • the problem is divided into several subproblems that are like the original but smaller in size. • small subproblems are solved straightforwardly • larger subproblems are further divided into smaller units • finally the solutions of the subproblems are combined to get the solution to the original problem Let’s get back to the claim made earlier about the complexity notation not being fixed to programs and take an everyday, concrete example TIE-20106 49 Example: finding the false goldcoin • The problem is well-known from logic problems. • We have n gold coins, one of which is false. The false coin looks the same as the real ones but is lighter than the others. We have a scale we can use and our task is to find the false coin. • We can solve the problem with Decrease and conquer by choosing a random coin and by comparing it to the other coins one at a time. ⇒At least 1 and at most n - 1 weighings are needed. The best-case efficiency is Θ(1) and the worst and average case efficiencies are Θ(n). • Alternatively we can always take two coins at random and weigh them. At most n/2 weighings are needed and the efficiency of the solution is still the same. TIE-20106 50 The same problem can be solved more efficiently with divide and conquer: • Divide the coins into the two pans on the scales. The coins on the heavier side are all authentic, so they don’t need to be investigated further. • Continue the search similarly with the lighter half, i.e. the half that con- tains the false coin, until there is only one coin in the pan, the coin that we know is false. possible false ones genuine for sure • The solution is recursive and the base case is the situation where there is only one possible coin that can be false. TIE-20106 51 • The amount of coins on each weighing is 2 to the power of the amount of weighings still required: on the highest level there are 2weighings coins, so based on the definition of the logarithm: 2weighings = n ⇒log2n = weighings • Only log2n weighings is needed, which is significantly fewer than n/2 when the amount of coins is large. ⇒The complexity of the solution is Θ(lg n) both in the best and the worst-case. TIE-20106 52 3.4 QUICKSORT Let’s next cover a very efficient sorting algorithm QUICKSORT. QUICKSORT is a divide and conquer algorithm. The division of the problem into smaller subproblems • Select one of the elements in the array as a pivot, i.e. the element which partitions the array. • Change the order of the elements in the array so that all elements smaller or equal to the pivot are placed before it and the larger elements after it. • Continue dividing the upper and lower halves into smaller subarrays, until the subarrays contain 0 or 1 elements. TIE-20106 53 Smaller subproblems: • Subarrays of the size 0 and 1 are already sorted Combining the sorted subarrays: • The entire (sub) array is automatically sorted when its upper and lower halves are sorted. – all elements in the lower half are smaller than the elements in the upper half, as they should be QUICKSORT-algorithm QUICKSORT(A, p, r) 1 if p < r then (do nothing in the trivial case) 2 q := PARTITION(A, p, r) (partition in two) 3 QUICKSORT(A, p, q −1) (sort the elements smaller than the pivot) 4 QUICKSORT(A, q + 1, r) (sort the elements larger than the pivot) TIE-20106 54 The partition algorithm rearranges the subarray in place PARTITION(A, p, r) 1 x := A[r] (choose the last element as the pivot) 2 i := p −1 (use i to mark the end of the smaller elements) 4 for j := p to r −1 do (scan to the second to last element) 6 if A[j] ≤x (if A[j] goes to the half with the smaller elements...) 9 i := i + 1 (... increment the amount of the smaller elements...) 12 exchange A[i] ↔A[j] (... and move A[j] there) 12 exchange A[i + 1] ↔A[r] (place the pivot between the halves) 13 return i + 1 (return the location of the pivot) How fast is PARTITION? • The for-loop is executed n - 1 times when n is r - p • All other operations are constant time. ⇒The running-time is Θ(n). TIE-20106 55 Determining the running-time of QUICKSORT is more difficult since it is recursive. Therefore the equation for its running time would also be recursives. Finding the recursive equation is, however, beyond the goals of this course so we’ll settle for a less formal approach • As all the operations of QUICKSORT except PARTITION and the recursive call are constant time, let’s concentrate on the time used by the instances of PARTITION. 1 1 1 1 1 n n n 1 1 1 1 1 2 2 n − n − 1 n − n − 1 TIE-20106 56 • The total time is the sum of the running times of the nodes in the picture abowe. • The execution is constant time for an array of size 1. • For the other the execution is linear to the size of the array. ⇒The total time is Θ(the sum of the numbers of the nodes). TIE-20106 57 Worst-case running time • The number of a node is always smaller than the number of its parent, since the pivot is already in its correct location and doesn’t go into either of the sorted subarrays ⇒there can be atmost n levels in the tree • the worst case is realized when the smallest or the largest element is always chosen as the pivot – this happens, for example, with an array already sorted • the sum of the node numbers is n + n - 1 + · · · + 2 + 1 ⇒the worst case running time of QUICKSORT is Θ(n2) n−1 n−2 k 2 1 n TIE-20106 58 The best-case is when the array is always divided evenly in half. • The picture below shows how the subarrays get smaller. – The grey boxes mark elements already in their correct position. • The amount of work on each level is in Θ(n). – a pessimistic estimate on the height of the execution tree is in the best-case ⇒Θ(lg n) ⇒The upper limit for the best-case efficiency is Θ(n lg n). O(lg n) O(n) O(n) O(n) O(n) O(n) TIE-20106 59 The best-case and the worst-case efficiencies of QUICKSORT differ significantly. • It would be interesting to know the average-case running-time. • Analyzing it is beyond the goals of the course but it has been shown that if the data is evenly distributed its average running-time is Θ(n lg n). • Thus the average running-time is quite good. TIE-20106 60 An unfortunate fact with QUICKSORT is that its worst-case efficiency is poor and in practise the worst-case situation is quite probable. • It is easy to see that there can be situations where the data is already sorted or almost sorted. ⇒A way to decrease the risk of the systematic occurence of the worst-case situation’s likelyhood is needed. Randomization has proved to be quite efficient. TIE-20106 61 Advantages and disadvantages of QUICKSORT Advantages: • sorts the array very efficiently in average – the average-case running-time is Θ(n lg n) – the constant coefficient is small • requires only a constant amount of extra memory • if well-suited for the virtual memory environment Disadvantages: • the worst-case running-time is Θ(n2) • without randomization the worst-case input is far too common • the algorithm is recursive ⇒the stack uses extra memory • instability TIE-20106 62 3.5 Algorithm Design Technique: Randomization Randomization is one of the design techniques of algorithms. • A pathological occurence of the worst-case inputs can be avoided with it. • The best-case and the worst-case running-times don’t usually change, but their likelyhood in practise decreases. • Disadvantageous inputs are exactly as likely as any other inputs regardless of the original distribution of the inputs. • The input can be randomized either by randomizing it before running the algorithm or by embedding the randomization into the algorithm. – the latter approach usually gives better results – often it is also easier than preprocessing the input. TIE-20106 63 • Randomization is usually a good idea when – the algorithm can continue its execution in several ways – it is difficult to see which way is a good one – most of the ways are good – a few bad guesses among the good ones don’t make much damage • For example, QUICKSORT can choose any element in the array as the pivot – besides the almost smallest and the almost largest elements, all other elements are a good choise – it is difficult to guess when making the selection whether the element is almost the smallest/largest – a few bad guesses now and then doesn’t ruin the efficiency of QUICKSORT ⇒randomization can be used with QUICKSORT TIE-20106 64 With randomization an algorithm RANDOMIZED-QUICKSORT which uses a randomized PARTITION can be written • A[r] is not always chosen as the pivot. Instead, a random element from the entire subarray is selected as the pivot • In order to keep PARTITION correct, the pivot is still placed in the index r in the array ⇒Now the partition is quite likely even regardless of the input and how the array has earlier been processed. RANDOMIZED-PARTITION(A, p, r) 1 i := RANDOM(p, r) (choose a random element as pivot) 2 exchange A[r] ↔A[i] (store it as the last element) 3 return PARTITION(A, p, r) (call the normal partition) RANDOMIZED-QUICKSORT(A, p, r) 1 if p < r then 2 q := RANDOMIZED-PARTITION(A, p, r) 3 RANDOMIZED-QUICKSORT(A, p, q −1) 4 RANDOMIZED-QUICKSORT(A, q + 1, r) TIE-20106 65 The running-time of RANDOMIZED-QUICKSORT is Θ(n lg n) on average just like with normal QUICKSORT. • However, the assumption made in analyzing the average-case running-time that the pivot-element is the smallest, the second smallest etc. element in the subarray with the same likelyhood holds for RANDOMIZED-QUICKSORT for sure. • This holds for the normal QUICKSORT only if the data is evenly distributed. ⇒RANDOMIZED-QUICKSORT is better than the normal QUICKSORT in general TIE-20106 66 QUICKSORT can be made more efficient with other methods: • An algorithm efficient with small inputs (e.g.INSERTIONSORT) can be used to sort the subarrays. – they can also be left unsorted and in the end sort the entire array with INSERTIONSORT • The median of three randomly selected elements can be used as the pivot. • It’s always possible to use the median as the pivot. TIE-20106 67 The median can be found efficiently with the so called lazy QUICKSORT. • Divide the array into a “small elements” lower half and a “large elements” upper half like in QUICKSORT. • Calculate which half the ith element belongs to and continue recursively from there. • The other half does not need to be processed further. RANDOMIZED-SELECT(A, p, r, i) 1 if p = r then (if the subarray is of size 1...) 2 return A[p] (... return the only element) 3 q := RANDOMIZED-PARTITION(A, p, r) (divide the array into two halves) 4 k := q −p + 1 (calculate the number of the pivot) 5 if i = k then (if the pivot is the ith element in the array...) 6 return A[q] (...return it) 7 else if i < k then (continue the search from the small ones) 8 return RANDOMIZED-SELECT(A, p, q −1, i) 9 else (continue on the large ones) 10 return RANDOMIZED-SELECT(A, q + 1, r, i −k) TIE-20106 68 The lower-bound for the running-time of RANDOMIZED-SELECT: • Again everything else is constant time except the call of RANDOMIZED-PARTITION and the recursive call. • In the best-case the pivot selected by RANDOMIZED-PARTITION is the ith element and the execution ends. • RANDOMIZED-PARTITION is run once for the entire array. ⇒The algorithm’s best case running-time is Θ(n). The upper-bound for the running-time of RANDOMIZED-SELECT: • RANDOMIZED-PARTITION always ends up choosing the smallest or the largest element and the ith element is left in the larger half. • the amount of work is decreased only by one step on each level of recursion. ⇒The worst case running-time of the algorithm is Θ(n2). TIE-20106 69 The average-case running-time is however Θ(n). The algorithm is found in STL under the name nth_element. The algorithm can also be made to always work in linear time. TIE-20106 70 4 Sorting algorithms This chapter covers two efficient sorting algorithms that sort the data in place. In addition their central ideas are applied to solving two separate problems - priority queue and finding the median. Finally the maximum efficiency of comparison sorts, i.e. sorting based on the comparisons of the elements, is discussed. Sorting algorithms that use other approaches than comparisons are also examined. TIE-20106 71 4.1 Sorting with a heap This chapter introduces a sorting algorithm HEAPSORT that uses a very important data structure, a heap, to manage data during execution. Binary trees Before we get our hands on the heap, let’s define what a binary tree is • a structure that consists of nodes who each have 0, 1 or 2 chidren • the children are called left and right • a node is the parent of its children • a childless node is called a leaf, and the other nodes are internal nodes • a binary tree has at most one node that has no parent, i.e. the root – all other nodes are the root’s children, grandchildren etc. TIE-20106 72 • the descendants of each node form the subtree of the binary tree with the node as the root • The height of a node in a binary tree is the length of the longest simple down- ward path from the node to a leaf – the edges are counted into the height, the height of a leaf is 0 • the height of a binary tree is the height of it’s root inner nodes root leaves • a binary tree is completely balanced if the difference between the height of the root’s left and right subtrees is atmost one and the subtrees are completely balanced • the height of a binary tree with n nodes is at least ⌊lg n⌋and atmost n - 1 ⇒O(n) and Ω(lg n) TIE-20106 73 Heap An array A[1 . . . n] is a heap, if A[i] ≥A[2i] and A[i] ≥A[2i + 1] always when 1 ≤i ≤⌊n 2⌋(and 2i + 1 ≤n). The structure is easier to understand if we define the heap as a completely balanced binary tree, where • the root is stored in the array at index 1 • the children of the node at in- dex i are stored at 2i and 2i + 1 (if they exist) • the parent of the node at index i is stored at ⌊i 2⌋ 12 15 14 7 17 5 10 8 2 7 Thus, the value of each node is larger or equal to the values of its children TIE-20106 74 Each level in the heap tree is full, except maybe the last one, where only some rightmost leaves may be missing In order to make it easier to see the heap as a tree, let’s define subroutines that find the parent and the children. • they can be implemented very efficiently by shifting bits • the running time of each is always Θ(1) PARENT(i) return ⌊i/2⌋ LEFT(i) return 2i RIGHT(i) return 2i + 1 ⇒Now the heap property can be given with: A[PARENT(i)] ≥A[i] always when 2 ≤i ≤A.heapsize • A.heapsize gives the size of the heap (we’ll later see that it’s not necessarily the size of the array) TIE-20106 75 Due to the heap property, the largest element of the heap is always its root, i.e. at the first index in the array. If the height of the heap is h, the amount of its nodes is between 2h . . . 2h+1 −1. ⇒If there are n nodes in the heap its height is Θ(lg n). Adding an element to the heap from the top: • let’s assume that A[1 . . . n] is oth- erwise a heap, except that the heap property does not hold for the root of the heap tree – in other words A[1] < A[2] or A[1] < A[3] 12 10 9 8 3 4 5 6 7 2 1 15 14 7 5 9 8 10 2 7 TIE-20106 76 • the problem can be moved downwards in the tree by se- lecting the largest of the root’s children and swapping it with the root – in order to maintain the heap property the largest of the children needs to be chosen - it is going to become the par- ent of the other child 12 10 9 8 3 4 5 6 7 2 1 9 7 14 5 10 8 15 7 2 • the same can be done to the subtree, whose root thus turned problematic and again to its subtree etc. until the problem disappears – the problem is solved for sure once the execution reaches a leaf ⇒the tree becomes a heap 12 10 9 8 3 4 5 6 7 2 1 14 7 5 10 15 8 9 7 2 TIE-20106 77 The same is pseudocode HEAPIFY(A, i) (i is the index where the element might be too small) 1 repeat (repeat until the heap is fixed) 2 old_i := i (store the value of i) 3 l := LEFT(i) 4 r := RIGHT(i) 5 if l ≤A.heapsize and A[l] > A[i] then (the left child is larger than i) 6 i := l 7 if r ≤A.heapsize and A[r] > A[i] then (right child is even larger) 8 i := r 9 if i ̸= old_i then (if a larger child was found...) 10 exchange A[old_i] ↔A[i] (...move the problem downwards) 11 until i = old_i (if the heap is already fixed, exit) • The execution is constant time if the condition on line 11 is true the first time it is met: Ω(1). • In the worst case the new element needs to be moved all the way down to the leaf. ⇒The running time is O(h) = O(lg n). TIE-20106 78 Building a heap • the following algorithm converts an array into a heap: BUILD-HEAP(A) 1 A.heapsize := A.length (the heap is built out of the entire array) 2 for i := ⌊A.length/2⌋downto 1 do (scan through the lower half of the array ) 3 HEAPIFY(A, i) (call Heapify) • The array is scanned from the end towards the beginning and HEAPIFY is called for each node. – before calling HEAPIFY the heap property always holds for the subtree rooted at i except that the element in i may be too small – subtrees of size one don’t need to be fixed as the heap property trivially holds – after HEAPIFY(A, i) the subtree rooted at i is a heap ⇒after HEAPIFY(A, 1) the entire array is a heap TIE-20106 79 • BUILD-HEAP executes the for-loop ⌊n 2⌋times and HEAPIFY is Ω(1) and O(lg n) so – the best case running time is ⌊n 2⌋· Ω(1) + Θ(n) = Ω(n) – the program never uses more than ⌊n 2⌋· O(lg n) + Θ(n) = O(n lg n) • The worst-case running time we get this way is however too pessimistic: – HEAPIFY is O(h), where h is the height of the heap tree – as i changes the height of the tree changes level h executions times of HEAPIFY lowest 0 0 2nd 1 ⌊n 4⌋ 3rd 2 ⌊n 8⌋ ... ... ... topmost ⌊lg n⌋ 1 – thus the worst case runnign time is n 4 · 1 + n 8 · 2 + n 16 · 3 + · · · = n 2 · P∞ i=1 i 2i = n 2 · 2 = n ⇒O(n) ⇒the running time of BUILD-HEAP is always Θ(n) TIE-20106 80 Sorting with a heap The following algorithm can be used to sort the contents of the array efficiently: HEAPSORT(A) 1 BUILD-HEAP(A) (convert the array into a heap) 2 for i := A.length downto 2 do (scan the array from the last to the first element) 3 exchange A[1] ↔A[i] (move the heap’s largest element to the end) 4 A.heapsize := A.heapsize −1 (move the largest element outside the heap) 5 HEAPIFY(A, 1) (fix the heap, which is otherwise fine...) (... except the first element may be too small) Let’s draw a picture of the situation: • first the array is converted into a heap • it’s easy to see from the exam- ple, that the operation is not too laborous – the heap property is obviously weaker than the order 9 12 8 14 7 15 9 9 9 12 12 12 8 8 8 7 7 7 14 14 14 15 15 15 TIE-20106 81 • the picture shows how the sorted range at the end of the array gets larger until the entire array is sorted • the heap property is fixed each time the sorted range gets larger • the fixing process seems complex in such a small example – the fixing process doesn’t take a lot of steps even with large ar- rays, only a logarithmic amount 7 8 14 9 12 15 15 15 15 15 15 12 8 7 9 14 7 14 14 15 9 12 8 7 14 15 8 14 9 12 7 14 14 14 14 12 12 12 12 7 9 8 9 9 7 8 8 7 8 7 9 14 12 9 8 12 7 9 8 8 7 9 12 12 14 7 8 9 15 15 15 15 TIE-20106 82 The running time of HEAPSORT consists of the following: • BUILD-HEAP on line 1 is executed once: Θ(n) • the contents of the for-loop is executed n - 1 times – operations on lines 3 and 4 are constant time – HEAPIFY uses Ω(1) and O(lg n) ⇒in total Ω(n) and O(n lg n) • the lower bound is exact – if all the elements have the same value the heap doesn’t need to be fixed at all and HEAPIFY is always constant time • the upper bound is also exact – proving this is more difficult and we find the upcoming result from the efficiency of sorting by counting sufficient Note! The efficiency calculations abowe assume that the data structure used to store the heap provides a constant time indexing. • Heap is worth using only when this is true TIE-20106 83 Advantages and disadvantages of HEAPSORT Advantages: • sorts the array in place • never uses more than Θ(n lg n) time Disadvantages: • the constant coefficient in the running time is quite large • instability – elements with the same value don’t maintain their order TIE-20106 84 4.2 Priority queue A priority queue is a data structure for maintaining a set S of elements, each associated with a key value. The following operations can be performed: • INSERT(S, x) inserts the element x into the set S • MAXIMUM(S) returns the element with the largest key – if there are several elements with the same key, the operation can choose any one of them • EXTRACT-MAX(S) removes and returns the element with the largest key • alternatively the operations MINIMUM(S) and EXTRACT-MIN(S) can be implemented – there can be only the maximum or only the minimun operations implemented in the same queue TIE-20106 85 Priority queues can be used widely • prioritizing tasks in an operating system – new tasks are added with the command INSERT – as the previous task is completed or interrupted the next one is chosen with EXTRACT-MAX • action based simulation – the queue stores incoming (not yet simulated) actions – the key is the time the action occurs – an action can cause new actions ⇒they are added to the queue with INSERT – EXTRACT-MIN gives the next simulated action • finding the shortest route on a map – cars driving at constant speed but choosing different routes are simulated until the first one reaches the destination – a priority queue is needed in practise in an algorithm for finding shortest paths, covered later TIE-20106 86 In practise, a priority queue could be implemented with an unsorted or sorted array, but that would be inefficient • the operations MAXIMUM and EXTRACT-MAX are slow in an unsorted array • INSERT is slow in a sorted array A heap can be used to implement a priority queue efficiently instead. • The elements of the set S are stored in the heap A. • MAXIMUM(S) is really simple and works in Θ(1) running-time HEAP-MAXIMUM(A) 1 if A.heapsize < 1 then (there is no maximum in an empty heap) 2 error “heap underflow” 3 return A[1] (otherwise return the first element in the array) TIE-20106 87 • EXTRACT-MAX(S) can be implemented by fixing the heap after the extraction with HEAPIFY. • HEAPIFY dominates the running-time of the algorithm: O(lg n). HEAP-EXTRACT-MAX(A) 1 if A.heapsize < 1 then (no maximum in an empty heap) 2 error “heap underflow” 3 max := A[1] (the largest element is at the first index) 4 A[1] := A[A.heapsize] (make the last element the root) 5 A.heapsize := A.heapsize −1 (decrement the size of the heap) 6 HEAPIFY(A, 1) (fix the heap) 7 return max TIE-20106 88 • INSERT(S, x) adds a new element into the heap by making it a leaf and then by lifting it to its correct height based on its size – is works like HEAPIFY, but from bottom up – in the worst-case, the leaf needs to be lifted all the way up to the root: running-time O(lg n) HEAP-INSERT(A, key) 1 A.heapsize := A.heapsize + 1 (increment the size of the heap) 2 i := A.heapsize (start from the end of the array) 3 while i > 1 and A[PARENT(i)] < key do (continue until the root or ...) (... or a parent with a larger value is reached) 4 A[i] := A[PARENT(i)] (move the parent downwards) 5 i := PARENT(i) (move upwards) 6 A[i] := key (place the key into its correct location) ⇒Each operation in the priority queue can be made O(lg n) by using a heap. TIE-20106 89 A priority queue can be thought of as an abstract data type which stores the data (the set S) and provides the operations INSERT, MAXIMUM,EXTRACT-MAX. • the user sees the names and the purpose of the operations but not the implementation • the implementation is encapsulated into a package (Ada), a class (C++) or an independent file (C) ⇒It’s easy to maintain and change the implementation when needed without needing to change the code using the queue. TIE-20106 90 4.3 QUICKSORT This chapter covers a very efficient sorting algorithm QUICKSORT. Like MERGE-SORT, QUICKSORT is a divide and conquer algorithm. However, with MERGE-SORT the division is simple and combining the results is complex, with QUICKSORT it’s vice versa The division of the problem into smaller subproblems • Select one of the elements in the array as a pivot, i.e. the element which partitions the array. • Change the order of the elements in the array so that all elements smaller or equal to the pivot are placed before it and the larger elements after it. • Continue dividing the upper and lower halves into smaller subarrays, until the subarrays contain 0 or 1 elements. TIE-20106 91 Smaller subproblems: • Subarrays of the size 0 and 1 are already sorted Combining the sorted subarrays: • The entire (sub) array is automatically sorted when its upper and lower halves are sorted. – all elements in the lower half are smaller than the elements in the upper half, as they should be QUICKSORT-algorithm QUICKSORT(A, p, r) 1 if p < r then (do nothing in the trivial case) 2 q := PARTITION(A, p, r) (partition in two) 3 QUICKSORT(A, p, q −1) (sort the elements smaller than the pivot) 4 QUICKSORT(A, q + 1, r) (sort the elements larger than the pivot) TIE-20106 92 The partition algorithm rearranges the subarray in place PARTITION(A, p, r) 1 x := A[r] (choose the last element as the pivot) 2 i := p −1 (use i to mark the end of the smaller elements) 4 for j := p to r −1 do (scan to the second to last element) 6 if A[j] ≤x (if A[j] goes to the half with the smaller elements...) 9 i := i + 1 (... increment the amount of the smaller elements...) 12 exchange A[i] ↔A[j] (... and move A[j] there) 12 exchange A[i + 1] ↔A[r] (place the pivot between the halves) 13 return i + 1 (return the location of the pivot) How fast is PARTITION? • The for-loop is executed n - 1 times when n is r - p • All other operations are constant time. ⇒The running-time is Θ(n). TIE-20106 93 Determining the running-time of QUICKSORT is more difficult. We’ll analyze in the same way we did with MERGE-SORT • As all the operations of QUICKSORT except PARTITION and the recursive call are constant time, let’s concentrate on the time used by the instances of PARTITION. 1 1 1 1 1 n n n 1 1 1 1 1 2 2 n − n − 1 n − n − 1 TIE-20106 94 • The total time is the sum of the running times of the nodes in the picture abowe. • The execution is constant time for an array of size 1. • For the other the execution is linear to the size of the array. ⇒The total time is Θ(the sum of the numbers of the nodes). TIE-20106 95 Worst-case running time • The number of a node is always smaller than the number of its parent, since the pivot is already in its correct location and doesn’t go into either of the sorted subarrays ⇒there can be atmost n levels in the tree • the worst case is realized when the smallest or the largest element is always chosen as the pivot – this happens, for example, with an array already sorted • the sum of the node numbers is n + n - 1 + · · · + 2 + 1 ⇒the running time of QUICKSORT is O(n2) n−1 n−2 k 2 1 n TIE-20106 96 The best-case is when the array is always divided evenly in half. • The picture below shows how the subarrays get smaller. – The grey boxes mark elements already in their correct position. • The amount of work on each level is in Θ(n). • If the pivot would be kept either in the half with the smaller elements or with the larger elements the situation would be the same as in MERGE-SORT (page ??). – a pessimistic estimate on the height of the execution tree is in the best-case ⇒O(lg n) ⇒The upper limit for the best-case efficiency is O(n lg n). O(lg n) O(n) O(n) O(n) O(n) O(n) TIE-20106 97 The best-case and the worst-case efficiencies of QUICKSORT differ significantly. • It would be interesting to know the average-case running-time. • Analyzing it is beyond the goals of the course but it has been shown that if the data is evenly distributed its average running-time is Θ(n lg n). • Thus the average running-time is quite good. An unfortunate fact with QUICKSORT is that its worst-case efficiency is poor and in practise the worst-case situation is quite probable. • It is easy to see that there can be situations where the data is already sorted or almost sorted. ⇒A way to decrease the risk of the systematic occurence of the worst-case situation’s likelyhood is needed. Randomization has proved to be quite efficient. TIE-20106 98 Advantages and disadvantages of QUICKSORT Advantages: • sorts the array very efficiently in average – the average-case running-time is Θ(n lg n) – the constant coefficient is small • requires only a constant amount of extra memory • if well-suited for the virtual memory environment Disadvantages: • the worst-case running-time is Θ(n2) • without randomization the worst-case input is far too common • the algorithm is recursive ⇒the stack uses extra memory • instability TIE-20106 99 4.4 Randomization Randomization is one of the design techniques of algorithms. • A pathological occurence of the worst-case inputs can be avoided with it. • The best-case and the worst-case running-times don’t usually change, but their likelyhood in practise decreases. • Disadvantageous inputs are exactly as likely as any other inputs regardless of the original distribution of the inputs. • The input can be randomized either by randomizing it before running the algorithm or by embedding the randomization into the algorithm. – the latter approach usually gives better results – often it is also easier than preprocessing the input. TIE-20106 100 • Randomization is usually a good idea when – the algorithm can continue its execution in several ways – it is difficult to see which way is a good one – most of the ways are good – a few bad guesses among the good ones don’t make much damage • For example, QUICKSORT can choose any element in the array as the pivot – besides the almost smallest and the almost largest elements, all other elements are a good choise – it is difficult to guess when making the selection whether the element is almost the smallest/largest – a few bad guesses now and then doesn’t ruin the efficiency of QUICKSORT ⇒randomization can be used with QUICKSORT TIE-20106 101 With randomization an algorithm RANDOMIZED-QUICKSORT which uses a randomized PARTITION can be written • A[r] is not always chosen as the pivot. Instead, a random element from the entire subarray is selected as the pivot • In order to keep PARTITION correct, the pivot is still placed in the index r in the array ⇒Now the partition is quite likely even regardless of the input and how the array has earlier been processed. RANDOMIZED-PARTITION(A, p, r) 1 i := RANDOM(p, r) (choose a random element as pivot) 2 exchange A[r] ↔A[i] (store it as the last element) 3 return PARTITION(A, p, r) (call the normal partition) RANDOMIZED-QUICKSORT(A, p, r) 1 if p < r then 2 q := RANDOMIZED-PARTITION(A, p, r) 3 RANDOMIZED-QUICKSORT(A, p, q −1) 4 RANDOMIZED-QUICKSORT(A, q + 1, r) TIE-20106 102 The running-time of RANDOMIZED-QUICKSORT is Θ(n lg n) on average just like with normal QUICKSORT. • However, the assumption made in analyzing the average-case running-time that the pivot-element is the smallest, the second smallest etc. element in the subarray with the same likelyhood holds for RANDOMIZED-QUICKSORT for sure. • This holds for the normal QUICKSORT only if the data is evenly distributed. ⇒RANDOMIZED-QUICKSORT is better than the normal QUICKSORT in general QUICKSORT can be made more efficient with other methods: • An algorithm efficient with small inputs (e.g.INSERTIONSORT) can be used to sort the subarrays. – they can also be left unsorted and in the end sort the entire array with INSERTIONSORT • The median of three randomly selected elements can be used as the pivot. TIE-20106 103 • It’s always possible to use the median as the pivot. The median can be found efficiently with the so called lazy QUICKSORT. • Divide the array into a “small elements” lower half and a “large elements” upper half like in QUICKSORT. • Calculate which half the ith element belongs to and continue recursively from there. • The other half does not need to be processed further. RANDOMIZED-SELECT(A, p, r, i) 1 if p = r then (if the subarray is of size 1...) 2 return A[p] (... return the only element) 3 q := RANDOMIZED-PARTITION(A, p, r) (divide the array into two halves) 4 k := q −p + 1 (calculate the number of the pivot) 5 if i = k then (if the pivot is the ith element in the array...) 6 return A[q] (...return it) 7 else if i < k then (continue the search from the small ones) 8 return RANDOMIZED-SELECT(A, p, q −1, i) 9 else (continue on the large ones) 10 return RANDOMIZED-SELECT(A, q + 1, r, i −k) TIE-20106 104 The lower-bound for the running-time of RANDOMIZED-SELECT: • Again everything else is constant time except the call of RANDOMIZED-PARTITION and the recursive call. • In the best-case the pivot selected by RANDOMIZED-PARTITION is the ith element and the execution ends. • RANDOMIZED-PARTITION is run once for the entire array. ⇒The algorithm’s running-time is Ω(n). The upper-bound for the running-time of RANDOMIZED-SELECT: • RANDOMIZED-PARTITION always ends up choosing the smallest or the largest element and the ith element is left in the larger half. • the amount of work is decreased only by one step on each level of recursion. ⇒The running-time of the algorithm is O(n2). TIE-20106 105 The average-case running-time is however O(n). The algorithm is found in STL under the name nth_element. The algorithm can also be made to always work in linear time. TIE-20106 106 4.5 Other sorting algorithms All sorting algorithms covered so far have been based on comparisons. • They determine the correct order only based on comparing the values of the elements to eachother. It is possible to use information other than comparisons to sort the data. Sorting by counting Let’s assume that the value range of the keys is small, atmost on the same scale with the amount of the elements. • For simplicity we assume that the keys of the elements are from the set {1, 2, . . . , k}, and k = O(n). • For each key the amount of elements with the given key is calculated. • Based on the result the elements are placed directly into their correct positions. TIE-20106 107 COUNTING-SORT(A, B, k) 1 for i := 1 to k do 2 C[i] := 0 (initialize a temp array C with zero) 3 for j := 1 to A.length do 4 C[A[j].key] := C[A[j].key] + 1 (calculate the amount of elements with key = i) 5 for i := 2 to k do 6 C[i] := C[i] + C[i −1] (calculate how many keys ≤i) 7 for j := A.length downto 1 do (scan the array from end to beginning) 8 B[C[A[j].key]] := A[j] (place the element into the output array) 9 C[A[j].key] := C[A[j].key] −1 (the next correct location is a step to the left) The algorithm places the elements to their correct location in a reverse order in order to quarantee stability. Running-time: • The first and the third for-loop take Θ(k) time. • The second and the last for-loop take Θ(n) time. ⇒The running time is Θ(n + k). • If k = O(n), the running-time is Θ(n). TIE-20106 108 • All basic operations are simple and there are only a few of them in each loop so the constant coefficient of the running-time is small. COUNTING-SORT is not worth using if k ≫n. • The memory consumption of the algorithm is Θ(k). • Usually k ≫n. – for example: all possible social security numbers ≫the social security numbers of TUT personnel Sometimes there is a need to be able to sort based on a key with several parts. • the list of exam results first based on the department and then into an alphabetical order • dates first based on the year, then the month and then the day • a deck of cards first based on the suit and then according to the numbers TIE-20106 109 The different criteris are taken into account as follows • The most significant criterion according to which the values of the elements differs determines the result of the comparison. • If the elements are equal with each criteria they are considered equal. The problem can be solved with a comparison sort (e.g. by using a suitable comparison operator in QUICKSORT) • example: comparing dates DATE-COMPARE(x, y) 1 if x.year < y.year then return “smaller” 2 if x.year > y.year then return “greater” 3 if x.month < y.month then return “smaller” 4 if x.month > y.month then return “greater” 5 if x.day < y.day then return “smaller” 6 if x.day > y.day then return “greater” 7 return “equal” TIE-20106 110 Sometimes it makes sense to handle the input one criterion at a time. • For example it’s easiest to sort a deck of cards into four piles based on the suits and then each suit separately. The range of values in the significant criteria is often small when compared to the amount of element and COUNTING-SORT can be used. TIE-20106 111 There are two different algorithms available for sorting with multiple keys. • LSD-RADIX-SORT – the array is sorted first according to the least significant digit, then the second least significant etc. – the sorting algorithm needs to be stable - otherwise the array would be sorted only according to the most significant criterion – COUNTING-SORT is a suitable algorithm – comparison algorithms are not worth using since they would sort the array with approximately the same amount of effort directly at one go LSD-RADIX-SORT(A, d) 1 for i := 1 to d do (run through the criteria, least significant first) 2 ▷sort A with a stable sort according to criterion i TIE-20106 112 • MSD-RADIX-SORT – the array is first sorted according to the most significant digit and then the subarrays with equal keys according to the next significant digit etc. – does not require the sorting algorithm to be stable – usable when sorting character strings of different lengths – checks only as many of the sorting criterions as is needed to determine the order – more complex to implement than LSD-RADIX-SORT ⇒the algorithm is not given here The efficiency of RADIX-SORT when using COUNTING-SORT: • sorting according to one criterion: Θ(n + k) • amount of different criteria is d ⇒total efficiency Θ(dn + dk) • k is usually constant ⇒total efficiency Θ(dn), or Θ(n), if d is also constant TIE-20106 113 RADIX-SORT appears to be a O(n) sorting algorithm with certain assumptions. Is is better than the comparison sorts in general? When analyzing the efficiency of sorting algorithms it makes sense to assume that all (or most) of the elements have different values. • For example INSERTION-SORT is O(n), if all elements are equal. • If the elements are all different and the size of value range of one criterion is constant k, kd ≥n ⇒d ≥logk n = Θ(lg n) ⇒RADIX-SORT is Θ(dn) = Θ(n lg n), if we assume that the element values are mostly different from each other. RADIX-SORT is asymptotically as slow as other good sorting algorithms. • By assuming a constant d, RADIX-SORT is Θ(n), but then with large values of n most elements are equal to eachother. TIE-20106 114 Advantages and disadvantages of RADIX-SORT Advantages: • RADIX-SORT is able to compete in efficiency with QUICKSORT for example – if the keys are 32-bit numbers and the array is sorted according to 8 bits at a time ⇒k = 28 and d = 4 ⇒COUNTING-SORT is called four times • RADIX-SORT is well suited for sorting according to keys with multiple parts when the parts of the key have a small value range. – e.g. sorting a text file according to the characters on the given columns (cmp. Unix or MS/DOS sort) Disadvantages: • COUNTING-SORT requires another array B of n elements where it builds the result and a temp array of k elements. ⇒It requires Θ(n) extra memory which is significantly larger than for example with QUICKSORT and HEAPSORT. TIE-20106 115 Bucket sort Let’s assume that the keys are within a known range of values and the key values are evenly distributed. • Each key is just as probable. • For the sake of an example we’ll assume that the key values are between zero and one. • Let’s use n buckets B[0] . . . B[n −1]. BUCKET-SORT(A) 1 n := A.length 2 for i := 1 to n do (go through the elements) 3 INSERT(B[⌊n · A[i]⌋], A[i]) (throw the element into the correct bucket) 4 k := 1 (start filling the array from index 1) 5 for i := 0 to n −1 do (go through the buckets) 6 while B[i] not empty do (empty non-empty buckets...) 7 A[k] := EXTRACT-MIN(B[i]) (... by moving the elements, smallest first...) 8 k := k + 1 (... into the correct location in the result array) TIE-20106 116 Implementation of the buckets: • Operations INSERT and EXTRACT-MIN are needed. ⇒The bucket is actually a priority queue. • The size of the buckets varies a lot. – usually the amount of elements in the bucket is ≈1 – however it is possible that every element end up in the same bucket ⇒an implementation that uses a heap would require Θ(n) for each bucket Θ(n2) in total • On the other hand, the implementation does not need to be very efficient for large buckets since they are rare. ⇒In practise the buckets should be implemented as lists. – INSERT links the incoming element to its correct location in the list, Θ(list length) time is used – EXTRACT-MIN removes and returns the first element in the list, Θ(1) time is used TIE-20106 117 the average efficiency of BUCKET-SORT: • We assumed the keys are evenly distributed. ⇒On average one element falls into each bucket and very rarely a significantly larger amount of elements fall into the same bucket. • The first for-loop runs through all of the elements, Θ(n). • The second for-loop runs through the buckets, Θ(n). • The while-loop runs through all of the elements in all of its iterations in total once, Θ(n). • INSERT is on average constant time, since there is on average one element in the bucket. • EXTRACT-MIN is constant time. ⇒The total running-time is Θ(n) on average. In the slowest case all elements fall into the same bucket in an ascending order. • INSERT takes a linear amount of time ⇒The total running-time is Θ(n2) in the worst-case. TIE-20106 118 4.6 How fast can we sort? Sorting an array actually creates the per- mutation of its elements where the origi- nal array is completely sorted. • If the elements are all different, the permutation is unique. ⇒Sorting searches for that permutation from the set of all possible permutations. 31 41 59 26 58 41 31 26 41 58 59 41 For example the functionality of INSERTION-SORT, MERGE-SORT, HEAPSORT and QUICKSORT is based on comparisons between the elements. • Information about the correct permutation is collected only by comparing the elements together. What would be the smallest amount of comparisons that is enough to find the correct permutation for sure? • An array of n elements of different values has 1 · 2 · 3 · . . . · n i.e. n! permutations. TIE-20106 119 • So many comparisons need to be made that the only correct alternative gets chosen from the set. • Each comparison A[i] ≤A[j] (or A[i] < A[j]) divides the permutations into two groups: those where the order of A[i] and A[j] must be switched and those where the order is correct so... – one comparison in enough to pick the right alternative from atmost two – two comparisons in enough to pick the right one from atmost four – . . . – k comparisons in enough to pick the right alternative from atmost 2k ⇒choosing the right one from x alternatives requires at least ⌈lg x⌉comparisons • If the size of the array is n, there are n! permutations ⇒At least ⌈lg n!⌉comparisons is required ⇒a comparison sort algorithm needs to use Ω(⌈lg n!⌉) time. TIE-20106 120 How large is ⌈lg n!⌉? • ⌈lg n!⌉≥lg n! = Pn k=1 lg k ≥ Pn k=⌈n 2⌉lg n 2 ≥n 2 · lg n 2 = 1 2n lg n −1 2n = Ω(n lg n) −Ω(n) = Ω(n lg n) • on the other hand ⌈lg n!⌉< n lg n + 1 = O(n lg n) ⇒⌈lg n!⌉= Θ(n lg n) Every comparison sort algorithm needs to use Ω(n lg n) time in the slowest case. • On the other hand HEAPSORT and MERGE-SORT are O(n lg n) in the slowest case. ⇒In the slowest case sorting based on comparisons between elements is possible in Θ(n lg n) time, but no faster. • HEAPSORT and MERGE-SORT have an optimal asymptotic running-time in the slowest case. • Sorting is for real asymptotically more time consuming than finding the median calue, which can be done in the slowest possible case in O(n). TIE-20106 121 4.7 Hash table The basic idea behind hash tables is to reduce the range of possible key values in a dynamic set by using a hash function h so that the keys can be stored in an array. • the advantatage of an array is the efficient, constant-time indexing it provides Reducing the range of the keys creates a problem: collisions. • more than one element can hash into the same slot in the hash table TIE-20106 122 The most common way to solve the problem is called chaining. • all the elements that hash to the same slot are put into a linked list • there are other alternatives – in open addressing the element is put into a secondary slot if the primary slot is unavailable – in some situations the range of key values is so small, that it doesn’t need to be reduced and therefore there are no collisions either – this direct-access table is very simple and efficient – this course covers hashing with chaining only TIE-20106 123 The picture below shows a chained hash table, whose keys have been hashed based on the first letter according the the table given. h(k) first letter 0 H P X 1 A I Q Y 2 B J R Z 3 C K S Ä 4 D L T Ö 5 E M U Å 6 F N V 7 G O W Is this a good hash? • No. Let’s see why. TIE-20106 124 The chained hash table provides the dictionary operations only, but those are very simple: CHAINED-HASH-SEARCH(T, k) ▷find the element with key k from the list T[h(k)] CHAINED-HASH-INSERT(T, x) ▷add x to the beginning of the list T[h(x→key)] CHAINED-HASH-DELETE(T, x) ▷remove x from the list T[h(x→key)] TIE-20106 125 Running-times: • addition: Θ(1) • search: worst-case Θ(n) • removal: if the list is doubly-linked Θ(1); with a singly linked list worst-case Θ(n), since the predecessor of the element under removal needs to be searched from the list – in practise the difference is not significant since usually the element to be removed needs to be searched from the list anyway The average running-times of the operations of a chained hash table depend on the lengths of the lists. TIE-20106 126 • in the worst-case all elements end up in the same list and the running-times are Θ(n) • to determine the average-case running time we’ll use the following: – m = size of the hash table – n = amount of elements in the table – α = n m = load factor i.e. the average length of the list • in addition, in order to evaluate the average-case efficiency an estimate on how well the hash function h hashes the elements is needed – if for example h(k) = the 3 highest bits in the name, all elements hash into the same list – it is often assumed that all elements are equally likely to hash into any of the slots – simple uniform hashing – we’ll also assume that evaluating h(k) is Θ(1) TIE-20106 127 • if an element that is not in the table is searched for, the entire list needs to be scanned through ⇒on average α elements need to be investigated ⇒the running-time is on average Θ(1 + α) • if we assume that any of the elements in the list is the key with the same likelyhood, on average half of the list needs to be searched through in the case where the key is found in the list ⇒the running-time is Θ(1 + α 2) = Θ(1 + α) on average • if the load factor is kept under some fixed constant (e.g. α < 50 %), then Θ(1 + α) = Θ(1) ⇒all operations of a chained hash table can be implemented in Θ(1) running-time on average – this requires that the size of the hash table is around the same as the amount of elements stored in the table TIE-20106 128 When evaluating the average-case running-time we assumed that the hash-function hashes evenly. However, it is in no way obvious that this actually happens. The quality of the hash function is the most critical factor in the efficiency of the hash table. Properties of a good hash function: • the hash function must be deterministic – otherwise an element once placed into the hash table may never be found! • despite this, it would be good that the hash function is as “random” as possible – 1 m of the keys should be hashed into each slot as closely as possible TIE-20106 129 • unfortunately implementing a completely evenly hashing hash function is most of the time impossible – the probability distribution of the keys is not usually known – the data is usually not evenly distributed * almost any sensible hash function hashes an evenly distributed data perfectly • Often the hash function is created so that it is independent of any patterns occuring in the input data, i.e. such patterns are broken by the function – for example, single letters are not investigated when hashing names but all the bits in the name are taken into account TIE-20106 130 • two methods for creating hash functions that usually behave well are introduced here • lets assume that the keys are natural numbers 0, 1, 2, . . . – if this is not the case the key can be interpreted as a natural number – e.g. a name can be converted into a number by calculating the ASCII-values of the letters and adding them together with appropriate weights TIE-20106 131 Creating hash functions with the division method is simple and fast. • h(k) = k mod m • it should only be used if the value of m is suitable • e.g. if m = 2b for some b ∈N = {0, 1, 2, . . .}, then h(k) = k’s b lowest bits ⇒the function doesn’t even take a look at all the bits in k ⇒the function probably hashes binary keys poorly TIE-20106 132 • for the same reason, values of m in the format m = 10b should be avoided with decimal keys • if the keys have been formed by interpreting a character string as a value in the 128-system, then m = 127 is a poor choise, as then all the permutations of the same string end up into the same slot • prime numbers are usually good choises for m, provided they are not close to a power of two – e.g. ≈700 lists is needed ⇒701 is OK • it’s worth checking with a small “real” input data set whether the function hashes efficiently TIE-20106 133 The multiplication method for creating hash functions doesn’t have large requirements for the values of m. • the constant A is chosen so that 0 < A < 1 • h(k) = ⌊m(kA −⌊kA⌋)⌋ • if m = 2b, the word length of the machine is w, and k and 2w · A fit into a single word, then h(k) can be calculated easily as follows: h(k) = ⌊(((2w · A) · k) mod 2w) 2w−b ⌋ • which value should be chosen for A? – all of the values of A work at least somehow – the rumor has it that A ≈ √ 5−1 2 often works quite well TIE-20106 134 4.8 B-trees B-trees are rapidly branching search trees that are designed for storing large dynamic sets on a disk • the goal is to keep the number of search/write operations as small as possible • all leaves have the same depth • one node fills one disk unit as closely as possible ⇒B-tree often branches rapidly: each node has tens, hundreds or thousands of children ⇒B-trees are very shallow in practise • the tree is kept balanced by alternating the amount of the node’s children between t, . . . , 2t for some t ∈N , t ≥2 – each internal node except the root always has at least 1 2 children from the maximum amount TIE-20106 135 The picture shows how the keys of a B-tree divide the search area. Searching in a B-tree is done in the same way as in an ordinary binary search tree. • travel from the root towards the leaves • in each node, choose the branch where the searched element must be in - there are just much more brances TIE-20106 136 Inserting an element into a B-tree • travel from the root to a leaf and on the way split each full node into half ⇒when a node is reached, its parent is not full • the new key is added into a leaf • if the root is split, a new root is created and the halves of the old root are made the children of the new root ⇒B-tree gets more height only by splitting roots • a single pass down the tree is needed and no passes upwards A node in a B-tree is split by making room for one more key in the parent and the median key in the node is then lifted into the parent. The rest of the keys are split around the median key into a node with the smaller keys and a node with the larger keys. TIE-20106 137 TIE-20106 138 Deleting a key from a B-tree is a similar operation to the addition. • travel from the root to a leaf and always before entering a node make sure there is at least the mimimun amount + 1 keys in it – this quarantees that the amount of the keys is kept legal although one is removed • once the searched key is found, it is deleted and if necessary the node in combined with either of its siblings – this can be done for sure, since the parent node has at least one extra key • if the root of the end result has only one child, the root is removed and the child is turned into the new root"
            },
            {
                "role": "user",
                "content": "Can you summarize the key topics covered in this lecture?"
            },
            {
                "role": "assistant",
                "content": "This lecture covers foundational algorithmic concepts including problem formulation, correctness proof using induction, efficiency analysis using asymptotic notation, data structures (arrays, linked lists, BSTs, heaps), and sorting methods (selection sort, merge sort, counting sort). Additionally, it introduces the Word-RAM computational model, providing a theoretical framework for measuring algorithm efficiency."
            }
        ]
    },
    {
        "system": "Hello! I am Kiran, an AI assistant from Tampere University, dedicated to providing you with accurate and insightful assistance. My goal is to help you understand concepts, summarize lectures, and answer questions effectively.",
        "conversations": [
            {
                "role": "host",
                "content": "Tree data structures and tree traversals 1. Background 2. Data structure 3. Traversals 1. Background Why does one need to know how to form and use a tree data structure? - STL does not have 'tree' in its containers. - A tree is useful for storing data that is hierarchic. University of Sciences and Applied Sciences Faculty of Engineering Faculty of Science Civil Mechanical IT Chemical Chemistry Biology Physics Geology Tree parts: nodes, edges, root, children, parents, leaves (external node), internal node, subtree Note: 1. All trees are assumed to be rooted trees. 2. Children and subtrees are ordered from left to right. Hence the leftmost child is the first child and the rightmost child is the last child. 2. Data structure Usually in a tree the nodes hold the information. We need a device (a data type) to hold node data. We will use a Node. In pseudocode we will use the following for general tree: • Node.key = the key of Node • Node.parent = pointer to parent of Node or NIL if root • Node.children = all children of Node • Node.children[i] = pointer to i'th child of Node, or NIL if child does not exist • The root node is root. In pseudocode we will use the following for binary tree: • Node.key = the key of Node • Node.parent = pointer to parent of Node or NIL if root • Node.left = pointer to left child or NIL if child does not exist • Node.right = pointer lto right child or NIL if child does not exist. • The root node is root. NOTE • individual node can be accessed by its key In C++ for general tree: In C++ for binary tree: NOTES - If node does not exist, then pointer value is nullptr. - Individual node is accessed by its key. - If a node can have many children, then a vector container is probably inefficient. Some other container should be used, e.g. underordered_set. - All tree nodes should be stored in a container. For example: 3. Traversals Q: What is a traversal? A: When all of the elements of a data structure are visited. Single dimensional array traversal: We consider three tree traversals: preorder traversal, postorder traversal and inorder traversal. These three differ in the order in which the nodes are visited. Preorder traversal First visit the parent and then visit its subtrees in order. Example Compute PRETRAVERSAL( ) Assume: operation in line 4 is outputting node key to user recursion level line computation PRETRAVERSAL( ) PRETRAVERSAL( ) PRETRAVERSAL( ) order of all output: 8, 9, 4, 5, 3, 7, 2, 6, 0, 1 Postorder traversal First visit the subtrees in order and then visit the parent. Example Compute POSTTRAVERSAL( ) recursion level line computation POSTTRAVERSAL( ) POSTTRAVERSAL( ) POSTTRAVERSAL( ) order of all output: 4, 5, 9, 3, 7, 0, 6, 1, 2, 8 Inorder traversal This order is only valid for binary trees. First the left subtree is visited, then the parent is visited, and finally the right subtree is visited. Example Compute INTRAVERSAL( ) recursion level line computation INTRAVERSAL( ) INTRAVERSAL( ) INTRAVERSAL( ) INTRAVERSAL( ) NOTES - PRETRAVERSAL, POSTTRAVERSAL and INTRAVERSAL are all recursive. - Preorder traversal is useful when some information about the parent should be available or relayed to the children. - Postorder traversal is useful when some information about the children should be available or relayed to the parent. - Inorder traversal is useful for sorting all nodes by theirs keys when using a binary search tree. order of all output: 4, 9, 3, 5, 7, 8, 0, 6, 2, 1 Tämä teos on lisensoitu Creative Commons Nimeä-EiKaupallinen- EiMuutoksia 4.0 Kansainvälinen -lisenssillä. Tarkastele lisenssiä osoitteessa http://creativecommons.org/licenses/by-nc-nd/4.0/. tekijä: Frank Cameron This work is licensed under the Creative Commons Attribution- NonCommercial-NoDerivatives 4.0 International License. To view a copy of this license, visit http://creativecommons.org/licenses/by-nc-nd/4.0/. made by Frank Cameron"
            },
            {
                "role": "user",
                "content": "Can you summarize the key topics covered in this lecture?"
            },
            {
                "role": "assistant",
                "content": "This lecture covers foundational algorithmic concepts including problem formulation, correctness proof using induction, efficiency analysis using asymptotic notation, data structures (arrays, linked lists, BSTs, heaps), and sorting methods (selection sort, merge sort, counting sort). Additionally, it introduces the Word-RAM computational model, providing a theoretical framework for measuring algorithm efficiency."
            }
        ]
    },
    {
        "system": "Hello! I am Kiran, an AI assistant from Tampere University, dedicated to providing you with accurate and insightful assistance. My goal is to help you understand concepts, summarize lectures, and answer questions effectively.",
        "conversations": [
            {
                "role": "host",
                "content": "Amortized performance and std::vector's memory management COMP.CS.300 Data structures and algorithms 1 Matti Rintala (matti.rintala@tuni.fi) STL vector memory management •Vector reserves a continuous memory block for its elements •What to do when it needs more space? 1 •Attempt 1: reserve new memory block with as much more space as you need, copy old elements there 1 2 1 2 3 1 2 3 4 1 2 3 4 5 STL vector memory management 1 1 2 1 2 3 1 2 3 4 1 2 3 4 5 •Vector reserves a continuous memory block for its elements •What to do when it needs more space? •Attempt 1: reserve new memory block with as much more space as you need, copy old elements there STL vector memory management •Vector reserves a continuous memory block for its elements •What to do when it needs more space? 1 •Attempt 2: reserve twice as large memory block, copy old elements there •Note: Some memory remains unused! 1 2 1 2 3 1 2 3 4 1 2 3 4 5 1 2 3 4 5 6 7 1 2 3 4 5 6 1 2 3 4 5 6 7 8 STL vector memory management 1 1 2 1 2 3 1 2 3 4 1 2 3 4 5 1 2 3 4 5 6 7 1 2 3 4 5 6 1 2 3 4 5 6 7 8 •Attempt 2: reserve twice as large memory block, copy old elements there •Note: Some memory remains unused! •Vector reserves a continuous memory block for its elements •What to do when it needs more space? Amortized tehokkuus •Counts average performance of operation sequences •Cost of expensive rare operation can be spread evenly on cheap operations •E.g., append an element to vector: – Individual insertion can be linear (gets rarer and rarer) – Insertions are still amortized constant time (on average) std::vector •STL vector contains operations for tweaking memory management •vec.reserve(n): Reserves memory at least for n elements, elements are still not (yet) added •vec.capacity(): Maximum number of elements without new memory allocation •vec.shrink_to_fit(): Move elements to memory block that is just the right size •(vec.erase() does not free memory!)"
            },
            {
                "role": "user",
                "content": "Can you summarize the key topics covered in this lecture?"
            },
            {
                "role": "assistant",
                "content": "This lecture covers foundational algorithmic concepts including problem formulation, correctness proof using induction, efficiency analysis using asymptotic notation, data structures (arrays, linked lists, BSTs, heaps), and sorting methods (selection sort, merge sort, counting sort). Additionally, it introduces the Word-RAM computational model, providing a theoretical framework for measuring algorithm efficiency."
            }
        ]
    },
    {
        "system": "Hello! I am Kiran, an AI assistant from Tampere University, dedicated to providing you with accurate and insightful assistance. My goal is to help you understand concepts, summarize lectures, and answer questions effectively.",
        "conversations": [
            {
                "role": "host",
                "content": "Weighted graphs and their implementations in C++ Q: How does a weighted graph differ from an ordinary graph? A: In a weighted graph each edge (x,y) has associated with it a weight w((x,y)), which is a number. Common interpretations of weights - graph represents a road map and nodes are locations or intersections on the map: w((x,y)) can be the distance from x to y or the time required to get from x to y - graph represents a project and the nodes are different tasks in the project: w((x,y)) is the time it takes to complete the task x - graph represents a pipe network under construction and the nodes represent junctions in the pipe network: w((x,y)) can be the cost of constructing the pipe from x to y Common use - find the shortest/fastest/cheapest path from some node to some other node - BFS assumes w((x,y)) = 1 for all edges Example Goal: shortest path from F to H. One BFS solution: <F, D, H > True solution: <F, A, C, B, H > Implementation 1 For each node y adjacent to x, store pair <y, w((x,y)) > struct Node { // All the data stored in the node int id; std::string name; // ... std::vector<std::pair<Node*,Cost>> to_neighbours; }; Suitable when - only need to move forward along edges - edges added or deleted infrequently - only data associated with edge is cost (weight) Implementation 2 struct Node { // All the data stored in the node int id; std::string name; // ... // ...map, not set! std::unordered_map<Node*,Cost> to_neighbours; }; Suitable when - only need to move forward along edges - edges added or deleted frequently - only data associated with edge is cost (weight) Implementation 3 struct Edge { int cost; string name; // ... }; struct Node { // All the data stored in the node int id; std::string name; // ... // ...map, not set! std::unordered_map<Node*,Edge> to_neighbours; }; Suitable when - only need to move forward along edges - edges added or deleted frequently - there is much data associated with edge Implementation 4 // In undirected graphs, egde data can be shared between directions struct Edge { int cost; string name; Hugedata data; // ... too much data or changing data }; struct Node { // All the data stored in the node int id; std::string name; // ... // ...map, not set! std::unordered_map<Node*,std::shared_ptr<Edge>> to_neighbours; }; Suitable when - only need to move forward along edges - edges added or deleted frequently - there is much data associated with edge - the graph is undirected and we do not wish to keep two copies of the same edge Tämä teos on lisensoitu Creative Commons Nimeä-EiKaupallinen- EiMuutoksia 4.0 Kansainvälinen -lisenssillä. Tarkastele lisenssiä osoitteessa http://creativecommons.org/licenses/by-nc-nd/4.0/. tekijä: Frank Cameron This work is licensed under the Creative Commons Attribution- NonCommercial-NoDerivatives 4.0 International License. To view a copy of this license, visit http://creativecommons.org/licenses/by-nc-nd/4.0/. made by Frank Cameron"
            },
            {
                "role": "user",
                "content": "Can you summarize the key topics covered in this lecture?"
            },
            {
                "role": "assistant",
                "content": "This lecture covers foundational algorithmic concepts including problem formulation, correctness proof using induction, efficiency analysis using asymptotic notation, data structures (arrays, linked lists, BSTs, heaps), and sorting methods (selection sort, merge sort, counting sort). Additionally, it introduces the Word-RAM computational model, providing a theoretical framework for measuring algorithm efficiency."
            }
        ]
    },
    {
        "system": "Hello! I am Kiran, an AI assistant from Tampere University, dedicated to providing you with accurate and insightful assistance. My goal is to help you understand concepts, summarize lectures, and answer questions effectively.",
        "conversations": [
            {
                "role": "host",
                "content": "COMP.CS.300 Data structures and algorithms 1, autumn 2022 Programming assignment 1: Railroads Last modified on 11/24/2022 Change log Below is a list of substantial changes to this document after its initial publication: • 11th November: removed the separate readme-document, it is no longer required. Contents Change log............................................................................................................................................1 Introduction to the assignment.............................................................................................................1 Terminology.....................................................................................................................................3 On sorting....................................................................................................................................4 About implementing the program and using C++......................................................................4 Structure and functionality of the program...........................................................................................4 Parts provided by the course.......................................................................................................4 Files mainprogram.hh, mainprogram.cc, mainwindow.hh, mainwindow.cc, mainwindow.ui ................................................................................................................................................4 File datastructures.hh..............................................................................................................5 File datastructures.cc..............................................................................................................5 On using the graphical user interface..........................................................................................6 Parts of the program to be implemented as the assignment........................................................6 Commands recognized by the program and the public interface of the Datastructures class.....6 \"Data files\".........................................................................................................................................12 example-stations.txt.......................................................................................................................12 example-regions.txt........................................................................................................................12 Screenshot of user interface................................................................................................................14 Example run........................................................................................................................................14 example-compulsory......................................................................................................................15 example-all.....................................................................................................................................16 Introduction to the assignment In this programming assignment you will practise implementing simple algorithms and evaluating their performance. This autumn’s programming assignment is about railroads, in other words railway stations and trains running between them. The goal of the first programming assignment is to create a program into which you can enter information about railway stations, train departures, and administrative regions surrounding the stations (municipalities, provinces, states, etc.). In the 1 second assignment the program will be extended to also include train connections between stations and doing route searches. Some operations in the assignment are compulsory, others are not. A compulsory operation is required to be implemented to pass the assignment, the non-compulsory parts are not required to pass the assignment, but they are still part of grading. In practice the assignment consists of implementing a given class, which stores the required information in its data structures, and whose methods implement the required functionality. The main program and the Qt-based graphical user interface are provided by the course. (Running the program in text-only mode is also possible). One goal in the assignment is to practise how to efficiently use ready-made data structures and algorithms (STL). Another goal is to write one’s own efficient algorithms and estimating their performance (of course it’s a good idea to favour STL’s ready-made algorithms/data structures when their can be justified by performance). In grading the assignment, both asymptotic and real-life performance (= sensible and efficient implementation aspects) are taken into account. “Micro optimizations” (like ”do I write a = a+b or a += b”, or ”how do I tweak compiler’s optimization flags”) will not improve the grade. The goal is to produce code that runs as efficiently as possible, under the assumption that all operations are executed equally often (unless specified otherwise on the command table). In many cases you’ll probably have to make compromises about the performance. In particular, note the following (some of these are new, some are repeated because of their importance): • The main program given by the course can be run either with a graphical user interface (when compiled with QtCreator/qmake), or as a textual command line program (when compiled just with g++ or some other C++ compiler). In both cases the basic functionality and students' implementation is exactly the same. • Hint: If the performance of any of your operations is worse than ϴ(nlog n) on average, the performance is definitely not ok. Most operations can be implemented much faster. This doesn’t mean that n log n would be a good performance for many operations. Especially for often called operations even linear performance is quite bad. • As part of the assignment, you should write your estimate of the asymptotic performance and a short rationale for each operation you implement. The file datastructures.hh contains a code comment for this before each operation. • Implementing operations all_subregions_in_region, stations_closest_to, remove_station, and common_parent_of_regions are not compulsory to pass the assignment. If you only implement the compulsory parts, the maximum grade for the assignment is 3. • If the implementation is bad enough, the assignment can be rejected. 2 • In judging runtime performance the essential thing is how the execution time changes with more data, not just the measured seconds. More points are given if operations are implemented with better performance. • More points will also be given for better real performance in seconds (if the required minimum asymptotic performance requirements are met). But points are only given for performance that comes from algorithmic choices and design of the program (for example setting compiler optimization flags, using concurrency or hacker optimizing individual C++ lines doesn't give extra points). • The performance of an operation includes all work done for the operation, also work possibly done during the addition of elements. Terminology Below is explanation for the most important terms in the assignment: • Station. Every station has a unique string id, a name, and a location (x,y), where x and y are integers (the scale and the origin (0,0) of the coordinate system are arbitrary, x coordinates grow to the right, y grows up). You can assume that two stations cannot have the same coordinate. • Region. Regions are arbitrary (administrative) regions on a map, defined by a polygon. Each region has a unique integer ID, a name, and a list of coordinates that describe the shape of the region. Each region can contain stations, and also an arbitrary number of (sub)regions, so that every region can belong to at most one “upper” region. An example of this could be a town that contains its stations, and is part of a province, which in turn is part of a state. The relationships between regions and stations are given to the program, i.e. the program doesn’t have to deduce them by analyzing regions’ coordinates. The region relationships cannot form cycles, i.e. region 1 cannot be a subregion of region 2, if region 2 is already a direct or indirect subregion of region 1. The assignment does not have to prepare for attempts to add cyclic regions in any way. On sorting Sorting names should by done using the default string class “<” comparison, which works because only characters a-z, A-Z, 0-9, space, and a dash - are allowed in names. Multiple equal names can be in any order with respect to each other. The operation stations_distance_increasing() requires the comparison of coordinates. The comparison is based on the “normal” euclidean distance from the origin √x 2+ y 2 (the coordinate closer to origin comes first). If the distance to the origin is the same, the coordinate with the smaller y-value comes first. Coordinates with with equal distances and y-values can be in any order with respect to each other. 3 In the non-compulsory operation stations_closest_to() stations are ordered based on their distance from a given position. For this operation the distance is the “normal” euclidean distance √(x1−x2) 2+( y1−y2) 2 , and again if the distances are equal, the coordinate with smaller y comes first. About implementing the program and using C++ The programming language in this assignment is C++17. The purpose of this programming assignment is to learn how to use ready-made data structures and algorithms, so using C++ STL is highly recommended and part of the grading. There are no restrictions in using the C++ standard library, including STL. Using libraries not part of the language is not allowed (for example libraries provided by Windows, Qt libraries, Boost, etc.). Please note however, that it's very likely you'll have to implement some algorithms completely by yourself. Structure and functionality of the program. Some parts of the program are provided by the course, some parts have to be implemented by the student. Parts provided by the course Files mainprogram.hh, mainprogram.cc, mainwindow.hh, mainwindow.cc, mainwindow.ui • You are NOT ALLOWED TO MAKE ANY CHANGES TO THESE FILES)! • The files contain the main program, which takes care of reading input, interpreting commands and printing out results. The main routine contains also commands for testing. • If you compile the program with QtCreator or qmake, you'll also get a graphical user interface, with an embedded command interpreter and buttons for pasting commands, file names, etc in addition to keyboard input. The graphical user interface also shows a visual representation of stations, their regions, results of operations, etc. File datastructures.hh • class Datastructures: The implementation of this class is where students write their code. The public interface of the class is provided by the course. You are NOT ALLOWED TO CHANGE THE PUBLIC INTERFACE (change names, return type or parameters of the given public member functions, etc., of course you are allowed to add new methods and data members to the private side). • Type definition StationID: Used as a unique identifier for each station (consists of characters A-Z, a-z, 0-9, and dash -). There can be several stations with the same name, but every station has a different id. 4 • Type definition Coord: Used in the public interface to represent (x,y) coordinates. As an example, some comparison operations (==, !=, <) and a hash function have been implemented for this type. • Type definition TrainID: Used as a unique identifier for each train (consists of characters A-Z, a-z, 0-9, and dash -). • Type definition RegionID: Used as a unique identifier for each region (it is a non- negative integer). There can be several regions with the same name, but every region has a different id. • Type definition NAME: Used for names of stations and regions (consists of characters A-Z, a-z, 0-9, space, and dash -). • Type definition Time: An integer, which represents a time of day in format HHMM (in 24 hour notation). The main program ensures that all entered times are valid. Note, that the format of the times makes it possible to simply use the < operator to compare two times. In the assignment you don't have to handle the changing of the day after midnight. • Constants NO_STATION, NO_REGION, NO_TRAIN, NO_NAME, NO_COORD, NO_TYPE, and NO_DISTANCE: Used as error codes, if information is requested for a station or region that doesn't exist. File datastructures.cc • Here you write the code for the your operations. • Function random_in_range: Returns a random value in given range (start and end of the range are included in the range). You can use this function if your implementation requires random numbers. On using the graphical user interface When compiled with QtCreator, a graphical user interface is provided. It allows running operations and test scripts, and visualizing the data. In assignment 1 the UI has some disabled (greyed out) controls needed for assignment 2. The UI has a command line interface, which accepts commands described later in this document. In addition to this, the UI shows graphically created stations and regions (if you have implemented necessary operations, see below). The graphical view can be scrolled and zoomed. Clicking on a station name (or region border, which requires precision) prints out its information, and also inserts the ID on the command line (a handy way to give commands ID parameters). The user interface has selections for what to show graphically. Note! The graphical representation gets all its information from the student's code! It's not a depiction of what the \"right\" result is, but what information students' code gives out. The UI 5 uses the operation all_stations() to get a list of stations, and asks their information with get_...() operations. If the checkbox for drawing regions is on, regions are obtained with the operation all_regions(), and the coordinates of each region with get_region_coords(). Parts of the program to be implemented as the assignment Files datastructure.hpp and datastructure.cpp • class Datastructures: The given public member functions of the class have to be implemented. You can add your own stuff into the class (new data members, new member functions, etc.) • In file datastructures.hh, for each member function you implement, write an estimation of the asymptotic performance of the operation (with a short rationale for your estimate) as a comment above the member function declaration. Note! The code implemented by the student does not print out any output related to the expected functionality, the main program does that. If you want to do debug-output while testing, use the cerr stream instead of cout (or qDebug, if you you use Qt), so that debug output does not interfere with the tests. Commands recognized by the program and the public interface of the Datastructures class When the program is run, it awaits for commands explained below. The commands, whose explanation mentions a member function, call the respective member function of the Datastructure class (which the student must implement). Some commands are completely implemented by the code provided by the course. If the program is given a file as a command line parameter, the program executes commands from that file and then quits. A program compiled with QtCreator can also be started from the command prompt in a purely textual form with command line argument --console. The operations below are listed in the order in which we recommend them to be implemented (of course you should first design the class taking into account all operations). Command Public member function (Optional parameters of commands are in []-brackets and alternatives separated by |) Explanation station_count int station_count() Returns the number of stations currently in the data structure. 6 Command Public member function (Optional parameters of commands are in []-brackets and alternatives separated by |) Explanation clear_all void clear_all() Clears out the data structures (after this all_stations() and all_regions() return empty vectors). This operation is not included in the default performance tests. all_stations std::vector<StationID> all_stations() Returns all the stations in any (arbitrary) order (the main routine sorts them based on their ID). add_station ID \"Name\" (x,y) bool add_station(StationID id, Name const& name, Coord xy) Adds a station to the data structure with the given unique id, name, type, and coordinates. If there already is a station with the given id, nothing is done and false is returned, otherwise true is returned. station_info ID Name get_station_name(StationID id) Returns the name and type of the station with the given ID, or NO_NAME if such a station doesn't exist. (The main program calls this in various stations.) This operation is called more often than others (it is also called by the perftest command with parameter “all” or “compulsory”). station_info ID Coord get_station_coord(StationID id) Returns the name of the station with the given ID, or value NO_COORD, if such a station doesn't exist. (The main program calls this in various stations.) This operation is called more often than others (it is also called by the perftest command with parameter “all” or “compulsory”). (The operations below should probably be implemented only after the ones above have been implemented.) stations_alphabetically std::vector<StationID> stations_alphabetically() Returns station IDs sorted according to alphabetical order of station names. Stations with the same name can be in any order with respect to each other. stations_distance_increasing std::vector<StationID> stations_distance_increasing() Returns station IDs sorted according to their coordinates (defined earlier in this document). find_station_with_coord (x,y) StationID find_station_with_coord(Coord xy) Returns a station with the given coordinate, or NO_STATION, if no such station exists. change_station_coord ID (x,y) bool change_station_coord(StationID id, Coord newcoord) Changes the location of the station with given ID. If such station doesn't exist, returns false, otherwise true. 7 Command Public member function (Optional parameters of commands are in []-brackets and alternatives separated by |) Explanation add_departure StationID TrainID Time bool add_departure(StationID stationid, TrainID trainid, Time time) Adds information that the given train leaves from the given station at the given time. If such station doesn't exist or the departure has already been added (train already leaves from given station at the given time), returns false, otherwise true. remove_departure StationID TrainID Time bool remove_departure(StationID stationid, TrainID trainid, Time time) Removes the given train departure from the given station at the given time. If such a station or such departure from the station doesn't exist, returns false, otherwise true. station_departures_after StationID Time std::vector<std::pair<Time, TrainID>> station_departures_after(StationI D stationid, Time time) Lists all train departures from the given station at or after the given time. Departures should be sorted based on departure time. If trains have the same departure time, they should be sorted based on their ids. If the given station doesn't exist, pair {NO_TIME, NO_TRAIN} is returned. (The operations below should probably be implemented only after the ones above have been implemented.) add_region ID Name Coord1 Coord2... bool add_region(RegionID id, Name const& name, std::vector<Coord> coords) Adds a region to the data structure with given unique id, name and polygon (coordinates). Initially the added region is not a subregion of any region, and it doesn’t contain any subregions or stations. If there already is a region with the given id, nothing is done and false is returned, otherwise true is returned. all_regions std::vector<RegionID> all_regions() Returns all the regions in any (arbitrary) order (the main routine sorts them based on their ID). This operation is not included in the default performance tests. region_info ID Name get_region_name(RegionID id) Returns the name of the region with the given ID, or NO_NAME if such region doesn't exist. (Main program calls this in various stations.) This operation is called more often than others (it is also called by the perftest command with parameter “all” or “compulsory”). region_info ID std::vector<Coord> get_region_coords(RegionID id) Returns the coordinate vector of the region with the given ID, or a vector with single item NO_COORD, if such region doesn't exist. (The main program calls this in various stations.) This operation is not part of performance tests. 8 Command Public member function (Optional parameters of commands are in []-brackets and alternatives separated by |) Explanation add_subregion_to_region RegionID RegionID bool add_subregion_to_region(RegionID id, RegionID parentid) Adds the first given region as a subregion to the second region. If no regions exist with the given IDs, or if the first region is already a subregion of some region, nothing is done and false is returned, otherwise true is returned. add_station_to_region StationID RegionID bool add_station_to_region(StationID id, RegionID parentid) Adds the given station to the given region. If no station or region exist with the given IDs, or if the station already belongs to some region, nothing is done and false is returned, otherwise true is returned. station_in_regions StationID std::vector<RegionID> station_in_regions(StationID id) Returns a list of regions to which the given station belongs either directly or indirectly. The returned vector first contains the region to which the given station belongs directly, then the region that this region belongs to, etc. If no station with the given ID exists, a vector with a single element NO_REGION is returned. (Implementing the following operations is not compulsory, but they improve the grade of the assignment.) all_subregions_of_region RegionID std::vector<RegionID> all_subregions_of_region(RegionID id) Returns a list of regions which belong either directly or indirectly to the given region. The order of regions in the returned vector can be arbitrary (the main program sorts them in increasing ID order). If no region with the given ID exists, a vector with a single element NO_REGION is returned. stations_closest_to Coord std::vector<StationID> stations_closest_to(Coord xy) Returns the three stations closest to the given coordinate in order of increasing distance (based on the ordering of coordinates described earlier). If there are less than three stations in total, of course less stations are returned. Implementing this command is not compulsory (but is taken into account in the grading of the assignment). remove_station ID bool remove_station(StationID id) Removes the station with the given id. If a station with the given id does not exist, does nothing and returns false, otherwise returns true. Implementing this command is not compulsory (but is taken into account in the grading of the assignment). 9 Command Public member function (Optional parameters of commands are in []-brackets and alternatives separated by |) Explanation common_parent_of_regions RegionID RegionID RegionID common_parent_of_regions(RegionID id1, RegionID id2) Returns the “nearest” region in the subregion hierarchy, to which both given regions belong either directly or indirectly. The returned region must be the \"nearest\" in the following sense: it does not have a subregion containing both given regions. If either of the region ids do not correspond to any region, or if no common region exists, returns NO_REGION. (The following operations are already implemented by the main program.) random_add n (implemented by main program) Add n new stations and n/10 regions (for testing) with random ids, names, and coordinates. The added regions are added to random regions as subregions, and stations to regions with 50 % probability. Note! The values really are random, so they can be different for each run. Also all the coordinates are random, so stations and subregions are probably outside their parent region etc. random_seed n (implemented by main program) Sets a new seed to the main program's random number generator. By default the generator is initialized to a different value each time the program is run, i.e. random data is different from one run to another. By setting the seed you can get the random data to stay same between runs (can be useful in debugging). read ”filename” [silent] (implemented by main program) Reads and executes more commands from the given file. If optional parameter ‘silent’ is given, outputs of the commands are not displayed. (This can be used to read a list of stations from a file, run tests, etc.) stopwatch on|off|next (implemented by main program) Switch time measurement on or off. When the program starts, the stopwatch is \"off\". When it is turned \"on\", the time it takes to execute each command is printed after the command. The option \"next\" switches the measurement on only for the next command (handy with command \"read\" to measure the total time of a command file). 10 Command Public member function (Optional parameters of commands are in []-brackets and alternatives separated by |) Explanation perftest all|compulsory|cmd1[;cmd2...] timeout repeat n1[;n2...] (implemented by main program) Runs performance tests. First the command clears out the data structure and adds n1 random stations and regions (see random_add). After this a random command is performed repeat times. The time for adding elements and running commands is measured and printed out. Then the same is repeated for n2 elements, etc. If any test round takes more than timeout seconds, the test is interrupted (this is not necessarily a failure, just an arbitrary time limit). If the first parameter of the command is all, commands are selected from all commands. If it is compulsory, random commands are selected from the list of operations that have to be implemented. If the parameter is a list of commands, commands are selected from that list (in this case it's a good idea to include also random_add so that elements are also added during the test loop, not just in the beginning). If the program is run from the graphical user interface, the \"stop test\" button can be used to interrupt the performance test (it may take a while for the program to react to the button). testread \"in-filename\" \"out-filename\" (implemented by main program) Runs a correctness test and compares the results. This command reads command from file in-filename and shows the output of the commands next to the expected output in file out-filename. Each line with differences is marked with a question mark. Finally the last line tells whether there are any differences. help (implemented by main program) Prints out a list of known commands. quit (implemented by main program) Quit the program. (If this is read from a file, stops processing that file.) \"Data files\" The easiest way to test the program is to create \"data files\", which can add a bunch of stations, trains and regions. Those files can then be read in using the \"read\" command, after which other commands can be tested without having to enter the data every time by hand. What follows are examples of such data files: 11 example-stations.txt add_station kuo \"kuopio\" (945,767) add_station tpe \"tampere\" (542,455) add_station kli \"kolari\" (579,1758) add_station tus \"turku satama\" (366,219) add_station roi \"rovaniemi\" (740,1569) example-regions.txt add_region 6440429 \"tampereen seutukunta\" (442,495) (535,586) (729,518) (597,396) (442,495) add_station_to_region tpe 6440429 add_region 2528474 \"rovaniemi\" (656,1714) (737,1500) (848,1525) (823,1641) (656,1714) add_region 1724359 \"lappi\" (327,2139) (1020,2232) (1006,1566) (556,1525) (327,2139) add_station_to_region roi 2528474 add_subregion_to_region 2528474 1724359 add_station_to_region kli 1724359 add_region 54224 \"suomi - finland\" (23,189) (188,827) (598,1300) (590,1641) (567,1894) (415,2187) (672,2026) (906,2369) (960,2023) (1030,1664) (1111,1219) (1164,958) (1308,666) (143,0) (23,189) add_subregion_to_region 6440429 54224 add_subregion_to_region 1724359 54224 add_station_to_region kuo 54224 12 Screenshot of user interface Below is a screenshot of the graphical user interface after example-stations.txt and example- regions.txt have been read in. Example run Below are example outputs from the program. The example's commands can be found in files example-compulsory-in.txt and example-all-in.txt, and the outputs in files example-compulsory- out.txt and example-all-out.txt. If you wish to perform a small test on all the compulsory commands, you can do this by running the following: testread \"example-compulsory-in.txt\" \"example-compulsory-out.txt\" 13 example-compulsory > clear_all Cleared all stations > # Stations > station_count Number of stations: 0 > read \"example-stations.txt\" silent ** Commands from 'example-stations.txt' ...(output discarded in silent mode)... ** End of commands from 'example-stations.txt' > station_count Number of stations: 5 > station_info tpe Station: tampere: pos=(542,455), id=tpe > station_info kli Station: kolari: pos=(579,1758), id=kli > stations_alphabetically Stations: 1. kolari: pos=(579,1758), id=kli 2. kuopio: pos=(945,767), id=kuo 3. rovaniemi: pos=(740,1569), id=roi 4. tampere: pos=(542,455), id=tpe 5. turku satama: pos=(366,219), id=tus > stations_distance_increasing Stations: 1. turku satama: pos=(366,219), id=tus 2. tampere: pos=(542,455), id=tpe 3. kuopio: pos=(945,767), id=kuo 4. rovaniemi: pos=(740,1569), id=roi 5. kolari: pos=(579,1758), id=kli > change_station_coord tpe (600,500) Station: tampere: pos=(600,500), id=tpe > find_station_with_coord (600,500) Station: tampere: pos=(600,500), id=tpe > add_departure tpe ic20 1000 Train ic20 leaves from station tampere (tpe) at 1000 > add_departure tpe ic22 1200 Train ic22 leaves from station tampere (tpe) at 1200 > add_departure kli pyo276 1942 Train pyo276 leaves from station kolari (kli) at 1942 > station_departures_after tpe 1100 Departures from station tampere (tpe) after 1100: ic22 at 1200 > station_departures_after kli 1900 Departures from station kolari (kli) after 1900: pyo276 at 1942 > remove_departure kli pyo276 1942 Removed departure of train pyo276 from station kolari (kli) at 1942 > station_departures_after kli 1900 14 No departures from station kolari (kli) after 1900 > # Regions > read \"example-regions.txt\" silent ** Commands from 'example-regions.txt' ...(output discarded in silent mode)... ** End of commands from 'example-regions.txt' > all_regions Regions: 1. suomi - finland: id=54224 2. lappi: id=1724359 3. rovaniemi: id=2528474 4. tampereen seutukunta: id=6440429 > region_info 6440429 Region: tampereen seutukunta: id=6440429 > station_in_regions kuo Station: kuopio: pos=(945,767), id=kuo Region: suomi - finland: id=54224 > station_in_regions kli Station: kolari: pos=(579,1758), id=kli Regions: 1. lappi: id=1724359 2. suomi - finland: id=54224 example-all > # First read in compulsory example > read \"example-compulsory-in.txt\" ** Commands from 'example-compulsory-in.txt' ... ** End of commands from 'example-compulsory-in.txt' > all_subregions_of_region 54224 Regions: 1. suomi - finland: id=54224 2. lappi: id=1724359 3. rovaniemi: id=2528474 4. tampereen seutukunta: id=6440429 > stations_closest_to (500,400) Stations: 1. tampere: pos=(600,500), id=tpe 2. turku satama: pos=(366,219), id=tus 3. kuopio: pos=(945,767), id=kuo > common_parent_of_regions 2528474 6440429 Regions: 1. rovaniemi: id=2528474 2. tampereen seutukunta: id=6440429 3. suomi - finland: id=54224 > remove_station tpe tampere removed. > stations_alphabetically Stations: 15 1. kolari: pos=(579,1758), id=kli 2. kuopio: pos=(945,767), id=kuo 3. rovaniemi: pos=(740,1569), id=roi 4. turku satama: pos=(366,219), id=tus > stations_distance_increasing Stations: 1. turku satama: pos=(366,219), id=tus 2. kuopio: pos=(945,767), id=kuo 3. rovaniemi: pos=(740,1569), id=roi 4. kolari: pos=(579,1758), id=kli > find_station_with_coord (600,600) Failed (NO_STATION returned)! > stations_closest_to (500,400) Stations: 1. turku satama: pos=(366,219), id=tus 2. kuopio: pos=(945,767), id=kuo 3. rovaniemi: pos=(740,1569), id=roi 16"
            },
            {
                "role": "user",
                "content": "Can you summarize the key topics covered in this lecture?"
            },
            {
                "role": "assistant",
                "content": "This lecture covers foundational algorithmic concepts including problem formulation, correctness proof using induction, efficiency analysis using asymptotic notation, data structures (arrays, linked lists, BSTs, heaps), and sorting methods (selection sort, merge sort, counting sort). Additionally, it introduces the Word-RAM computational model, providing a theoretical framework for measuring algorithm efficiency."
            }
        ]
    },
    {
        "system": "Hello! I am Kiran, an AI assistant from Tampere University, dedicated to providing you with accurate and insightful assistance. My goal is to help you understand concepts, summarize lectures, and answer questions effectively.",
        "conversations": [
            {
                "role": "host",
                "content": "COMP.CS.300 Tietorakenteet ja algoritmit 1, syksy 2022 Harjoitustyö 1: Rautatiet Viimeksi päivitetty 24.11.2022 Muutoshistoria Alla ohjeeseen tehdyt merkittävät muutokset julkaisun jälkeen: • 11.11.: Poistettu vaatimus erillisesti readme-dokumentista. Sisällys Muutoshistoria......................................................................................................................................1 Harjoitustyön esittely............................................................................................................................1 Terminologiaa..................................................................................................................................3 Järjestämisestä.............................................................................................................................3 Harjoitustyön toteuttamisesta ja C++:n käytöstä........................................................................4 Ohjelman toiminta ja rakenne...............................................................................................................4 Valmiit osat, jotka tarjotaan kurssin puolesta..............................................................................4 Tiedostot mainprogram.hh, mainprogram.cc, mainwindow.hh, mainwindow.cc, mainwindow.ui.......................................................................................................................4 Tiedosto datastructures.hh.....................................................................................................4 Tiedosto datastructures.cc......................................................................................................5 Graafisen käyttöliittymän käytöstä.............................................................................................5 Harjoitustyönä toteutettavat osat.................................................................................................6 Ohjelman tuntemat komennot ja luokan julkinen rajapinta........................................................6 \"Datatiedostot\"....................................................................................................................................12 example-stations.txt.......................................................................................................................12 example-regions.txt........................................................................................................................12 Kuvakaappaus käyttöliittymästä.........................................................................................................13 Esimerkki ohjelman toiminnasta........................................................................................................13 example-compulsory......................................................................................................................13 example-all.....................................................................................................................................15 Harjoitustyön esittely Tämän syksyn harjoitustyö liittyy rautateihin, eli rautatieasemiin, niiden sijaintiin ja niiden välillä kulkeviin juniin. Ensimmäisessä harjoitustyössä tehdään ohjelma, johon pystyy syöttämään tietoa rautatieasemista, junalähdöistä ja alueista, joihin asemat kuuluvat (kunnat, seutukunnat, maakunnat 1 jne.). Toisessa harjoitustyössä ohjelmaa laajennetaan käsittelemään asemien välisiä junayhteyksiä ja tekemään niihin liittyviä reittihakuja. Osa harjoitustyön operaatioista on pakollisia, osa vapaaehtoisia (pakollinen = vaaditaan läpipääsyyn, vapaaehtoinen = ei-pakollinen, mutta silti osa arvostelua arvosanan kannalta). Käytännössä harjoitustyönä koodataan luokka, joka tallettaa tietorakenteisiinsa tarvitut tiedot ja jonka metodit suorittavat tarvitut operaatiot. Kurssin puolesta tulee luokan käyttöön tarvittava pääohjelma ja graafinen Qt-käyttöliittymä (myös pelkkä tekstipohjainen käyttö on mahdollista). Harjoitustyössä harjoitellaan valmiiden tietorakenteiden ja algoritmien tehokasta käyttöä (STL), mutta siinä harjoitellaan myös omien algoritmien tehokasta toteuttamista ja niiden tehokkuuden arvioimista (kannattaa tietysti suosia STL:ää omien algoritmien/tietorakenteiden sijaan silloin, kun se on tehokkuuden kannalta järkevää ). Toisin sanoen arvostelussa otetaan huomioon valintojen asymptoottinen tehokkuus, mutta sen lisäksi myös ohjelman yleinen tehokkuus (= järkevät ja tehokkaat toteutusratkaisut). ”Mikro-optimoinnista” (tyyliin kirjoitanko ”a = a+b;” vai ”a += b;” tai kääntäjän optimointivipujen säätäminen) ei saa pisteitä. Tavoitteena on tehdä mahdollisimman tehokas toteutus, kun oletetaan että kaikki ohjelman tuntemat komennot ovat suunnilleen yhtä yleisiä (ellei komentotaulukossa toisin mainita). Usein tehokkuuden kannalta joutuu tekemään joissain tilanteissa kompromisseja. Huomaa erityisesti seuraavat asiat: • Valmiina annetun pääohjelman voi ajaa joko graafisen käyttöliittymän kanssa QtCreatorilla/qmakella käännettynä, tai tekstipohjaisena pelkällä g++:lla käännettynä. Itse ohjelman toiminnallisuus ja opiskelijan toteuttama osa on täsmälleen sama molemmissa tapauksissa. • Vihje tehokkuudesta: Jos minkään operaation keskimääräinen tehokkuus on huonompi kuin ϴ(nlog n) , ratkaisun tehokkuus ei ole hyvä. Suurin osa operaatioista on mahdollista toteuttaa paljon nopeamminkin. Tämä ei tarkoita sitä, että n log n olisi hyvä tehokkuus monelle operaatiolle. Erityisesti usein kutsutut operaatiot kannattaa pyrkiä toteuttamaan nopeiksi, niille lineaarinen tehokkuuskin on varsin huono. • Osana ohjelman palautusta tiedostoon datastructures.hh on jokaisen operaation oheen laitettu kommentti, johon lisätään oma arvio kunkin toteutetun operaation asymptoottisesta tehokkuudesta lyhyiden perusteluiden kera. • Operaatioiden all_subregions_in_region, stations_closest_to, remove_station ja common_parent_of_regions toteuttaminen ei ole pakollista läpipääsyn kannalta. Ne ovat kuitenkin osa arvostelua. Vain pakolliset osat toteuttamalla harjoitustyön maksimiarvosana on 3. • Riittävän huonolla toteutuksella työ voidaan hylätä. 2 • Tehokkuudessa olennaista on erityisesti, miten ohjelman tehokkuus muuttuu datan kasvaessa, eivät pelkät sekuntimäärät. Plussaa tietysti saa, jos operaatioita saa toteutettua mahdollisimman tehokkaasti. • Samoin plussaa saa, mitä nopeammaksi operaatiot saa sekunteinakin mitattuna (jos siis kertaluokka on vähintään vaadittu). Mutta plussaa saa vain tehokkuudesta, joka syntyy omista algoritmivalinnoista ja suunnittelusta. (Esim. kääntäjän optimointivipujen vääntely, rinnakkaisuuden käyttö, häkkerioptimoinnilla kellojaksojen viilaaminen eivät tuo pisteitä.) • Operaation tehokkuuteen lasketaan kaikki siihen liittyvä työ, myös mahdollisesti alkioiden lisäyksen yhteydessä tehty operaation hyväksi liittyvä työ. Terminologiaa Menossa olevan englanninkielisen sisarkurssin vuoksi ohjelman käyttöliittymä ja rajapinta ovat englanniksi. Tässä selitys tärkeimmistä harjoitustyön termeistä (lista täydentyy harjoitustyössä 2): • Station = asema. Asemalla on yksilöivä merkkijono-ID, nimi sekä sijainti (x,y), jossa x ja y ovat kokonaislukuja (mittakaava ja koordinaatiston origo on mielivaltainen, x-koordinaatti kasvaa oikealle ja y-koordinaatti ylös). Harjoitustyössä saa olettaa, että samassa koordinaatissa ei voi olla kahta asemaa. • Region = (hallinnollinen) alue. Alueet ovat mielivaltaisia monikulmion rajoittamia alueita kartalla. Jokaisella alueella on yksilöivä kokonaisluku-ID, nimi sekä lista koordinaatteja, jotka kuvaavat alueen muodon. Alueeseen voi kuulua asemia ja myös toisia (ali-)alueita, niin että jokainen alue voi kuulua korkeintaan yhteen ”ylempään” alueeseen. Esimerkkinä voisi olla kunta, joka sisältää kunnan asemat ja joka kuuluu seutukuntaan, joka kuuluu johonkin maakuntaan jne. Alueiden ja asemien suhteet annetaan erikseen, ts. niitä ei tarvitse päätellä monikulmioiden sisäkkäisyyksistä. Aluesuhteet eivät voi muodostaa syklejä, ts. alue 1 ei voi olla alueen 2 alialue, jos alue 2 on jo alueen 1 suora tai epäsuora alialue. Harjoitustyön ei tarvitse huomioida tällaista mahdollisuutta. Järjestämisestä Nimien järjestämisessä voi käyttää suoraan string-luokan ”<”-vertailua, joka toimii koska nimissä ei sallita kuin kirjaimet a-z, A-Z, 0-9, sanaväli ja väliviiva -. Samannimisten nimien keskinäinen järjestys voi olla mikä tahansa. Operaatio stations_distance_increasing() vaatii koordinaattien vertailua. Tällöin vertaillaan ensisijaisesti koordinaatin ”normaalia” euklidista etäisyyttä origosta √x 2+ y 2 (lähempänä origoa oleva koordinaatti tulee ensin). Jos etäisyys origosta on sama, tulee ensin koordinaatti, jonka y-koordinaatti on pienempi. Koordinaattien, joiden etäisyys ja y-koordinaatti on sama, keskinäinen järjestys voi olla mikä tahansa. 3 Vastaavasti vapaaehtoisessa operaatiossa stations_closest_to() järjestetään asemat niiden annetusta sijainnista etäisyyden perusteella. Silloin etäisyys on ”normaali” kahden pisteen välinen euklidinen etäisyys, ts. √(x1−x2) 2+( y1−y2) 2 , ja jälleen jos etäisyys on sama, tulee ensin koordinaatti, jonka y-koordinaatti on pienempi. Harjoitustyön toteuttamisesta ja C++:n käytöstä Harjoitustyön kielenä on C++17. Tämän harjoitustyön tarkoituksena on opetella valmiiden tietorakenteiden ja algoritmien käyttöä, joten C++:n STL:n käyttö on erittäin suotavaa ja osa arvostelua. Mitään erityisiä rajoituksia C++:n standardikirjaston käytössä ei ole. Kielen ulkopuolisten kirjastojen käyttö ei ole sallittua (esim. Windowsin omat kirjastot, Qt:n kirjastot, Boost tms.). Huomaa kuitenkin, että jotkut operaatiot joudut todennäköisesti toteuttamaan myös kokonaan itse. Ohjelman toiminta ja rakenne Osa ohjelmasta tulee valmiina kurssin puolesta, osa toteutetaan itse. Valmiit osat, jotka tarjotaan kurssin puolesta Tiedostot mainprogram.hh, mainprogram.cc, mainwindow.hh, mainwindow.cc, mainwindow.ui • Näihin tiedostoihin EI SAA TEHDÄ MITÄÄN MUUTOKSIA! • Pääohjelma, joka hoitaa syötteiden lukemisen, komentojen tulkitsemisen ja tulostusten tulostamisen. Pääohjelmassa on myös valmiina komentoja testaamista varten. • QtCreatorilla tai qmakella käännettäessä graafinen käyttöliittymä, jonka \"komentotulkkiin\" voi näppäimistön lisäksi hiirellä lisätä komentoja, tiedostoja yms. Graafinen käyttöliittymä näyttää myös luodut asemat ja alueet graafisesti samoin kuin suoritettujen operaatioiden tulokset. Tiedosto datastructures.hh • class Datastructures: Luokka, johon harjoitustyö kirjoitetaan. Luokasta annetaan valmiina sen julkinen rajapinta (johon EI SAA TEHDÄ MITÄÄN MUUTOKSIA, luonnollisesti luokkaan saa private-puolelle lisätä omia jäsenmuuttujia ja -funktioita). • Tyyppimäärittely StationID: Käytetään aseman yksilöivänä tunnisteena (koostuu merkeistä A-Z, a-z, 0-9 ja väliviiva -). Samannimisiä asemia voi olla monta, mutta jokaisella on eri id. 4 • Tyyppimäärittely Coord: Käytetään rajapinnassa (x,y)-koordinaattien esitykseen. Tälle tyypille on esimerkinomaisesti valmiiksi määritelty vertailuoperaatiot ==, != ja < sekä hajautusfunktio. • Tyyppimäärittely TrainID: käytetään junan yksilöivänä tunnisteena (koostuu merkeistä A-Z, a-z, 0-9 ja väliviiva -). • Tyyppimäärittely RegionID: Ei-negatiivinen kokonaisluku, jota käytetään alueen yksilöivänä tunnisteena. Samannimisiä alueita voi olla monta, mutta jokaisella on eri id. • Tyyppimäärittely Name: Käytetään asemien ja alueiden nimenä (koostuu merkeistä A-Z, a- z, 0-9, sanaväli ja väliviiva -). • Tyyppimäärittely Time: Kokonaisluku, joka kuvaa kellonaikaa muodossa HHMM. Pääohjelma hoitaa syötettyjen kellonaikojen oikeellisuuden tarkastelun. Huomaa, että kellonajan esitysmuodosta johtuen kellonaikojen vertailu onnistuu yksinkertaisesti operaattorilla <. Harjoitustyössä ei tarvitse huomioida tilanteita, joissa vuorokausi vaihtuu. • Vakiot NO_STATION, NO_REGION, NO_TRAIN, NO_NAME, NO_COORD, NO_TYPE ja NO_DISTANCE: Käytetään virhekoodeina, jos tietoja kysytään asemasta tai alueesta, jota ei ole olemassa. Tiedosto datastructures.cc • Tänne luonnollisesti kirjoitetaan luokan operaatioiden toteutukset. • Funktio random_in_range: Arpoo luvun annetulla välillä (alku- ja loppuarvo ovat molemmat välissä mukana). Voit käyttää tätä funktiota, jos tarvitset toteutuksessasi satunnaislukuja. Graafisen käyttöliittymän käytöstä QtCreatorilla käännettäessä harjoitustyön valmis koodi tarjoaa graafisen käyttöliittymän, jolla ohjelmaa voi testata ja ajaa valmiita testejä sekä visualisoida ohjelman toimintaa. Ensimmäisen harjoitustyön käyttöliittymässä on ei-aktiivisena (harmaana) mukana myös muutama toisen harjoitustyön tarvitsema asetus. Käyttöliittymässä on komentotulkki, jolle voi antaa myöhemmin kuvattuja komentoja, jotka kutsuvat opiskelijan toteuttamia operaatioita. Käyttöliittymä näyttää myös luodut asemat ja alueet graafisesti (jos olet toteuttanut tarvittavat operaatiot, ks. alla). Graafista näkymää voi vierittää ja sen skaalausta voi muuttaa. Asemien nimien (ja alueiden ääriviivojen, joihin osuminen voi olla hankalaa) klikkaaminen hiirellä tulostaa sen tiedot ja tuottaa sen ID:n komentoriville (kätevä tapa syöttää komentojen parametreja). Käyttöliittymästä voi myös valita, mitä graafisessa näkymässä näytetään. 5 Huom! Käyttöliittymän graafinen esitys kysyy kaikki tiedot opiskelijoiden koodista! Se ei siis ole \"oikea\" lopputulos vaan graafinen esitys siitä, mitä tietoja opiskelijoiden koodi antaa. Jos asemien piirto on päällä, käyttöliittymä hakee kaikki asemat operaatiolla all_stations() ja kysyy asemien tiedot operaatioilla get_...(). Jos alueiden piirtäminen on päällä, ne kysytään operaatiolla all_regions(), ja alueen koordinaatit operaatiolla get_region_coords(). Harjoitustyönä toteutettavat osat Tiedostot datastructures.hh ja datastructures.cc • class Datastructures: Luokan julkisen rajapinnan jäsenfunktiot tulee toteuttaa. Luokkaan saa lisätä omia määrittelyitä (jäsenmuuttujat, uudet jäsenfunktiot yms.) • Tiedostoon datastructures.hh kirjoitetaan jokaisen toteutetun operaation yläpuolelle kommentteihin oma arvio ko. operaation toteutuksen asymptoottisesti tehokkuudesta ja lyhyt perustelu arviolle. Huom! Omassa koodissa ei ole tarpeen tehdä ohjelman varsinaiseen toimintaan liittyviä tulostuksia, koska pääohjelma hoitaa ne. Mahdolliset Debug-tulostukset kannattaa tehdä cerr-virtaan (tai qDebug:lla, jos käytät Qt:ta), jotta ne eivät sotke testejä. Ohjelman tuntemat komennot ja luokan julkinen rajapinta Kun ohjelma käynnistetään, se jää odottamaan komentoja, jotka on selitetty alla. Komennot, joiden yhteydessä mainitaan jäsenfunktio, kutsuvat ko. Datastructure-luokan operaatioita, jotka siis opiskelijat toteuttavat. Osa komennoista on taas toteutettu kokonaan kurssin puolesta pääohjelmassa. Jos ohjelmalle antaa komentoriviltä tiedoston parametriksi, se lukee komennot ko. tiedostosta ja lopettaa sen jälkeen. QtCreatorillakin käännetyn ohjelman voi käynnistää komentoriviltä tekstimuotoisena antamalla komentoriviparametrin --console. Alla operaatiot on listattu siinä järjestyksessä, kun ne suositellaan toteutettavaksi (tietysti suunnittelu kannattaa tehdä kaikki operaatiot huomioon ottaen jo alun alkaen). Komento Julkinen jäsenfunktio (Komennoissa ei-pakolliset parametrit hakasuluissa ja vaihtoehdot erotettu |-merkillä.) Selitys station_count int station_count() Palauttaa tietorakenteessa olevien asemien lukumäärän. 6 Komento Julkinen jäsenfunktio (Komennoissa ei-pakolliset parametrit hakasuluissa ja vaihtoehdot erotettu |-merkillä.) Selitys clear_all void clear_all() Tyhjentää tietorakenteet (tämän jälkeen all_stations() ja all_regions() palauttavat tyhjän vektorin). Tämä operaatio ei ole mukana tehokkuustesteissä. all_stations std::vector<StationID> all_stations() Palauttaa kaikki tietorakenteessa olevat asemat mielivaltaisessa järjestyksessä, ts. järjestyksellä ei ole väliä (pääohjelma järjestää ne id:n mukaan). add_station ID \"Name\" (x,y) bool add_station(StationID id, Name const& name, Coord xy) Lisää tietorakenteeseen uuden aseman annetulla uniikilla id:llä, nimellä ja sijainnilla. Jos annetulla id:llä on jo asema, ei tehdä mitään ja palautetaan false, muuten palautetaan true. station_info ID Name get_station_name(StationID id) Palauttaa annetulla ID:llä olevan aseman nimen tai NO_NAME, jos id:llä ei löydy asemaa. (Pääohjelma kutsuu tätä eri paikoissa.) Tätä operaatiota kutsutaan useammin kuin muita (myös perftest-komennossa parametrilla ”all” tai ”compulsory”). station_info ID Coord get_station_coord(StationID id) Palauttaa annetulla ID:llä olevan aseman sijainnin tai arvon NO_COORD, jos id:llä ei löydy asemaa. (Pääohjelma kutsuu tätä eri paikoissa.) Tätä operaatiota kutsutaan useammin kuin muita (myös perftest-komennossa parametrilla ”all” tai ”compulsory”). (Allaolevat kannattaa toteuttaa todennäköisesti vasta, kun ylläolevat on toteutettu.) stations_alphabetically std::vector<StationID> stations_alphabetically() Palauttaa id:t asemien nimen mukaan aakkosjärjestyksessä. Keskenään samannimiset asemat saavat olla missä järjestyksessä tahansa. stations_distance_increasing std::vector<StationID> stations_distance_increasing() Palauttaa id:t asemien koordinaattien mukaan järjestettynä (koordinaattien järjestys on määritelty aiemmin tässä dokumentissa). find_station_with_coord (x,y) StationID find_station_with_coord(Coord xy) Palauttaa aseman, joka on annetussa koordinaatissa tai NO_STATION, jos sellaista ei ole. change_station_coord ID (x,y) bool change_station_coord(StationID id, Coord newcoord) Muuttaa annetulla ID:llä olevan aseman sijainnin. Jos asemaa ei löydy, palauttaa false, muuten true. 7 Komento Julkinen jäsenfunktio (Komennoissa ei-pakolliset parametrit hakasuluissa ja vaihtoehdot erotettu |-merkillä.) Selitys add_departure StationID TrainID Time bool add_departure(StationID stationid, TrainID trainid, Time time) Lisää tiedon siitä, että annetulta asemalta lähtee annettu juna annetulla kellonajalla. Jos asemaa ei ole, tai jos lähtö on jo lisätty (annettu juna lähtee jo annettuun aikaan annetulta asemalta) palautetaan false, muuten true. remove_departure StationID TrainID Time bool remove_departure(StationID stationid, TrainID trainid, Time time) Poistaa annetulta asemalta annetun junan lähdön annetulla kellonajalla. Jos asemaa tai junaa annetulla lähtöajalla asemalta ei löydy, palautetaa false, muuten true. station_departures_after StationID Time std::vector<std::pair<Time, TrainID>> station_departures_after(StationI D stationid, Time time) Listaa kaikki lähdöt annetulta asemalta annettuna kellonaikana tai sen jälkeen. Lähdöt listataan kellonaikajärjestyksessä, ja samalla kellonlyömällä lähtevät juna-id:n mukaan. Jos annettua asemaa ei löydy, palautetaan {NO_TIME, NO_TRAIN}. (Allaolevat kannattaa toteuttaa todennäköisesti vasta, kun ylläolevat on toteutettu.) add_region ID Name Coord1 Coord2... bool add_region(RegionID id, Name const& name, std::vector<Coord> coords) Lisää tietorakenteeseen uuden alueen annetulla uniikilla id:llä, nimellä ja monikulmiolla (koordinaatit). Aluksi alueeseen ei kuulu toisia alueita tai asemia, eikä alue ole minkään alueen alialue. Jos annetulla id:llä on jo alue, ei tehdä mitään ja palautetaan false, muuten palautetaan true. all_regions std::vector<RegionID> all_regions() Palauttaa kaikki tietorakenteessa olevat alueet mielivaltaisessa järjestyksessä, ts. järjestyksellä ei ole väliä (pääohjelma järjestää ne id:n mukaan). Tämä operaatio ei ole oletuksena mukana tehokkuustesteissä. region_info ID Name get_region_name(RegionID id) Palauttaa annetulla ID:llä olevan alueen nimen, tai NO_NAME, jos id:llä ei löydy aluetta. (Pääohjelma kutsuu tätä eri paikoissa.) Tätä operaatiota kutsutaan useammin kuin muita (myös perftest-komennossa parametrilla ”all” tai ”compulsory”). region_info ID std::vector<Coord> get_region_coords(RegionID id) Palauttaa annetulla ID:llä olevan alueen koordinaattivektorin, tai vektorin, jonka ainoa alkio on NO_COORD, jos id:llä ei löydy aluetta. (Pääohjelma kutsuu tätä eri paikoissa.) Tämä operaatio ei ole mukana tehokkuustesteissä. 8 Komento Julkinen jäsenfunktio (Komennoissa ei-pakolliset parametrit hakasuluissa ja vaihtoehdot erotettu |-merkillä.) Selitys add_subregion_to_region RegionID RegionID bool add_subregion_to_region(RegionID id, RegionID parentid) Lisää ensimmäisen alueen alialueeksi toiseen alueeseen. Jos annetuilla id:illä ei löydy alueita, tai jos annettu alialue kuuluu jo johonkin alueeseen, ei tehdä mitään ja palautetaan false, muuten palautetaan true. add_station_to_region StationID RegionID bool add_statj ion_to_region(StationID id, RegionID parentid) Lisää aseman annettuun alueeseen. Jos annetuilla id:illä ei löydy asemaa tai aluetta, tai jos annettu asema kuuluu jo johonkin alueeseen, ei tehdä mitään ja palautetaan false, muuten palautetaan true. station_in_regions StationID std::vector<RegionID> station_in_regions(StationID id) Palauttaa kaikki alueet, joihin annettu asema kuuluu suoraan tai epäsuorasti. Paluuarvossa on ensin alue, johon annettu asema kuuluu suoraan, sitten alue, johon tämä alue kuuluu jne. Jos asema ei kuulu mihinkään alueeseen, palautetaan tyhjä vektori. Jos id:llä ei ole asemaa, palautetaan vektori, jonka ainoa alkio on NO_REGION. (Seuraavien operaatioiden toteuttaminen ei ole pakollista, mutta ne parantavat arvosanaa.) all_subregions_of_region RegionID std::vector<RegionID> all_subregions_of_region(RegionID id) Palauttaa kaikki alueet, jotka kuuluvat annettuun alueeseen suoraan tai epäsuorasti alialueina. Alueiden järjestys paluuarvossa saa olla mikä tahansa (pääohjelma järjestää ne ID:n mukaan). Jos annettuun alueeseen ei kuulu muita alueita, palautetaan tyhjä vektori. Jos id:llä ei ole aluetta, palautetaan vektori, jonka ainoa alkio on NO_REGION. stations_closest_to Coord std::vector<StationID> stations_closest_to(Coord xy) Palauttaa etäisyysjärjestyksessä kolme annettua koordinaattia lähinnä olevaa asemaa. Jos sopivia asemia ei ole kolmea, palautetaan luonnollisesti vähemmän asemia. Tämän komennon toteutus ei ole pakollinen (mutta se vaikuttaa arvosteluun). remove_station ID bool remove_station(StationID id) Poistaa annetulla id:llä olevan aseman. Jos id ei vastaa mitään asemaa, ei tehdä mitään ja palautetaan false, muuten palautetaan true. Tämän operaation toteuttaminen ei ole pakollista (mutta otetaan huomioon arvostelussa). 9 Komento Julkinen jäsenfunktio (Komennoissa ei-pakolliset parametrit hakasuluissa ja vaihtoehdot erotettu |-merkillä.) Selitys common_parent_of_regions RegionID RegionID RegionID common_parent_of_regions(RegionID id1, RegionID id2) Palauttaa ”lähimmän” alueen aluehierarkiassa, jonka alialueita molemmat annetut alueet ovat suoraan tai epäsuorasti. \"Lähimmän\" tarkoittaa, että alueet eivät enää kuulu yhdessä mihinkään palautetun alueen alialueeseen. Jos jompikumpi id ei vastaa mitään aluetta, tai yhteistä ylempää aluetta ei löydy, palautetaan NO_REGION. (Seuraavat komennot on toteutettu valmiiksi pääohjelmassa.) random_add n (pääohjelman toteuttama) Lisää tietorakenteeseen (testausta varten) n kpl asemia ja n/10 kpl alueita, joilla on satunnainen id, nimi ja sijainti. Lisätyt alueet lisätään alialueina satunnaisiin alueisiin ja asemat 50 % todennäköisyydellä johonkin alueeseen. Huom! Arvot ovat tosiaan satunnaisia, eli saattavat olla kerrasta toiseen eri. Myös koordinaatit ovat satunnaisia, eli alialueet ja asemat sijaitsevat todennäköisesti \"isäntäalueensa\" ulkopuolella yms. random_seed n (pääohjelman toteuttama) Asettaa pääohjelman satunnaislukugeneraattorille uuden siemenarvon. Oletuksena generaattori alustetaan joka kerta eri arvoon, eli satunnainen data on eri ajokerroilla erilaista. Siemenarvon asettamalla arvotun datan saa toistumaan samanlaisena kerrasta toiseen (voi olla hyödyllistä debuggaamisessa). read \"tiedostonimi\" [silent] (pääohjelman toteuttama) Lukee lisää komentoja annetusta tiedostosta. Jos parametri ’silent’ annetaan, luettujen komentojen tulostusta ei näytetä. (Tällä voi esim. lukea tiedostossa olevilla komennoilla asioita tietorakenteeseen, ajaa valmiita testejä yms.) stopwatch on|off|next (pääohjelman toteuttama) Aloittaa tai lopettaa komentojen ajanmittauksen. Ohjelman alussa mittaus on pois päältä (\"off\"). Kun mittaus on päällä (\"on\"), tulostetaan jokaisen komennon jälkeen siihen kulunut aika. Vaihtoehto \"next\" kytkee mittauksen päälle vain seuraavan komennon ajaksi (kätevää read-komennon kanssa, kun halutaan mitata vain komentotiedoston kokonaisaika). 10 Komento Julkinen jäsenfunktio (Komennoissa ei-pakolliset parametrit hakasuluissa ja vaihtoehdot erotettu |-merkillä.) Selitys perftest all|compulsory|cmd1[;cmd2...] timeout repeat n1[;n2...] (pääohjelman toteuttama) Ajaa ohjelmalle tehokkuustestit. Tyhjentää tietorakenteen ja lisää sinne n1 kpl satunnaisia asemia ja alueita (ks. random_add). Sen jälkeen arpoo repeat kertaa satunnaisen komennon. Mittaa ja tulostaa sekä lisäämiseen että komentoihin menneen ajan. Sen jälkeen sama toistetaan n2:lle jne. Jos jonkin testikierroksen suoritusaika ylittää timeout sekuntia, keskeytetään testien ajaminen (tämä ei välttämättä ole mikään ongelma, vaan mielivaltainen aikaraja). Jos ensimmäinen parametri on all, arvotaan lisäyksen jälkeen kaikista komennoista, joita on ilmoitettu kutsuttavan usein. Jos se on compulsory, testataan vain komentoja, jotka on pakko toteuttaa. Jos parametri on lista komentoja, arvotaan komento näiden joukosta (tällöin kannattaa mukaan ottaa myös random_add, jotta lisäyksiä tulee myös testikierroksen aikana). Jos ohjelmaa ajaa graafisella käyttöliittymällä, \"stop test\" nappia painamalla testi keskeytetään (nappiin reagointi voi kestää hetken). testread \"in-tiedostonimi\" \"out- tiedostonimi\" (pääohjelman toteuttama) Ajaa toiminnallisuustestin ja vertailee tulostuksia. Lukee komennot tiedostosta in-tiedostonimi ja näyttää ohjelman tulostuksen rinnakkain tiedoston out- tiedostonimi sisällön kanssa. Jokainen eroava rivi merkitään kysymysmerkillä, ja lopuksi tulostetaan vielä tieto, oliko eroavia rivejä. help (pääohjelman toteuttama) Tulostaa listan tunnetuista komennoista. quit (pääohjelman toteuttama) Lopettaa ohjelman. (Tiedostosta luettaessa lopettaa vain ko. tiedoston lukemisen.) \"Datatiedostot\" Kätevin tapa testata ohjelmaa on luoda \"datatiedostoja\", jotka add-komennolla lisäävät joukon asemia, alueita ja junia ohjelmaan. Tiedot voi sitten kätevästi lukea sisään tiedostosta read- komennolla ja sitten kokeilla muita komentoja ilman, että tiedot täytyisi joka kerta syöttää sisään käsin. Alla on esimerkki datatiedostoista: 11 example-stations.txt add_station kuo \"kuopio\" (945,767) add_station tpe \"tampere\" (542,455) add_station kli \"kolari\" (579,1758) add_station tus \"turku satama\" (366,219) add_station roi \"rovaniemi\" (740,1569) example-regions.txt add_region 6440429 \"tampereen seutukunta\" (442,495) (535,586) (729,518) (597,396) (442,495) add_station_to_region tpe 6440429 add_region 2528474 \"rovaniemi\" (656,1714) (737,1500) (848,1525) (823,1641) (656,1714) add_region 1724359 \"lappi\" (327,2139) (1020,2232) (1006,1566) (556,1525) (327,2139) add_station_to_region roi 2528474 add_subregion_to_region 2528474 1724359 add_station_to_region kli 1724359 add_region 54224 \"suomi - finland\" (23,189) (188,827) (598,1300) (590,1641) (567,1894) (415,2187) (672,2026) (906,2369) (960,2023) (1030,1664) (1111,1219) (1164,958) (1308,666) (143,0) (23,189) add_subregion_to_region 6440429 54224 add_subregion_to_region 1724359 54224 add_station_to_region kuo 54224 Kuvakaappaus käyttöliittymästä Alla vielä kuvakaappaus käyttöliittymästä sen jälkeen, kun example-stations.txt~example- regions.txt on luettu sisään. 12 Esimerkki ohjelman toiminnasta Alla on esimerkki ohjelman toiminnasta. Esimerkin syötteet löytyvät tiedostoista example- compulsory-in.txt ja example-all-in.txt, tulostukset tiedostoista example-compulsory-out.txt ja example-all-out.txt. Eli esimerkkiä voi käyttää pienenä testinä pakollisten toimintojen toimimisesta antamalla käyttöliittymästä komennon testread \"example-compulsory-in.txt\" \"example-compulsory-out.txt\" example-compulsory > clear_all Cleared all stations > # Stations > station_count Number of stations: 0 > read \"example-stations.txt\" silent ** Commands from 'example-stations.txt' ...(output discarded in silent mode)... ** End of commands from 'example-stations.txt' > station_count Number of stations: 5 > station_info tpe Station: tampere: pos=(542,455), id=tpe > station_info kli Station: kolari: pos=(579,1758), id=kli > stations_alphabetically Stations: 1. kolari: pos=(579,1758), id=kli 2. kuopio: pos=(945,767), id=kuo 3. rovaniemi: pos=(740,1569), id=roi 4. tampere: pos=(542,455), id=tpe 5. turku satama: pos=(366,219), id=tus > stations_distance_increasing Stations: 1. turku satama: pos=(366,219), id=tus 2. tampere: pos=(542,455), id=tpe 3. kuopio: pos=(945,767), id=kuo 4. rovaniemi: pos=(740,1569), id=roi 5. kolari: pos=(579,1758), id=kli > change_station_coord tpe (600,500) Station: tampere: pos=(600,500), id=tpe > find_station_with_coord (600,500) Station: tampere: pos=(600,500), id=tpe > add_departure tpe ic20 1000 Train ic20 leaves from station tampere (tpe) at 1000 > add_departure tpe ic22 1200 Train ic22 leaves from station tampere (tpe) at 1200 > add_departure kli pyo276 1942 13 Train pyo276 leaves from station kolari (kli) at 1942 > station_departures_after tpe 1100 Departures from station tampere (tpe) after 1100: ic22 at 1200 > station_departures_after kli 1900 Departures from station kolari (kli) after 1900: pyo276 at 1942 > remove_departure kli pyo276 1942 Removed departure of train pyo276 from station kolari (kli) at 1942 > station_departures_after kli 1900 No departures from station kolari (kli) after 1900 > # Regions > read \"example-regions.txt\" silent ** Commands from 'example-regions.txt' ...(output discarded in silent mode)... ** End of commands from 'example-regions.txt' > all_regions Regions: 1. suomi - finland: id=54224 2. lappi: id=1724359 3. rovaniemi: id=2528474 4. tampereen seutukunta: id=6440429 > region_info 6440429 Region: tampereen seutukunta: id=6440429 > station_in_regions kuo Station: kuopio: pos=(945,767), id=kuo Region: suomi - finland: id=54224 > station_in_regions kli Station: kolari: pos=(579,1758), id=kli Regions: 1. lappi: id=1724359 2. suomi - finland: id=54224 example-all > # First read in compulsory example > read \"example-compulsory-in.txt\" ** Commands from 'example-compulsory-in.txt' ... ** End of commands from 'example-compulsory-in.txt' > all_subregions_of_region 54224 Regions: 1. suomi - finland: id=54224 2. lappi: id=1724359 3. rovaniemi: id=2528474 4. tampereen seutukunta: id=6440429 > stations_closest_to (500,400) Stations: 1. tampere: pos=(600,500), id=tpe 2. turku satama: pos=(366,219), id=tus 14 3. kuopio: pos=(945,767), id=kuo > common_parent_of_regions 2528474 6440429 Regions: 1. rovaniemi: id=2528474 2. tampereen seutukunta: id=6440429 3. suomi - finland: id=54224 > remove_station tpe tampere removed. > stations_alphabetically Stations: 1. kolari: pos=(579,1758), id=kli 2. kuopio: pos=(945,767), id=kuo 3. rovaniemi: pos=(740,1569), id=roi 4. turku satama: pos=(366,219), id=tus > stations_distance_increasing Stations: 1. turku satama: pos=(366,219), id=tus 2. kuopio: pos=(945,767), id=kuo 3. rovaniemi: pos=(740,1569), id=roi 4. kolari: pos=(579,1758), id=kli > find_station_with_coord (600,600) Failed (NO_STATION returned)! > stations_closest_to (500,400) Stations: 1. turku satama: pos=(366,219), id=tus 2. kuopio: pos=(945,767), id=kuo 3. rovaniemi: pos=(740,1569), id=roi 15"
            },
            {
                "role": "user",
                "content": "Can you summarize the key topics covered in this lecture?"
            },
            {
                "role": "assistant",
                "content": "This lecture covers foundational algorithmic concepts including problem formulation, correctness proof using induction, efficiency analysis using asymptotic notation, data structures (arrays, linked lists, BSTs, heaps), and sorting methods (selection sort, merge sort, counting sort). Additionally, it introduces the Word-RAM computational model, providing a theoretical framework for measuring algorithm efficiency."
            }
        ]
    },
    {
        "system": "Hello! I am Kiran, an AI assistant from Tampere University, dedicated to providing you with accurate and insightful assistance. My goal is to help you understand concepts, summarize lectures, and answer questions effectively.",
        "conversations": [
            {
                "role": "host",
                "content": "COMP.CS.300 Data structures and algorithms 1, autumn 2022 Programming assignment 2: Railroads Last modified on 11/21/2022 Changelog Below is a list of substantial changes to this document after its initial publication: • 11/17/2022: Added a note to next_stations_from saying that the returned stations can be in arbitrary order. Contents Changelog.............................................................................................................................................1 Topic of the assignment........................................................................................................................1 Terminology.....................................................................................................................................2 Structure and functionality of the program...........................................................................................2 Parts provided by the course.......................................................................................................2 On using the graphical user interface..........................................................................................3 Commands recognized by the program and the public interface of the Datastructures class.....3 \"Data files\"...........................................................................................................................................7 Screenshot of user interface..................................................................................................................8 Example run..........................................................................................................................................8 example-compulsory........................................................................................................................8 example-all.......................................................................................................................................9 Topic of the assignment In the second programming assignment the first programming assignment will be extended to also include whole train connections and route searches related to them. Some operations in the assignment are compulsory, others are not A compulsory operation is one that the student must do to pass assignment 2. Non-compulsory operations will still affect the student's grade.. This assignment 2 document only describes new assignment 2 features. The idea is to copy the assignment 1 implementation as the starting point of assignment 2, and continue from there. All assignment 1 features and commands of the main program are also available in assignment 2, even if they are not repeated in this document. However, operations graded in assignment 1 are not graded again in assignment 2, and they don't affect the grade from assignment. 1 Terminology The most important new terms: • Train. (Describes the path of a single train through stations) Every train has a unique string id, and a list of stations through which the train runs, as well as departure times from each station (and the arrival time to the final station). • Route. A route is a sequence of stations from the origin station to the goal station using train connections. The length of a route is calculated (to minimize rounding errors) in the following way: the distance from a station to the next station ( √(x1−x2) 2+( y1−y2) 2 ) is first rounded down to an integer, and then these added together. • Cycle. A route has a cycle, if the route arrives again to a station through which it has already passed. In addition to the specifications given in programming assignment 1, the following must be taken into account: • In this assignment you cannot necessarily have much choice in the asymptotic performance of the new operations, because that's dictated by the typical algorithms. For this reason the implementation of the algorithms and correct behaviour are a more important grading criteria than asymptotic performance alone. • Implementing operations route_least_stations, route_shortest_distance, route_with_cycle, and route_earliest_arrival are not compulsory to pass assignment 2. If you only implement the compulsory parts, the maximum grade for assignment 2 is 2. • If the implementation is bad enough, the assignment can be rejected. Structure and functionality of the program. Part of the program code is provided by the course, part has to be implemented by students. Parts provided by the course The following files are provided to the students: mainprogram.hh, mainprogram.cc, mainwindow.hh, mainwindow.cc, mainwindow.ui. You are NOT ALLOWED TO MAKE ANY CHANGES TO THESE FILES. File datastructures.hh • class Datastructures: The implementation of this class is where students write their code. The public interface of the class is provided by the course. You are NOT ALLOWED TO CHANGE THE PUBLIC INTERFACE. That is, you are now allowed to change the 2 names, return types or parameters of the given public member functions, etc. Of course you are still allowed to add new methods and data members to the private side. • Type definition TrainID (consists of characters A-Z, a-z, 0-9, and dash -), which used as a unique identifier for each train. • Constant NO_TRAIN, which is used as a return value in some operations, if the operation fails. File datastructures.cc • Here you write the code for the your operations. On using the graphical user interface When compiled with QtCreator, a graphical user interface is provided. It allows running operations and test scripts, and visualizing the data. Note! The graphical representation gets all its information from student code! It's not a depiction of what the \"right\" result is, but what output a student's code provides. The UI uses operation all_stations() to get a list of stations, and asks their information with get_...() operations. If the drawing feature of regions is on, they are obtained with operation all_regions(), and the coordinates of each region with get_region_coords(). If the drawing feature of trains is on, they are obtained by calling operation next_stations_from() for each station. Commands recognized by the program and the public interface of the Datastructures class When the program is run, it waits for the user to input one or more of the commands given in the table below. The commands, whose explanation mentions a member function, call the respective member function of the Datastructure class (implemented by students). Some commands are completely implemented by the code provided by the course. If the program is given a file as a command line parameter, the program executes commands from that file and then quits. The operations below are listed in the order in which we recommend them to be implemented (of course you should first design the class taking into account all operations). 3 Command Public member function (In commands optional parameters are in []-brackets and alternatives separated by |) Explanation (All programming assignment 1 commands and operations are also available.) (And they do the same thing as in programming assignment 1.) clear_trains void clear_trains() Clears out all trains, but doesn’t touch stations or regions. This operation is not included in the performance tests. add_train ID Station1:Time1 CStation2:Time2... StationN:TimeN bool add_train(TrainID trainid, std::vector<std::pair<StationID, Time> stationtimes) Adds a train to the data structure with given unique id. The train runs through the given stations and departs from them at given times. The time of the last station is the arrival time to the final station (only needed for the route_earliest_arrival command). The departure times are also added to the station's info so that the assignment 1 operation station_departures_after shows them. If there already is a train with the given id or some station id is not found, nothing is done and false is returned, otherwise true is returned. next_stations_from ID std::vector<StationID> next_stations_from(StationID id) Returns the stations that are immediate next stations on trains running through the given station. If no trains leave from the station, an empty vector is returned. If a station with given id doesn't exist, a vector with single element NO_STATION is returned. The main program sorts the result based on the id, so the stations can be returned in arbitrary order. (Main program calls this in various places.) train_stations_from StationID TrainID std::vector<StationID> train_stations_from(StationID stationid, TrainID trainid) Returns a list of stations, through which the given train runs after leaving the given station. If there is no station or train with the given id, or the train does not leave from the given station, a vector with single element NO_STATION is returned. (The operations below should probably be implemented only after the ones above have been implemented.) 4 Command Public member function (In commands optional parameters are in []-brackets and alternatives separated by |) Explanation route_any StationID1 StationID2 std::vector<std::pair<StationID, Distance>> route_any(StationID fromid, StationID toid) Returns any (arbitrary) route between the given stations. The vector returned contains pairs StationID & Distance. The first pair contains the departure station and a distance of 0. Subsequent pairs come in order along the route, where the distance is always the total distance from the departure station. The final pair contains the destination station and the total distance of the trip. If no route can be found between the stations, an empty vector is returned. If either of the station ids is not found, a vector with one element {NO_STATION, NO_DISTANCE} is returned. (Implementing the following operations is not compulsory, but they improve the grade of the assignment.) route_least_stations StationID1 StationID2 std::vector<std::pair<StationID, Distance>> route_least_stations(StationID fromid, StationID toid) Returns a route between the given stations so that it contains as few stations as possible. If several routes exist with equally few stations, any of them can be returned. The vector returned contains pairs StationID & Distance, whose contents are described in operation route_any. If no route can be found between the stations, an empty vector is returned. If either of the station ids is not found, a vector with one element {NO_STATION, NO_DISTANCE} is returned. route_with_cycle StationID std::vector<StationID> route_with_cycle(StationID fromid) Returns a route starting from the given station such that the route has a single cycle, i.e. the route returns to a station it has already passed through. If several routes with a cycle exist, any of them can be returned. The returned vector first has the starting station, and then the rest of the stations along the route. The vector's final element should be the repeated station which causes the cycle. If no cyclic route can be found, an empty vector is returned. If the station id is not found, a vector with one element NO_STATION is returned. 5 Command Public member function (In commands optional parameters are in []-brackets and alternatives separated by |) Explanation route_shortest_distance StationID1 StationID2 std::vector<std::pair<StationID, Distance>> route_shortest_distance(StationID fromid, StationID toid) Returns a route between the given stations so that its length is as small as possible. If several equally short routes exist, any of them can be returned. The vector returned contains pairs StationID & Distance, whose contents are described in operation route_any. If no route can be found between the stations, an empty vector is returned. If either of the station ids is not found, a vector with one element {NO_STATION, NO_DISTANCE} is returned. route_earliest_arrival StationID1 StationID2 StartTime std::vector<std::pair<StationID, Time>> route_earliest_arrival(StationID fromid, StationID toid, Time starttime) Note! This is the most challenging non-compulsory operation! (Tip: try to come up with a suitable cost function.) Returns a route between the given stations that arrives to the destination as early as possible. If several routes with the same arrival time exist, any of them can be returned. The vector returned contains pairs StationID & Time. The first pair contains the departure station and the departure time from it. Subsequent pairs contain the stations along the route, in order and the departure times from each station. The final pair contains the destination station and the train's arrival time to it. If no route can be found between the stations, an empty vector is returned. If either of the station ids is not found, a vector with one element {NO_STATION, NO_TIME} is returned. Note! The operation doesn't have to find routes where the day changes during the route. (The following operations are already implemented by the main program.) (Here only changes to phase 1 are mentioned.) random_trains n (implemented by main program) Add n new trains running between random stations. Note! The values really are random, so they can be different for each run, and they don’t in any way form a sensible “map”. 6 Command Public member function (In commands optional parameters are in []-brackets and alternatives separated by |) Explanation perftest all|compulsory|cmd1[;cmd2...] timeout repeat n1[;n2...] (implemented by main program) Run performance tests. Clears out the data structure and add n1 random stations, regions, and trains. Then a random command is performed repeat times. The time for adding elements and running commands is measured and printed out. Then the same is repeated for n2 elements, etc. If any test round takes more than timeout seconds, the test is interrupted (this is not necessarily a failure, just arbitrary time limit). If the first parameter of the command is all, commands are selected from all commands. If it is compulsory, random commands are selected only from operations that have to be implemented. If the parameter is a list of commands, commands are selected from that list (in this case it's a good idea to include also random_add so that elements are also added during the test loop). If the program is run with a graphical user interface, \"stop test\" button can be used to interrupt the performance test (it may take a while for the program to react to the button). \"Data files\" The easiest way to test the program is to create \"data files\", which can add a bunch of places, areas, and trains. Those files can then be read in using the \"read\" command, after which other commands can be tested without having to enter the information every time by hand. Below is an examples of a data file, which adds trains to the application: • example-trains.txt # Some imaginary example trains add_train tukutuku tus:0800 tpe:0900 kuo:1000 add_train upwards tpe:0930 roi:1600 kli:2000 add_train fast1 tus:1000 kuo:1200 add_train fast2 kuo:1100 kli:1900 add_train downwards kli:1300 vs:1500 kuo:2000 7 Screenshot of user interface Above is a screenshot of the graphical user interface after example-stations.txt, example-regions.txt, and example-trains.txt have been read in. Example run Below are example outputs from the program. The example's commands can be found in files example-compulsory-in.txt and example-all-in.txt, and the outputs in files example-compulsory- out.txt and example-all-out.txt. If you wish to perform a small test on all the compulsory commands, you can do this by running the following: testread \"example-compulsory-in.txt\" \"example-compulsory-out.txt\" example-compulsory > clear_all Cleared all stations > clear_trains All trains removed. > all_stations 8 No stations! > read \"example-stations.txt\" silent ** Commands from 'example-stations.txt' ...(output discarded in silent mode)... ** End of commands from 'example-stations.txt' > read \"example-trains.txt\" ** Commands from 'example-trains.txt' > # Some imaginary example trains > add_train tukutuku tus:0800 tpe:0900 kuo:1000 1. turku satama (tus) -> tampere (tpe): tukutuku (at 0800) 2. tampere (tpe) -> kuopio (kuo): tukutuku (at 0900) 3. kuopio (kuo): tukutuku (at 1000) > add_train upwards tpe:0930 roi:1600 kli:2000 1. tampere (tpe) -> rovaniemi (roi): upwards (at 0930) 2. rovaniemi (roi) -> kolari (kli): upwards (at 1600) 3. kolari (kli): upwards (at 2000) > add_train fast1 tus:1000 kuo:1200 1. turku satama (tus) -> kuopio (kuo): fast1 (at 1000) 2. kuopio (kuo): fast1 (at 1200) > add_train fast2 kuo:1100 kli:1900 1. kuopio (kuo) -> kolari (kli): fast2 (at 1100) 2. kolari (kli): fast2 (at 1900) > add_train downwards kli:1300 vs:1500 kuo:2000 1. kolari (kli) -> vaasa (vs): downwards (at 1300) 2. vaasa (vs) -> kuopio (kuo): downwards (at 1500) 3. kuopio (kuo): downwards (at 2000) > ** End of commands from 'example-trains.txt' > next_stations_from tpe 1. tampere (tpe) -> kuopio (kuo) 2. tampere (tpe) -> rovaniemi (roi) > train_stations_from tpe upwards 1. tampere (tpe) -> rovaniemi (roi) 2. rovaniemi (roi) -> kolari (kli) > route_any tus roi 1. turku satama (tus) -> tampere (tpe) (distance 0) 2. tampere (tpe) -> rovaniemi (roi) (distance 294) 3. rovaniemi (roi) (distance 1425) example-all > # First read in compulsory example > read \"example-compulsory-in.txt\" ** Commands from 'example-compulsory-in.txt' ... ** End of commands from 'example-compulsory-in.txt' > route_least_stations tus kli 1. turku satama (tus) -> kuopio (kuo) (distance 0) 2. kuopio (kuo) -> kolari (kli) (distance 797) 3. kolari (kli) (distance 1853) > route_with_cycle kuo 1. kuopio (kuo) -> kolari (kli) 2. kolari (kli) -> vaasa (vs) 3. vaasa (vs) -> kuopio (kuo) 9 4. kuopio (kuo) > route_shortest_distance tus kli 1. turku satama (tus) -> tampere (tpe) (distance 0) 2. tampere (tpe) -> rovaniemi (roi) (distance 294) 3. rovaniemi (roi) -> kolari (kli) (distance 1425) 4. kolari (kli) (distance 1673) > route_earliest_arrival tus kli 0700 1. turku satama (tus) -> tampere (tpe) (at 0800) 2. tampere (tpe) -> kuopio (kuo) (at 0900) 3. kuopio (kuo) -> kolari (kli) (at 1100) 4. kolari (kli) (at 1900) 10"
            },
            {
                "role": "user",
                "content": "Can you summarize the key topics covered in this lecture?"
            },
            {
                "role": "assistant",
                "content": "This lecture covers foundational algorithmic concepts including problem formulation, correctness proof using induction, efficiency analysis using asymptotic notation, data structures (arrays, linked lists, BSTs, heaps), and sorting methods (selection sort, merge sort, counting sort). Additionally, it introduces the Word-RAM computational model, providing a theoretical framework for measuring algorithm efficiency."
            }
        ]
    },
    {
        "system": "Hello! I am Kiran, an AI assistant from Tampere University, dedicated to providing you with accurate and insightful assistance. My goal is to help you understand concepts, summarize lectures, and answer questions effectively.",
        "conversations": [
            {
                "role": "host",
                "content": "COMP.CS.300 Tietorakenteet ja algoritmit 1, syksy 2022 Harjoitustyö 2: Rautatiet Viimeksi päivitetty 17.11.2022 Muutoshistoria Alla ohjeeseen tehdyt merkittävät muutokset julkaisun jälkeen: • 17.11. Lisätty huomautus, että next_stations_from-operaatiossa asemat saa palauttaa mielivaltaisessa järjestyksessä. Sisällys Muutoshistoria......................................................................................................................................1 Harjoitustyön aihe.................................................................................................................................1 Terminologiaa..................................................................................................................................2 Ohjelman toiminta ja rakenne...............................................................................................................2 Valmiit osat, jotka tarjotaan kurssin puolesta..............................................................................2 Graafisen käyttöliittymän käytöstä.............................................................................................3 Ohjelman tuntemat komennot ja luokan julkinen rajapinta........................................................3 \"Datatiedostot\"......................................................................................................................................7 Kuvakaappaus käyttöliittymästä...........................................................................................................7 Esimerkki ohjelman toiminnasta..........................................................................................................8 example-compulsory........................................................................................................................8 example-all.......................................................................................................................................9 Harjoitustyön aihe Toisessa harjoitustyössä ensimmäisen harjoitustyön ohjelmaa laajennetaan käsittelemään kokonaisia junayhteyksiä ja tekemään niihin liittyviä reittihakuja. Osa harjoitustyön operaatioista on pakollisia, osa vapaaehtoisia (pakollinen = vaaditaan läpipääsyyn, vapaaehtoinen = ei-pakollinen, mutta silti osa arvostelua arvosanan kannalta). Tässä kakkostyön dokumentissa esitellään vain kakkostyön uudet asiat. Tarkoitus on kopioida ykköstyön toteutus kakkostyön pohjaksi ja jatkaa siitä. Kaikki pääohjelman ykköstyön ominaisuudet ja komennot ovat käytössä myös kakkosessa, vaikka niitä ei toisteta tässä dokumentissa. Ykköstyössä arvosteltuja operaatioita ei kuitenkaan arvostella uudelleen, eivätkä ne vaikuta kakkostyön arvosteluun. 1 Terminologiaa Tärkeimmät uudet termit • Train = juna. (Kuvaa yhden junan kulkua lähtöasemalta pääteasemalle.) Junalla on yksilöivä merkkijono-ID ja lista asemia, joiden läpi juna kulkee, sekä lähtöajat ko. asemilta (ja saapumisaika pääteasemalle). • Route = reitti. Reitti on jono asemia lähtöasemalta kohdeasemalle junayhteyksiä käyttäen. Reitin pituus lasketaan (pyöristysvirheiden minimoimiseksi) niin, että etäisyys kahden peräkkäisen aseman välillä ( √(x1−x2) 2+( y1−y2) 2 ) pyöristetään ensin alaspäin kokonaisluvuksi, ja sitten nämä lasketaan yhteen. • Cycle = silmukka. Reitissä on silmukka, jos reittiä kulkemalla päädytään jossain vaiheessa takaisin sellaiselle asemalle, jonka läpi reitti on jo kulkenut. Huomaa erityisesti seuraavat asiat (myös kaikki harjoitustyön 1 huomioitavat asiat pätevät edelleen): • Tämän harjoitustyön uusissa operaatioissa asymptoottiseen tehokkuuteen ei välttämättä pysty hirveästi vaikuttamaan, koska käytetyt algoritmit määräävät sen. Sen vuoksi harjoitustyössä algoritmien toteutukseen ja toiminnallisuuteen kiinnitetään enemmän huomiota kuin vain asymptoottiseen tehokkuuteen. • Operaatioiden route_least_stations, route_shortest_distance, route_with_cycle ja route_earliest_arrival toteuttaminen ei ole pakollista läpipääsyn kannalta. Ne ovat kuitenkin osa arvostelua. Vain pakolliset osat toteuttamalla vaiheen maksimiarvosana on 2. • Riittävän huonolla toteutuksella työ voidaan hylätä. Ohjelman toiminta ja rakenne Osa ohjelmasta tulee valmiina kurssin puolesta, osa toteutetaan itse. Valmiit osat, jotka tarjotaan kurssin puolesta Tiedostot mainprogram.hh, mainprogram.cc, mainwindow.hh, mainwindow.cc, mainwindow.ui (joihin EI SAA TEHDÄ MITÄÄN MUUTOKSIA) Tiedosto datastructures.hh • class Datastructures: Luokka, johon harjoitustyö kirjoitetaan. Luokasta annetaan valmiina sen julkinen rajapinta (johon EI SAA TEHDÄ MITÄÄN MUUTOKSIA, luonnollisesti luokkaan saa private-puolelle lisätä omia jäsenmuuttujia ja -funktioita). 2 • Tyyppimäärittely TrainID (koostuu merkeistä A-Z, a-z, 0-9 ja väliviiva -), jota käytetään junan yksilöivänä tunnisteena. • Vakiot NO_TRAIN, jota käytetään joissain operaatioissa paluuarvona, jos operaatio epäonnistuu. Tiedosto datastructures.cc • Tänne luonnollisesti kirjoitetaan luokan operaatioiden toteutukset. Graafisen käyttöliittymän käytöstä QtCreatorilla käännettäessä harjoitustyön valmis koodi tarjoaa graafisen käyttöliittymän, jolla ohjelmaa voi testata ja ajaa valmiita testejä sekä visualisoida ohjelman toimintaa. Huom! Käyttöliittymän graafinen esitys kysyy kaikki tiedot opiskelijoiden koodista! Se ei siis ole \"oikea\" lopputulos vaan graafinen esitys siitä, mitä tietoja opiskelijoiden koodi antaa. Jos paikkojen piirto on päällä, käyttöliittymä hakee kaikki asemat operaatiolla all_stations() ja kysyy asemien tiedot operaatioilla get_...(). Jos alueiden piirtäminen on päällä, ne kysytään operaatiolla all_regions(), ja alueen koordinaatit operaatiolla get_region_coords(). Jos junien piirto on päällä, ne kysytään joka aseman osalta operaatiolla next_stations_from(). Ohjelman tuntemat komennot ja luokan julkinen rajapinta Kun ohjelma käynnistetään, se jää odottamaan komentoja, jotka on selitetty alla. Komennot, joiden yhteydessä mainitaan jäsenfunktio, kutsuvat ko. Datastructure-luokan operaatioita, jotka siis opiskelijat toteuttavat. Osa komennoista on taas toteutettu kokonaan kurssin puolesta pääohjelmassa. Jos ohjelmalle antaa komentoriviltä tiedoston parametriksi, se lukee komennot ko. tiedostosta ja lopettaa sen jälkeen. Alla operaatiot on listattu siinä järjestyksessä, kun ne suositellaan toteutettavaksi (tietysti suunnittelu kannattaa tehdä kaikki operaatiot huomioon ottaen jo alun alkaen). Komento Julkinen jäsenfunktio (Komennoissa ei-pakolliset parametrit hakasuluissa ja vaihtoehdot erotettu |-merkillä.) Selitys (Kaikki harjoitustyön 1 operaatiot ovat myös saatavilla.) (Ja tekevät saman asian kuin harjoitustyössä 1.) clear_trains void clear_trains() Poistaa kaikki junat, mutta ei koske asemiin eikä alueisiin. Tämä operaatio ei ole mukana tehokkuustesteissä. 3 Komento Julkinen jäsenfunktio (Komennoissa ei-pakolliset parametrit hakasuluissa ja vaihtoehdot erotettu |-merkillä.) Selitys add_train ID Station1:Time1 CStation2:Time2... StationN:TimeN bool add_train(TrainID trainid, std::vector<std::pair<StationID, Time> stationtimes) Lisää tietorakenteeseen uuden junan annetulla uniikilla id:llä. Juna kulkee annettujen asemien läpi ja lähtee niiltä annettuina aikoina. Viimeisen aseman aika on saapumisaika pääteasemalle (jolla on merkitystä vain route_earliest_arrival-komennossa). Lähtöajat lisätään myös aseman tietoihin niin, että harjoitustyö 1:n station_departures_after näytää ne. Jos annetulla id:llä on jo juna tai jotain asemaa ei löydy, ei tehdä mitään ja palautetaan false, muuten palautetaan true. next_stations_from ID std::vector<StationID> next_stations_from(StationID id) Palauttaa asemat, jotka ovat heti seuraavia asemia annetun aseman läpi kulkevissa junayhteyksissä. Jos asemalta ei lähde junia, palautetaan tyhjä vektori. Jos id:tä vastaavaa asemaa ei löydy, palautetaan vektori, jonka ainoa alkio on NO_STATION. Pääohjelma järjestää tuloksen id:n mukaan, joten asemat voi palauttaa mielivaltaisessa järjestyksessä. (Pääohjelma kutsuu tätä eri paikoissa.) train_stations_from StationID TrainID std::vector<StationID> train_stations_from(StationID stationid, TrainID trainid) Palauttaa luettelon asemista, joiden läpi annettu juna kulkee lähdettyään annetulta asemalta. Jos id:illä ei löydy asemaa tai junaa, tai juna ei lähde annetulta asemalta, palautetaan vektori, jonka ainoa alkio on NO_STATION. (Allaolevat kannattaa toteuttaa todennäköisesti vasta, kun ylläolevat on toteutettu.) route_any StationID1 StationID2 std::vector<std::pair<StationID, Distance>> route_any(StationID fromid, StationID toid) Palauttaa jonkin (mielivaltaisen) reitin annettujen asemien välillä. Palautetussa vektorissa on ensimmäisenä alkuasema ja matka 0. Sitten tulevat kaikki reitin varrella olevat asemat, ja kokonaismatka ko. asemaan saakka. Viimeisenä alkiona on kohdeasema ja reitin kokonaismatka. Jos reittiä ei löydy, palautetaan tyhjä vektori. Jos jompaakumpaa id:tä ei ole, palautetaan vektori, jonka ainoa alkio on {NO_STATION, NO_DISTANCE}. (Seuraavien operaatioiden toteuttaminen ei ole pakollista, mutta ne parantavat arvosanaa.) 4 Komento Julkinen jäsenfunktio (Komennoissa ei-pakolliset parametrit hakasuluissa ja vaihtoehdot erotettu |-merkillä.) Selitys route_least_stations StationID1 StationID2 std::vector<std::pair<StationID, Distance>> route_least_stations(StationID fromid, StationID toid) Palauttaa sellaisen reitin annettujen asemien välillä, jossa on mahdollisimman vähän asemia. Jos useilla mahdollisilla reiteillä on yhtä monta asemaa, voi niistä palauttaa minkä tahansa. Palautetussa vektorissa on ensimmäisenä alkuasema ja matka 0. Sitten tulevat kaikki reitin varrella olevat asemat, ja kokonaismatka ko. asemaan saakka. Viimeisenä alkiona on kohdeasema ja reitin kokonaismatka. Jos reittiä ei löydy, palautetaan tyhjä vektori. Jos jompaakumpaa id:tä ei ole, palautetaan vektori, jonka ainoa alkio on {NO_STATION, NO_DISTANCE}. route_with_cycle StationID std::vector<StationID> route_with_cycle(StationID fromid) Palauttaa annetulta asemalta lähtevän reitin, jossa on sykli, ts. reitti päätyy uudelleen jollekin reitillä jo oleva asemalle. Palautetussa vektorissa on ensimmäisenä alkuasema, sitten tulevat kaikki reitin varrella olevat asemat. Viimeisenä on syklin aiheuttava toiseen kertaan tuleva asema. Jos useilla mahdollisilla reiteillä on sykli, voi niistä palauttaa minkä tahansa. Jos syklistä reittiä ei löydy, palautetaan tyhjä vektori. Jos id:tä vastaavaa asemaa ei ole, palautetaan vektori, jonka ainoa alkio on NO_STATION. route_shortest_distance StationID1 StationID2 std::vector<std::pair<StationID, Distance>> route_shortest_distance(StationID fromid, StationID toid) Palauttaa annettujen asemien välillä kokonaismatkaltaan mahdollisimman lyhyen reitin. Jos useilla reiteillä on sama kokonaismatka, voi niistä palauttaa minkä tahansa. Palautetussa vektorissa on ensimmäisenä alkuasema ja matka 0. Sitten tulevat kaikki reitin varrella olevat asemat, ja kokonaismatka ko. asemaan saakka. Viimeisenä alkiona on kohdeasema ja reitin kokonaismatka. Jos reittiä ei löydy, palautetaan tyhjä vektori. Jos jompaakumpaa id:tä ei ole, palautetaan vektori, jonka ainoa alkio on {NO_STATION, NO_DISTANCE}. 5 Komento Julkinen jäsenfunktio (Komennoissa ei-pakolliset parametrit hakasuluissa ja vaihtoehdot erotettu |-merkillä.) Selitys route_earliest_arrival StationID1 StationID2 StartTime std::vector<std::pair<StationID, Time>> route_earliest_arrival(StationID fromid, StationID toid, Time starttime) Huom! Tämä on haastavin ei-pakollinen operaatio! (Vinkki: yritä keksiä sopiva kustannusfunktio.) Palauttaa annettujen asemien välillä reitin, jolla päästään perille mahdollisimman aikaisin. Jos useilla reiteillä on sama saapumisaika, voi niistä palauttaa minkä tahansa. Palautetussa vektorissa on ensimmäisenä alkuasema ja lähtöaika ko. asemalta. Sitten tulevat kaikki reitin varrella olevat asemat, ja lähtöajat niiltä. Viimeisenä alkiona on kohdeasema ja saapumisaika sille. Jos reittiä ei löydy, palautetaan tyhjä vektori. Jos jompaakumpaa id:tä ei ole, palautetaan vektori, jonka ainoa alkio on {NO_STATION, NO_TIME}. Huom! operaation ei tarvitse löytää reittejä, joissa vuorokausi vaihtuu kesken reitin. (Seuraavat komennot on toteutettu valmiiksi pääohjelmassa.) (Tässä mainitaan vain muutokset vaiheen 1 toiminnallisuuteen) random_trains n (pääohjelman toteuttama) Lisää tietorakenteeseen (testausta varten) n kpl junia, jotka kulkevat satunnaisten asemien välillä. Huom! Arvot ovat tosiaan satunnaisia, eli saattavat olla kerrasta toiseen eri, eivätkä ne graafisesti muodosta järkevää ”karttaa”. perftest all|compulsory|cmd1[;cmd2...] timeout repeat n1[;n2...] (pääohjelman toteuttama) Ajaa ohjelmalle tehokkuustestit. Tyhjentää tietorakenteen ja lisää sinne n1 kpl satunnaisia asemia, alueita ja junia. Sen jälkeen arpoo repeat kertaa satunnaisen komennon. Mittaa ja tulostaa sekä lisäämiseen että komentoihin menneen ajan. Sen jälkeen sama toistetaan n2:lle jne. Jos jonkin testikierroksen suoritusaika ylittää timeout sekuntia, keskeytetään testien ajaminen (tämä ei välttämättä ole mikään ongelma, vaan mielivaltainen aikaraja). Jos ensimmäinen parametri on all, arvotaan lisäyksen jälkeen kaikista komennoista, joita on ilmoitettu kutsuttavan usein. Jos se on compulsory, testataan vain komentoja, jotka on pakko toteuttaa. Jos parametri on lista komentoja, arvotaan komento näiden joukosta (tällöin kannattaa mukaan ottaa myös random_add, jotta lisäyksiä tulee myös testikierroksen aikana). Jos ohjelmaa ajaa graafisella käyttöliittymällä, \"stop test\" nappia painamalla testi keskeytetään (nappiin reagointi voi kestää hetken). 6 \"Datatiedostot\" Kätevin tapa testata ohjelmaa on luoda \"datatiedostoja\", jotka add-komennolla lisäävät joukon paikkoja, alueita ja väyliä ohjelmaan. Tiedot voi sitten kätevästi lukea sisään tiedostosta read- komennolla ja sitten kokeilla muita komentoja ilman, että tiedot täytyisi joka kerta syöttää sisään käsin. Alla on esimerkit datatiedostoista, joka lisää väyliä sovellukseen: • example-trains.txt # Some imaginary example trains add_train tukutuku tus:0800 tpe:0900 kuo:1000 add_train upwards tpe:0930 roi:1600 kli:2000 add_train fast1 tus:1000 kuo:1200 add_train fast2 kuo:1100 kli:1900 add_train downwards kli:1300 vs:1500 kuo:2000 Kuvakaappaus käyttöliittymästä Yllä on kuvakaappaus käyttöliittymästä sen jälkeen, kun example-stations.txt, example-regions.txt ja example-trains.txt on luettu sisään. 7 Esimerkki ohjelman toiminnasta Alla on esimerkki ohjelman toiminnasta. Esimerkin syötteet löytyvät tiedostoista example- compulsory-in.txt ja example-all-in.txt, tulostukset tiedostoista example-compulsory-out.txt ja example-all-out.txt. Eli esimerkkiä voi käyttää pienenä testinä pakollisten toimintojen toimimisesta antamalla käyttöliittymästä komennon testread \"example-compulsory-in.txt\" \"example-compulsory-out.txt\" example-compulsory > clear_all Cleared all stations > clear_trains All trains removed. > all_stations No stations! > read \"example-stations.txt\" silent ** Commands from 'example-stations.txt' ...(output discarded in silent mode)... ** End of commands from 'example-stations.txt' > read \"example-trains.txt\" ** Commands from 'example-trains.txt' > # Some imaginary example trains > add_train tukutuku tus:0800 tpe:0900 kuo:1000 1. turku satama (tus) -> tampere (tpe): tukutuku (at 0800) 2. tampere (tpe) -> kuopio (kuo): tukutuku (at 0900) 3. kuopio (kuo): tukutuku (at 1000) > add_train upwards tpe:0930 roi:1600 kli:2000 1. tampere (tpe) -> rovaniemi (roi): upwards (at 0930) 2. rovaniemi (roi) -> kolari (kli): upwards (at 1600) 3. kolari (kli): upwards (at 2000) > add_train fast1 tus:1000 kuo:1200 1. turku satama (tus) -> kuopio (kuo): fast1 (at 1000) 2. kuopio (kuo): fast1 (at 1200) > add_train fast2 kuo:1100 kli:1900 1. kuopio (kuo) -> kolari (kli): fast2 (at 1100) 2. kolari (kli): fast2 (at 1900) > add_train downwards kli:1300 vs:1500 kuo:2000 1. kolari (kli) -> vaasa (vs): downwards (at 1300) 2. vaasa (vs) -> kuopio (kuo): downwards (at 1500) 3. kuopio (kuo): downwards (at 2000) > ** End of commands from 'example-trains.txt' > next_stations_from tpe 1. tampere (tpe) -> kuopio (kuo) 2. tampere (tpe) -> rovaniemi (roi) > train_stations_from tpe upwards 1. tampere (tpe) -> rovaniemi (roi) 2. rovaniemi (roi) -> kolari (kli) > route_any tus roi 1. turku satama (tus) -> tampere (tpe) (distance 0) 2. tampere (tpe) -> rovaniemi (roi) (distance 294) 8 3. rovaniemi (roi) (distance 1425) example-all > # First read in compulsory example > read \"example-compulsory-in.txt\" ** Commands from 'example-compulsory-in.txt' ... ** End of commands from 'example-compulsory-in.txt' > route_least_stations tus kli 1. turku satama (tus) -> kuopio (kuo) (distance 0) 2. kuopio (kuo) -> kolari (kli) (distance 797) 3. kolari (kli) (distance 1853) > route_with_cycle kuo 1. kuopio (kuo) -> kolari (kli) 2. kolari (kli) -> vaasa (vs) 3. vaasa (vs) -> kuopio (kuo) 4. kuopio (kuo) > route_shortest_distance tus kli 1. turku satama (tus) -> tampere (tpe) (distance 0) 2. tampere (tpe) -> rovaniemi (roi) (distance 294) 3. rovaniemi (roi) -> kolari (kli) (distance 1425) 4. kolari (kli) (distance 1673) > route_earliest_arrival tus kli 0700 1. turku satama (tus) -> tampere (tpe) (at 0800) 2. tampere (tpe) -> kuopio (kuo) (at 0900) 3. kuopio (kuo) -> kolari (kli) (at 1100) 4. kolari (kli) (at 1900) 9"
            },
            {
                "role": "user",
                "content": "Can you summarize the key topics covered in this lecture?"
            },
            {
                "role": "assistant",
                "content": "This lecture covers foundational algorithmic concepts including problem formulation, correctness proof using induction, efficiency analysis using asymptotic notation, data structures (arrays, linked lists, BSTs, heaps), and sorting methods (selection sort, merge sort, counting sort). Additionally, it introduces the Word-RAM computational model, providing a theoretical framework for measuring algorithm efficiency."
            }
        ]
    },
    {
        "system": "Hello! I am Kiran, an AI assistant from Tampere University, dedicated to providing you with accurate and insightful assistance. My goal is to help you understand concepts, summarize lectures, and answer questions effectively.",
        "conversations": [
            {
                "role": "host",
                "content": "Algoritmi & pseudokoodi COMP.CS.300 Tietorakenteet ja algoritmit 1 Matti Rintala (matti.rintala@tuni.fi) Algoritmi vs tietokoneohjelma Algoritmin \"määritelmiä\" •Joukko ohjeita tai askeleita jonkin ongelman ratkaisemiseksi •Hyvin määritelty laskentamenetelmä, joka saa syötteenään alkion tai joukon alkioita ja ja tuottaa tuloksenaan alkion tai joukon alkioita Algoritmin \"määritelmä\" •Hyvin määritelty = – jokainen askel on kuvattu niin tarkasti, että lukija osaa suorittaa sen (tai ohjelmoida koneen suorittamaan) – jokainen askel on määritelty yksikäsitteisesti – samat vaatimukset pätevät askelten suoritusjärjestykselle – suorituksen tulee päättyä äärellisen askelmäärän jälkeen Algoritmin \"määritelmä\" •Algoritmi saa rajoittaa sille annettua syötettä •Ts. \"lupaa toimia\" vain, kun tietyt reunaehdot toteutuvat Pseudokoodi COMP.CS.300 Tietorakenteet ja algoritmit 1 Matti Rintala (matti.rintala@tuni.fi) Pseudokoodi EtsiAlkio(TAUL, arvo, järjestyksessä) 1 paikka := 0 (kommentti: sijoitus :=-merkillä!) 2 if järjestyksessä then (järjestyksessä on true/false) 3 paikka := Puolitushaku(TAUL, arvo) (kutsu toista algoritmia) 4 else 5 for i in 1..TAUL.size (käy läpi taulukon indeksit) 6 if TAUL[i] = arvo then 7 paikka := i 8 break (hyppää ulos silmukasta) 9 ▷tulosta paikka (vapaamuotoinen ohje)"
            },
            {
                "role": "user",
                "content": "Can you summarize the key topics covered in this lecture?"
            },
            {
                "role": "assistant",
                "content": "This lecture covers foundational algorithmic concepts including problem formulation, correctness proof using induction, efficiency analysis using asymptotic notation, data structures (arrays, linked lists, BSTs, heaps), and sorting methods (selection sort, merge sort, counting sort). Additionally, it introduces the Word-RAM computational model, providing a theoretical framework for measuring algorithm efficiency."
            }
        ]
    },
    {
        "system": "Hello! I am Kiran, an AI assistant from Tampere University, dedicated to providing you with accurate and insightful assistance. My goal is to help you understand concepts, summarize lectures, and answer questions effectively.",
        "conversations": [
            {
                "role": "host",
                "content": "Asymptoottisen tehokkuuden idea COMP.CS.300 Tietorakenteet ja algoritmit 1 Matti Rintala (matti.rintala@tuni.fi) Asymptoottinen tehokkuus •Miten algoritmin käyttäytyminen muuttuu, kun datan määrä kasvaa? •Ajankulutus, muistinkulutus tms. •Asymptoottinen: miltä muutos näyttää suurilla datamäärillä (\"lähestyy ääretöntä\") Datan määrä (n) Aika (t) Kertaluokat •Kuvataan datan kasvun vaikutusta funktiolla, jota todellisuus lähestyy määrän kasvaessa •Algoritmin ajankäyttö: Suoritettujen \"askelten\" määrä •Askel: jokin syötekoosta riippumaton operaatio •Lasketaan yhteen askelten määrä, ilmaistaan syötteen koon n funktiona •Yksinkertaistus: jätetään vain \"ylimmän asteen\" termi, poistetaan vakiokertoimet (miksi? selviää myöhemmin) •Merkintä: Θ(funktio) Esimerkki: summaus Summa(A) 1 summa := 0 2 for i := 1 to A.length 3 summa := summa + A[i] Esimerkki: summaus n 1 10 100 1000 10000 Esimerkki: summaus Datan määrä (n) Aika (t) Esimerkki: etsiminen Etsi(A, arvo) 1 for i := 1 to A.length 2 if A[i] = arvo then 3 return i 4 return 0 (ei löytynyt) Esimerkki: etsiminen •Paras tapaus? •Huonoin tapaus? •Keskimääräinen tapaus? Esimerkki: etsiminen •( ) 1 p n +2 p n +…+n p n +n(1−p)"
            },
            {
                "role": "user",
                "content": "Can you summarize the key topics covered in this lecture?"
            },
            {
                "role": "assistant",
                "content": "This lecture covers foundational algorithmic concepts including problem formulation, correctness proof using induction, efficiency analysis using asymptotic notation, data structures (arrays, linked lists, BSTs, heaps), and sorting methods (selection sort, merge sort, counting sort). Additionally, it introduces the Word-RAM computational model, providing a theoretical framework for measuring algorithm efficiency."
            }
        ]
    },
    {
        "system": "Hello! I am Kiran, an AI assistant from Tampere University, dedicated to providing you with accurate and insightful assistance. My goal is to help you understand concepts, summarize lectures, and answer questions effectively.",
        "conversations": [
            {
                "role": "host",
                "content": "Tehokkuuden analysointi pseudokoodista COMP.CS.300 Tietorakenteet ja algoritmit 1 Matti Rintala (matti.rintala@tuni.fi) Asymptoottinen tehokkuus •O: yläraja (saa olla todellista hitaampi) •Ω: alaraja (saa olla todellista nopeampi) •Analysoidaan rajat \"niin hyvin kuin pystytään\" •(Tarkempi analyysi voi laskea ylärajaa ja/tai nostaa alarajaa) Insertion-Sort(A) 1 for next_elem := 2 to A.length do 2 key := A[next_elem] 3 place := next_elem − 1 4 while place > 0 and A[place] > key do 5 A[place + 1] := A[place] 6 place := place − 1 7 A[place + 1] := key"
            },
            {
                "role": "user",
                "content": "Can you summarize the key topics covered in this lecture?"
            },
            {
                "role": "assistant",
                "content": "This lecture covers foundational algorithmic concepts including problem formulation, correctness proof using induction, efficiency analysis using asymptotic notation, data structures (arrays, linked lists, BSTs, heaps), and sorting methods (selection sort, merge sort, counting sort). Additionally, it introduces the Word-RAM computational model, providing a theoretical framework for measuring algorithm efficiency."
            }
        ]
    },
    {
        "system": "Hello! I am Kiran, an AI assistant from Tampere University, dedicated to providing you with accurate and insightful assistance. My goal is to help you understand concepts, summarize lectures, and answer questions effectively.",
        "conversations": [
            {
                "role": "host",
                "content": "Asymptoottisen tehokkuuden testaaminen COMP.CS.300 Tietorakenteet ja algoritmit 1 Matti Rintala (matti.rintala@tuni.fi) Asymptoottisen tehokkuuden testaaminen •Pseudokoodi/suunnittelu – Arvio asymptoottisesta tehokkuudesta •Toteutus/koodaus – Mahd. tehokas toteutus – Toivottavasti haluttu asymptoottinen tehokkuus •Testaus – Vastaako toteutuksen tehokkuus suunniteltua???"
            },
            {
                "role": "user",
                "content": "Can you summarize the key topics covered in this lecture?"
            },
            {
                "role": "assistant",
                "content": "This lecture covers foundational algorithmic concepts including problem formulation, correctness proof using induction, efficiency analysis using asymptotic notation, data structures (arrays, linked lists, BSTs, heaps), and sorting methods (selection sort, merge sort, counting sort). Additionally, it introduces the Word-RAM computational model, providing a theoretical framework for measuring algorithm efficiency."
            }
        ]
    },
    {
        "system": "Hello! I am Kiran, an AI assistant from Tampere University, dedicated to providing you with accurate and insightful assistance. My goal is to help you understand concepts, summarize lectures, and answer questions effectively.",
        "conversations": [
            {
                "role": "host",
                "content": "Binäärihakupuut COMP.CS.300 Tietorakenteet ja algoritmit 1 Matti Rintala (matti.rintala@tuni.fi) Binäärihakupuu •Solmulla max. 2 lasta, vasen ja oikea •Joka solmun vasemman alipuun alkiot solmua pienempiä, oikean suurempia •Ei välttämättä tasapainossa •Ei siis voi esittää yleensä taulukkona 9 10 6 4 8 15 2 5 7 Haku binäärihakupuusta B-tree-search(root, value) 1 if root = NIL or root→key = value then (avain löytyi) 2 return root 3 if value < root→key then (jos haetaan pienempää...) 4 return B-tree-search(root→left, value) (...etsitään vasemmasta alipuusta) 5 else 6 return B-tree-search(root→right, value) (muuten etsitään oikeasta) Haku binäärihakupuusta B-tree-search(root, value) 1 if root = NIL or root→key = value then 2 return root 3 if value < root→key then 4 return B-tree-search(root→left, value) 5 else 6 return B-tree-search(root→right, value) 9 10 6 4 8 15 2 5 7 B-tree-search(root, value) 1 while root ≠ NIL and root→key ≠ value do 2 if value < root→key then 3 root := root→left 4 else 5 root := root→right 6 return root Lisäys bin:hakupuuhun B-tree-insert(root, newnode) 1 if root = NIL then 2 ▷(Puu on tyhjä, lisää newnode juureksi) 3 parent := NIL 4 node := root 5 while node ≠ NIL do (mennään alaspäin kunnes puu loppuu) 6 parent := node 7 if newnode→key < node→key then (valitaan suunta avaimia vertailemalla) 8 node := node→left 9 else 10 node := node→right 11 newnode→parent := parent (lehtisolmu on uuden solmun vanhempi) 12 if newnode→key < parent→key then (ja uusi solmu lehtisolmun lapsi...) 13 parent→left := newnode (...joko vasen...) 14 else 15 parent→right := newnode (...tai oikea) 16 newnode→left := NIL (nollataan uuden solmun lapsiosoittimet) 17 newnode→right := NIL Lisäys bin:hakupuuhun B-tree-insert(root, newnode) 1 if root = NIL then 2 ▷(Puu on tyhjä, lisää newnode juureksi) 3 parent := NIL 4 node := root 5 while node ≠ NIL do 6 parent := node 7 if newnode→key < node→key then 8 node := node→left 9 else 10 node := node→right 11 newnode→parent := parent 12 if newnode→key < parent→key then 13 parent→left := newnode 14 else 15 parent→right := newnode 16 newnode→left := NIL 17 newnode→right := NIL Minimi ja maksimi B-tree-maximum(root) 1 while root→right ≠ NIL do 2 root := root→right 3 return root B-tree-minimum(root) 1 while root→left ≠ NIL do 2 root := root→left 3 return root Läpikäynti suuruusjärj. Inorder-tree-walk(node) 1 if node ≠ NIL then 2 Inorder-tree-walk(node→left-child) 3 ▷(käsittele alkio node) 4 Inorder-tree-walk(node→right-child) •Binäärihakupuun läpikäynti suuruusjärjestyksessä välijärjestyksellä 10 4 11 6 1 5 9 2 7 8 3 Solmua järj. seuraava B-tree-successor(node) 1 if node→right ≠ NIL then (jos löytyy oikea alipuu) 2 return B-tree-minimum(node→right) (seuraaja on sen pienin alkio) 3 parent := node→parent (muuten lähdetään ylöspäin) 4 while parent ≠ NIL and node ≠ parent→left do (kunnes tultiin vasemmalta) 5 node := parent 6 parent := parent→parent 7 return parent (tulos on löydetty vanhempi) Solmua järj. seuraava B-tree-successor(node) 1 if node→right ≠ NIL then 2 return B-tree-minimum(node→right) 3 parent := node→parent 4 while parent ≠ NIL and node ≠ parent→left do 5 node := parent 6 parent := parent→parent 7 return parent Binäärihakupuiden tehokkuus, tasapainotetut bin:hakupuut COMP.CS.300 Tietorakenteet ja algoritmit 1 Matti Rintala (matti.rintala@tuni.fi) Epätasapainon vaikutus 9 10 6 4 8 15 2 5 7 Rotaatiot 9 10 6 4 8 15 2 5 7 9 10 6 4 8 15 2 5 7 Rotaatiot 9 10 6 4 8 15 2 5 7 9 10 6 4 8 15 2 5 7 \"Riittävä\" tasapaino 9 13 6 4 8 15 12 11 10 Esimerkkejä binäärihakupuista COMP.CS.300 Tietorakenteet ja algoritmit 1 Matti Rintala (matti.rintala@tuni.fi) AVL-puut •Tasapainotettu binäärihakupuu, tasapaino rotaatioilla •Joka solmun alipuiden korkeudet eroavat korkeintaan yhdellä •Joka solmussa kirjanpito – joko alipuiden korkeuseroista -1 / 0 / +1 – tai solmusta alkavan puun korkeudesta 9 4 7 2 1 12 10 11 20 15 25 13 17 +1 +1 +1 -1 -1 -1 0 0 0 0 0 0 0 Puna-mustat puut •\"Riittävän\" tasapainotettu binäärihakupuu, tasapaino rotaatioilla •Pitkät haarat korkeintaan 2x lyhimmät •Joka solmussa kirjanpitona \"väri\" punainen/musta •Invariantit auttavat tasapainossa: – kahta punaista solmua ei peräkkäin – mustia solmuja haaroissa aina yhtä monta – (juuri aina musta) 9 4 12 10 20 15 25 Splay-puut •Eivät tasapainotettuja, mutta muuten mielenkiintoisia •Lisäyksessä ja haussa lisätty/löydetty solmu rotatoidaan juureksi •Tuloksena usein haetut/viimeksi lisätyt solmut löytyvät nopeasti läheltä juurta"
            },
            {
                "role": "user",
                "content": "Can you summarize the key topics covered in this lecture?"
            },
            {
                "role": "assistant",
                "content": "This lecture covers foundational algorithmic concepts including problem formulation, correctness proof using induction, efficiency analysis using asymptotic notation, data structures (arrays, linked lists, BSTs, heaps), and sorting methods (selection sort, merge sort, counting sort). Additionally, it introduces the Word-RAM computational model, providing a theoretical framework for measuring algorithm efficiency."
            }
        ]
    },
    {
        "system": "Hello! I am Kiran, an AI assistant from Tampere University, dedicated to providing you with accurate and insightful assistance. My goal is to help you understand concepts, summarize lectures, and answer questions effectively.",
        "conversations": [
            {
                "role": "host",
                "content": "Datan jakaminen tietorakenteiden kesken COMP.CS.300 Tietorakenteet ja algoritmit 1 Matti Rintala (matti.rintala@tuni.fi) Arvosemantiikka (C++) Viitesemantiikka (Python...) Tapaus 1: ei vielä ongelmia? nimi: Jussi ikä: 32 nimi: Katja ikä: 45 nimi: Simo ikä: 21 nimi: Bob ikä: 31 nimi: Äijä ikä: 65 Tapaus 2: Data monistaminen? nimi: Jussi ikä: 32 nimi: Katja ikä: 45 nimi: Simo ikä: 21 nimi: Bob ikä: 31 nimi: Äijä ikä: 65 Tapaus 3: Datan jakaminen? nimi: Jussi ikä: 32 nimi: Katja ikä: 45 nimi: Simo ikä: 21 nimi: Bob ikä: 31 nimi: Äijä ikä: 65 Tapaus 4: Dataan viittaaminen? Vaihtoehtoja epäsuoraan viittaamiseen •Osoittimet (viitteet) •Älykkäät osoittimet (muistinhallinta) •Indeksit (jos data taulukossa tms.) •Iteraattorit (jos data tietorakenteessa) •Mikä tahansa tapa päästä dataan käsiksi! •(Huom. mitätöitymisvaara!)"
            },
            {
                "role": "user",
                "content": "Can you summarize the key topics covered in this lecture?"
            },
            {
                "role": "assistant",
                "content": "This lecture covers foundational algorithmic concepts including problem formulation, correctness proof using induction, efficiency analysis using asymptotic notation, data structures (arrays, linked lists, BSTs, heaps), and sorting methods (selection sort, merge sort, counting sort). Additionally, it introduces the Word-RAM computational model, providing a theoretical framework for measuring algorithm efficiency."
            }
        ]
    },
    {
        "system": "Hello! I am Kiran, an AI assistant from Tampere University, dedicated to providing you with accurate and insightful assistance. My goal is to help you understand concepts, summarize lectures, and answer questions effectively.",
        "conversations": [
            {
                "role": "host",
                "content": "Graafit COMP.CS.300 Tietorakenteet ja algoritmit 1 Matti Rintala (matti.rintala@tuni.fi) Graafit Graafit Graafit Graafit Lista-puu-graafi Lista-puu-graafi Lista-puu-graafi Termejä •Solmut (node, vertex) – Sisältävät usein dataa – \"Samanarvoisia\" •Kaaret (edge, arc) – Voivat myös sisältää dataa – Yhteys solmusta toiseen •Suunnattu / suuntaamaton •Monigraafi (multigraph) •Kytketty/yhtenäinen graafi Termejä •Solmut (node, vertex) – Sisältävät usein dataa – \"Samanarvoisia\" •Kaaret (edge, arc) – Voivat myös sisältää dataa – Yhteys solmusta toiseen •Suunnattu / suuntaamaton •Monigraafi (multigraph) •Kytketty/yhtenäinen graafi Termejä •Solmut (node, vertex) – Sisältävät usein dataa – \"Samanarvoisia\" •Kaaret (edge, arc) – Voivat myös sisältää dataa – Yhteys solmusta toiseen •Suunnattu / suuntaamaton •Monigraafi (multigraph) •Kytketty/yhtenäinen graafi Graafihaut: Leveys-ensin-haku (BFS) COMP.CS.300 Tietorakenteet ja algoritmit 1 Matti Rintala (matti.rintala@tuni.fi) Graafialgoritmeista ●Askel ●Reitti ●Solmujen etäisyys (v1,v2) ẟ ●Solmun \"väri\" (vain apukeino) – Valkea: alkutila – Harmaa: löydetty, \"kesken\" – Musta: löydetty, loppuun käsitelty ●Aputietorakenne kesken oleville (harmaille) solmuille, \"muistilista\" Graafialgoritmeista ●Algoritmit liittävät joka solmuun v lisädataa – v→d: etäisyys lähtösolmusta solmuun (∞ jos solmua ei (vielä) löydetty) – v→π: mistä solmusta saavuttiin tähän solmuun – v→colour: solmun väri ●Lisäksi merkitään – v→Adj: v:n naapurisolmut (mihin solmuihin v:stä lähtee kaaret) Leveys-ensin-haku (BFS) ●Etsii kaikki solmut, joihin pääsee annetusta lähtösolmusta ●Laskee em. solmujen etäisyydet lähtösolmusta ●Laskee lyhimmän polun lähtösolmusta joka em. solmuun ●Jono (queue) harmaiden solmujen muistilistana ●(lisäksi tarvitaan solmujen lisäkentät) Leveys-ensin-haku (BFS) Breath-first-search(s) (s on haun lähtösolmu) 1 ▷(Kaikki solmut: colour := white, d := ∞, π := NIL) 2 ▷(Q on muistilistana toimiva jonotietorakenne) 3 s→colour := gray (alkusolmu on kesken) 4 s→d := 0 (etäisyys alkusolmuun 0) 5 Push(Q, s) (alkusolmu muistilistalle) 6 while Q ≠ empty do (kunnes muistilista on tyhjä) 7 u := Pop(Q) (vanhin alkio listasta) 8 for v in u→Adj do (käy läpi solmun naapurit) 9 if v→colour = white then (jos uusi solmu...) 10 v→colour := gray (...se on nyt kesken) 11 v→d := u→d + 1 (...ja yhtä kauempana) 12 v→π := u (...ja siihen tultiin u:sta) 13 Push(Q, v) (Lisätään v listalle) 14 u→colour := black (Nyt s on loppuun käsitelty) Leveys-ensin-haku (BFS) Breath-first-search(s) 1 ▷(Kaikki solmut: ...) 2 ▷(Q on ...) 3 s→colour := gray 4 s→d := 0 5 Push(Q, s) 6 while Q ≠ empty do 7 u := Pop(Q) 8 for v in u→Adj do 9 if v→colour = white then 10 v→colour := gray 11 v→d := u→d + 1 12 v→π := u 13 Push(Q, v) 14 u→colour := black C B H A I E G D J K F Graafihaut: Syvyys-ensin-haku (DFS) COMP.CS.300 Tietorakenteet ja algoritmit 1 Matti Rintala (matti.rintala@tuni.fi) Syvyys-ensin-haku (DFS) ●Etsii myös kaikki solmut, joihin pääsee annetusta lähtösolmusta ●Laskee jonkin polun lähtösolmusta joka em. solmuun ●Selvittää, löytyykö poluista silmukoita ●Pino (stack) harmaiden solmujen muistilistana ●(muistinkulutus usein BFS:ää pienempi) Syvyys-ensin-haku (DFS) Depth-first-search(s) (s on haun lähtösolmu) 1 ▷(Kaikki solmut: colour := white) 2 ▷(S on muistilistana toimiva pinotietorakenne) 3 Push(S, s) (alkusolmu muistilistalle (valkoisena!)) 4 while S ≠ empty do (kunnes muistilista on tyhjä) 5 u := Pop(S) (uusin alkio listasta) 6 if u→colour = white then (jos kyse on uudesta alkiosta...) 7 u→colour = gray (...niin se on nyt kesken...) 8 Push(S, u) (...ja laitetaan uudelleen listalle) 9 for v in u→Adj do (käy läpi solmun naapurit) 10 if v→colour = white then (jos uusi solmu...) 11 Push(S, v) (...lisätään se listalle) 12 else if v→colour = gray then (päästiin takaisin, silmukka!) 13 ▷(Käsittele silmukka) 14 else 15 u→colour := black (Solmu tuli uudelleen, nyt valmis) Syvyys-ensin-haku (DFS) Depth-first-search(s) 1 ▷(Kaikki solmut: colour := white) 2 ▷(S on muistilistana toimiva pinotietorakenne) 3 Push(S, s) 4 while S ≠ empty do 5 u := Pop(S) 6 if u→colour = white then 7 u→colour = gray 8 Push(S, u) 9 for v in u→Adj do 10 if v→colour = white then 11 Push(S, v) 12 else if v→colour = gray then 13 ▷(Käsittele silmukka) 14 else 15 u→colour := black C B H A I E G D J K F Rekursiivinen (DFS) ●Ihan aluksi kaikki solmut: colour := white ●Muistilista-pinona toimii funktioiden kutsupino! Depth-first-search(s) (s on haun lähtösolmu) 1 s→colour := gray 2 for v in s→Adj do (käy läpi solmun naapurit) 3 if v→colour = white then (jos uusi solmu...) 4 Depth-first-search(v) (...käy se läpi rekursiivisesti) 5 else if v→colour = gray then (päästiin takaisin, silmukka!) 6 ▷(Käsittele silmukka) 7 s→colour := black (Aivan lopuksi väri mustaksi) Graafien toteuttaminen COMP.CS.300 Tietorakenteet ja algoritmit 1 Matti Rintala (matti.rintala@tuni.fi) Graafien toteuttaminen Painotetut graafit ja niiden toteuttaminen COMP.CS.300 Tietorakenteet ja algoritmit 1 Matti Rintala (matti.rintala@tuni.fi) Painotetut graafit ●Paino/kustannus joka kaaressa ●Etsitään keveimpiä/halvimpia polkuje solmujen välillä ●Leveys-ensin ei enää kelpaa! (siinä kaaren \"hinta\" on aina 1) ●Halvin reitti ≠ lyhin reitti (ehkä) ●(Kaaret voivat sisältää myös muuta dataa) ●Usein rajoitetaan paino/kustannus positiiviseksi 3 3 3 1 1 1 3 1 2 1 2 5 6 1 1 Painotetun graafin toteuttaminen 3 3 3 1 1 1 3 1 2 1 2 5 6 1 1 ●Kaaren tiedoksi solmussa ei riitä enää vain sen päätepiste Halvimmat reitit: Dijkstran algoritmi COMP.CS.300 Tietorakenteet ja algoritmit 1 Matti Rintala (matti.rintala@tuni.fi) Dijkstran algoritmi Dijkstra(s) (s on haun lähtösolmu) 1 ▷(Kaikki solmut: colour := white, d := ∞, π := NIL) 2 ▷(Q on muistilistana toimiva prioriteettijono-tietorakenne) 3 s→colour := gray (alkusolmu on kesken) 4 s→d := 0 (etäisyys alkusolmuun 0) 5 Push(Q, s) (alkusolmu muistilistalle) 6 while Q ≠ empty do (kunnes muistilista on tyhjä) 7 u := Extract-min(Q) (halvin alkio listasta) 8 for v in u→Adj do (käy läpi solmun naapurit) 9 Relax(u, v) (laske (tai korjaa) hinta) 10 if v→colour = white then (jos uusi solmu...) 11 v→colour := gray (...se on nyt kesken) 12 Push(Q, v, v→d) (...ja laitetaan jonoon) 13 else 14 ▷Jos hinta pieneni, korjaa prioriteetti Q:ssa! 15 u→colour := black (Nyt s on loppuun käsitelty) 16 Relax(u, v) (Laske/korjaa hinta u:n kautta solmuun v) 17 if v→d > u→d + cost(u, v) then (Jos hinta v:hen u:n kautta on entistä halvempi) 18 v→d := u→d + cost(u, v) (...päivitä hinta) 19 v→π := u (...ja se, että tultiin u:sta) Dijkstran algoritmi C B H A I E G D J K F 3 3 3 1 1 1 3 1 2 1 2 5 6 1 1 Dijkstra(s) 1 ▷(Alusta kaikki solmut) 2 ▷(Q on prioriteettijono) 3 s→colour := gray 4 s→d := 0 5 Push(Q, s) 6 while Q ≠ empty do 7 u := Extract-min(Q) 8 for v in u→Adj do 9 Relax(u, v) 10 if v→colour = white then 11 v→colour := gray 12 Push(Q, v, v→d) 13 else 14 ▷Jos hinta pieneni, korjaa prioriteetti Q:ssa! 15 u→colour := black 16 Relax(u, v) 17 if v→d > u→d + cost(u, v) then 18 v→d := u→d + cost(u, v) 19 v→π := u Prioriteettijono & Dijkstra •Dijkstran toteutus edellyttää minimi-prioriteettijonon – std::priority_queue on maksimi- prioriteettijono ●Tallenna prioriteetiksi -d! 😜 ●(Tai mukauta prioriteettijonoa) •Prioriteettia pystyttävä päivittämään – std::priority_queue ei tarjoa tätä ●Lisää solmu aina uudelleen (prioriteetti aina parempi), jätä myöhemmät kopiot huomiotta – Tai käytä jotain muuta prioriteettijonona (esim. std::set<std::pair<int, Node*>>) Halvimmat reitit: A* ja heuristiikat COMP.CS.300 Tietorakenteet ja algoritmit 1 Matti Rintala (matti.rintala@tuni.fi) A*-algoritmi A*(s, g) (s on lähtösolmu, g maali) 1 ▷(Kaikki solmut: colour := white, d := ∞, de := ∞, π := NIL) 2 ▷(Q on muistilistana toimiva prioriteettijono-tietorakenne) 3 s→colour := gray (alkusolmu on kesken) 4 s→d := 0 (etäisyys alkusolmuun 0) 5 Push(Q, s) (alkusolmu muistilistalle) 6 while Q ≠ empty do (kunnes muistilista on tyhjä) 7 u := Extract-min(Q) (halvin alkio listasta) 8 if u = g then Maali löytynyt, lopeta! ▷ 9 for v in u→Adj do (käy läpi solmun naapurit) 10 Relax-A*(u, v, g) (laske (tai korjaa) hinta) 11 if v→colour = white then (jos uusi solmu...) 12 v→colour := gray (...se on nyt kesken) 13 Push(Q, v, v→de) (...ja laitetaan jonoon) 14 else 15 ▷Jos hinta pieneni, korjaa prioriteetti Q:ssa! 16 u→colour := black (Nyt s on loppuun käsitelty) 17 Relax-A*(u, v, g) (Laske/korjaa hinta+arvio u:n kautta) 18 if v→d > u→d + cost(u, v) then (Jos hinta v:hen u:n kautta on entistä halvempi) 19 v→d := u→d + cost(u, v) (...päivitä hinta) 20 v→de := v→d + min-est(v, g) (...päivitä minimiarvio kokonaishinnasta) 21 v→π := u (...ja se, että tultiin u:sta) A*-algoritmi C B H A I E G D J K F 3 3 3 1 1 1 3 1 2 1 2 5 6 1 1 A*(s, g) 1 ▷(Alusta kaikki solmut) 2 ▷(Q on prioriteettijono) 3 s→colour := gray 4 s→d := 0 5 Push(Q, s) 6 while Q ≠ empty do 7 u := Extract-min(Q) 8 if u = g then Maali löytynyt, lopeta! ▷ 9 for v in u→Adj do 10 Relax-A*(u, v, g) 11 if v→colour = white then 12 v→colour := gray 13 Push(Q, v, v→de) 14 else 15 ▷Jos hinta pieneni, korjaa prioriteetti Q:ssa! 16 u→colour := black 17 Relax-A*(u, v, g) 18 if v→d > u→d + cost(u, v) then 19 v→d := u→d + cost(u, v) 20 v→de := v→d + min-est(v, g) 21 v→π := u Graafihakujen tehokkuus ja yhteenveto COMP.CS.300 Tietorakenteet ja algoritmit 1 Matti Rintala (matti.rintala@tuni.fi) Leveys-ensin-haku (BFS) Breath-first-search(s) (s on haun lähtösolmu) 1 ▷(Kaikki solmut: colour := white, d := ∞, π := NIL) 2 ▷(Q on muistilistana toimiva jonotietorakenne) 3 s→colour := gray (alkusolmu on kesken) 4 s→d := 0 (etäisyys alkusolmuun 0) 5 Push(Q, s) (alkusolmu muistilistalle) 6 while Q ≠ empty do (kunnes muistilista on tyhjä) 7 u := Pop(Q) (vanhin alkio listasta) 8 for v in u→Adj do (käy läpi solmun naapurit) 9 if v→colour = white then (jos uusi solmu...) 10 v→colour := gray (...se on nyt kesken) 11 v→d := u→d + 1 (...ja yhtä kauempana) 12 v→π := u (...ja siihen tultiin u:sta) 13 Push(Q, v) (Lisätään v listalle) 14 u→colour := black (Nyt s on loppuun käsitelty) Syvyys-ensin-haku (DFS) Depth-first-search(s) (s on haun lähtösolmu) 1 ▷(Kaikki solmut: colour := white) 2 ▷(S on muistilistana toimiva pinotietorakenne) 3 Push(S, s) (alkusolmu muistilistalle (valkoisena!)) 4 while S ≠ empty do (kunnes muistilista on tyhjä) 5 u := Pop(S) (uusin alkio listasta) 6 if u→colour = white then (jos kyse on uudesta alkiosta...) 7 u→colour = gray (...niin se on nyt kesken...) 8 Push(S, u) (...ja laitetaan uudelleen listalle) 9 for v in u→Adj do (käy läpi solmun naapurit) 10 if v→colour = white then (jos uusi solmu...) 11 Push(S, v) (...lisätään se listalle) 12 else if v→colour = gray then (päästiin takaisin, silmukka!) 13 ▷(Käsittele silmukka) 14 else 15 u→colour := black (Solmu tuli uudelleen, nyt valmis) Dijkstran algoritmi Dijkstra(s) (s on haun lähtösolmu) 1 ▷(Kaikki solmut: colour := white, d := ∞, π := NIL) 2 ▷(Q on muistilistana toimiva prioriteettijono-tietorakenne) 3 s→colour := gray (alkusolmu on kesken) 4 s→d := 0 (etäisyys alkusolmuun 0) 5 Push(Q, s) (alkusolmu muistilistalle) 6 while Q ≠ empty do (kunnes muistilista on tyhjä) 7 u := Extract-min(Q) (halvin alkio listasta) 8 for v in u→Adj do (käy läpi solmun naapurit) 9 Relax(u, v) (laske (tai korjaa) hinta) 10 if v→colour = white then (jos uusi solmu...) 11 v→colour := gray (...se on nyt kesken) 12 Push(Q, v, v→d) (...ja laitetaan jonoon) 13 else 14 ▷Jos hinta pieneni, korjaa prioriteetti Q:ssa! 15 u→colour := black (Nyt s on loppuun käsitelty) 16 Relax(u, v) (Laske/korjaa hinta u:n kautta solmuun v) 17 if v→d > u→d + cost(u, v) then (Jos hinta v:hen u:n kautta on entistä halvempi) 18 v→d := u→d + cost(u, v) (...päivitä hinta) 19 v→π := u (...ja se, että tultiin u:sta) A*-algoritmi A*(s, g) (s on lähtösolmu, g maali) 1 ▷(Kaikki solmut: colour := white, d := ∞, de := ∞, π := NIL) 2 ▷(Q on muistilistana toimiva prioriteettijono-tietorakenne) 3 s→colour := gray (alkusolmu on kesken) 4 s→d := 0 (etäisyys alkusolmuun 0) 5 Push(Q, s) (alkusolmu muistilistalle) 6 while Q ≠ empty do (kunnes muistilista on tyhjä) 7 u := Extract-min(Q) (halvin alkio listasta) 8 for v in u→Adj do (käy läpi solmun naapurit) 9 Relax-A*(u, v, g) (laske (tai korjaa) hinta) 10 if v→colour = white then (jos uusi solmu...) 11 v→colour := gray (...se on nyt kesken) 12 Push(Q, v, v→de) (...ja laitetaan jonoon) 13 else 14 ▷Jos hinta pieneni, korjaa prioriteetti Q:ssa! 15 u→colour := black (Nyt s on loppuun käsitelty) 16 Relax-A*(u, v, g) (Laske/korjaa hinta+arvio u:n kautta) 17 if v→d > u→d + cost(u, v) then (Jos hinta v:hen u:n kautta on entistä halvempi) 18 v→d := u→d + cost(u, v) (...päivitä hinta) 19 v→de := v→d + min-est(v, g) (...päivitä minimiarvio kokonaishinnasta) 20 v→π := u (...ja se, että tultiin u:sta) Algoritmi(par) 1 koodia (kommentti) 2 koodia (kommentti)"
            },
            {
                "role": "user",
                "content": "Can you summarize the key topics covered in this lecture?"
            },
            {
                "role": "assistant",
                "content": "This lecture covers foundational algorithmic concepts including problem formulation, correctness proof using induction, efficiency analysis using asymptotic notation, data structures (arrays, linked lists, BSTs, heaps), and sorting methods (selection sort, merge sort, counting sort). Additionally, it introduces the Word-RAM computational model, providing a theoretical framework for measuring algorithm efficiency."
            }
        ]
    },
    {
        "system": "Hello! I am Kiran, an AI assistant from Tampere University, dedicated to providing you with accurate and insightful assistance. My goal is to help you understand concepts, summarize lectures, and answer questions effectively.",
        "conversations": [
            {
                "role": "host",
                "content": "Hajautustaulut (hash tables) COMP.CS.300 Tietorakenteet ja algoritmit 1 Matti Rintala (matti.rintala@tuni.fi) Hajautustaulun tarve •Vektorin indeksointi nopea •Assosiatiivinen säiliö: hakuavain •Vektorin indeksi on hakuavain! •Miten mielivaltainen hakuavain? •Ämpärit (buckets) •Hajautusfunktio (hash function) •Vektori nopea •Vektori myös (nopea) hakurakenne •Vektorin hakurajoitukset (int, peräkkäiset arvot) •Hajautustaulun tavoite •Hajautusfunktio avain->int •Sitten int -> % n •Useat osumat Hajautustaulujen toteutus COMP.CS.300 Tietorakenteet ja algoritmit 1 Matti Rintala (matti.rintala@tuni.fi) Ketjutettu hajautus (chained hashing) Esimerkki hash alkukirjain 0 _ H P X 1 A I Q Y 2 B J R Z 3 C K S Å 4 D L T Ä 5 E M U Ö 6 F N V 7 G O W Matti Ulla Erkki Olli Ilkka Tiina Anne 0 7 6 5 4 3 2 1 Suljettu hajautus (closed hashing) •Ämpäri terminä, sen idea •Vaihtoehtoja samaan ämpäriin osumiselle – Ketjutettu hajautus – Suljettu hajautus •Esimerkki (huonosta) hajautuksesta (vanhasta prujusta) •C++:n ämpärirajapinta Hajautusfunktiot COMP.CS.300 Tietorakenteet ja algoritmit 1 Matti Rintala (matti.rintala@tuni.fi) Hajautusfunktiot ●Hyvä hajautusfunktio – Nopea – Deterministinen (aina sama tulos samalla avaimella)! – Tulos mahd. tasaisesti jakautunut kokonaisluku – Käyttää kaikkea hakuavaimen dataa – Tasoittaa avaimissa esiintyvät säännönmukaisuudet Hajautusfunktiot Matti Ulla Erkki Olli Ilkka Tiina Anne 0 7 6 5 4 3 2 1 Hajautusfunktiot ●Käytännössä – Hyvän hajautusfunktion itse keksiminen vaikeaa! – (Huonon keksiminen turhankin helppoa) – Älä keksi itse, etsi hyväksi todettu funktio hakuavaimellesi – Ohjelmointikielissä ja kirjastoissa usein valmiit (toivottavasti hyvät) hajautusfunktiot perustyypeille, esim. std::hash<tyyppi> – Jos avaimessa monta osaa, voi joka osan hajauttaa erikseen ja yhdistää osat sopivalla kaavalla (älä keksi sitäkään itse!) Jakomenetelmä & ämpäreiden määrä • Lasketaan datasta nopeasti & helposti yksi kok.luku k (esim. summa tms.) • Jakojäännös (k % ämpärilkm) • Jos tuloksessa säännöllisyyttä: riski epätasaisesta hajautumisesta ämpäreihin • Jos ämpäreiden määrä (sopiva) alkuluku, riski pienempi Matti Ulla Erkki Olli Ilkka Tiina Anne 0 7 6 5 4 3 2 1 Toinen esimerkki hajautusfunktiosta •Kertomenetelmä – Yhdistetään data yhdeksi kokonaisluvuksi k – m on ämpäreiden määrä – Valitaan vakio A siten, että 0.0 < A < 1.0 – – Mikä on hyvä arvo A:lle? – Suurin osa arvoista ok – Kuulemma A ≈ usein hyvä h(k)=⌊m(kA−⌊kA ⌋)⌋ √5−1 2 •Millainen on hyvä hajautusfunktio •Valmiit hajautusfunktiot •Esimerkkejä hajautuksesta •Hajautusten yhdistäminen •Miksi ämpäreiden määrä alkuluku? •C++ ja omat hajautusfunktiot unordered_map/set:ssä Hajautustaulun tehokkuus, uudelleenhajautus (rehashing) COMP.CS.300 Tietorakenteet ja algoritmit 1 Matti Rintala (matti.rintala@tuni.fi) Hajautustaulun tehokkuus Matti Ulla Erkki Olli Ilkka Tiina Anne 0 7 6 5 4 3 2 1 Uudelleenhajautus ●H:taulun täyttöaste (load factor): n / ämpärilkm ●Asetetaan tälle yläraja ●Yläraja ylitys → varataan isompi vektori, hajautetaan alkiot sinne ●Esim. ~ tuplataan ämpärit → lisäys amortisoidusti keskim. vakio ●Huom! Yksittäinen ämpäri voi edelleen olla iso! Matti Ulla Erkki Olli Ilkka Tiina Anne 0 7 6 5 4 3 2 1 •Hajautustaulun tehokkuus •Uudelleenhajautuksen tarve, vaikutus asympt. tehokkuuteen •Täyttöaste (load factor) •Uudelleenhajautus •Pahin tapaus uudelleenhajautuksesta riippumatta •C++:n uudelleenhajautusrajapinta Algoritmi(par) 1 koodia (kommentti) 2 koodia (kommentti)"
            },
            {
                "role": "user",
                "content": "Can you summarize the key topics covered in this lecture?"
            },
            {
                "role": "assistant",
                "content": "This lecture covers foundational algorithmic concepts including problem formulation, correctness proof using induction, efficiency analysis using asymptotic notation, data structures (arrays, linked lists, BSTs, heaps), and sorting methods (selection sort, merge sort, counting sort). Additionally, it introduces the Word-RAM computational model, providing a theoretical framework for measuring algorithm efficiency."
            }
        ]
    },
    {
        "system": "Hello! I am Kiran, an AI assistant from Tampere University, dedicated to providing you with accurate and insightful assistance. My goal is to help you understand concepts, summarize lectures, and answer questions effectively.",
        "conversations": [
            {
                "role": "host",
                "content": "Keko taulukkona COMP.CS.300 Tietorakenteet ja algoritmit 1 Matti Rintala (matti.rintala@tuni.fi) Kertaus : keko ●Binääripuu ●Solmun arvo ≥ lapsien arvot ●Täydellisesti tasapainotettu ●Haarojen pituusero korkeintaan 1 ●(Pisimmät haarat vasemmalla) 17 12 15 7 14 8 10 2 7 9 1 3 Keko taulukkona ●Säästää muistia, on tehokkaampi ●Solmun yksilöi indeksi i ●Keon juuri: A[1] ●Solmun i lapset Left-Child(i): return 2*i Right-Child(i): return 2*i + 1 ●Solmun vanhempi Parent(i): return ⌊i / 2 (pyöristys alas) ⌋ ●(A.heapsize pitää kirjaa, \"mihin keko loppuu\") 17 12 15 7 14 8 10 2 7 9 1 3 1 2 3 4 5 6 7 8 9 10 11 12 17 15 12 7 14 8 10 2 7 9 1 3 Kekoon lisääminen (Heap Insert) Heap-Insert(A, value): 1 Insert-Last(A, value) 2 A.heapsize := A.heapsize + 1 3 i := A.heapsize 4 while i > 1 and A[i] > A[Parent(i)] 5 A[i] ⇄ A[Parent(i)] 6 i := Parent(i) Idea: Laitetaan uusi alkio keon “pohjalle” ja liu’utetaan sitä ylöspäin, kunnes se on arvonsa kannalta mahdollisessa paikassa. Keon korjaaminen (Heapify) ●Oletus: keko ok, paitsi ehkä solmu i Heapify(A, i): 1 repeat 2 orig_i := i 3 lc := Left-Child(i) 4 rc := Right-Child(i) 5 if lc < A.heapsize and A[lc] > A[i] then 6 i := lc 7 if rc < A.heapsize and A[rc] > A[i] then 8 i := rc 9 if i ≠ orig_i then 10 A[i] ⇄A[orig_i] 11 until i = orig_i Keon luominen (Build Heap) ●Oletus: Puun alkiot missä tahansa järjestyksessä Build-Heap(A): 1 A.heapsize := A.length (alustetaan koko) 2 for i in ⌊A.heapsize / 2⌋ downto 1 3 Heapify(A, i) Suurimman poistaminen (Heap Extract Max) ●(Tämä jättää A:n loppuun ylimääräisen alkion, jonka voi poistaa) Heap-Extract-Max(A): 1 max := A[1] 2 last_node := A.heapsize 3 A[1] := A[last_node] 4 A.heapsize := A.heapsize - 1 5 Heapify(A, 1) 6 return max Suunnitteluperiaate Muunna-ja-hallitse, kekolajittelu COMP.CS.300 Tietorakenteet ja algoritmit 1 Matti Rintala (matti.rintala@tuni.fi) Muunna-ja-hallitse (Transform and conquer) ●Strategia ●Muunnetaan ongelma toiseen muotoon ●Ratkaistaan tämä helpommin/tehokkaammin ●Ratkaisu muutetaan alkup. ongelman ratkaisuksi ●Esimerkkejä muunnoksista: ●Yksinkertaistaminen: \"kätevämpi\" instanssi samasta ongelmasta ●Esitystavan muutos: saman instanssin toinen esitystapa ●Ongelman muunnos: muutetaan ongelmaksi, jolle on jo valmis algoritmi Kekolajittelu (heap sort) ●Ongelma: Miten lajitella taulukko? ●Muunnetaan taulukko keoksi ●(Tehostetaan kekoa toteuttamalla taulukkona) ●Poimitaan alkiot keosta suuruusjärjestyksessä Heap-Sort(A): 1 Build-Heap(A) 2 while A.heapsize > 1 do 3 A[1] ⇄ A[A.heapsize] 4 A.heapsize := A.heapsize - 1 5 Heapify(A, 1) Kekolajittelu (Heap Sort) Heap-Sort(A): 1 Build-Heap(A) 2 while A.heapsize > 1 do 3 A[1] ⇄ A[A.heapsize] 4 A.heapsize := A.heapsize - 1 5 Heapify(A, 1) 17 12 15 7 14 8 10 2 7 9 1 3 1 2 3 4 5 6 7 8 9 10 11 12 17 15 12 7 14 8 10 2 7 9 1 3 Tehokkuuksien vertailua Tapaus Insertion sort Quick sort Merge sort Heap sort Paras n n log n n log n Keskimäärin n² n log n n log n Huonoin n² n² n log n Lisämuistitarve Θ(n)!"
            },
            {
                "role": "user",
                "content": "Can you summarize the key topics covered in this lecture?"
            },
            {
                "role": "assistant",
                "content": "This lecture covers foundational algorithmic concepts including problem formulation, correctness proof using induction, efficiency analysis using asymptotic notation, data structures (arrays, linked lists, BSTs, heaps), and sorting methods (selection sort, merge sort, counting sort). Additionally, it introduces the Word-RAM computational model, providing a theoretical framework for measuring algorithm efficiency."
            }
        ]
    },
    {
        "system": "Hello! I am Kiran, an AI assistant from Tampere University, dedicated to providing you with accurate and insightful assistance. My goal is to help you understand concepts, summarize lectures, and answer questions effectively.",
        "conversations": [
            {
                "role": "host",
                "content": "Keko (heap) COMP.CS.300 Tietorakenteet ja algoritmit 1 Matti Rintala (matti.rintala@tuni.fi) Keon operaatiot COMP.CS.300 Tietorakenteet ja algoritmit 1 Matti Rintala (matti.rintala@tuni.fi) Keko ●Binääripuu ●Solmun arvo ≥ lapsien arvot (max-keko) ●tai solmun arvo ≤ lapsien arvot (min-keko) ●Täydellisesti tasapainotettu ●Haarojen pituusero korkeintaan 1 ●(Pisimmät haarat vasemmalla) 17 12 15 7 14 8 10 2 7 9 1 3 Kekoon lisääminen (Heap Insert) Heap-Insert(root, node): 1 ▷Add node as the lowest-level rightmost leaf. 2 while node ≠ root and node.value > parent(node).value 3 node.value ⇄ parent(node).value 4 node := node.parent Idea: Laitetaan uusi alkio keon “pohjalle” ja liu’utetaan sitä ylöspäin, kunnes se on arvonsa kannalta mahdollisessa paikassa. Keon korjaaminen (Heapify) ●Oletus: keko ok, paitsi ehkä node Heapify(root, node): 1 repeat 2 orig_node := node 3 l := Left-Child(node) 4 r := Right-Child(node) 5 if l exists and l.value > node.value then 6 node := l 7 if r exists and r.value > node.value then 8 node := r 9 if node ≠ orig_node then 10 node.value ⇄orig_node.value 11 until node = orig_node 10 12 15 7 14 8 10 2 7 9 1 3 Keon luominen (Build Heap) ●Oletus: Puun alkiot missä tahansa järjestyksessä Build-Heap(root): 1 for node in ▷heap non-leaf nodes in bottom-to-up, right-to-left order 2 Heapify(root, node) Suurimman poistaminen (Heap Extract Max) Heap-Extract-Max(root): 1 max := root.value 2 node := lowest-level rightmost leaf ▷ 3 root.value := node.value 4 ▷remove node 5 Heapify(root, root) (Heapify from root down) 6 return max 17 12 15 7 14 8 10 2 7 9 1 3 Keon operaatioiden tehokkuus COMP.CS.300 Tietorakenteet ja algoritmit 1 Matti Rintala (matti.rintala@tuni.fi)"
            },
            {
                "role": "user",
                "content": "Can you summarize the key topics covered in this lecture?"
            },
            {
                "role": "assistant",
                "content": "This lecture covers foundational algorithmic concepts including problem formulation, correctness proof using induction, efficiency analysis using asymptotic notation, data structures (arrays, linked lists, BSTs, heaps), and sorting methods (selection sort, merge sort, counting sort). Additionally, it introduces the Word-RAM computational model, providing a theoretical framework for measuring algorithm efficiency."
            }
        ]
    },
    {
        "system": "Hello! I am Kiran, an AI assistant from Tampere University, dedicated to providing you with accurate and insightful assistance. My goal is to help you understand concepts, summarize lectures, and answer questions effectively.",
        "conversations": [
            {
                "role": "host",
                "content": "Lisäysjärjestäminen (Insertion sort) COMP.CS.300 Tietorakenteet ja algoritmit 1 Matti Rintala (matti.rintala@tuni.fi) Insertion-Sort(A) 1 for j := 2 to A.length do 2 key := A[j] 3 i := j − 1 4 while i > 0 and A[i] > key do 5 A[i + 1] := A[i] 6 i := i − 1 7 A[i + 1] := key Insertion-Sort(A) 1 for next_elem := 2 to A.length do 2 key := A[next_elem] 3 place := next_elem − 1 4 while place > 0 and A[place] > key do 5 A[place + 1] := A[place] 6 place := place − 1 7 A[place + 1] := key (syöte saadaan taulukossa A) (siirretään osien välistä rajaa) (otetaan uusi alkio käsittelyyn) (etsitään uudelle alkiolle oikea paikka) (raivataan uudelle alkiolle tilaa) (asetetaan uusi alkio oikealle paikalleen) 31 41 59 26 41 58 31 41 59 26 41 58 31 41 59 26 41 58 31 41 59 26 41 58 31 41 59 26 41 58 31 41 59 26 41 58 31 41 59 26 41 58 31 41 59 26 41 58 31 41 59 26 41 58 26 31 41 59 41 58 31 41 59 26 41 58 31 41 59 26 41 58 31 41 59 26 41 58 26 31 41 59 41 58 26 31 41 41 59 58 31 41 59 26 41 58 31 41 59 26 41 58 31 41 59 26 41 58 26 31 41 59 41 58 26 31 41 41 59 58 26 31 41 41 58 59"
            },
            {
                "role": "user",
                "content": "Can you summarize the key topics covered in this lecture?"
            },
            {
                "role": "assistant",
                "content": "This lecture covers foundational algorithmic concepts including problem formulation, correctness proof using induction, efficiency analysis using asymptotic notation, data structures (arrays, linked lists, BSTs, heaps), and sorting methods (selection sort, merge sort, counting sort). Additionally, it introduces the Word-RAM computational model, providing a theoretical framework for measuring algorithm efficiency."
            }
        ]
    },
    {
        "system": "Hello! I am Kiran, an AI assistant from Tampere University, dedicated to providing you with accurate and insightful assistance. My goal is to help you understand concepts, summarize lectures, and answer questions effectively.",
        "conversations": [
            {
                "role": "host",
                "content": "Järjestysalgoritmien tehokkuuden vertailu COMP.CS.300 Tietorakenteet ja algoritmit 1 Matti Rintala (matti.rintala@tuni.fi) Mitä voidaan vertailla? ●Asymptoottinen tehokkuus – Huonoin tapaus (tai ainakin O) – Paras tapaus (tai ainakin Ω) – Keskimääräinen tapaus (mitä tarkoittaa?) Mitä voidaan vertailla? ●Todellinen tehokkuus – Riippuu toteutuksesta! – Usein voidaan vain valistuneesta arvailla – ...tai testata jollain toteutuksella Mitä voidaan vertailla? ●Asymptoottinen tehokkuus – Huonoin tapaus (tai ainakin O) – Paras tapaus (tai ainakin Ω) – Keskimääräinen tapaus (mitä tarkoittaa?) ●Todellinen tehokkuus – Riippuu toteutuksesta! – Usein voidaan vain valistuneesta arvailla – ...tai testata jollain toteutuksella Insertion-Sort(A) 1 for next_elem := 2 to A.length do 2 key := A[next_elem] 3 place := next_elem − 1 4 while place > 0 and A[place] > key do 5 A[place + 1] := A[place] 6 place := place − 1 7 A[place + 1] := key Quicksort(A, left, right) 1 if left < right then (triviaalitapaukselle ei tehdä mitään) 2 pivot := Partition(A, left, right) (jaetaan pieniin ja suuriin, pivot jakokohta) 3 Quicksort(A, left, pivot−1 ) (järjestetään jakoalkiota pienemmät) 4 Quicksort(A, pivot+1, right) (järjestetään jakoalkiota suuremmat) Partition(A, left, right) 1 pivot := A[right] (otetaan pivotiksi viimeinen alkio) 2 cut := left − 1 (merkitään cut:lla pienten puolen loppua) 3 for i := left to right−1 do (käydään läpi toiseksi viimeiseen alkioon asti) 4 if A[i] ≤ pivot (jos A[i] kuuluu pienten puolelle...) 5 cut := cut + 1 (... kasvatetaan pienten puolta...) 6 A[cut] ⇄ A[i] (... ja siirretään A[i] sinne) 7 A[ cut+1 ] ⇄ A[ right ] (sijoitetaan pivot pienten ja isojen puolten väliin) 8 return cut+1 (palautetaan pivot-alkion uusi sijainti) Mergesort(A, left, right) 1 if left < right then (triviaalitapaukselle ei tehdä mitään) 2 mid := (⌊left + right) / 2⌋ (lasketaan puoliväli) 3 Mergesort(A, left, mid ) (järjestetään vasen puoli) 4 Mergesort(A, mid+1, right) (järjestetään oikea puoli) 5 Merge(A, left, mid, right) (lomitetaan järjestetyt puolikkaat yhteen) Merge(A, left, mid, right) 1 for i := left to right do 2 Tmp[i] := A[i] 3 out := left 4 in_l := left; in_r := mid + 1 5 while in_l ≤ mid and in_r ≤ right do 6 if Tmp[in_l] ≤ Tmp[in_r] then 7 A[out] := Tmp[in_l] 8 in_l := in_l + 1 9 else 10 A[out] := Tmp[in_r] 11 in_r := in_r + 1 12 out := out + 1 13 if in_l > mid then 14 in_rest := in_r 15 else 16 in_rest := in_l 17 for i := 0 to right-out do 18 A[out + i] := Tmp[in_rest + i]"
            },
            {
                "role": "user",
                "content": "Can you summarize the key topics covered in this lecture?"
            },
            {
                "role": "assistant",
                "content": "This lecture covers foundational algorithmic concepts including problem formulation, correctness proof using induction, efficiency analysis using asymptotic notation, data structures (arrays, linked lists, BSTs, heaps), and sorting methods (selection sort, merge sort, counting sort). Additionally, it introduces the Word-RAM computational model, providing a theoretical framework for measuring algorithm efficiency."
            }
        ]
    },
    {
        "system": "Hello! I am Kiran, an AI assistant from Tampere University, dedicated to providing you with accurate and insightful assistance. My goal is to help you understand concepts, summarize lectures, and answer questions effectively.",
        "conversations": [
            {
                "role": "host",
                "content": "Lambda-funktiot COMP.CS.300 Tietorakenteet ja algoritmit 1 Matti Rintala (matti.rintala@tuni.fi) Motivaatio •Nimetty muuttuja vs. literaali Motivaatio •Nimetty funktio vs. lambda-funktio C++:n lambdojen syntaksi •(Paluutyyppi päätellään) [](int x, int y) { return x+y; } •(Pakotettu paluutyyppi) [](double a)->int { return a/2; } auto-parametrit •Kääntäjän päättelemät parametrityypit [](auto x, auto y) { return x+y; } [](auto& x){ x = x+1; } Lambda-funktiot ja muuttujien kaappaaminen COMP.CS.300 Tietorakenteet ja algoritmit 1 Matti Rintala (matti.rintala@tuni.fi) Muuttujien \"kaappaaminen\" (capture) •Lambda voi kopioida tarvitsemiaan muuttujia (tai viitata niihin) •[x](auto y){ return y>x; } •[&x](auto y){ x = x + y; } •(Vrt. funktioiden arvo- ja viiteparametrit)"
            },
            {
                "role": "user",
                "content": "Can you summarize the key topics covered in this lecture?"
            },
            {
                "role": "assistant",
                "content": "This lecture covers foundational algorithmic concepts including problem formulation, correctness proof using induction, efficiency analysis using asymptotic notation, data structures (arrays, linked lists, BSTs, heaps), and sorting methods (selection sort, merge sort, counting sort). Additionally, it introduces the Word-RAM computational model, providing a theoretical framework for measuring algorithm efficiency."
            }
        ]
    },
    {
        "system": "Hello! I am Kiran, an AI assistant from Tampere University, dedicated to providing you with accurate and insightful assistance. My goal is to help you understand concepts, summarize lectures, and answer questions effectively.",
        "conversations": [
            {
                "role": "host",
                "content": "Lomitusjärjestäminen (Merge Sort) COMP.CS.300 Tietorakenteet ja algoritmit 1 Matti Rintala (matti.rintala@tuni.fi) Lomitusjärjestäminen (Merge Sort) ●Hajoita ja hallitse -algoritmi ●Rekursiivinen hajoita: ●Pilkotaan data aina puoliksi ●(Palat aina (±1) yhtä suuria) ●Järjestetään palat rekursiivisesti samalla algoritmilla ●1:n mittaista ei enää tarvitse hajottaa ●Hallitse: ●\"Lomitetaan\" järjestetyt puolikkaat yhdeksi järjestetyksi vastaukseksi ●Tehdään joka rekursiotasolla Mergesort(A, left, right) 1 if left < right then (triviaalitapaukselle ei tehdä mitään) 2 mid := (⌊left + right) / 2⌋ (lasketaan puoliväli) 3 Mergesort(A, left, mid ) (järjestetään vasen puoli) 4 Mergesort(A, mid+1, right) (järjestetään oikea puoli) 5 Merge(A, left, mid, right) (lomitetaan järjestetyt puolikkaat yhteen) 10 4 2 6 9 3 14 8 5 10 4 2 6 9 3 14 8 5 10 4 2 6 9 10 4 2 10 4 6 9 3 14 8 5 3 14 3 14 8 5 10 4 2 6 9 3 14 8 5 10 4 2 6 9 10 4 2 10 4 10 4 2 6 9 6 9 3 14 8 5 3 14 8 5 3 14 3 14 8 5 Lomitusjärjestäminen: Lomitus (Merge) COMP.CS.300 Tietorakenteet ja algoritmit 1 Matti Rintala (matti.rintala@tuni.fi) Merge(A, left, mid, right) 1 for i := left to right do 2 Tmp[i] := A[i] 3 out := left 4 in_l := left; in_r := mid + 1 5 while in_l ≤ mid and in_r ≤ right do 6 if Tmp[in_l] ≤ Tmp[in_r] then 7 A[out] := Tmp[in_l] 8 in_l := in_l + 1 9 else 10 A[out] := Tmp[in_r] 11 in_r := in_r + 1 12 out := out + 1 13 if in_l > mid then 14 in_rest := in_r 15 else 16 in_rest := in_l 17 for i := 0 to right-out do 18 A[out + i] := Tmp[in_rest + i] (käydään koko alue läpi...) (... ja kopioidaan se aputaulukkoon) (minne seuraava alkio limitetään) (mistä luetaan vasemmasta ja oikeasta puolesta) (käydään läpi, kunnes jompikumpi osa loppuu) (jos vasemmalla on pienempi kuin oikealla...) (... sijoitetaan se tulostaulukkoon...) (... ja siirretään lukukohtaa) (... muuten otetaan alkio oikealta puolelta.) (siirretään myös tuloksen kirjoituskohtaa) (jos vasen puoli tuli luettua loppuun...) (...täytyy vielä lukea alkioita oikealta...) (...muuten vasemmalta) (siirretään loput alkiot tuloksen loppuun) 2 4 6 9 10 3 5 8 14 2 4 6 9 10 3 5 8 14"
            },
            {
                "role": "user",
                "content": "Can you summarize the key topics covered in this lecture?"
            },
            {
                "role": "assistant",
                "content": "This lecture covers foundational algorithmic concepts including problem formulation, correctness proof using induction, efficiency analysis using asymptotic notation, data structures (arrays, linked lists, BSTs, heaps), and sorting methods (selection sort, merge sort, counting sort). Additionally, it introduces the Word-RAM computational model, providing a theoretical framework for measuring algorithm efficiency."
            }
        ]
    },
    {
        "system": "Hello! I am Kiran, an AI assistant from Tampere University, dedicated to providing you with accurate and insightful assistance. My goal is to help you understand concepts, summarize lectures, and answer questions effectively.",
        "conversations": [
            {
                "role": "host",
                "content": "O-notaatio COMP.CS.300 Tietorakenteet ja algoritmit 1 Matti Rintala (matti.rintala@tuni.fi) Asymptoottinen yläraja, O •Algoritmin käytös: f(n) •f(n) kuuluu kertaluokkaan O(g(n)), jos: •löytyy raja n0 niin, että kun n > n0: •löytyy kerroin c1 niin, että: •f(n) ≤ c1 g(n) •Merkitään: f(n) ∈O(g(n)) Asymptoottinen yläraja, O Datan määrä (n) Aika (t) Ω(Omega)- ja Θ(Theta)- notaatiot COMP.CS.300 Tietorakenteet ja algoritmit 1 Matti Rintala (matti.rintala@tuni.fi) Asymptoottinen alaraja, Ω (Omega) •Algoritmin käytös: f(n) •f(n) kuuluu kertaluokkaan Ω(g(n)), jos: •löytyy raja n0 niin, että kun n > n0: •löytyy kerroin c2 niin, että: •c2 g(n) ≤ f(n) •Merkitään: f(n) ∈Ω(g(n)) Asymptoottinen alaraja, Ω (Omega) Datan määrä (n) Aika (t) Asymptoottinen tarkka raja, Θ (Theta) •Algoritmin käytös: f(n) •f(n) kuuluu kertaluokkaan Θ(g(n)), jos: •löytyy raja n0 niin, että kun n > n0: •löytyy kertoimet c1 ja c2 niin, että: •c2 g(n) ≤ f(n) ≤ c1 g(n) •Merkitään: f(n) ∈Θ(g(n)) •(Θ on sekä O että Ω) Asymptoottinen tarkka raja, Θ (Theta) Datan määrä (n) Aika (t) O, Ω, Θ, paras, huonoin, keskimääräinen... COMP.CS.300 Tietorakenteet ja algoritmit 1 Matti Rintala (matti.rintala@tuni.fi) Insertion-Sort(A) 1 for next_elem := 2 to A.length do 2 key := A[next_elem] 3 place := next_elem − 1 4 while place > 0 and A[place] > key do 5 A[place + 1] := A[place] 6 place := place − 1 7 A[place + 1] := key Datan määrä (n) Aika (t)"
            },
            {
                "role": "user",
                "content": "Can you summarize the key topics covered in this lecture?"
            },
            {
                "role": "assistant",
                "content": "This lecture covers foundational algorithmic concepts including problem formulation, correctness proof using induction, efficiency analysis using asymptotic notation, data structures (arrays, linked lists, BSTs, heaps), and sorting methods (selection sort, merge sort, counting sort). Additionally, it introduces the Word-RAM computational model, providing a theoretical framework for measuring algorithm efficiency."
            }
        ]
    },
    {
        "system": "Hello! I am Kiran, an AI assistant from Tampere University, dedicated to providing you with accurate and insightful assistance. My goal is to help you understand concepts, summarize lectures, and answer questions effectively.",
        "conversations": [
            {
                "role": "host",
                "content": "Pala kerrallaan (Decrease and conquer) COMP.CS.300 Tietorakenteet ja algoritmit 1 Matti Rintala (matti.rintala@tuni.fi) Algoritmin suunnitteluperiaate ●\"Strategia\", \"paradigma\" ●Yleinen lähestymistapa ongelman ratkaisemiseksi ●Muutamia \"perustekniikoita\", jotka toistuvat ●Hyväksi todettuja, tuottavat (riittävän) tehokkaan ratkaisun ●Auttavat ymmärtämään algoritmeja Pala kerrallaan (Decrease and conquer) ●Suoraviivainen ja intuitiivinen lähestymistapa ●Joka kierroksella käsitellään \"pala\" ongelmaa ●Joka kierroksella ongelma pienenee, ratkaisu kasvaa ●Lopuksi ongelma on tyhjä ja ratkaisu valmis Pala kerrallaan edut ja \"haitat\" ●Etuja: – Selkeys, intuitiivisuus: helppo toteuttaa – Yksinkertainen: pieni määrä koodia, CPU:n helppo suorittaa (välimuisti yms.) ●\"Haittoja\" – Hidastuu usein nopeasti datan määrän kasvaessa – Isoilla datamäärillä tarjolla parempia vaihtoehtoja (toiset suunnitteluperiaatteet) Muita tämän kurssin suunnitteluperiaatteita ●Hajota ja hallitse (divide and conquer) – Jaa ongelma pienemmiksi (samankokoisiksi) osaongelmiksi, ratkaise rekursiivisesti ●Muunna ja hallitse (transform and conquer) – Muunna ongelma ensin toiseen, tehokkaammin ratkaistavaan muotoon ●(Muitakin on: satunnaistaminen, laiska laskenta, ...)"
            },
            {
                "role": "user",
                "content": "Can you summarize the key topics covered in this lecture?"
            },
            {
                "role": "assistant",
                "content": "This lecture covers foundational algorithmic concepts including problem formulation, correctness proof using induction, efficiency analysis using asymptotic notation, data structures (arrays, linked lists, BSTs, heaps), and sorting methods (selection sort, merge sort, counting sort). Additionally, it introduces the Word-RAM computational model, providing a theoretical framework for measuring algorithm efficiency."
            }
        ]
    },
    {
        "system": "Hello! I am Kiran, an AI assistant from Tampere University, dedicated to providing you with accurate and insightful assistance. My goal is to help you understand concepts, summarize lectures, and answer questions effectively.",
        "conversations": [
            {
                "role": "host",
                "content": "Ohjelmointiprojekti, vaihe 1 Game of Taxes COMP.CS.300 Tietorakenteet ja algoritmit 1 Matti Rintala (matti.rintala@tuni.fi) Programming project Phase 1 Game of Taxes COMP.CS.300 Data structures and algorithms 1 Matti Rintala (matti.rintala@tuni.fi)"
            },
            {
                "role": "user",
                "content": "Can you summarize the key topics covered in this lecture?"
            },
            {
                "role": "assistant",
                "content": "This lecture covers foundational algorithmic concepts including problem formulation, correctness proof using induction, efficiency analysis using asymptotic notation, data structures (arrays, linked lists, BSTs, heaps), and sorting methods (selection sort, merge sort, counting sort). Additionally, it introduces the Word-RAM computational model, providing a theoretical framework for measuring algorithm efficiency."
            }
        ]
    },
    {
        "system": "Hello! I am Kiran, an AI assistant from Tampere University, dedicated to providing you with accurate and insightful assistance. My goal is to help you understand concepts, summarize lectures, and answer questions effectively.",
        "conversations": [
            {
                "role": "host",
                "content": "Ohjelmointiprojekti, vaihe 2 Connecting Towns COMP.CS.300 Tietorakenteet ja algoritmit 1 Matti Rintala (matti.rintala@tuni.fi) Programming project Phase 2 Connecting Towns COMP.CS.300 Data structures and algorithms 1 Matti Rintala (matti.rintala@tuni.fi)"
            },
            {
                "role": "user",
                "content": "Can you summarize the key topics covered in this lecture?"
            },
            {
                "role": "assistant",
                "content": "This lecture covers foundational algorithmic concepts including problem formulation, correctness proof using induction, efficiency analysis using asymptotic notation, data structures (arrays, linked lists, BSTs, heaps), and sorting methods (selection sort, merge sort, counting sort). Additionally, it introduces the Word-RAM computational model, providing a theoretical framework for measuring algorithm efficiency."
            }
        ]
    },
    {
        "system": "Hello! I am Kiran, an AI assistant from Tampere University, dedicated to providing you with accurate and insightful assistance. My goal is to help you understand concepts, summarize lectures, and answer questions effectively.",
        "conversations": [
            {
                "role": "host",
                "content": "Pseudokoodista toteutukseen COMP.CS.300 Tietorakenteet ja algoritmit 1 Matti Rintala (matti.rintala@tuni.fi) Pseudokoodin toteutus •Sopeuttaminen käyttötilanteeseen •Syötteiden laillisuuden tarkistukset •Virhetilanteiden käsittely •Ohjelmointikielen rajoitukset •Laitteiston ja kielen aiheuttamat nopeus- ja tarkoituksenmukaisuusnäkökohdat •Ylläpidettävyys modulaarisuus jne. ⇒ Pseudokoodi Insertion-Sort(A) 1 for next_elem := 2 to A.length do 2 key := A[next_elem] 3 place := next_elem − 1 4 while place > 0 and A[place] > key do 5 A[place + 1] := A[place] 6 place := place − 1 7 A[place + 1] := key Ohjelmointikielen vaikutus •Indeksointi alkaa 0:sta (pseudokoodissa usein 1:stä) •Käytetäänkö edes indeksointia (tai taulukoita, tai...) •Kopiodaanko data, vai viitataanko siihen epäsuorasti •Jos ulkopuoliseen dataan viitataan epäsuorasti, tapahtuuko se: osoittimella, älyosoittimella (esim. shared_ptr), iteraattorilla (jos data tietorakenteessa), indeksillä (jos data vektorissa tms.), hakuavaimella (jos data tietorakenteessa, josta haku nopeaa) •Ovatko algoritmin \"parametrit\" oikeasti parametreja, vai vain muuttujia tms. name: \"Matti\" age: 23 height: 1.82 name: \"Elisa\" age: 32 height: 1.87 name: \"Lumi\" age: 20 height: 1.65 name: \"Siri\" age: 45 height: 1.50 name: \"Bob\" age: 30 height: 1.97 Toteutus 1 #include <vector> 2 struct Data { string name; int age; float height; }; 3 using Taulukko = std::vector<Data*>; 4 // Täytetään taulukko osoittimilla dataan 5 void insertion_sort(Taulukko& A) { 6 Data* keyp = nullptr; int place = 0; 7 for (Taulukko::size_type next_elem = 1; next_elem < A.size(); ++next_elem) { 8 keyp = Taulukko.at(next_elem); 9 place = next_elem-1; 10 while (place >= 0) { 11 elemp = Taulukko.at(place); 12 assert(elem != nullptr); 13 if (keyp->name < elemp->name) { break; } 14 Taulukko.at(place+1) = elemp; --place; 15 } 16 Taulukko.at(place+1) = keyp; 17 } 18 } Insertion-Sort(A) 1 for next_elem := 2 to A.length do 2 key := A[next_elem] 3 place := next_elem − 1 4 while place > 0 and A[place] > key do 5 A[place + 1] := A[place] 6 place := place − 1 7 A[place + 1] := key"
            },
            {
                "role": "user",
                "content": "Can you summarize the key topics covered in this lecture?"
            },
            {
                "role": "assistant",
                "content": "This lecture covers foundational algorithmic concepts including problem formulation, correctness proof using induction, efficiency analysis using asymptotic notation, data structures (arrays, linked lists, BSTs, heaps), and sorting methods (selection sort, merge sort, counting sort). Additionally, it introduces the Word-RAM computational model, providing a theoretical framework for measuring algorithm efficiency."
            }
        ]
    },
    {
        "system": "Hello! I am Kiran, an AI assistant from Tampere University, dedicated to providing you with accurate and insightful assistance. My goal is to help you understand concepts, summarize lectures, and answer questions effectively.",
        "conversations": [
            {
                "role": "host",
                "content": "Puu-tietorakenteet COMP.CS.300 Tietorakenteet ja algoritmit 1 Matti Rintala (matti.rintala@tuni.fi) Hierakkinen data Hierarkkinen data Lista vs puu Puu-tietorakenne Puu-tietorakenne •Hierarkkinen, rekursiivinen •Data solmuissa •Juuri, sisäsolmut, lehdet •Lapsi, vanhempi •Lasten maksimilukumäärä? (esim. binääripuu) •Alipuut •Korkeus Puu-tietorakenne •Hierarkkinen, rekursiivinen •Data solmuissa •Juuri, sisäsolmut, lehdet •Lapsi, vanhempi •Lasten maksimilukumäärä? (esim. binääripuu) •Alipuut •Korkeus Puiden toteuttaminen COMP.CS.300 Tietorakenteet ja algoritmit 1 Matti Rintala (matti.rintala@tuni.fi) Puu-tietorakenne Puiden läpikäynti (iterointi) COMP.CS.300 Tietorakenteet ja algoritmit 1 Matti Rintala (matti.rintala@tuni.fi) Esijärjestys (preorder) Preorder-tree-walk(node) 1 if node ≠ NIL then 2 ▷(käsittele alkio node) 3 for child in node→children do 4 Preorder-tree-walk(child) 7 2 11 1 4 5 8 3 9 10 6 Jälkijärjestys (postorder) Postorder-tree-walk(node) 1 if node ≠ NIL then 2 for child in node→children do 3 Postorder-tree-walk(child) 4 ▷(käsittele alkio node) 10 5 9 11 1 3 8 2 7 6 4 Välijärjestys (inorder) Inorder-tree-walk(node) 1 if node ≠ NIL then 2 Inorder-tree-walk(node→left-child) 3 ▷(käsittele alkio node) 4 Inorder-tree-walk(node→right-child) ●Huom! Järkevä vain binääripuille 10 4 11 6 1 5 9 2 7 8 3"
            },
            {
                "role": "user",
                "content": "Can you summarize the key topics covered in this lecture?"
            },
            {
                "role": "assistant",
                "content": "This lecture covers foundational algorithmic concepts including problem formulation, correctness proof using induction, efficiency analysis using asymptotic notation, data structures (arrays, linked lists, BSTs, heaps), and sorting methods (selection sort, merge sort, counting sort). Additionally, it introduces the Word-RAM computational model, providing a theoretical framework for measuring algorithm efficiency."
            }
        ]
    },
    {
        "system": "Hello! I am Kiran, an AI assistant from Tampere University, dedicated to providing you with accurate and insightful assistance. My goal is to help you understand concepts, summarize lectures, and answer questions effectively.",
        "conversations": [
            {
                "role": "host",
                "content": "Pikajärjestäminen (Quick Sort) COMP.CS.300 Tietorakenteet ja algoritmit 1 Matti Rintala (matti.rintala@tuni.fi) Pikajärjestäminen (Quick Sort) ●Hajoita ja hallitse -algoritmi ●Hajoita: ●Pilkotaan data \"pieniin\" ja \"suuriin\" ●(Ideaalitapauksessa palat yhtä suuria) ●Rekursiivinen hallitse: ●Järjestetään palat rekursiivisesti samalla algoritmilla ●1:n mittaista ei enää tarvitse järjestää Quicksort(A, left, right) 1 if left < right then (triviaalitapaukselle ei tehdä mitään) 2 pivot := Partition(A, left, right) (jaetaan pieniin ja suuriin, pivot jakokohta) 3 Quicksort(A, left, pivot−1 ) (järjestetään jakoalkiota pienemmät) 4 Quicksort(A, pivot+1, right) (järjestetään jakoalkiota suuremmat) 10 4 2 6 9 3 14 8 5 4 2 3 5 9 10 14 8 6 2 3 4 5 9 10 14 8 6 2 3 4 5 6 10 14 8 9 2 3 4 5 6 8 9 10 14 2 3 4 5 6 8 9 10 14 2 3 4 5 6 8 9 10 14 2 3 4 5 6 8 9 10 14 2 3 4 5 6 10 14 8 9 2 3 4 5 9 10 14 8 6 10 4 2 6 9 3 14 8 5 4 2 3 5 9 10 14 8 6 Pikajärjestäminen: Ositus (Partition) COMP.CS.300 Tietorakenteet ja algoritmit 1 Matti Rintala (matti.rintala@tuni.fi) Partition(A, left, right) 1 pivot := A[right] (otetaan pivotiksi viimeinen alkio) 2 cut := left − 1 (merkitään cut:lla pienten puolen loppua) 3 for i := left to right−1 do (käydään läpi toiseksi viimeiseen alkioon asti) 4 if A[i] ≤ pivot (jos A[i] kuuluu pienten puolelle...) 5 cut := cut + 1 (... kasvatetaan pienten puolta...) 6 A[cut] ⇄ A[i] (... ja siirretään A[i] sinne) 7 A[ cut+1 ] ⇄ A[ right ] (sijoitetaan pivot pienten ja isojen puolten väliin) 8 return cut+1 (palautetaan pivot-alkion uusi sijainti) 10 4 2 6 9 3 14 8 5"
            },
            {
                "role": "user",
                "content": "Can you summarize the key topics covered in this lecture?"
            },
            {
                "role": "assistant",
                "content": "This lecture covers foundational algorithmic concepts including problem formulation, correctness proof using induction, efficiency analysis using asymptotic notation, data structures (arrays, linked lists, BSTs, heaps), and sorting methods (selection sort, merge sort, counting sort). Additionally, it introduces the Word-RAM computational model, providing a theoretical framework for measuring algorithm efficiency."
            }
        ]
    },
    {
        "system": "Hello! I am Kiran, an AI assistant from Tampere University, dedicated to providing you with accurate and insightful assistance. My goal is to help you understand concepts, summarize lectures, and answer questions effectively.",
        "conversations": [
            {
                "role": "host",
                "content": "Rekursio (perusteiden kertaus) COMP.CS.300 Tietorakenteet ja algoritmit 1 Matti Rintala (matti.rintala@tuni.fi) Mitä on rekursio? •\"Funktio, joka kutsuu itseään\"? int fibonacci(int n) { if (n<2) { return 1; } else { return fibonacci(n-1) + fibonacci(n-2); } } Mitä on rekursio? •Asia, jonka osat ovat samanlaisia kuin kokonaisuus (vain pienempiä) Rekursiiviset funktiot •Useita funktio suorituksia kesken \"sisäkkäin\" •Joka kutsulla omat parametrit, omat paikalliset muuttujat •\"Kutsupino\" •Triviaalitapaus! Rekursiiviset funktiot •Rekursio → silmukka? (\"häntärekursio\") •Silmukka → rekursio? Puolitushaku BinarySearch(A, left, right, value) (A nousevassa järjestyksessä) 1 if left = right then (vain yksi alkio) 2 if A[left] = value then (ainoa alkio on etsitty) 3 return left (palauta paikka) 4 else 5 ▷Ei löytynyt! 6 else 7 middle := (⌊left+right)/2⌋ (alue puoliksi) 8 if value ≤ A[middle] then 9 return BinarySearch(A, left, middle, value) 10 else 11 return BinarySearch(A, middle+1, right, value) Puolitushaku BinarySearch(A, left, right, value) (A nousevassa järjestyksessä) 1 if left = right then 2 if A[left] = value then 3 return left 4 else 5 ▷Ei löytynyt! 6 else 7 middle := (⌊left+right)/2⌋ 8 if value ≤ A[middle] then 9 return BinarySearch(A, left, middle, value) 10 else 11 return BinarySearch(A, middle+1, right, value) 2 4 5 6 8 9 9 10 15 Hajoita ja hallitse -suunnitteluperiaate COMP.CS.300 Tietorakenteet ja algoritmit 1 Matti Rintala (matti.rintala@tuni.fi) Hajoita ja hallitse ●Jako (noin) yhtäsuuriin samanlaisiin osaongelmiin ●Pienimmät ongelmat triviaaleja ●Isommat ratkaistaan rekursiivisesti ●Lopuksi kootaan ratkaisu osaratkaisuista ●(Kaikki rekursio ei ole hajoita & hallitse) Miksi hajoita & hallitse (rekursio)? •Rekursiiviset ongelmat •Hierarkkiset ongelmat Miksi hajoita & hallitse (rekursio)? Miksi hajoita & hallitse (rekursio)? Miksi hajoita & hallitse (rekursio)? (Photo by Leon Brocard) Miksi hajoita & hallitse (rekursio)? •Algoritmit, joiden tehokkuus perustuu hajoita & hallitse -ideaan •Yleensä rekursiivisia algoritmeja •Esim. puolitushaku vs lineaarinen haku 2 4 5 6 8 9 9 10 15 Rekursiiviset tietorakenteet •Hierarkiset tietorakenteet, esim. puut •Hierarkisten tietorakenteiden algoritmit (usein) rekursiivisia 9 10 6 4 9 15 2 5 8"
            },
            {
                "role": "user",
                "content": "Can you summarize the key topics covered in this lecture?"
            },
            {
                "role": "assistant",
                "content": "This lecture covers foundational algorithmic concepts including problem formulation, correctness proof using induction, efficiency analysis using asymptotic notation, data structures (arrays, linked lists, BSTs, heaps), and sorting methods (selection sort, merge sort, counting sort). Additionally, it introduces the Word-RAM computational model, providing a theoretical framework for measuring algorithm efficiency."
            }
        ]
    },
    {
        "system": "Hello! I am Kiran, an AI assistant from Tampere University, dedicated to providing you with accurate and insightful assistance. My goal is to help you understand concepts, summarize lectures, and answer questions effectively.",
        "conversations": [
            {
                "role": "host",
                "content": "Satunnaistaminen (Randomization) COMP.CS.300 Tietorakenteet ja algoritmit 1 Matti Rintala (matti.rintala@tuni.fi) Ongelma •Pahin vs. \"keskimääräinen\" tapaus •Tapausten todennäköisyydet •Todellinen data usein painottunut Satunnaistaminen •Ratkaisu: \"sotketaan\" datan järjestys alussa tai algoritmin aikana •Tulos: kaikki tapaukset yhtä todennäköisiä •Pahin tapaus ei enää yleisempi kuin muut •Satunnaistaminen vie myös aikaa! •Pahin tapaus edelleen yhtä paha! Randomized-Quicksort(A, left, right) 1 if left < right then 2 pivot := Randomized-Partition(A, left, right) 3 Randomized-Quicksort(A, left, pivot−1 ) 4 Randomized-Quicksort(A, pivot+1, right) Satunnaistettu pikalajittelu Randomized-Partition(A, left, right) 1 pivot := Random(left, right) (valitaan satunnainen vertailualkio) 2 A[ right ] ⇄ A[ pivot ] (vaihdetaan pivot viimeiseksi) 3 return Partition(A, left, right) (normaali ositus) Randomized-Search(A, value) 1 Shuffle(A) (sekoita A:n järjestys) 2 for i in 1..A.size (käy läpi taulukon indeksit) 3 if A[i] = value then 4 return i Satunnaistettu haku?"
            },
            {
                "role": "user",
                "content": "Can you summarize the key topics covered in this lecture?"
            },
            {
                "role": "assistant",
                "content": "This lecture covers foundational algorithmic concepts including problem formulation, correctness proof using induction, efficiency analysis using asymptotic notation, data structures (arrays, linked lists, BSTs, heaps), and sorting methods (selection sort, merge sort, counting sort). Additionally, it introduces the Word-RAM computational model, providing a theoretical framework for measuring algorithm efficiency."
            }
        ]
    },
    {
        "system": "Hello! I am Kiran, an AI assistant from Tampere University, dedicated to providing you with accurate and insightful assistance. My goal is to help you understand concepts, summarize lectures, and answer questions effectively.",
        "conversations": [
            {
                "role": "host",
                "content": "STL:n algoritmit COMP.CS.300 Tietorakenteet ja algoritmit 1 Matti Rintala (matti.rintala@tuni.fi) STL:n algoritmit •Paljon valmiita algoritmeja •Tutustu algoritmilistaan (esim. cppreference.com) •Parametreina iteraattoreita •Eivät koskaan lisää/poista alkioita! (mitätöityminen) •(Tarjolla myös rinnakkaisuutta hyödyntäviä versioita) STL:n algoritmit - esimerkkejä COMP.CS.300 Tietorakenteet ja algoritmit 1 Matti Rintala (matti.rintala@tuni.fi) Selection-Sort(A) 1 for next_elem := 1 to A.length-1 do 2 smallest := next_elem 3 for place := next_elem+1 to A.length do 4 if A[place] < A[smallest] then 5 smallest := place 6 A[next_elem] ⇄ A[smallest] (siirretään osien välistä rajaa) (mahd. pienimmän alkion paikka = eka alkio) (jos löytyy vielä pienempi alkio...) (...otetaan sen paikka talteen) (vaihdetaan pienin alkio alkuun) Mergesort(A, left, right) 1 if left < right then (triviaalitapaukselle ei tehdä mitään) 2 mid := (⌊left + right) / 2⌋ (lasketaan puoliväli) 3 Mergesort(A, left, mid ) (järjestetään vasen puoli) 4 Mergesort(A, mid+1, right) (järjestetään oikea puoli) 5 Merge(A, left, mid, right) (lomitetaan järjestetyt puolikkaat yhteen) Insertion-Sort(A) 1 for next_elem := 2 to A.length do 2 key := A[next_elem] 3 place := next_elem − 1 4 while place > 0 and A[place] > key do 5 A[place + 1] := A[place] 6 place := place − 1 7 A[place + 1] := key (siirretään osien välistä rajaa) (otetaan uusi alkio käsittelyyn) (etsitään uudelle alkiolle oikea paikka) (raivataan uudelle alkiolle tilaa) (asetetaan uusi alkio oikealle paikalleen) STL:n algoritmien ja säiliöiden mukauttaminen COMP.CS.300 Tietorakenteet ja algoritmit 1 Matti Rintala (matti.rintala@tuni.fi) STL:n mukauttaminen •Monet algoritmit ottavat parametrinaan lambdan/funktion, jota algoritmi käyttää (=kutsuu) •Esim. sort ja alkioiden vertailu, find_if ja millaista etsitään... •Joidenkin säiliöiden toimita myös mukautettavissa •Esim. map ja avainten järjestys Quicksort(A, left, right) 1 if left < right then (triviaalitapaukselle ei tehdä mitään) 2 pivot := Partition(A, left, right) (jaetaan pieniin ja suuriin, pivot jakokohta) 3 Quicksort(A, left, pivot−1 ) (järjestetään jakoalkiota pienemmät) 4 Quicksort(A, pivot+1, right) (järjestetään jakoalkiota suuremmat)"
            },
            {
                "role": "user",
                "content": "Can you summarize the key topics covered in this lecture?"
            },
            {
                "role": "assistant",
                "content": "This lecture covers foundational algorithmic concepts including problem formulation, correctness proof using induction, efficiency analysis using asymptotic notation, data structures (arrays, linked lists, BSTs, heaps), and sorting methods (selection sort, merge sort, counting sort). Additionally, it introduces the Word-RAM computational model, providing a theoretical framework for measuring algorithm efficiency."
            }
        ]
    },
    {
        "system": "Hello! I am Kiran, an AI assistant from Tampere University, dedicated to providing you with accurate and insightful assistance. My goal is to help you understand concepts, summarize lectures, and answer questions effectively.",
        "conversations": [
            {
                "role": "host",
                "content": "C++:n Standard Template Library (STL) COMP.CS.300 Tietorakenteet ja algoritmit 1 Matti Rintala (matti.rintala@tuni.fi) Valmis kirjasto vs oma toteutus •Itse toteutettu tietorakenne/algoritmi: – Miten operaatiot toteutetaan? •Valmis tietorakenne/algoritmikirjasto (STL): – Toteutus piilotettu – Miten operaatioita käytetään? (Rajapinnat) – Miten valita sopiva tietorakenne/algoritmi? – Miten valita tehokas tietorakenne/algoritmi? – Miten yhdistellä valmiita tietorakenteita/algoritmeja? – Miten säätää/mukauttaa valmiiden operaatioiden toimintaa? STL:n (tietorakenne)osat Säiliöt (containers) Geneeriset algoritmit Iteraattorit STL:n (tietorakenne)osat Perusoperaatiot: • [] at • push_back • erase • size • clear • ... STL:n (tietorakenne)osat Yleiskäyttöiset algoritmit: • for_each • find • binary_search • set_symmetric_ difference • transform_reduce • ... STL:n (tietorakenne)osat Perusoperaatiot: • [] at • push_back • erase • size • clear • ... Säiliöt (containers) Geneeriset algoritmit Iteraattorit Yleiskäyttöiset algoritmit: • for_each • find • binary_search • set_symmetric_ difference • transform_reduce • ... STL ja asymptoottinen tehokkuus •STL:n operaatioiden tehokkuus luvattu asymptoottisesti – Usein lupaus O-notaatio (\"ei hitaampi\") – Joskus myös keskimääräinen tehokkuus tai Θ-notaatio STL ja asymptoottinen tehokkuus •STL:n operaatioiden tehokkuus luvattu asymptoottisesti – Usein lupaus O-notaatio (\"ei hitaampi\") – Joskus myös keskimääräinen tehokkuus tai Θ-notaatio •Mutta mikä on \"n\"? Riippuu tilanteesta: – Alkioiden lukumäärä – Tietyn alkio-osajoukon lukumäärä – Joskus useita muuttujia (O(m*n)) STL:n säiliöt (containers) COMP.CS.300 Tietorakenteet ja algoritmit 1 Matti Rintala (matti.rintala@tuni.fi) STL:n säiliöt (containers) •Sarjat (sequences) – Alkioiden järjestys itse valittavissa – Alkiot löytyvät järjestyksen perusteella •Assosiatiiviset säiliöt – Säiliö päättää alkioiden järjestyksen (voi olla myös määrittelemätön) – Alkiot löytyvät hakuavaimen perusteella •(Säiliösovittimet (adaptors) stack, queue, priority_queue) STL:n sarjat (sequences) •vector, deque, list •(array, forward_list) •Alkioiden järjestys itse valittavissa •Haku indeksoimalla (järjestysnumero) tai iteroimalla (läpikäynti järjestyksessä) •Lisäys/poisto annetusta kohdasta (iteraattori) STL:n assosiatiiviset säiliöt •Järjestetyt map, set – Järjestys hakuavaimen mukainen •unordered_map/_set – Järjestys määrittelemätön, voi vaihtua koska vain! • Monta alkiota per hakuavain: (unordered_)multimap/set •Poisto annetusta kohdasta (iteraattori) (kohta löytyy haulla) STL:n assosiatiiviset säiliöt avain = data? monta dataa/avain? järjestys avainten perusteella? E: ...map K: ...set E K: ...multi... K E: unordered_... STL:n assosiatiiviset säiliöt •Järjestetyt map, set – Järjestys hakuavaimen mukainen •unordered_map/_set – Järjestys määrittelemätön, voi vaihtua koska vain! • Monta alkiota per hakuavain: (unordered_)multimap/set •Poisto annetusta kohdasta (iteraattori) (kohta löytyy haulla) avain = data? monta dataa/avain? järjestys avainten perusteella? E: ...map K: ...set E K: ...multi... K E: unordered_... STL:n säiliöiden tehokkuus COMP.CS.300 Tietorakenteet ja algoritmit 1 Matti Rintala (matti.rintala@tuni.fi) Säiliöiden (asymptoottinen) tehokkuus •Asymptoottinen tehokkuus määrätty (yleensä yläraja O) •Rajapinnat keskenään samantapaiset, erot tehokkuudessa •Jos operaatio \"tehoton\", se voi puuttua säiliöstä •Säiliön valinta: – Kategoria (sarja/assosiatiivinen) – Toistuvat operaatiot tehokkaita – (Mitätöityminen, muut pienemmät erot) Säiliön valinta tehokkuuden perusteella Säiliö Yleinen lisäys/poisto Erik. lisäys alku/loppu Erik. poisto alku/loppu Haku (järjestysnro) Haku (arvo/avain) Suurin/pienin yms. vector O(n) - / O(1)* - / O(1)* O(1) ( O(n) ) ( O(n) ) deque O(n) O(1) / O(1) O(1) / O(1) O(1) ( O(n) ) ( O(n) ) list O(1) O(1) / O(1) O(1) / O(1) ( O(n) ) ( O(n) ) ( O(n) ) (array) - - - O(1) ( O(n) ) ( O(n) ) (forward_list) O(1) O(1) / ( O(n) ) O(1) / ( O(n) ) O(n) ( O(n) ) ( O(n) ) unordered_ map/set O(n) ≈ Θ(1) - - - O(n) ≈ Θ(1) ( O(n) ) map/set O(log n) - - ( O(n) ) O(log n) O(1) (stack) - - / O(1) - / O(1) - - - (queue) - O(1) / - - / O(1) - - - (priority_queue) O(log n) - - - - O(1) STL:n iteraattorit COMP.CS.300 Tietorakenteet ja algoritmit 1 Matti Rintala (matti.rintala@tuni.fi) Iteraattoreiden idea •Kirjanmerkki säiliöön •Säiliön läpikäynti •Osaväli: 2 iteraattoria (C++20 myös: ranges) 10 4 2 6 9 3 14 8 5 i1 i2 b e Iteraattoreiden rooli STL:ssä •Säiliöt – begin(), end() – Alkion lisäys tiettyyn paikkaan – Alkion poisto (tai välin) – Operaation tuloksena paikka (tai väli) •Algoritmit – Paikan ja säiliön ilmaiseminen – Toimintavälin ilmaiseminen – Operaation tuloksena paikka (tai väli) •Käänteisiteraattorit rbegin(), rend() Iteraattoreiden tehokkuus, iteraattorikategoriat COMP.CS.300 Tietorakenteet ja algoritmit 1 Matti Rintala (matti.rintala@tuni.fi) Iteraattoreiden tehokkuus & kategoriat •Iteraattoreiden operaatiot (++, *, ...) vakioaikaisia O(1) •Säiliöstä riippuu, mitä operaatioita iteraattori tukee •Iteraattorikategoriat: mitä operaatioita tuetaan •Algoritmit voivat vaatia tietyn kategorian iteraattoreita Iteraattoreiden tehokkuus & kategoriat Lukuiteraattori input iterator *(luku), ++, ==, !=, =, -> Kirjoitusiteraattori output iterator *(kirjoitus), ++, ==, !=, =, -> Eteenpäin-iteraattori forward iterator * (luku & kirjoitus) 2-suuntainen iteraattori bidirectional iterator -- hajasaanti-iteraattori random access iterator +=, -=, +, -, <, >, <=, =>, [] forward_list list (unordered_) (multi) map/set vector deque array Iteraattoreiden tehokkuus & kategoriat •Iteraattoreiden operaatiot (++, *, ...) vakioaikaisia O(1) •Säiliöstä riippuu, mitä operaatioita iteraattori tukee •Iteraattorikategoriat: mitä operaatioita tuetaan •Algoritmit voivat vaatia tietyn kategorian iteraattoreita Lukuiteraattori input iterator *(luku), ++, ==, !=, =, -> Kirjoitusiteraattori output iterator *(kirjoitus), ++, ==, !=, =, -> Eteenpäin-iteraattori forward iterator * (luku & kirjoitus) 2-suuntainen iteraattori bidirectional iterator -- hajasaanti-iteraattori random access iterator +=, -=, +, -, <, >, <=, =>, [] forward_list list (unordered_) (multi) map/set vector deque array Säiliöviittausten mitätöityminen (invalidation) COMP.CS.300 Tietorakenteet ja algoritmit 1 Matti Rintala (matti.rintala@tuni.fi) Mitätöityminen (invalidation) • Mitätöitynyt iteraattori ei enää viittaa (oikeaan) paikkaan säiliössä lisäyksen/poiston jälkeen • Mitätöitynyttä iteraattoria ei saa käyttää (uuden arvon saa sijoittaa) • Käytön seuraus määrittelemätön (kaatuminen/sekoaminen/???) Mitätöityminen (invalidation) vector<int> v={ 1,2,3,4}; auto i = v.begin(); auto j = i+1; // i:tä seuraava v.erase(i); *j = 3; // !!! j mitätöitynyt! Mitätöityminen ja säiliön valinta •Eri säiliöillä eri säännöt mitätöitymiseen •Toinen valintakriteeri tehokkuuden lisäksi (usein kompromissi) •vector ja deque: säännöt monimutkaiset •unordered_map/set: turvallinen poiston suhteen, lisäys mitätöi •map/set ja (forward_)list lähes turvallisia Mitätöityminen ja säiliön valinta Säiliö Lisäyksen jälkeen Poiston jälkeen Huom. Mitätön! - Kapasiteetti muuttui vector Ok Ok * Ennen muutospaikkaa Mitätön! Mitätön! Muutospaikan jälkeen deque Mitätön! Ok * 1./viim. lisäys/poisto Mitätön! Mitätön! muun lisäys/poisto (forward_)list Ok Ok * (multi)map/set Ok Ok * unordered_(multi) map/set Mitätön! - Uudelleenhajautus tapahtui Ok Ok * Miten huomata mitätöityminen •Huolellinen suunnittelu!!! •Joissakin kääntäjissä STL-debug-tiloja Gcc: -D_GLIBCXX_DEBUG -D_GLIBCXX_DEBUG_PEDANTIC •Ohjelma kaatuu: debuggeri kertoo missä? •Ohjelma sekoaa: debuggeri/tulostukset Mitätöityminen, osoittimet ja indeksit •Mikä tahansa paikan ilmaisin voi mitätöityä! •Osoitin alkioon: alkio siirtyy muistissa •Indeksi alkioon: alkiota ennen lisää/pois alkioita •cppreference.com:ssa täydellisempi taulukko"
            },
            {
                "role": "user",
                "content": "Can you summarize the key topics covered in this lecture?"
            },
            {
                "role": "assistant",
                "content": "This lecture covers foundational algorithmic concepts including problem formulation, correctness proof using induction, efficiency analysis using asymptotic notation, data structures (arrays, linked lists, BSTs, heaps), and sorting methods (selection sort, merge sort, counting sort). Additionally, it introduces the Word-RAM computational model, providing a theoretical framework for measuring algorithm efficiency."
            }
        ]
    },
    {
        "system": "Hello! I am Kiran, an AI assistant from Tampere University, dedicated to providing you with accurate and insightful assistance. My goal is to help you understand concepts, summarize lectures, and answer questions effectively.",
        "conversations": [
            {
                "role": "host",
                "content": "Tehokkuuden mittaaminen ja sen rajat COMP.CS.300 Tietorakenteet ja algoritmit 1 Matti Rintala (matti.rintala@tuni.fi) Mitä voidaan mitata/arvioida? •Algoritmin kuvaus suomeksi •Pseudokoodi •Ohjelma (ohjelmointikielellä) •Tietokoneessa ajettu ohjelma Insertion-Sort(A) 1 for next_elem := 2 to A.length do 2 key := A[next_elem] 3 place := next_elem − 1 4 while place > 0 and A[place] > key do 5 A[place + 1] := A[place] 6 place := place − 1 7 A[place + 1] := key Mitä voidaan arvioida pseudokoodista? Datan määrä (n) Aika (t)"
            },
            {
                "role": "user",
                "content": "Can you summarize the key topics covered in this lecture?"
            },
            {
                "role": "assistant",
                "content": "This lecture covers foundational algorithmic concepts including problem formulation, correctness proof using induction, efficiency analysis using asymptotic notation, data structures (arrays, linked lists, BSTs, heaps), and sorting methods (selection sort, merge sort, counting sort). Additionally, it introduces the Word-RAM computational model, providing a theoretical framework for measuring algorithm efficiency."
            }
        ]
    },
    {
        "system": "Hello! I am Kiran, an AI assistant from Tampere University, dedicated to providing you with accurate and insightful assistance. My goal is to help you understand concepts, summarize lectures, and answer questions effectively.",
        "conversations": [
            {
                "role": "host",
                "content": "Vinkkejä todellisen tehokuuden parantamiseen COMP.CS.300 Tietorakenteet ja algoritmit 1 Matti Rintala (matti.rintala@tuni.fi) Suunnittelu/toteutus •Pseudokoodi/suunnittelu – Arvio asymptoottisesta tehokkuudesta – Asymptoottisesti mahd. tehokkaat algoritmit •Toteutus/koodaus – Mahd. tehokas toteutus – Optimointi vaikuttaa usein vain vakiokertoimeen (silti tärkeää!) Asymptoottisen tehokkuuden parantaminen •Valitse asymptoottisesti paras algoritmi •Vältä ylimääräistä työtä – Valitse algoritmi, joka tekee vain sen, mitä tarvitaan •Varmistu, että valittujen säilöiden yms. asymptoottiset tehokkuudet ovat sitä, mitä algoritmi olettaa Vinkkejä todellisen tehokkuuden optimointiin •Vältä saman asian (turhaan) uudelleen tekemistä – Ota (väli)tulos talteen – Käytä osoittimia yms. •Pidä muistissa usein tarvittavia (pieniä) tuloksia – Silloin niitä ei tarvitse laskea uudelleen •Data jakaminen vs. kopioiminen •\"Pienen\" asian tekeminen ison ohessa"
            },
            {
                "role": "user",
                "content": "Can you summarize the key topics covered in this lecture?"
            },
            {
                "role": "assistant",
                "content": "This lecture covers foundational algorithmic concepts including problem formulation, correctness proof using induction, efficiency analysis using asymptotic notation, data structures (arrays, linked lists, BSTs, heaps), and sorting methods (selection sort, merge sort, counting sort). Additionally, it introduces the Word-RAM computational model, providing a theoretical framework for measuring algorithm efficiency."
            }
        ]
    },
    {
        "system": "Hello! I am Kiran, an AI assistant from Tampere University, dedicated to providing you with accurate and insightful assistance. My goal is to help you understand concepts, summarize lectures, and answer questions effectively.",
        "conversations": [
            {
                "role": "host",
                "content": "Amortisoitu tehokkuus ja std::vector:n muistinhallinta COMP.CS.300 Tietorakenteet ja algoritmit 1 Matti Rintala (matti.rintala@tuni.fi) STL:n vektorin muistinhallinta •Vektori varaa yhtenäisen muistialueen alkioilleen •Mitä tehdä, kun tarvitaan lisää tilaa? 1 •Yritys 1: varataan tarvittavan verran suurempi muistialue, kopioidaan vanhat alkiot sinne 1 2 1 2 3 1 2 3 4 1 2 3 4 5 STL:n vektorin muistinhallinta •Vektori varaa yhtenäisen muistialueen alkioilleen •Mitä tehdä, kun tarvitaan lisää tilaa? 1 1 2 1 2 3 1 2 3 4 1 2 3 4 5 •Yritys 1: varataan tarvittavan verran suurempi muistialue, kopioidaan vanhat alkiot sinne STL:n vektorin muistinhallinta •Vektori varaa yhtenäisen muistialueen alkioilleen •Mitä tehdä, kun tarvitaan lisää tilaa? 1 •Yritys 2: varataan kaksi kertaa vanhan kokoinen muistialue, kopioidaan vanhat alkiot sinne •Huom: Osa lopusta jää vielä käyttämättä! 1 2 1 2 3 1 2 3 4 1 2 3 4 5 1 2 3 4 5 6 7 1 2 3 4 5 6 1 2 3 4 5 6 7 8 STL:n vektorin muistinhallinta •Vektori varaa yhtenäisen muistialueen alkioilleen •Mitä tehdä, kun tarvitaan lisää tilaa? 1 1 2 1 2 3 1 2 3 4 1 2 3 4 5 1 2 3 4 5 6 7 1 2 3 4 5 6 1 2 3 4 5 6 7 8 •Yritys 2: varataan kaksi kertaa vanhan kokoinen muistialue, kopioidaan vanhat alkiot sinne •Huom: Osa lopusta jää vielä käyttämättä! Amortisoitu tehokkuus •\"Tasattu (asymptoottinen) tehokkuus\" •Lasketaan operaatiosarjojen keskimääräinen tehokkuus •Kalliin harvinaisen operaation kustannus voidaan jakaa tasan halvoille •Esim. vektoriin lisäys: – Yksittäinen vektorin lisäys voi olla lineaarinen (yhä harvinaisempaa) – Lisäykset silti amortisoidusti vakioaikaisia (keskimäärin) std::vector •STL:n vektori sisältää operaatioita muistinhallinnan säätämiseen •vec.reserve(n): Varaa muistia väh. n alkiolle, alkioita ei silti (vielä) lisätä •vec.capacity(): Alkioiden maksimimäärä ilman uutta muistinvarausta •vec.shrink_to_fit(): Siirrä alkiot muistialueeseen, joka on juuri sopivan kokoinen •(vec.erase() ei vapauta muistia!)"
            },
            {
                "role": "user",
                "content": "Can you summarize the key topics covered in this lecture?"
            },
            {
                "role": "assistant",
                "content": "This lecture covers foundational algorithmic concepts including problem formulation, correctness proof using induction, efficiency analysis using asymptotic notation, data structures (arrays, linked lists, BSTs, heaps), and sorting methods (selection sort, merge sort, counting sort). Additionally, it introduces the Word-RAM computational model, providing a theoretical framework for measuring algorithm efficiency."
            }
        ]
    }
]